{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project seeks to build machine learning models to achieve two things:\n",
    "\n",
    "1. Predict the highest ranking a song will achieve on the Billboard Hot 100 list.\n",
    "2. Predict the largest week over week increase in a given song's ranking on the Hot 100 list.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "- A song's genre has a minimal impact on its ranking on the Billboard Hot 100 list.\n",
    "- Song characteristics such as tempo, danceability, etc. appear to be the strongest drivers of their performance on the Hot 100 list.\n",
    "- Analyzing music can quickly lead to an overwhelming number of features, so thoughtful and careful data selection is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mean_squared_error, r2_score, pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 327895 entries, 0 to 327894\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   327895 non-null  int64  \n",
      " 1   url                     327895 non-null  object \n",
      " 2   WeekID                  327895 non-null  object \n",
      " 3   Week Position           327895 non-null  int64  \n",
      " 4   Song                    327895 non-null  object \n",
      " 5   Performer               327895 non-null  object \n",
      " 6   SongID                  327895 non-null  object \n",
      " 7   Instance                327895 non-null  int64  \n",
      " 8   Previous Week Position  295941 non-null  float64\n",
      " 9   Peak Position           327895 non-null  int64  \n",
      " 10  Weeks on Chart          327895 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29503 entries, 0 to 29502\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index                      29503 non-null  int64  \n",
      " 1   SongID                     29503 non-null  object \n",
      " 2   Performer                  29503 non-null  object \n",
      " 3   Song                       29503 non-null  object \n",
      " 4   spotify_genre              27903 non-null  object \n",
      " 5   spotify_track_id           24397 non-null  object \n",
      " 6   spotify_track_preview_url  14491 non-null  object \n",
      " 7   spotify_track_duration_ms  24397 non-null  float64\n",
      " 8   spotify_track_explicit     24397 non-null  object \n",
      " 9   spotify_track_album        24391 non-null  object \n",
      " 10  danceability               24334 non-null  float64\n",
      " 11  energy                     24334 non-null  float64\n",
      " 12  key                        24334 non-null  float64\n",
      " 13  loudness                   24334 non-null  float64\n",
      " 14  mode                       24334 non-null  float64\n",
      " 15  speechiness                24334 non-null  float64\n",
      " 16  acousticness               24334 non-null  float64\n",
      " 17  instrumentalness           24334 non-null  float64\n",
      " 18  liveness                   24334 non-null  float64\n",
      " 19  valence                    24334 non-null  float64\n",
      " 20  tempo                      24334 non-null  float64\n",
      " 21  time_signature             24334 non-null  float64\n",
      " 22  spotify_track_popularity   24397 non-null  float64\n",
      "dtypes: float64(14), int64(1), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveness_dist = df_cleaned_genre['liveness'].value_counts()\n",
    "df_liveness_dist = pd.DataFrame(liveness_dist)\n",
    "df_liveness_dist = df_liveness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_liveness_dist['liveness'], df_liveness_dist['count'], color='orange')\n",
    "plt.xlabel('Liveness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Liveness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "danceability_dist = df_cleaned_genre['danceability'].value_counts()\n",
    "df_danceability_dist = pd.DataFrame(danceability_dist)\n",
    "df_danceability_dist = df_danceability_dist.reset_index()\n",
    "\n",
    "plt.bar(df_danceability_dist['danceability'], df_danceability_dist['count'], color='skyblue')\n",
    "plt.xlabel('Danceability Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Danceability Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acousticness_dist = df_cleaned_genre['acousticness'].value_counts()\n",
    "df_acousticness_dist = pd.DataFrame(acousticness_dist)\n",
    "df_acousticness_dist = df_acousticness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_acousticness_dist['acousticness'], df_acousticness_dist['count'], color='orange')\n",
    "plt.xlabel('Acousticness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Acousticness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_pos_dist = df_cleaned_genre['Max_Peak_Position'].value_counts()\n",
    "df_peak_pos_dist = pd.DataFrame(peak_pos_dist)\n",
    "df_peak_pos_dist = df_peak_pos_dist.reset_index()\n",
    "print(f\"Mean Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='skyblue')\n",
    "plt.xlabel('Highest Ranking')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Peak Rankings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank_change = df_cleaned_genre['Max_Rank_Change'].value_counts()\n",
    "df_max_rank_change = pd.DataFrame(max_rank_change)\n",
    "df_max_rank_change = df_max_rank_change.reset_index()\n",
    "df_max_rank_change = df_max_rank_change[df_max_rank_change['Max_Rank_Change'] > 0]\n",
    "print(f\"Mean Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_max_rank_change['Max_Rank_Change'], df_max_rank_change['count'], color='orange')\n",
    "plt.xlabel('Largest Week over Week Rank Change')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Largest Week over Week Rank Change')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markh\\AppData\\Local\\Temp\\ipykernel_9796\\3177740419.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
      "C:\\Users\\markh\\AppData\\Local\\Temp\\ipykernel_9796\\3177740419.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer', 'Instance'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "def diff(a, b):\n",
    "    return a - b\n",
    "\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_id', 'spotify_track_preview_url',  \n",
    "                                        'spotify_track_explicit', 'spotify_track_popularity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112098 entries, 0 to 112097\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   WeekID                     112098 non-null  datetime64[ns]\n",
      " 1   Week Position              112098 non-null  int64         \n",
      " 2   SongID                     112098 non-null  object        \n",
      " 3   Previous Week Position     101571 non-null  float64       \n",
      " 4   Peak Position              112098 non-null  int64         \n",
      " 5   Weeks on Chart             112098 non-null  int64         \n",
      " 6   Rank_Change                112098 non-null  float64       \n",
      " 7   spotify_genre              108091 non-null  object        \n",
      " 8   spotify_track_duration_ms  104590 non-null  float64       \n",
      " 9   danceability               104287 non-null  float64       \n",
      " 10  energy                     104287 non-null  float64       \n",
      " 11  key                        104287 non-null  float64       \n",
      " 12  loudness                   104287 non-null  float64       \n",
      " 13  mode                       104287 non-null  float64       \n",
      " 14  speechiness                104287 non-null  float64       \n",
      " 15  acousticness               104287 non-null  float64       \n",
      " 16  instrumentalness           104287 non-null  float64       \n",
      " 17  liveness                   104287 non-null  float64       \n",
      " 18  valence                    104287 non-null  float64       \n",
      " 19  tempo                      104287 non-null  float64       \n",
      " 20  time_signature             104287 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(3), object(2)\n",
      "memory usage: 18.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# combining the hotlist and features into one dataframe\n",
    "\n",
    "df_hotlist_and_features_2000s = pd.merge(df_hotlist_2000s, df_features_all, on='SongID', how='left')\n",
    "df_hotlist_and_features_2000s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset has genre in a single column and the entry for each song has a variety of genres listed in that single column. \n",
    "This does not allow me to explore genre in a systematic way.\n",
    "I'll need to break genre out so that each genre has its own column with a 1 or 0 to indicate whether each song is tagged with that genre (ending with a one-hot encoded structure).\n",
    "\"\"\"\n",
    "\n",
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_hotlist_and_features_2000s['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I'm only including genres that appear in 100 or more songs (i.e. at least 0.1% of songs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading list of genres with 100 or more instances in df_cleaned\n",
    "df_genres_100_up = pd.read_csv('genre_counts_100+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_100_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating a list of genres and counts\n",
    "genre_data = []\n",
    "\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre'] :\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        row_dict = {genre: (1 if genre in genre_list else 0) for genre in final_genres_list} # dict with 1 if genre exists in list, 0 if not\n",
    "    else:\n",
    "         row_dict = {genre: 0 for genre in final_genres_list} # 0 of genre does not exist in list\n",
    "    genre_data.append(row_dict)\n",
    "\n",
    "# creating a df with the list of dicts\n",
    "genre_df = pd.DataFrame(genre_data)\n",
    "\n",
    "# concatenating genre data into df_clean\n",
    "df_hotlist_and_features_2000s = pd.concat([df_hotlist_and_features_2000s, genre_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  \n",
       "0           0                 0           0  \n",
       "1           0                 0           0  \n",
       "2           0                 0           0  \n",
       "3           0                 0           0  \n",
       "4           0                 0           0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing spotify_genre and spot-checking resulting df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_hotlist_and_features_2000s = df_hotlist_and_features_2000s.drop(['spotify_genre'], axis=1)\n",
    "df_hotlist_and_features_2000s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new df with most 3 most popular genres by week\n",
    "\n",
    "# Get all genre column names\n",
    "genre_start_idx = df_hotlist_and_features_2000s.columns.get_loc('pop')\n",
    "genre_cols = df_hotlist_and_features_2000s.columns[genre_start_idx:].tolist()\n",
    "\n",
    "# Group by WeekID and sum the genre columns to get counts\n",
    "genre_counts = df_hotlist_and_features_2000s.groupby('WeekID')[genre_cols].sum()\n",
    "\n",
    "# For each week, find the top 3 genres\n",
    "top_genres = []\n",
    "for week_id in genre_counts.index:\n",
    "    # Get the genre counts for this week and sort them\n",
    "    week_genres = genre_counts.loc[week_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Get the top 3 genre names\n",
    "    top_3 = week_genres.head(3).index.tolist()\n",
    "    \n",
    "    # Pad with None if there are fewer than 3 genres\n",
    "    while len(top_3) < 3:\n",
    "        top_3.append(None)\n",
    "    \n",
    "    top_genres.append({\n",
    "        'WeekID': week_id,\n",
    "        'Most_Popular_Genre': top_3[0],\n",
    "        '2nd_Most_Popular_Genre': top_3[1],\n",
    "        '3rd_Most_Popular_Genre': top_3[2]\n",
    "    })\n",
    "\n",
    "# Create the new dataframe\n",
    "df_top_genres = pd.DataFrame(top_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Most_Popular_Genre</th>\n",
       "      <th>2nd_Most_Popular_Genre</th>\n",
       "      <th>3rd_Most_Popular_Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>urban contemporary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>urban contemporary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-15</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>urban contemporary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-22</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>hip hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-29</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>urban contemporary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>pop</td>\n",
       "      <td>contemporary country</td>\n",
       "      <td>dance pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>pop</td>\n",
       "      <td>contemporary country</td>\n",
       "      <td>dance pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2020-12-12</td>\n",
       "      <td>adult standards</td>\n",
       "      <td>pop</td>\n",
       "      <td>contemporary country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2020-12-19</td>\n",
       "      <td>adult standards</td>\n",
       "      <td>pop</td>\n",
       "      <td>contemporary country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2020-12-26</td>\n",
       "      <td>pop</td>\n",
       "      <td>adult standards</td>\n",
       "      <td>post-teen pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         WeekID Most_Popular_Genre 2nd_Most_Popular_Genre  \\\n",
       "0    2000-01-01          dance pop                    r&b   \n",
       "1    2000-01-08          dance pop                    r&b   \n",
       "2    2000-01-15          dance pop                    r&b   \n",
       "3    2000-01-22          dance pop                    r&b   \n",
       "4    2000-01-29          dance pop                    r&b   \n",
       "...         ...                ...                    ...   \n",
       "1091 2020-11-28                pop   contemporary country   \n",
       "1092 2020-12-05                pop   contemporary country   \n",
       "1093 2020-12-12    adult standards                    pop   \n",
       "1094 2020-12-19    adult standards                    pop   \n",
       "1095 2020-12-26                pop        adult standards   \n",
       "\n",
       "     3rd_Most_Popular_Genre  \n",
       "0        urban contemporary  \n",
       "1        urban contemporary  \n",
       "2        urban contemporary  \n",
       "3                   hip hop  \n",
       "4        urban contemporary  \n",
       "...                     ...  \n",
       "1091              dance pop  \n",
       "1092              dance pop  \n",
       "1093   contemporary country  \n",
       "1094   contemporary country  \n",
       "1095          post-teen pop  \n",
       "\n",
       "[1096 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the main df indicating whether each song is in a genre that's in the top 3 genres for a given week\n",
    "\n",
    "def is_in_top3_genres(row, df_top_genres):\n",
    "    week = row['WeekID']\n",
    "\n",
    "    top_genres = df_top_genres[df_top_genres['WeekID'] == week]\n",
    "\n",
    "    if len(top_genres) == 0:\n",
    "        return 0\n",
    "    \n",
    "    top_3 = [\n",
    "        top_genres.iloc[0]['Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['2nd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['3rd_Most_Popular_Genre']\n",
    "        ]\n",
    "\n",
    "    for genre in top_3:\n",
    "        if genre in row.index and row[genre] == 1:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "df_hotlist_and_features_2000s['in_top3_genres'] = df_hotlist_and_features_2000s.apply(\n",
    "    lambda row: is_in_top3_genres(row, df_top_genres), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new df with the max weekly rank change for each song \n",
    "df_max_rank_change = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song \n",
    "df_max_peak_pos = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates and rechecking\n",
    "df_features_2000s = df_features_2000s.drop_duplicates(subset='SongID')\n",
    "\n",
    "print(len(df_features_2000s))\n",
    "print(len(pd.unique(df_features_2000s['SongID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>in_top3_genres</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  in_top3_genres  \\\n",
       "0           0                 0           0               0   \n",
       "1           0                 0           0               0   \n",
       "2           0                 0           0               1   \n",
       "3           0                 0           0               1   \n",
       "4           0                 0           0               0   \n",
       "\n",
       "   Max_Peak_Position  Max_Rank_Change  \n",
       "0                 69             -8.0  \n",
       "1                 69             10.0  \n",
       "2                  1              9.0  \n",
       "3                 26             31.0  \n",
       "4                 19             19.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding max peak position to main df\n",
    "df_2000s_data = df_hotlist_and_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to features df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "df_2000s_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'WeekID'), (1, 'Week Position'), (2, 'SongID'), (3, 'Previous Week Position'), (4, 'Peak Position'), (5, 'Weeks on Chart'), (6, 'Rank_Change'), (7, 'spotify_track_duration_ms'), (8, 'danceability'), (9, 'energy'), (10, 'key'), (11, 'loudness'), (12, 'mode'), (13, 'speechiness'), (14, 'acousticness'), (15, 'instrumentalness'), (16, 'liveness'), (17, 'valence'), (18, 'tempo'), (19, 'time_signature'), (20, 'pop'), (21, 'dance pop'), (22, 'pop rap'), (23, 'rap'), (24, 'contemporary country'), (25, 'country'), (26, 'country road'), (27, 'post-teen pop'), (28, 'hip hop'), (29, 'r&b'), (30, 'urban contemporary'), (31, 'trap'), (32, 'southern hip hop'), (33, 'pop rock'), (34, 'hip pop'), (35, 'modern country rock'), (36, 'neo mellow'), (37, 'post-grunge'), (38, 'gangster rap'), (39, 'atl hip hop'), (40, 'alternative metal'), (41, 'neo soul'), (42, 'dirty south rap'), (43, 'country dawn'), (44, 'modern rock'), (45, 'nu metal'), (46, 'canadian pop'), (47, 'rock'), (48, 'melodic rap'), (49, 'deep pop r&b'), (50, 'edm'), (51, 'new jack swing'), (52, 'permanent wave'), (53, 'miami hip hop'), (54, 'country pop'), (55, 'oklahoma country'), (56, 'latin'), (57, 'tropical house'), (58, 'electropop'), (59, 'uk pop'), (60, 'east coast hip hop'), (61, 'alternative rock'), (62, 'viral pop'), (63, 'quiet storm'), (64, 'chicago rap'), (65, 'redneck'), (66, 'pop punk'), (67, 'crunk'), (68, 'country rock'), (69, 'toronto rap'), (70, 'canadian hip hop'), (71, 'boy band'), (72, 'hardcore hip hop'), (73, 'queens hip hop'), (74, 'talent show'), (75, 'emo'), (76, 'europop'), (77, 'acoustic pop'), (78, 'alternative r&b'), (79, 'conscious hip hop'), (80, 'australian pop'), (81, 'detroit hip hop'), (82, 'rap rock'), (83, 'latin pop'), (84, 'barbadian pop'), (85, 'g funk'), (86, 'girl group'), (87, 'canadian rock'), (88, 'tropical'), (89, 'piano rock'), (90, 'indie pop'), (91, 'electro house'), (92, 'indie poptimism'), (93, 'rap metal'), (94, 'west coast rap'), (95, 'new orleans rap'), (96, 'metropopolis'), (97, 'candy pop'), (98, 'lilith'), (99, 'australian country'), (100, 'philly rap'), (101, 'funk metal'), (102, 'reggaeton'), (103, 'dfw rap'), (104, 'canadian contemporary r&b'), (105, 'soul'), (106, 'mexican pop'), (107, 'adult standards'), (108, 'nc hip hop'), (109, 'british soul'), (110, 'trap queen'), (111, 'hollywood'), (112, 'arkansas country'), (113, 'atl trap'), (114, 'underground hip hop'), (115, 'texas country'), (116, 'uk dance'), (117, 'house'), (118, 'new wave pop'), (119, 'brostep'), (120, 'dancehall'), (121, 'progressive house'), (122, 'funk'), (123, 'singer-songwriter'), (124, 'latin hip hop'), (125, 'idol'), (126, 'garage rock'), (127, 'mellow gold'), (128, 'baroque pop'), (129, 'big room'), (130, 'art pop'), (131, 'reggae fusion'), (132, 'cali rap'), (133, 'bronx hip hop'), (134, 'folk-pop'), (135, 'country rap'), (136, 'stomp and holler'), (137, 'neon pop punk'), (138, 'emo rap'), (139, 'punk'), (140, 'indie rock'), (141, 'funk rock'), (142, 'memphis hip hop'), (143, 'modern alternative rock'), (144, 'lgbtq+ hip hop'), (145, 'progressive electro house'), (146, 'alternative hip hop'), (147, 'blues rock'), (148, 'colombian pop'), (149, 'eurodance'), (150, 'classic rock'), (151, 'baton rouge rap'), (152, 'australian dance'), (153, 'folk'), (154, 'pop emo'), (155, 'soft rock'), (156, 'motown'), (157, 'pixie'), (158, 'canadian country'), (159, 'wrestling'), (160, 'glee club'), (161, 'complextro'), (162, 'vapor trap'), (163, 'etherpop'), (164, 'pittsburgh rap'), (165, 'escape room'), (166, 'indietronica'), (167, 'comic'), (168, 'german techno'), (169, 'new jersey rap'), (170, 'trap latino'), (171, 'houston rap'), (172, 'social media pop'), (173, 'puerto rican pop'), (174, 'deep southern trap'), (175, 'heartland rock'), (176, 'alternative dance'), (177, 'bubblegum dance'), (178, 'alberta country'), (179, 'outlaw country'), (180, 'country gospel'), (181, 'florida rap'), (182, 'hard rock'), (183, 'canadian metal'), (184, 'christian rock'), (185, 'soca'), (186, 'indiecoustica'), (187, 'harlem hip hop'), (188, 'new rave'), (189, 'electronic trap'), (190, 'christian music'), (191, 'grunge'), (192, 'show tunes'), (193, 'viral trap'), (194, 'la indie'), (195, 'swedish pop'), (196, 'swedish electropop'), (197, 'reggaeton flow'), (198, 'dance-punk'), (199, 'celtic rock'), (200, 'socal pop punk'), (201, 'lounge'), (202, 'chicano rap'), (203, 'stomp pop'), (204, 'ccm'), (205, 'vocal jazz'), (206, 'glam metal'), (207, 'worship'), (208, 'irish rock'), (209, 'electropowerpop'), (210, 'electro'), (211, 'indie pop rap'), (212, 'canadian contemporary country'), (213, 'bounce'), (214, 'christian alternative rock'), (215, 'south african rock'), (216, 'deep talent show'), (217, 'disco'), (218, 'hyphy'), (219, 'disco house'), (220, 'canadian latin'), (221, 'australian hip hop'), (222, 'nyc rap'), (223, 'brill building pop'), (224, 'k-pop'), (225, 'nz pop'), (226, 'minnesota hip hop'), (227, 'modern blues rock'), (228, 'album rock'), (229, 'modern folk rock'), (230, 'uk americana'), (231, 'old school hip hop'), (232, 'punk blues'), (233, 'dmv rap'), (234, 'industrial metal'), (235, 'skate punk'), (236, 'swedish synthpop'), (237, 'moombahton'), (238, 'Max_Peak_Position'), (239, 'Max_Rank_Change')]\n"
     ]
    }
   ],
   "source": [
    "all_cols = [(index, column) for index, column in enumerate(df_2000s_data.columns)]\n",
    "print(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m     distances \u001b[38;5;241m=\u001b[39m pairwise_distances(\n\u001b[0;32m     16\u001b[0m         week_df[attribute_columns]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     17\u001b[0m         top_song\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     18\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m     )\u001b[38;5;241m.\u001b[39mflatten() \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distances\n\u001b[1;32m---> 23\u001b[0m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan_dist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_2000s_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWeekID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalc_weekly_manhattan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# converting distance to similarity score   \u001b[39;00m\n\u001b[0;32m     28\u001b[0m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan_dist\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1353\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1352\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1353\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1402\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1402\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1404\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:767\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    766\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    769\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     distances \u001b[38;5;241m=\u001b[39m pairwise_distances(\n\u001b[0;32m     16\u001b[0m         week_df[attribute_columns]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     17\u001b[0m         top_song\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     18\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m     )\u001b[38;5;241m.\u001b[39mflatten() \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distances\n\u001b[0;32m     23\u001b[0m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan_dist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_2000s_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekID\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 24\u001b[0m                 \u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mcalc_weekly_manhattan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, index\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# converting distance to similarity score   \u001b[39;00m\n\u001b[0;32m     28\u001b[0m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m df_2000s_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan_dist\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m, in \u001b[0;36mcalc_weekly_manhattan\u001b[1;34m(week_df)\u001b[0m\n\u001b[0;32m     12\u001b[0m top_song \u001b[38;5;241m=\u001b[39m week_df[week_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeek Position\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][attribute_columns]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# calculating distance\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweek_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattribute_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_song\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmanhattan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten() \n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distances\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2480\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   2478\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1973\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1970\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(X, Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1976\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1112\u001b[0m, in \u001b[0;36mmanhattan_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1067\u001b[0m     {\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m )\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmanhattan_distances\u001b[39m(X, Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the L1 distances between the vectors in X and Y.\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <metrics>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;124;03m           [4., 4.]])\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1112\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mor\u001b[39;00m issparse(Y):\n\u001b[0;32m   1115\u001b[0m         X \u001b[38;5;241m=\u001b[39m csr_matrix(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:200\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    190\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    210\u001b[0m         Y,\n\u001b[0;32m    211\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I'd like to see how similar each song is to the #1 song on a week by week basis. \n",
    "I'll do this by doing a similarity analysis using Manhattan distance, since the data is both sparse and high-dimensional.\n",
    "\"\"\"\n",
    "attribute_columns = df_2000s_data.columns[7:238].tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_2000s_data[attribute_columns])\n",
    "\n",
    "def calc_weekly_manhattan(week_df):\n",
    "    # attributes of #1 song\n",
    "    top_song = week_df[week_df['Week Position'] == 1][attribute_columns].values[0]\n",
    "\n",
    "    # calculating distance\n",
    "    distances = pairwise_distances(\n",
    "        week_df[attribute_columns].values,\n",
    "        top_song.reshape(1, -1),\n",
    "        metric='manhattan'\n",
    "    ).flatten() \n",
    "\n",
    "    return distances\n",
    "\n",
    "df_2000s_data['manhattan_dist'] = df_2000s_data.groupby('WeekID', group_keys=False).apply(\n",
    "                lambda x: pd.Series(calc_weekly_manhattan(x), index=x.index)\n",
    ")\n",
    "\n",
    "# converting distance to similarity score   \n",
    "df_2000s_data['similarity'] = 1 / (1 + df_2000s_data['manhattan_dist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songID_genre = df_cleaned.drop(['spotify_track_id', 'spotify_track_duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
    "                                   'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "df_songID_genre.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing fields used for prep/cleaning but not needed for analysis\n",
    "df_cleaned = df_cleaned.drop(['SongID', 'spotify_genre', 'spotify_track_id'], axis=1)\n",
    "\n",
    "# my code added columns for all genres in spotify_genre, removing unwanted columns and creating a clean df with genre\n",
    "last_col_to_keep_genre = 'emo rap'\n",
    "df_cleaned_genre = df_cleaned.loc[:, :last_col_to_keep_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "\n",
    "# creating a clean df for analysis without genre\n",
    "last_col_to_keep_no_genre = 'Max_Rank_Change'\n",
    "df_cleaned_no_genre = df_cleaned.loc[:, :last_col_to_keep_no_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "df_cleaned_no_genre = df_cleaned_no_genre.dropna()\n",
    "\n",
    "df_cleaned_genre.info(), df_cleaned_no_genre.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (1__1 variables)\n",
    "2. Max Peak Position, with genre (1_2 variables)\n",
    "3. Max Rank Change, no genre (2_1 variables)\n",
    "4. Max Rank Change, with genre (2_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X1_1 = df_cleaned_no_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y1_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_1_train, X1_1_test, y1_1_train, y1_1_test = train_test_split(X1_1, y1_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_1_train_scaled = scaler.fit_transform(X1_1_train)\n",
    "X1_1_test_scaled = scaler.fit_transform(X1_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X1_2 = df_cleaned_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y1_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_2_train, X1_2_test, y1_2_train, y1_2_test = train_test_split(X1_2, y1_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_2_train_scaled = scaler.fit_transform(X1_2_train)\n",
    "X1_2_test_scaled = scaler.fit_transform(X1_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X2_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_1_train, X2_1_test, y2_1_train, y2_1_test = train_test_split(X2_1, y2_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_1_train_scaled = scaler.fit_transform(X2_1_train)\n",
    "X2_1_test_scaled = scaler.fit_transform(X2_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X2_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_2_train, X2_2_test, y2_2_train, y2_2_test = train_test_split(X2_2, y2_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_2_train_scaled = scaler.fit_transform(X2_2_train)\n",
    "X2_2_test_scaled = scaler.fit_transform(X2_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (3__1 variables)\n",
    "2. Max Peak Position, with genre (3_2 variables)\n",
    "3. Max Rank Change, no genre (4_1 variables)\n",
    "4. Max Rank Change, with genre (4_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X3_1 = df_cleaned_no_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y3_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_1_train, X3_1_test, y3_1_train, y3_1_test = train_test_split(X3_1, y3_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_1_train_final, X3_1_val, y3_1_train_final, y3_1_val = train_test_split(X3_1_train, y3_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X3_1_train_final)\n",
    "X3_1_train_scaled = scaler.transform(X3_1_train_final)\n",
    "X3_1_val_scaled = scaler.transform(X3_1_val)\n",
    "X3_1_test_scaled = scaler.transform(X3_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X3_2 = df_cleaned_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y3_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_2_train, X3_2_test, y3_2_train, y3_2_test = train_test_split(X3_2, y3_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_2_train_final, X3_2_val, y3_2_train_final, y3_2_val = train_test_split(X3_2_train, y3_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X3_2_train_final)\n",
    "X3_2_train_scaled = scaler.transform(X3_2_train_final)\n",
    "X3_2_val_scaled = scaler.transform(X3_2_val)\n",
    "X3_2_test_scaled = scaler.transform(X3_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X4_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_1_train, X4_1_test, y4_1_train, y4_1_test = train_test_split(X4_1, y4_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_1_train_final, X4_1_val, y4_1_train_final, y4_1_val = train_test_split(X4_1_train, y4_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_1_train_final)\n",
    "X4_1_train_scaled = scaler.transform(X4_1_train_final)\n",
    "X4_1_val_scaled = scaler.transform(X4_1_val)\n",
    "X4_1_test_scaled = scaler.transform(X4_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X4_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_2_train, X4_2_test, y4_2_train, y4_2_test = train_test_split(X4_2, y4_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_2_train_final, X4_2_val, y4_2_train_final, y4_2_val = train_test_split(X4_2_train, y4_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_2_train_final)\n",
    "X4_2_train_scaled = scaler.transform(X4_2_train_final)\n",
    "X4_2_val_scaled = scaler.transform(X4_2_val)\n",
    "X4_2_test_scaled = scaler.transform(X4_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_model1_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_1.fit(X1_1_train, y1_1_train)\n",
    "y1_1_pred = xgb_model1_1.predict(X1_1_test)\n",
    "y1_1_pred = np.clip(np.round(y1_1_pred), 1, 100)\n",
    "\n",
    "rmse1_1 = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred))\n",
    "r2_1_1 = r2_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_1:.3f}')\n",
    "print(f'R²: {r2_1_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [4, 5],\n",
    "    'learning_rate': [0.03, 0.05, 0.07],\n",
    "    'subsample': [0.75],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb1_1.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_1_pred_best = best_xgb1_1.predict(X1_1_test)\n",
    "y1_1_pred_best = np.clip(np.round(y1_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_1_best = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred_best))\n",
    "r2_1_1_best = r2_score(y1_1_test, y1_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_1_best:.3f}')\n",
    "print(f'R²: {r2_1_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_model1_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_2.fit(X1_2_train, y1_2_train)\n",
    "y1_2_pred = xgb_model1_2.predict(X1_2_test)\n",
    "y1_2_pred = np.clip(np.round(y1_2_pred), 1, 100)\n",
    "\n",
    "rmse1_2 = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred))\n",
    "r2_1_2 = r2_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_2:.3f}')\n",
    "print(f'R²: {r2_1_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'subsample': [0.9, 1.0, 1.1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [4, 5],\n",
    "    'learning_rate': [0.03, 0.05, 0.07],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model \n",
    "best_xgb1_2 = grid_search_xgb1_2.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_2_pred_best = best_xgb1_2.predict(X1_2_test)\n",
    "y1_2_pred_best = np.clip(np.round(y1_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_2_best = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred_best))\n",
    "r2_1_2_best = r2_score(y1_2_test, y1_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_2_best:.3f}')\n",
    "print(f'R²: {r2_1_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_1.fit(X2_1_train, y2_1_train)\n",
    "y2_1_pred = xgb_model2_1.predict(X2_1_test)\n",
    "y2_1_pred = np.clip(np.round(y2_1_pred), 1, 100)\n",
    "\n",
    "rmse2_1 = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred))\n",
    "r2_2_1 = r2_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_1:.3f}')\n",
    "print(f'R²: {r2_2_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3 for max_rank_change\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.9],\n",
    "        }\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_1 = grid_search_xgb2_1.best_estimator_\n",
    "\n",
    "y2_1_pred_best = best_xgb2_1.predict(X2_1_test)\n",
    "y2_1_pred_best = np.clip(np.round(y2_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_1_best = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred_best))\n",
    "r2_2_1_best = r2_score(y2_1_test, y2_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_1_best:.3f}')\n",
    "print(f'R²: {r2_2_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_2.fit(X2_2_train, y2_2_train)\n",
    "y2_2_pred = xgb_model2_2.predict(X2_2_test)\n",
    "y2_2_pred = np.clip(np.round(y2_2_pred), 1, 100)\n",
    "\n",
    "rmse2_2 = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred))\n",
    "r2_2_2 = r2_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_2:.3f}')\n",
    "print(f'R²: {r2_2_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_2 = grid_search_xgb2_2.best_estimator_\n",
    "\n",
    "y2_2_pred_best = best_xgb2_2.predict(X2_2_test)\n",
    "y2_2_pred_best = np.clip(np.round(y2_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_2_best = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred_best))\n",
    "r2_2_2_best = r2_score(y2_2_test, y2_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_2_best:.3f}')\n",
    "print(f'R²: {r2_2_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, the Max Rank Change models both had an r<sup>2</sup> value less than 0.001, essentially indicating no fit of the model to the test data. Max Peak Position performed better, but the highest r<sup>2</sup> value was 0.138 so their predictive value is low.\n",
    "\n",
    "Given the lack of predictive power in these outcomes, I'm shifting focus to the other two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_1.fit(X1_1_train_scaled, y1_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_1 = grid_search1_1.best_params_\n",
    "standard_best_score1_1 = grid_search1_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model1_1 = grid_search1_1.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y1_1_pred = final_model1_1.predict(X1_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_1 = accuracy_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_2.fit(X1_2_train_scaled, y1_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_2 = grid_search1_2.best_params_\n",
    "standard_best_score1_2 = grid_search1_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model1_2 = grid_search1_2.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y1_2_pred = final_model1_2.predict(X1_2_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_2 = accuracy_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_1.fit(X2_1_train_scaled, y2_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_1 = grid_search2_1.best_params_\n",
    "standard_best_score2_1 = grid_search2_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model2_1 = grid_search2_1.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y2_1_pred = final_model2_1.predict(X2_1_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_1 = accuracy_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_2.fit(X2_2_train_scaled, y2_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_2 = grid_search2_2.best_params_\n",
    "standard_best_score2_2 = grid_search2_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model2_2 = grid_search2_2.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y2_2_pred = final_model2_2.predict(X2_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_2 = accuracy_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed poorly on the Max Peak Position data, with a maximum accuracy of 0.052. The performance on the Max Rank Change was better, but the maximum accuracy was still just 0.217.\n",
    "\n",
    "I will not explore k-NN further and instead focus on the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"Reverting to 2 deep layers, increasing to 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model but increased overfitting a little bit. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing dropout rate\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"2 deep layers,  kernels per layer, increased dropout from 0.4 to 0.6\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"3 deep layers, 128 kernels per layer, 0.6 dropout rate\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model continues to have the best MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very nearly identical to the initial configurations. Doubling the kernels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model has the best MAE scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance\n",
    "\n",
    "**XGBoost**\n",
    "The XGBoost model did not return meaningful results for this dataset. The best metrics for XGBoost were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- RMSE: 22.35\n",
    "- r<sup>2</sup>: 0.138\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- RMSE: 11.73\n",
    "- - r<sup>2</sup>: 0.002\n",
    "\n",
    "**k-Nearest Neighbors**\n",
    "This model also did not perform well on this data set. Its best metrics were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- Accuracy: 0.052\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- Accuracy: 0.217\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "The deep learning model returned the most promising results and are described in the next section.\n",
    "\n",
    "### Final Models\n",
    "\n",
    "The final models are both based on a deep learning architecture. \n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "The best model for the Maximum Peak Position is named reg_model3_1. It is a 3-layer, regularized model trained on the song dataset that does not contain genre information.Its final metrics were:\n",
    "\n",
    "- Training mean absolute error: 16.99\n",
    "- Validation mean absolute error: 17.67\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "The best model for the Maximum Rank Increase is named reg_model4_2. It is also a 3-layer, regularized model trained on the song dataset without genre.\n",
    "Its final metrics:\n",
    "\n",
    "- Training mean absolute error: 7.75\n",
    "- Validation mean absolute error: 8.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genre of a song, at least how it is captured on Spotify, has very little influence on its movement on the Billboard Hot 100 list. This gives FutureProduct Advisors consultants and their customers significant leeway when they are selecting music for their social and advertising campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristics of a given song appear to be the most important factors in its performance on the Hot 100 list. Features such as tempo, danceability, etc. have the most predictive power for the Hot 100 list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital music has a large number of characteristics that can be pulled into machine learning models. The factors that have the largest influence on popularity are often unexpected, and modeling and analysis of songs requires methodical and careful selection and vetting of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "reg_model3_1 (predicting highest ranking on the Billboard Hot 100) and reg_model4_2 (predicting largest week over week ranking increase) are ready for initial deployment to FutureProduct Advisors. \n",
    "\n",
    "Detailed training will be provided, but at a high level here is how users will engage:\n",
    "\n",
    "1. Consultants will identify a list of songs they'd like to explore.\n",
    "    - We recommend using the songs in places 90-100 of the Billboard Hot 100 at a minimum\n",
    "2. Consultants download those songs' metadata from Spotify as a CSV file (detailed instructions to come)\n",
    "3. Consultants load the CSV file into these models, and the models will return predicted activity for each song.\n",
    "\n",
    "There is also an additional opportunity to built on and refine these models by indluding additional data and experimenting with other modeling approaches. If the FutureProduct Advisors consultants have positive feedback on these prototype tools, we will be happy to partner on future enhancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
