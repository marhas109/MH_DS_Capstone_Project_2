{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](Images/mick-haupt-fDW-BoHRMKE-unsplash_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Song Popularity on the BillBoard Hot 100 List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project seeks to build machine learning models to achieve two things:\n",
    "\n",
    "1. Predict the highest ranking a song will achieve on the Billboard Hot 100 list.\n",
    "2. Predict the largest week over week increase in a given song's ranking on the Hot 100 list.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "- A song's genre has a minimal impact on its ranking on the Billboard Hot 100 list.\n",
    "- Song characteristics such as tempo, danceability, etc. appear to be the strongest drivers of their performance on the Hot 100 list.\n",
    "- Analyzing music can quickly lead to an overwhelming number of features, so thoughtful and careful data selection is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 327895 entries, 0 to 327894\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   327895 non-null  int64  \n",
      " 1   url                     327895 non-null  object \n",
      " 2   WeekID                  327895 non-null  object \n",
      " 3   Week Position           327895 non-null  int64  \n",
      " 4   Song                    327895 non-null  object \n",
      " 5   Performer               327895 non-null  object \n",
      " 6   SongID                  327895 non-null  object \n",
      " 7   Instance                327895 non-null  int64  \n",
      " 8   Previous Week Position  295941 non-null  float64\n",
      " 9   Peak Position           327895 non-null  int64  \n",
      " 10  Weeks on Chart          327895 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29503 entries, 0 to 29502\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index                      29503 non-null  int64  \n",
      " 1   SongID                     29503 non-null  object \n",
      " 2   Performer                  29503 non-null  object \n",
      " 3   Song                       29503 non-null  object \n",
      " 4   spotify_genre              27903 non-null  object \n",
      " 5   spotify_track_id           24397 non-null  object \n",
      " 6   spotify_track_preview_url  14491 non-null  object \n",
      " 7   spotify_track_duration_ms  24397 non-null  float64\n",
      " 8   spotify_track_explicit     24397 non-null  object \n",
      " 9   spotify_track_album        24391 non-null  object \n",
      " 10  danceability               24334 non-null  float64\n",
      " 11  energy                     24334 non-null  float64\n",
      " 12  key                        24334 non-null  float64\n",
      " 13  loudness                   24334 non-null  float64\n",
      " 14  mode                       24334 non-null  float64\n",
      " 15  speechiness                24334 non-null  float64\n",
      " 16  acousticness               24334 non-null  float64\n",
      " 17  instrumentalness           24334 non-null  float64\n",
      " 18  liveness                   24334 non-null  float64\n",
      " 19  valence                    24334 non-null  float64\n",
      " 20  tempo                      24334 non-null  float64\n",
      " 21  time_signature             24334 non-null  float64\n",
      " 22  spotify_track_popularity   24397 non-null  float64\n",
      "dtypes: float64(14), int64(1), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Highest Ranking: 76\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peak_pos_dist = df_clean_withgenre['Max_Peak_Position'].value_counts()\n",
    "df_peak_pos_dist = pd.DataFrame(peak_pos_dist)\n",
    "df_peak_pos_dist = df_peak_pos_dist.reset_index()\n",
    "print(f\"Mean Highest Ranking: {df_clean_withgenre['Max_Peak_Position'].mean():.0f}\")\n",
    "\n",
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='skyblue')\n",
    "plt.xlabel('Highest Ranking')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Highest Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Largest Week over Week Rank Change: 13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_rank_change = df_clean_withgenre['Max_Rank_Change'].value_counts()\n",
    "df_max_rank_change = pd.DataFrame(max_rank_change)\n",
    "df_max_rank_change = df_max_rank_change.reset_index()\n",
    "df_max_rank_change = df_max_rank_change[df_max_rank_change['Max_Rank_Change'] > 0]\n",
    "print(f\"Mean Largest Week over Week Rank Change: {df_clean_withgenre['Max_Rank_Change'].mean():.0f}\")\n",
    "\n",
    "plt.bar(df_max_rank_change['Max_Rank_Change'], df_max_rank_change['count'], color='orange')\n",
    "plt.xlabel('Largest Week over Week Rank Change')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Biggest Week over Week Rank Change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveness_dist = df_clean_withgenre['liveness'].value_counts()\n",
    "df_liveness_dist = pd.DataFrame(liveness_dist)\n",
    "df_liveness_dist = df_liveness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_liveness_dist['liveness'], df_liveness_dist['count'], color='orange')\n",
    "plt.xlabel('Liveness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Liveness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "danceability_dist = df_clean_withgenre['danceability'].value_counts()\n",
    "df_danceability_dist = pd.DataFrame(danceability_dist)\n",
    "df_danceability_dist = df_danceability_dist.reset_index()\n",
    "\n",
    "plt.bar(df_danceability_dist['danceability'], df_danceability_dist['count'], color='skyblue')\n",
    "plt.xlabel('Danceability Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Danceability Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acousticness_dist = df_clean_withgenre['acousticness'].value_counts()\n",
    "df_acousticness_dist = pd.DataFrame(acousticness_dist)\n",
    "df_acousticness_dist = df_acousticness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_acousticness_dist['acousticness'], df_acousticness_dist['count'], color='orange')\n",
    "plt.xlabel('Acousticness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Acousticness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_10656\\1393120573.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s.loc[:, 'Rank_Change'] = df_hotlist_2000s['Week Position'] - df_hotlist_2000s['Previous Week Position']\n"
     ]
    }
   ],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer', 'Instance'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "df_hotlist_2000s.loc[:, 'Rank_Change'] = df_hotlist_2000s['Week Position'] - df_hotlist_2000s['Previous Week Position']\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s.loc[:, 'Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_id', 'spotify_track_preview_url',  \n",
    "                                        'spotify_track_explicit', 'spotify_track_popularity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112098 entries, 0 to 112097\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   WeekID                     112098 non-null  datetime64[ns]\n",
      " 1   Week Position              112098 non-null  int64         \n",
      " 2   SongID                     112098 non-null  object        \n",
      " 3   Previous Week Position     101571 non-null  float64       \n",
      " 4   Peak Position              112098 non-null  int64         \n",
      " 5   Weeks on Chart             112098 non-null  int64         \n",
      " 6   Rank_Change                112098 non-null  float64       \n",
      " 7   spotify_genre              108091 non-null  object        \n",
      " 8   spotify_track_duration_ms  104590 non-null  float64       \n",
      " 9   danceability               104287 non-null  float64       \n",
      " 10  energy                     104287 non-null  float64       \n",
      " 11  key                        104287 non-null  float64       \n",
      " 12  loudness                   104287 non-null  float64       \n",
      " 13  mode                       104287 non-null  float64       \n",
      " 14  speechiness                104287 non-null  float64       \n",
      " 15  acousticness               104287 non-null  float64       \n",
      " 16  instrumentalness           104287 non-null  float64       \n",
      " 17  liveness                   104287 non-null  float64       \n",
      " 18  valence                    104287 non-null  float64       \n",
      " 19  tempo                      104287 non-null  float64       \n",
      " 20  time_signature             104287 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(3), object(2)\n",
      "memory usage: 18.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# combining the hotlist and features into one dataframe\n",
    "\n",
    "df_hotlist_and_features_2000s = pd.merge(df_hotlist_2000s, df_features_all, on='SongID', how='left')\n",
    "df_hotlist_and_features_2000s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset has genre in a single column and the entry for each song has a variety of genres listed in that single column. \n",
    "This does not allow me to explore genre in a systematic way.\n",
    "I'll need to break genre out so that each genre has its own column with a 1 or 0 to indicate whether each song is tagged with that genre \n",
    "(ending with a one-hot encoded structure).\n",
    "\"\"\"\n",
    "\n",
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_hotlist_and_features_2000s['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I'm only including genres that appear in 100 or more songs (i.e. at least 0.1% of songs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading list of genres with 100 or more instances in df_cleaned\n",
    "df_genres_100_up = pd.read_csv('genre_counts_100+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_100_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating a list of genres and counts\n",
    "genre_data = []\n",
    "\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre'] :\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        row_dict = {genre: (1 if genre in genre_list else 0) for genre in final_genres_list} # dict with 1 if genre exists in list, 0 if not\n",
    "    else:\n",
    "         row_dict = {genre: 0 for genre in final_genres_list} # 0 of genre does not exist in list\n",
    "    genre_data.append(row_dict)\n",
    "\n",
    "# creating a df with the list of dicts\n",
    "genre_df = pd.DataFrame(genre_data)\n",
    "\n",
    "# concatenating genre data into df_clean\n",
    "df_hotlist_and_features_2000s = pd.concat([df_hotlist_and_features_2000s, genre_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  \n",
       "0           0                 0           0  \n",
       "1           0                 0           0  \n",
       "2           0                 0           0  \n",
       "3           0                 0           0  \n",
       "4           0                 0           0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing spotify_genre and spot-checking resulting df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_hotlist_and_features_2000s = df_hotlist_and_features_2000s.drop(['spotify_genre'], axis=1)\n",
    "df_hotlist_and_features_2000s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new df with most 5 most popular genres by week\n",
    "\n",
    "# Get all genre column names\n",
    "genre_start_idx = df_hotlist_and_features_2000s.columns.get_loc('pop')\n",
    "genre_cols = df_hotlist_and_features_2000s.columns[genre_start_idx:].tolist()\n",
    "\n",
    "# Group by WeekID and sum the genre columns to get counts\n",
    "genre_counts = df_hotlist_and_features_2000s.groupby('WeekID')[genre_cols].sum()\n",
    "\n",
    "# For each week, find the top 5 genres\n",
    "top_genres = []\n",
    "for week_id in genre_counts.index:\n",
    "    # Get the genre counts for this week and sort them\n",
    "    week_genres = genre_counts.loc[week_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Get the top 3 genre names\n",
    "    top_5 = week_genres.head(5).index.tolist()\n",
    "    \n",
    "    # Pad with None if there are fewer than 5 genres\n",
    "    while len(top_5) < 5:\n",
    "        top_5.append(None)\n",
    "    \n",
    "    top_genres.append({\n",
    "        'WeekID': week_id,\n",
    "        'Most_Popular_Genre': top_5[0],\n",
    "        '2nd_Most_Popular_Genre': top_5[1],\n",
    "        '3rd_Most_Popular_Genre': top_5[2],\n",
    "        '4th_Most_Popular_Genre': top_5[3],\n",
    "        '5th_Most_Popular_Genre': top_5[4]\n",
    "    })\n",
    "\n",
    "# Create the new dataframe\n",
    "df_top_genres = pd.DataFrame(top_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the main df indicating whether each song is in a genre that's in the top 5 genres for a given week\n",
    "\n",
    "def is_in_top5_genres(row, df_top_genres):\n",
    "    week = row['WeekID']\n",
    "\n",
    "    top_genres = df_top_genres[df_top_genres['WeekID'] == week]\n",
    "\n",
    "    if len(top_genres) == 0:\n",
    "        return 0\n",
    "    \n",
    "    top_5 = [\n",
    "        top_genres.iloc[0]['Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['2nd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['3rd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['4th_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['5th_Most_Popular_Genre'],\n",
    "        ]\n",
    "\n",
    "    for genre in top_5:\n",
    "        if genre in row.index and row[genre] == 1:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "df_hotlist_and_features_2000s['in_top5_genres'] = df_hotlist_and_features_2000s.apply(\n",
    "    lambda row: is_in_top5_genres(row, df_top_genres), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new df with mean score of appearance in top 5 weekly genres\n",
    "df_mean_genre_match = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['in_top5_genres'].mean()\n",
    "df_mean_genre_match.rename(columns={'in_top5_genres': 'In_Top5genres_Mean'}, inplace=True)\n",
    "df_mean_genre_match.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max weekly rank change for each song \n",
    "df_max_rank_change = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song \n",
    "df_max_peak_pos = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum(), df_mean_genre_match['In_Top5genres_Mean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>in_top5_genres</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  in_top5_genres  \\\n",
       "0           0                 0           0               0   \n",
       "1           0                 0           0               0   \n",
       "2           0                 0           0               1   \n",
       "3           0                 0           0               1   \n",
       "4           0                 0           0               0   \n",
       "\n",
       "   Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "0                 69             -8.0                 0.0  \n",
       "1                 69             10.0                 0.0  \n",
       "2                  1              9.0                 1.0  \n",
       "3                 26             31.0                 1.0  \n",
       "4                 19             19.0                 0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding max peak position to main df\n",
    "df_2000s_data = df_hotlist_and_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to main df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "# adding in top5genres mean to main df\n",
    "df_2000s_data = df_2000s_data.join(df_mean_genre_match, on='SongID')\n",
    "\n",
    "df_2000s_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103420, 112098)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a clean df with genre information\n",
    "df_clean_withgenre = df_2000s_data.drop(['WeekID', 'Week Position', 'Previous Week Position', 'Peak Position',\n",
    "                                         'Weeks on Chart', 'Rank_Change', 'in_top5_genres'], axis=1)\n",
    "\n",
    "# counting duplicates and all rows\n",
    "df_clean_withgenre.duplicated().sum(), len(df_clean_withgenre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 8678\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 820\n"
     ]
    }
   ],
   "source": [
    "# removing duplicate rows\n",
    "df_clean_withgenre = df_clean_withgenre.drop_duplicates()\n",
    "\n",
    "# checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7858\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# dropping rows with missing values (unfortunately there is no reliable way to infer or estimate song characteristics)\n",
    "df_clean_withgenre = df_clean_withgenre.dropna()\n",
    "\n",
    "# re-checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a clean df with no genre\n",
    "nogenre_cols = ['spotify_track_duration_ms', 'danceability', 'energy', 'key', 'loudness',\n",
    "                'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                'time_signature', 'Max_Peak_Position', 'Max_Rank_Change', 'In_Top5genres_Mean']\n",
    "df_clean_nogenre = df_clean_withgenre[nogenre_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.00000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>0.313566</td>\n",
       "      <td>0.278315</td>\n",
       "      <td>0.282260</td>\n",
       "      <td>0.252354</td>\n",
       "      <td>0.162001</td>\n",
       "      <td>0.153092</td>\n",
       "      <td>0.145839</td>\n",
       "      <td>0.181598</td>\n",
       "      <td>0.187580</td>\n",
       "      <td>0.113006</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.147875</td>\n",
       "      <td>0.116187</td>\n",
       "      <td>0.072410</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.049631</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.037414</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.032706</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.021125</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.017943</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.018962</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.00789</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.010562</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>0.463972</td>\n",
       "      <td>0.448198</td>\n",
       "      <td>0.450128</td>\n",
       "      <td>0.434391</td>\n",
       "      <td>0.368475</td>\n",
       "      <td>0.360099</td>\n",
       "      <td>0.352967</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.390401</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0.304414</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.320470</td>\n",
       "      <td>0.259183</td>\n",
       "      <td>0.268604</td>\n",
       "      <td>0.252559</td>\n",
       "      <td>0.219815</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.233815</td>\n",
       "      <td>0.234296</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>0.189786</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.176870</td>\n",
       "      <td>0.168216</td>\n",
       "      <td>0.160538</td>\n",
       "      <td>0.199717</td>\n",
       "      <td>0.177876</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>0.134133</td>\n",
       "      <td>0.144655</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.133675</td>\n",
       "      <td>0.127562</td>\n",
       "      <td>0.148392</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.130422</td>\n",
       "      <td>0.116964</td>\n",
       "      <td>0.124136</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>0.136398</td>\n",
       "      <td>0.119578</td>\n",
       "      <td>0.115365</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.138181</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>0.107574</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.092627</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>0.115901</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.104644</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.08848</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.095941</td>\n",
       "      <td>0.077111</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.089185</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.084865</td>\n",
       "      <td>0.076291</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.094630</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.078725</td>\n",
       "      <td>0.064672</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.073776</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.062689</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.159028</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature          pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003     0.313566   \n",
       "std       0.140660     0.223494    29.558487        0.273062     0.463972   \n",
       "min       0.020000     0.034900    48.718000        0.000000     0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000     0.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000     0.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000     1.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000     1.000000   \n",
       "\n",
       "         dance pop      pop rap          rap  contemporary country  \\\n",
       "count  7858.000000  7858.000000  7858.000000           7858.000000   \n",
       "mean      0.278315     0.282260     0.252354              0.162001   \n",
       "std       0.448198     0.450128     0.434391              0.368475   \n",
       "min       0.000000     0.000000     0.000000              0.000000   \n",
       "25%       0.000000     0.000000     0.000000              0.000000   \n",
       "50%       0.000000     0.000000     0.000000              0.000000   \n",
       "75%       1.000000     1.000000     1.000000              0.000000   \n",
       "max       1.000000     1.000000     1.000000              1.000000   \n",
       "\n",
       "           country  country road  post-teen pop      hip hop          r&b  \\\n",
       "count  7858.000000   7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean      0.153092      0.145839       0.181598     0.187580     0.113006   \n",
       "std       0.360099      0.352967       0.385538     0.390401     0.316620   \n",
       "min       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       urban contemporary         trap  southern hip hop     pop rock  \\\n",
       "count         7858.000000  7858.000000       7858.000000  7858.000000   \n",
       "mean             0.103334     0.147875          0.116187     0.072410   \n",
       "std              0.304414     0.354998          0.320470     0.259183   \n",
       "min              0.000000     0.000000          0.000000     0.000000   \n",
       "25%              0.000000     0.000000          0.000000     0.000000   \n",
       "50%              0.000000     0.000000          0.000000     0.000000   \n",
       "75%              0.000000     0.000000          0.000000     0.000000   \n",
       "max              1.000000     1.000000          1.000000     1.000000   \n",
       "\n",
       "           hip pop  modern country rock   neo mellow  post-grunge  \\\n",
       "count  7858.000000          7858.000000  7858.000000  7858.000000   \n",
       "mean      0.078264             0.068465     0.050904     0.049631   \n",
       "std       0.268604             0.252559     0.219815     0.217195   \n",
       "min       0.000000             0.000000     0.000000     0.000000   \n",
       "25%       0.000000             0.000000     0.000000     0.000000   \n",
       "50%       0.000000             0.000000     0.000000     0.000000   \n",
       "75%       0.000000             0.000000     0.000000     0.000000   \n",
       "max       1.000000             1.000000     1.000000     1.000000   \n",
       "\n",
       "       gangster rap  atl hip hop  alternative metal     neo soul  \\\n",
       "count   7858.000000  7858.000000        7858.000000  7858.000000   \n",
       "mean       0.058030     0.058285           0.034869     0.037414   \n",
       "std        0.233815     0.234296           0.183459     0.189786   \n",
       "min        0.000000     0.000000           0.000000     0.000000   \n",
       "25%        0.000000     0.000000           0.000000     0.000000   \n",
       "50%        0.000000     0.000000           0.000000     0.000000   \n",
       "75%        0.000000     0.000000           0.000000     0.000000   \n",
       "max        1.000000     1.000000           1.000000     1.000000   \n",
       "\n",
       "       dirty south rap  country dawn  modern rock     nu metal  canadian pop  \\\n",
       "count      7858.000000   7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean          0.037541      0.032324     0.029142     0.026470      0.041614   \n",
       "std           0.190096      0.176870     0.168216     0.160538      0.199717   \n",
       "min           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "75%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "max           1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "              rock  melodic rap  deep pop r&b          edm  new jack swing  \\\n",
       "count  7858.000000  7858.000000   7858.000000  7858.000000     7858.000000   \n",
       "mean      0.032706     0.034742      0.019598     0.019725        0.018325   \n",
       "std       0.177876     0.183136      0.138623     0.139063        0.134133   \n",
       "min       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "25%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "50%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "75%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "max       1.000000     1.000000      1.000000     1.000000        1.000000   \n",
       "\n",
       "       permanent wave  miami hip hop  country pop  oklahoma country  \\\n",
       "count     7858.000000    7858.000000  7858.000000       7858.000000   \n",
       "mean         0.021379       0.021125     0.018198          0.016544   \n",
       "std          0.144655       0.143810     0.133675          0.127562   \n",
       "min          0.000000       0.000000     0.000000          0.000000   \n",
       "25%          0.000000       0.000000     0.000000          0.000000   \n",
       "50%          0.000000       0.000000     0.000000          0.000000   \n",
       "75%          0.000000       0.000000     0.000000          0.000000   \n",
       "max          1.000000       1.000000     1.000000          1.000000   \n",
       "\n",
       "             latin  tropical house   electropop       uk pop  \\\n",
       "count  7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.022525        0.016671     0.017307     0.013871   \n",
       "std       0.148392        0.128043     0.130422     0.116964   \n",
       "min       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       east coast hip hop  alternative rock    viral pop  quiet storm  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.015653          0.017943     0.016671     0.015271   \n",
       "std              0.124136          0.132755     0.128043     0.122637   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       chicago rap      redneck     pop punk        crunk  country rock  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean      0.018962     0.014508     0.013489     0.012344      0.012471   \n",
       "std       0.136398     0.119578     0.115365     0.110423      0.110984   \n",
       "min       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       toronto rap  canadian hip hop     boy band  hardcore hip hop  \\\n",
       "count  7858.000000       7858.000000  7858.000000       7858.000000   \n",
       "mean      0.019598          0.019471     0.014762          0.012090   \n",
       "std       0.138623          0.138181     0.120607          0.109293   \n",
       "min       0.000000          0.000000     0.000000          0.000000   \n",
       "25%       0.000000          0.000000     0.000000          0.000000   \n",
       "50%       0.000000          0.000000     0.000000          0.000000   \n",
       "75%       0.000000          0.000000     0.000000          0.000000   \n",
       "max       1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       queens hip hop  talent show          emo      europop  acoustic pop  \\\n",
       "count     7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean         0.011708     0.010435     0.010435     0.009799      0.008654   \n",
       "std          0.107574     0.101625     0.101625     0.098510      0.092627   \n",
       "min          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max          1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       alternative r&b  conscious hip hop  australian pop  detroit hip hop  \\\n",
       "count      7858.000000        7858.000000     7858.000000      7858.000000   \n",
       "mean          0.010817           0.015526        0.008145         0.013617   \n",
       "std           0.103447           0.123639        0.089885         0.115901   \n",
       "min           0.000000           0.000000        0.000000         0.000000   \n",
       "25%           0.000000           0.000000        0.000000         0.000000   \n",
       "50%           0.000000           0.000000        0.000000         0.000000   \n",
       "75%           0.000000           0.000000        0.000000         0.000000   \n",
       "max           1.000000           1.000000        1.000000         1.000000   \n",
       "\n",
       "          rap rock    latin pop  barbadian pop       g funk  girl group  \\\n",
       "count  7858.000000  7858.000000    7858.000000  7858.000000  7858.00000   \n",
       "mean      0.007126     0.011072       0.005218     0.010817     0.00789   \n",
       "std       0.084123     0.104644       0.072049     0.103447     0.08848   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "50%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "75%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "max       1.000000     1.000000       1.000000     1.000000     1.00000   \n",
       "\n",
       "       canadian rock     tropical   piano rock    indie pop  electro house  \\\n",
       "count    7858.000000  7858.000000  7858.000000  7858.000000    7858.000000   \n",
       "mean        0.005218     0.009290     0.005981     0.007126       0.007126   \n",
       "std         0.072049     0.095941     0.077111     0.084123       0.084123   \n",
       "min         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "25%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "50%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "75%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "max         1.000000     1.000000     1.000000     1.000000       1.000000   \n",
       "\n",
       "       indie poptimism    rap metal  west coast rap  new orleans rap  \\\n",
       "count      7858.000000  7858.000000     7858.000000      7858.000000   \n",
       "mean          0.005727     0.007126        0.008017         0.010562   \n",
       "std           0.075462     0.084123        0.089185         0.102236   \n",
       "min           0.000000     0.000000        0.000000         0.000000   \n",
       "25%           0.000000     0.000000        0.000000         0.000000   \n",
       "50%           0.000000     0.000000        0.000000         0.000000   \n",
       "75%           0.000000     0.000000        0.000000         0.000000   \n",
       "max           1.000000     1.000000        1.000000         1.000000   \n",
       "\n",
       "       metropopolis    candy pop       lilith  australian country  \\\n",
       "count   7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean       0.004581     0.007254     0.005854            0.005090   \n",
       "std        0.067535     0.084865     0.076291            0.071169   \n",
       "min        0.000000     0.000000     0.000000            0.000000   \n",
       "25%        0.000000     0.000000     0.000000            0.000000   \n",
       "50%        0.000000     0.000000     0.000000            0.000000   \n",
       "75%        0.000000     0.000000     0.000000            0.000000   \n",
       "max        1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "        philly rap   funk metal    reggaeton      dfw rap  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.012471     0.005218     0.009035     0.005090   \n",
       "std       0.110984     0.072049     0.094630     0.071169   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       canadian contemporary r&b         soul  mexican pop  adult standards  \\\n",
       "count                7858.000000  7858.000000  7858.000000      7858.000000   \n",
       "mean                    0.007636     0.006108     0.006872         0.006108   \n",
       "std                     0.087053     0.077922     0.082617         0.077922   \n",
       "min                     0.000000     0.000000     0.000000         0.000000   \n",
       "25%                     0.000000     0.000000     0.000000         0.000000   \n",
       "50%                     0.000000     0.000000     0.000000         0.000000   \n",
       "75%                     0.000000     0.000000     0.000000         0.000000   \n",
       "max                     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        nc hip hop  british soul   trap queen    hollywood  arkansas country  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean      0.007126      0.004327     0.004709     0.029906          0.004072   \n",
       "std       0.084123      0.065640     0.068462     0.170338          0.063688   \n",
       "min       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "          atl trap  underground hip hop  texas country     uk dance  \\\n",
       "count  7858.000000          7858.000000    7858.000000  7858.000000   \n",
       "mean      0.007126             0.006236       0.004200     0.003181   \n",
       "std       0.084123             0.078725       0.064672     0.056318   \n",
       "min       0.000000             0.000000       0.000000     0.000000   \n",
       "25%       0.000000             0.000000       0.000000     0.000000   \n",
       "50%       0.000000             0.000000       0.000000     0.000000   \n",
       "75%       0.000000             0.000000       0.000000     0.000000   \n",
       "max       1.000000             1.000000       1.000000     1.000000   \n",
       "\n",
       "             house  new wave pop      brostep    dancehall  progressive house  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean      0.003309      0.004327     0.002800     0.003181           0.003818   \n",
       "std       0.057430      0.065640     0.052841     0.056318           0.061674   \n",
       "min       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "              funk  singer-songwriter  latin hip hop         idol  \\\n",
       "count  7858.000000        7858.000000    7858.000000  7858.000000   \n",
       "mean      0.004581           0.005090       0.004327     0.005472   \n",
       "std       0.067535           0.071169       0.065640     0.073776   \n",
       "min       0.000000           0.000000       0.000000     0.000000   \n",
       "25%       0.000000           0.000000       0.000000     0.000000   \n",
       "50%       0.000000           0.000000       0.000000     0.000000   \n",
       "75%       0.000000           0.000000       0.000000     0.000000   \n",
       "max       1.000000           1.000000       1.000000     1.000000   \n",
       "\n",
       "       garage rock  mellow gold  baroque pop     big room      art pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.004709     0.005218     0.002927     0.003945     0.004581   \n",
       "std       0.068462     0.072049     0.054026     0.062689     0.067535   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       reggae fusion     cali rap  bronx hip hop     folk-pop  country rap  \\\n",
       "count    7858.000000  7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean        0.003309     0.003054       0.002800     0.002418     0.002418   \n",
       "std         0.057430     0.055184       0.052841     0.049116     0.049116   \n",
       "min         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "75%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "max         1.000000     1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       stomp and holler  neon pop punk      emo rap         punk   indie rock  \\\n",
       "count       7858.000000    7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.003563       0.002418     0.006363     0.003054     0.004072   \n",
       "std            0.059590       0.049116     0.079519     0.055184     0.063688   \n",
       "min            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         funk rock  memphis hip hop  modern alternative rock  lgbtq+ hip hop  \\\n",
       "count  7858.000000      7858.000000              7858.000000     7858.000000   \n",
       "mean      0.002927         0.003436                 0.001782        0.003436   \n",
       "std       0.054026         0.058520                 0.042174        0.058520   \n",
       "min       0.000000         0.000000                 0.000000        0.000000   \n",
       "25%       0.000000         0.000000                 0.000000        0.000000   \n",
       "50%       0.000000         0.000000                 0.000000        0.000000   \n",
       "75%       0.000000         0.000000                 0.000000        0.000000   \n",
       "max       1.000000         1.000000                 1.000000        1.000000   \n",
       "\n",
       "       progressive electro house  alternative hip hop   blues rock  \\\n",
       "count                7858.000000          7858.000000  7858.000000   \n",
       "mean                    0.001654             0.003563     0.002545   \n",
       "std                     0.040643             0.059590     0.050389   \n",
       "min                     0.000000             0.000000     0.000000   \n",
       "25%                     0.000000             0.000000     0.000000   \n",
       "50%                     0.000000             0.000000     0.000000   \n",
       "75%                     0.000000             0.000000     0.000000   \n",
       "max                     1.000000             1.000000     1.000000   \n",
       "\n",
       "       colombian pop    eurodance  classic rock  baton rouge rap  \\\n",
       "count    7858.000000  7858.000000   7858.000000      7858.000000   \n",
       "mean        0.002800     0.002163      0.003309         0.005727   \n",
       "std         0.052841     0.046465      0.057430         0.075462   \n",
       "min         0.000000     0.000000      0.000000         0.000000   \n",
       "25%         0.000000     0.000000      0.000000         0.000000   \n",
       "50%         0.000000     0.000000      0.000000         0.000000   \n",
       "75%         0.000000     0.000000      0.000000         0.000000   \n",
       "max         1.000000     1.000000      1.000000         1.000000   \n",
       "\n",
       "       australian dance         folk      pop emo    soft rock       motown  \\\n",
       "count       7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.001909     0.002927     0.002927     0.003818     0.002800   \n",
       "std            0.043652     0.054026     0.054026     0.061674     0.052841   \n",
       "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             pixie  canadian country    wrestling    glee club   complextro  \\\n",
       "count  7858.000000       7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.002545          0.002418     0.001909     0.025961     0.002036   \n",
       "std       0.050389          0.049116     0.043652     0.159028     0.045081   \n",
       "min       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        vapor trap     etherpop  pittsburgh rap  escape room  indietronica  \\\n",
       "count  7858.000000  7858.000000     7858.000000  7858.000000   7858.000000   \n",
       "mean      0.002036     0.001273        0.004836     0.002036      0.002800   \n",
       "std       0.045081     0.035653        0.069376     0.045081      0.052841   \n",
       "min       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000        1.000000     1.000000      1.000000   \n",
       "\n",
       "             comic  german techno  new jersey rap  trap latino  houston rap  \\\n",
       "count  7858.000000    7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.003054       0.001782        0.001527     0.003818     0.002291   \n",
       "std       0.055184       0.042174        0.039051     0.061674     0.047809   \n",
       "min       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       social media pop  puerto rican pop  deep southern trap  heartland rock  \\\n",
       "count       7858.000000       7858.000000         7858.000000     7858.000000   \n",
       "mean           0.006108          0.002418            0.001527        0.002036   \n",
       "std            0.077922          0.049116            0.039051        0.045081   \n",
       "min            0.000000          0.000000            0.000000        0.000000   \n",
       "25%            0.000000          0.000000            0.000000        0.000000   \n",
       "50%            0.000000          0.000000            0.000000        0.000000   \n",
       "75%            0.000000          0.000000            0.000000        0.000000   \n",
       "max            1.000000          1.000000            1.000000        1.000000   \n",
       "\n",
       "       alternative dance  bubblegum dance  alberta country  outlaw country  \\\n",
       "count        7858.000000      7858.000000      7858.000000     7858.000000   \n",
       "mean            0.002163         0.001273         0.001527        0.002036   \n",
       "std             0.046465         0.035653         0.039051        0.045081   \n",
       "min             0.000000         0.000000         0.000000        0.000000   \n",
       "25%             0.000000         0.000000         0.000000        0.000000   \n",
       "50%             0.000000         0.000000         0.000000        0.000000   \n",
       "75%             0.000000         0.000000         0.000000        0.000000   \n",
       "max             1.000000         1.000000         1.000000        1.000000   \n",
       "\n",
       "       country gospel  florida rap    hard rock  canadian metal  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean         0.001400     0.003691     0.002291        0.001273   \n",
       "std          0.037391     0.060641     0.047809        0.035653   \n",
       "min          0.000000     0.000000     0.000000        0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "       christian rock         soca  indiecoustica  harlem hip hop  \\\n",
       "count     7858.000000  7858.000000    7858.000000     7858.000000   \n",
       "mean         0.001909     0.001400       0.002418        0.000891   \n",
       "std          0.043652     0.037391       0.049116        0.029835   \n",
       "min          0.000000     0.000000       0.000000        0.000000   \n",
       "25%          0.000000     0.000000       0.000000        0.000000   \n",
       "50%          0.000000     0.000000       0.000000        0.000000   \n",
       "75%          0.000000     0.000000       0.000000        0.000000   \n",
       "max          1.000000     1.000000       1.000000        1.000000   \n",
       "\n",
       "          new rave  electronic trap  christian music       grunge  \\\n",
       "count  7858.000000      7858.000000      7858.000000  7858.000000   \n",
       "mean      0.002036         0.001145         0.002036     0.002163   \n",
       "std       0.045081         0.033825         0.045081     0.046465   \n",
       "min       0.000000         0.000000         0.000000     0.000000   \n",
       "25%       0.000000         0.000000         0.000000     0.000000   \n",
       "50%       0.000000         0.000000         0.000000     0.000000   \n",
       "75%       0.000000         0.000000         0.000000     0.000000   \n",
       "max       1.000000         1.000000         1.000000     1.000000   \n",
       "\n",
       "        show tunes   viral trap     la indie  swedish pop  swedish electropop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean      0.001909     0.001145     0.000891     0.001145            0.001145   \n",
       "std       0.043652     0.033825     0.029835     0.033825            0.033825   \n",
       "min       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "       reggaeton flow   dance-punk  celtic rock  socal pop punk       lounge  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean         0.001654     0.002291     0.000891        0.001782     0.002036   \n",
       "std          0.040643     0.047809     0.029835        0.042174     0.045081   \n",
       "min          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000     1.000000   \n",
       "\n",
       "       chicano rap    stomp pop          ccm   vocal jazz   glam metal  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.001400     0.000382     0.001654     0.001654     0.001400   \n",
       "std       0.037391     0.019537     0.040643     0.040643     0.037391   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "           worship   irish rock  electropowerpop      electro  indie pop rap  \\\n",
       "count  7858.000000  7858.000000      7858.000000  7858.000000    7858.000000   \n",
       "mean      0.001527     0.001654         0.001654     0.001018       0.002036   \n",
       "std       0.039051     0.040643         0.040643     0.031893       0.045081   \n",
       "min       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "25%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "50%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "75%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "max       1.000000     1.000000         1.000000     1.000000       1.000000   \n",
       "\n",
       "       canadian contemporary country       bounce  christian alternative rock  \\\n",
       "count                    7858.000000  7858.000000                 7858.000000   \n",
       "mean                        0.001018     0.001018                    0.001909   \n",
       "std                         0.031893     0.031893                    0.043652   \n",
       "min                         0.000000     0.000000                    0.000000   \n",
       "25%                         0.000000     0.000000                    0.000000   \n",
       "50%                         0.000000     0.000000                    0.000000   \n",
       "75%                         0.000000     0.000000                    0.000000   \n",
       "max                         1.000000     1.000000                    1.000000   \n",
       "\n",
       "       south african rock  deep talent show        disco        hyphy  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.001018          0.004836     0.001145     0.001145   \n",
       "std              0.031893          0.069376     0.033825     0.033825   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       disco house  canadian latin  australian hip hop      nyc rap  \\\n",
       "count  7858.000000     7858.000000         7858.000000  7858.000000   \n",
       "mean      0.000891        0.000891            0.001273     0.001273   \n",
       "std       0.029835        0.029835            0.035653     0.035653   \n",
       "min       0.000000        0.000000            0.000000     0.000000   \n",
       "25%       0.000000        0.000000            0.000000     0.000000   \n",
       "50%       0.000000        0.000000            0.000000     0.000000   \n",
       "75%       0.000000        0.000000            0.000000     0.000000   \n",
       "max       1.000000        1.000000            1.000000     1.000000   \n",
       "\n",
       "       brill building pop        k-pop       nz pop  minnesota hip hop  \\\n",
       "count         7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean             0.001018     0.003436     0.001018           0.000382   \n",
       "std              0.031893     0.058520     0.031893           0.019537   \n",
       "min              0.000000     0.000000     0.000000           0.000000   \n",
       "25%              0.000000     0.000000     0.000000           0.000000   \n",
       "50%              0.000000     0.000000     0.000000           0.000000   \n",
       "75%              0.000000     0.000000     0.000000           0.000000   \n",
       "max              1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "       modern blues rock   album rock  modern folk rock  uk americana  \\\n",
       "count        7858.000000  7858.000000       7858.000000   7858.000000   \n",
       "mean            0.001273     0.002418          0.001400      0.001400   \n",
       "std             0.035653     0.049116          0.037391      0.037391   \n",
       "min             0.000000     0.000000          0.000000      0.000000   \n",
       "25%             0.000000     0.000000          0.000000      0.000000   \n",
       "50%             0.000000     0.000000          0.000000      0.000000   \n",
       "75%             0.000000     0.000000          0.000000      0.000000   \n",
       "max             1.000000     1.000000          1.000000      1.000000   \n",
       "\n",
       "       old school hip hop   punk blues      dmv rap  industrial metal  \\\n",
       "count         7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean             0.001145     0.001273     0.002418          0.001273   \n",
       "std              0.033825     0.035653     0.049116          0.035653   \n",
       "min              0.000000     0.000000     0.000000          0.000000   \n",
       "25%              0.000000     0.000000     0.000000          0.000000   \n",
       "50%              0.000000     0.000000     0.000000          0.000000   \n",
       "75%              0.000000     0.000000     0.000000          0.000000   \n",
       "max              1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "        skate punk  swedish synthpop   moombahton  Max_Peak_Position  \\\n",
       "count  7858.000000       7858.000000  7858.000000        7858.000000   \n",
       "mean      0.001145          0.000509     0.001018          75.775261   \n",
       "std       0.033825          0.022558     0.031893          24.561479   \n",
       "min       0.000000          0.000000     0.000000           1.000000   \n",
       "25%       0.000000          0.000000     0.000000          66.000000   \n",
       "50%       0.000000          0.000000     0.000000          84.000000   \n",
       "75%       0.000000          0.000000     0.000000          95.000000   \n",
       "max       1.000000          1.000000     1.000000         100.000000   \n",
       "\n",
       "       Max_Rank_Change  In_Top5genres_Mean  \n",
       "count      7858.000000         7858.000000  \n",
       "mean         12.531815            0.383706  \n",
       "std          11.760207            0.466419  \n",
       "min          -8.000000            0.000000  \n",
       "25%           3.000000            0.000000  \n",
       "50%          11.000000            0.000000  \n",
       "75%          17.000000            1.000000  \n",
       "max          79.000000            1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_withgenre = df_clean_withgenre.drop(['SongID'], axis=1)\n",
    "df_clean_withgenre.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003   \n",
       "std       0.140660     0.223494    29.558487        0.273062   \n",
       "min       0.020000     0.034900    48.718000        0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000   \n",
       "\n",
       "       Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "count        7858.000000      7858.000000         7858.000000  \n",
       "mean           75.775261        12.531815            0.383706  \n",
       "std            24.561479        11.760207            0.466419  \n",
       "min             1.000000        -8.000000            0.000000  \n",
       "25%            66.000000         3.000000            0.000000  \n",
       "50%            84.000000        11.000000            0.000000  \n",
       "75%            95.000000        17.000000            1.000000  \n",
       "max           100.000000        79.000000            1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_nogenre.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre__1 variables)\n",
    "2. Max Peak Position, with genre (withgenre_1 variables)\n",
    "3. Max Rank Change, no genre (nogenre_2 variables)\n",
    "4. Max Rank Change, with genre (withgenre_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X_nogenre_1 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_1 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_1_train, X_nogenre_1_test, y_nogenre_1_train, y_nogenre_1_test = train_test_split(X_nogenre_1, y_nogenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_1_train_scaled = scaler.fit_transform(X_nogenre_1_train)\n",
    "X_nogenre_1_test_scaled = scaler.fit_transform(X_nogenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X_withgenre_1 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_1 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_1_train, X_withgenre_1_test, y_withgenre_1_train, y_withgenre_1_test = train_test_split(X_withgenre_1, y_withgenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_1_train_scaled = scaler.fit_transform(X_withgenre_1_train)\n",
    "X_withgenre_1_test_scaled = scaler.fit_transform(X_withgenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X_nogenre_2 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_2 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_2_train, X_nogenre_2_test, y_nogenre_2_train, y_nogenre_2_test = train_test_split(X_nogenre_2, y_nogenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_2_train_scaled = scaler.fit_transform(X_nogenre_2_train)\n",
    "X_nogenre_2_test_scaled = scaler.fit_transform(X_nogenre_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X_withgenre_2 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_2 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_2_train, X_withgenre_2_test, y_withgenre_2_train, y_withgenre_2_test = train_test_split(X_withgenre_2, y_withgenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_2_train_scaled = scaler.fit_transform(X_withgenre_2_train)\n",
    "X_withgenre_2_test_scaled = scaler.fit_transform(X_withgenre_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data for the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre_3 variables)\n",
    "2. Max Peak Position, with genre (withgenre_3 variables)\n",
    "3. Max Rank Change, no genre (nogenre_4 variables)\n",
    "4. Max Rank Change, with genre (withgenre_4 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X_nogenre_3 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_3 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_3_train, X_nogenre_3_test, y_nogenre_3_train, y_nogenre_3_test = train_test_split(X_nogenre_3, y_nogenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_3_train_final, X_nogenre_3_val, y_nogenre_3_train_final, y_nogenre_3_val = train_test_split(X_nogenre_3_train, y_nogenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X_nogenre_3_train_final)\n",
    "X_nogenre_3_train_scaled = scaler.transform(X_nogenre_3_train_final)\n",
    "X_nogenre_3_val_scaled = scaler.transform(X_nogenre_3_val)\n",
    "X_nogenre_3_test_scaled = scaler.transform(X_nogenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X_withgenre_3 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_3 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_3_train, X_withgenre_3_test, y_withgenre_3_train, y_withgenre_3_test = train_test_split(X_withgenre_3, y_withgenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_3_train_final, X_withgenre_3_val, y_withgenre_3_train_final, y_withgenre_3_val = train_test_split(X_withgenre_3_train, y_withgenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_3_train_final)\n",
    "X_withgenre_3_train_scaled = scaler.transform(X_withgenre_3_train_final)\n",
    "X_withgenre_3_val_scaled = scaler.transform(X_withgenre_3_val)\n",
    "X_withgenre_3_test_scaled = scaler.transform(X_withgenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X_nogenre_4 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_4 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_4_train, X_nogenre_4_test, y_nogenre_4_train, y_nogenre_4_test = train_test_split(X_nogenre_4, y_nogenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_4_train_final, X_nogenre_4_val, y_nogenre_4_train_final, y_nogenre_4_val = train_test_split(X_nogenre_4_train, y_nogenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_nogenre_4_train_final)\n",
    "X_nogenre_4_train_scaled = scaler.transform(X_nogenre_4_train_final)\n",
    "X_nogenre_4_val_scaled = scaler.transform(X_nogenre_4_val)\n",
    "X_nogenre_4_test_scaled = scaler.transform(X_nogenre_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X_withgenre_4 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_4 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_4_train, X_withgenre_4_test, y_withgenre_4_train, y_withgenre_4_test = train_test_split(X_withgenre_4, y_withgenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_4_train_final, X_withgenre_4_val, y_withgenre_4_train_final, y_withgenre_4_val = train_test_split(X_withgenre_4_train, y_withgenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_4_train_final)\n",
    "X_withgenre_4_train_scaled = scaler.transform(X_withgenre_4_train_final)\n",
    "X_withgenre_4_val_scaled = scaler.transform(X_withgenre_4_val)\n",
    "X_withgenre_4_test_scaled = scaler.transform(X_withgenre_4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 25.189\n",
      "R²: 0.009\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_maxpeak_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "y_nogenre_1_pred = xgb_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "y_nogenre_1_pred = np.clip(np.round(y_nogenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_nogenre_1 = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred))\n",
    "r2_nogenre_1 = r2_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1:.3f}')\n",
    "print(f'R²: {r2_nogenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_1 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_1.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.14928596 0.1497933  0.14933232 0.1519076  0.1538763  0.15317112\n",
      " 0.14956039 0.14933785 0.15139946 0.15320263 0.15275146 0.1530459\n",
      " 0.14947191 0.14825419 0.14977659 0.13353322 0.13985333 0.13783634\n",
      " 0.15055884 0.14790804 0.15182688 0.13723545 0.1396863  0.13820686\n",
      " 0.12488542 0.12580568 0.13303351 0.14951099 0.14986331 0.15052954\n",
      " 0.15149311 0.15313528 0.15195441 0.15239897 0.15245192 0.15317686\n",
      " 0.15170226 0.15120819 0.15521759 0.1480341  0.14980488 0.15103229\n",
      " 0.1380379  0.14412221 0.14086841 0.14997244 0.15279588 0.15103062\n",
      " 0.13215508 0.13735918 0.14146951 0.11673868 0.12705541 0.12914339\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.9, 1.0, 1.1]\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_2 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_2.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [2],\n",
    "    'learning_rate': [0.07, 0.1, 0.13],\n",
    "    'subsample': [0.85],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_3 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_3.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.851\n",
      "R²: 0.185\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb_nogenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_nogenre_1_pred_best = best_xgb1_1.predict(X_nogenre_1_test)\n",
    "y_nogenre_1_pred_best = np.clip(np.round(y_nogenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_nogenre_1_best = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred_best))\n",
    "r2_nogenre_1_best = r2_score(y_nogenre_1_test, y_nogenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1_best:.3f}')\n",
    "print(f'R²: {r2_nogenre_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.751\n",
      "R²: 0.192\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_maxpeak_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_withgenre.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "y_withgenre_1_pred = xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred = np.clip(np.round(y_withgenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred))\n",
    "r2_withgenre_1 = r2_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_1 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_1.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.22290783 0.22476627        nan 0.22961248 0.23208994        nan\n",
      " 0.22492174 0.22395395        nan 0.22358264 0.22480186        nan\n",
      " 0.22946347 0.22976409        nan 0.22246672 0.22280164        nan\n",
      " 0.22506559 0.22599194        nan 0.22510593 0.22972054        nan\n",
      " 0.21419413 0.21712435        nan 0.22480632 0.22179151        nan\n",
      " 0.22939501 0.23055565        nan 0.22415408 0.22779516        nan\n",
      " 0.22766875 0.22672504        nan 0.2268309  0.22682267        nan\n",
      " 0.22377956 0.22375016        nan 0.22684716 0.22516409        nan\n",
      " 0.2178275  0.21981466        nan 0.20304011 0.21110715        nan\n",
      " 0.22454622 0.22334156        nan 0.22969245 0.2287244         nan\n",
      " 0.2288182  0.22858987        nan 0.22661289 0.22694678        nan\n",
      " 0.22954861 0.22734727        nan 0.21356106 0.22451347        nan\n",
      " 0.22868726 0.22631047        nan 0.21676042 0.22319322        nan\n",
      " 0.21143588 0.20957587        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.15, 0.2, 0.25],\n",
    "    'subsample': [0.9, 1.0, 1.1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_2 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_2.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.12, 0.15, 0.17],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.5, 0.6],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_3 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_3.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21.607\n",
      "R²: 0.271\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model \n",
    "best_xgb_maxpeak_withgenre = grid_search_xgb_withgenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_withgenre_1_pred_best = best_xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred_best = np.clip(np.round(y_withgenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred_best))\n",
    "r2_withgenre_2_best = r2_score(y_withgenre_1_test, y_withgenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.605\n",
      "R²: -0.097\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_nogenre.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "y_nogenre_2_pred = xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred = np.clip(np.round(y_nogenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxrank_nogenre_2 = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred))\n",
    "r2_maxrank_nogenre_2 = r2_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_1 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_1.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.00972437        nan 0.01147871        nan 0.0099106         nan\n",
      " 0.01288679        nan 0.01492339        nan 0.0120229         nan\n",
      " 0.01355836        nan 0.01520104        nan 0.011404          nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_2 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_2.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.918\n",
      "R²: 0.020\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_nogenre = grid_search_xgb_maxrank_nogenre_2.best_estimator_\n",
    "\n",
    "y_nogenre_2_pred_best = best_xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred_best = np.clip(np.round(y_nogenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxrank_nogenre_2_best = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred_best))\n",
    "r2_maxrank_nogenre_2_best = r2_score(y_nogenre_2_test, y_nogenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.342\n",
      "R²: -0.051\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_withgenre.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "y_withgenre_2_pred = xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred = np.clip(np.round(y_withgenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxpeak_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred))\n",
    "r2_maxpeak_withgenre_1 = r2_score(y_withgenre_2_test, y_withgenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_1 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_1.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.0224043  0.0229799  0.02310325 0.023507   0.02370381 0.02407839\n",
      " 0.03057899 0.03026649 0.0313546  0.03141223 0.0324422  0.03149776\n",
      " 0.03260914 0.03307942 0.03341982 0.0323358  0.03272635 0.03061318\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_2 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_2.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.013, 0.015, 0.017],\n",
    "    'subsample': [0.5, 0.6, 0.7],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_3 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_3.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.764\n",
      "R²: 0.045\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_withgenre = grid_search_xgb_maxrank_withgenre_3.best_estimator_\n",
    "\n",
    "y_withgenre_2_pred_best = best_xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred_best = np.clip(np.round(y_withgenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxpeak_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred_best))\n",
    "r2_maxpeak_withgenre_2_best = r2_score(y_withgenre_2_test, y_withgenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, the Max Rank Change models both had an r<sup>2</sup> value less than 0.001, essentially indicating no fit of the model to the test data. Max Peak Position performed better, but the highest r<sup>2</sup> value was 0.271 so their predictive value is low.\n",
    "\n",
    "Given the lack of predictive power in these outcomes, I'm shifting focus to the other two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 0.0439\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_params_\n",
    "standard_best_score_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0326\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_nogenre_1_pred = final_model_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_nogenre_1 = accuracy_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_nogenre_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.0409\n"
     ]
    }
   ],
   "source": [
    "# parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_withgenre.fit(X_withgenre_1_train_scaled, y_withgenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_params_\n",
    "standard_best_score_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0351\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_withgenre_1_pred = final_model_maxpeak_withgenre.predict(X_withgenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_withgenre = accuracy_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2350\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_nogenre.fit(X_nogenre_2_train_scaled, y_nogenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_nogenre = grid_search_maxrank_nogenre.best_params_\n",
    "standard_best_score_maxrank_nogenre = grid_search_maxrank_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2163\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_nogenre = grid_search_maxrank_nogenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_nogenre_2_pred = final_model_maxrank_nogenre.predict(X_nogenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_nogenre = accuracy_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_nogenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2379\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_withgenre.fit(X_withgenre_2_train_scaled, y_withgenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_withgenre = grid_search_maxrank_withgenre.best_params_\n",
    "standard_best_score_maxrank_withgenre = grid_search_maxrank_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2173\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_withgenre = grid_search_maxrank_withgenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_maxrank_withgenre_2_pred = final_model_maxrank_withgenre.predict(X_withgenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_withgenre = accuracy_score(y_withgenre_2_test, y_maxrank_withgenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed poorly on the Max Peak Position data, with a maximum accuracy of 0.035. The performance on the Max Rank Change was better, but the maximum accuracy was still just 0.217.\n",
    "\n",
    "I will not explore k-NN further and instead focus on the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2549.0750 - mae: 41.0085 - mse: 2549.0750 - val_loss: 681.8605 - val_mae: 21.4929 - val_mse: 681.8605\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 673.9514 - mae: 20.7414 - mse: 673.9514 - val_loss: 610.9291 - val_mae: 19.6272 - val_mse: 610.9291\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 622.3873 - mae: 19.7192 - mse: 622.3873 - val_loss: 579.3101 - val_mae: 19.5156 - val_mse: 579.3101\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 594.4716 - mae: 19.2081 - mse: 594.4716 - val_loss: 558.0447 - val_mae: 18.5191 - val_mse: 558.0447\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 572.6750 - mae: 18.7519 - mse: 572.6750 - val_loss: 544.1376 - val_mae: 18.5251 - val_mse: 544.1376\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 559.7896 - mae: 18.5117 - mse: 559.7896 - val_loss: 532.8557 - val_mae: 18.2148 - val_mse: 532.8557\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.3685 - mae: 18.2477 - mse: 549.3685 - val_loss: 524.1381 - val_mae: 18.0492 - val_mse: 524.1381\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 542.0280 - mae: 18.1367 - mse: 542.0280 - val_loss: 520.2755 - val_mae: 18.0432 - val_mse: 520.2755\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 536.7059 - mae: 17.9864 - mse: 536.7059 - val_loss: 514.2313 - val_mae: 17.9484 - val_mse: 514.2313\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 531.8787 - mae: 17.9223 - mse: 531.8787 - val_loss: 518.4539 - val_mae: 18.4349 - val_mse: 518.4539\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 526.3088 - mae: 17.8065 - mse: 526.3088 - val_loss: 518.2149 - val_mae: 18.5887 - val_mse: 518.2149\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 521.7917 - mae: 17.6641 - mse: 521.7917 - val_loss: 516.1942 - val_mae: 18.5408 - val_mse: 516.1942\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 516.4424 - mae: 17.6251 - mse: 516.4424 - val_loss: 506.4629 - val_mae: 17.7274 - val_mse: 506.4629\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 513.8203 - mae: 17.4537 - mse: 513.8203 - val_loss: 510.5822 - val_mae: 18.3104 - val_mse: 510.5822\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 509.2174 - mae: 17.3926 - mse: 509.2174 - val_loss: 509.4762 - val_mae: 18.3673 - val_mse: 509.4762\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 504.3606 - mae: 17.2996 - mse: 504.3606 - val_loss: 495.3358 - val_mae: 17.4661 - val_mse: 495.3358\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 501.2714 - mae: 17.2626 - mse: 501.2714 - val_loss: 494.4391 - val_mae: 17.5187 - val_mse: 494.4391\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 495.6008 - mae: 17.1466 - mse: 495.6008 - val_loss: 496.2470 - val_mae: 17.8326 - val_mse: 496.2470\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 493.8424 - mae: 17.0567 - mse: 493.8424 - val_loss: 492.4696 - val_mae: 17.6873 - val_mse: 492.4696\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 495.1079 - mae: 17.0856 - mse: 495.1079 - val_loss: 503.5040 - val_mae: 18.2004 - val_mse: 503.5040\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 489.1183 - mae: 16.9726 - mse: 489.1183 - val_loss: 514.9006 - val_mae: 16.9328 - val_mse: 514.9006\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 486.4953 - mae: 16.9034 - mse: 486.4953 - val_loss: 494.4450 - val_mae: 17.4978 - val_mse: 494.4450\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 482.2360 - mae: 16.8327 - mse: 482.2360 - val_loss: 509.7364 - val_mae: 17.1495 - val_mse: 509.7364\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 480.7697 - mae: 16.8147 - mse: 480.7697 - val_loss: 491.3487 - val_mae: 17.4188 - val_mse: 491.3487\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 481.0500 - mae: 16.8685 - mse: 481.0500 - val_loss: 502.3790 - val_mae: 18.0606 - val_mse: 502.3790\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 480.5851 - mae: 16.8054 - mse: 480.5851 - val_loss: 497.4374 - val_mae: 17.6258 - val_mse: 497.4374\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 475.5234 - mae: 16.7029 - mse: 475.5234 - val_loss: 495.7361 - val_mae: 17.7097 - val_mse: 495.7361\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 470.6005 - mae: 16.6718 - mse: 470.6005 - val_loss: 491.7675 - val_mae: 17.2827 - val_mse: 491.7675\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 472.2727 - mae: 16.6201 - mse: 472.2727 - val_loss: 500.9466 - val_mae: 17.1128 - val_mse: 500.9466\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 466.8601 - mae: 16.5299 - mse: 466.8601 - val_loss: 493.8946 - val_mae: 17.7804 - val_mse: 493.8946\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.3942 - mae: 16.4902 - mse: 465.3942 - val_loss: 504.8943 - val_mae: 17.5683 - val_mse: 504.8943\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 462.6771 - mae: 16.5106 - mse: 462.6771 - val_loss: 496.0166 - val_mae: 17.5385 - val_mse: 496.0166\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 463.2121 - mae: 16.4786 - mse: 463.2121 - val_loss: 498.6469 - val_mae: 17.5919 - val_mse: 498.6469\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.3859 - mae: 16.5524 - mse: 465.3859 - val_loss: 499.4373 - val_mae: 17.5562 - val_mse: 499.4373\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 455.5559 - mae: 16.3256 - mse: 455.5559 - val_loss: 506.7105 - val_mae: 17.4923 - val_mse: 506.7105\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 455.1943 - mae: 16.2893 - mse: 455.1943 - val_loss: 503.4457 - val_mae: 17.4952 - val_mse: 503.4457\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 454.1593 - mae: 16.2926 - mse: 454.1593 - val_loss: 506.7657 - val_mae: 17.2924 - val_mse: 506.7657\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 455.3600 - mae: 16.3374 - mse: 455.3600 - val_loss: 507.8246 - val_mae: 17.4728 - val_mse: 507.8246\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 450.2985 - mae: 16.2129 - mse: 450.2985 - val_loss: 507.5980 - val_mae: 17.5046 - val_mse: 507.5980\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 449.1112 - mae: 16.2219 - mse: 449.1112 - val_loss: 519.3218 - val_mae: 18.2537 - val_mse: 519.3218\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 447.5521 - mae: 16.1720 - mse: 447.5521 - val_loss: 511.3575 - val_mae: 17.4403 - val_mse: 511.3575\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 446.0062 - mae: 16.0941 - mse: 446.0062 - val_loss: 515.2209 - val_mae: 18.1452 - val_mse: 515.2209\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 441.9410 - mae: 16.1326 - mse: 441.9410 - val_loss: 510.7792 - val_mae: 17.7653 - val_mse: 510.7792\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 442.9892 - mae: 16.1013 - mse: 442.9892 - val_loss: 511.4094 - val_mae: 17.5160 - val_mse: 511.4094\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.3680 - mae: 15.9498 - mse: 438.3680 - val_loss: 527.1721 - val_mae: 18.5606 - val_mse: 527.1721\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.8554 - mae: 15.9707 - mse: 438.8554 - val_loss: 533.8622 - val_mae: 17.2993 - val_mse: 533.8622\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 437.8275 - mae: 15.9711 - mse: 437.8275 - val_loss: 521.9758 - val_mae: 17.5573 - val_mse: 521.9758\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 440.5385 - mae: 16.0202 - mse: 440.5385 - val_loss: 519.6147 - val_mae: 17.6974 - val_mse: 519.6147\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 436.2274 - mae: 15.9237 - mse: 436.2274 - val_loss: 532.5669 - val_mae: 18.4829 - val_mse: 532.5669\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 432.9760 - mae: 15.8730 - mse: 432.9760 - val_loss: 528.3448 - val_mae: 17.5636 - val_mse: 528.3448\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 429.9355 - mae: 15.7862 - mse: 429.9355 - val_loss: 520.5129 - val_mae: 18.1475 - val_mse: 520.5129\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 429.5984 - mae: 15.8318 - mse: 429.5984 - val_loss: 533.6429 - val_mae: 18.5007 - val_mse: 533.6429\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 428.5836 - mae: 15.7777 - mse: 428.5836 - val_loss: 529.5001 - val_mae: 17.7926 - val_mse: 529.5001\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 426.8431 - mae: 15.7715 - mse: 426.8431 - val_loss: 545.7497 - val_mae: 17.4533 - val_mse: 545.7497\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 422.7584 - mae: 15.6697 - mse: 422.7584 - val_loss: 531.4555 - val_mae: 18.4392 - val_mse: 531.4555\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 424.0595 - mae: 15.6901 - mse: 424.0595 - val_loss: 550.9021 - val_mae: 19.1439 - val_mse: 550.9021\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 429.7937 - mae: 15.8080 - mse: 429.7937 - val_loss: 539.9207 - val_mae: 17.5766 - val_mse: 539.9207\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 420.3087 - mae: 15.5960 - mse: 420.3087 - val_loss: 532.1300 - val_mae: 18.3324 - val_mse: 532.1300\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 419.1496 - mae: 15.6186 - mse: 419.1496 - val_loss: 537.1426 - val_mae: 18.5022 - val_mse: 537.1426\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 418.0583 - mae: 15.6269 - mse: 418.0583 - val_loss: 534.9975 - val_mae: 17.9396 - val_mse: 534.9975\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 417.7331 - mae: 15.5683 - mse: 417.7331 - val_loss: 536.4506 - val_mae: 17.4412 - val_mse: 536.4506\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 417.8084 - mae: 15.5245 - mse: 417.8084 - val_loss: 536.8383 - val_mae: 17.9583 - val_mse: 536.8383\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 415.6873 - mae: 15.5019 - mse: 415.6873 - val_loss: 543.6124 - val_mae: 18.2757 - val_mse: 543.6124\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 418.4914 - mae: 15.6454 - mse: 418.4914 - val_loss: 535.0582 - val_mae: 17.9633 - val_mse: 535.0582\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 410.3818 - mae: 15.3895 - mse: 410.3818 - val_loss: 537.8902 - val_mae: 17.7558 - val_mse: 537.8902\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 413.4824 - mae: 15.5081 - mse: 413.4824 - val_loss: 537.4714 - val_mae: 17.9830 - val_mse: 537.4714\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 408.2130 - mae: 15.3450 - mse: 408.2130 - val_loss: 547.2120 - val_mae: 17.6242 - val_mse: 547.2120\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 413.8862 - mae: 15.4787 - mse: 413.8862 - val_loss: 534.3747 - val_mae: 17.8074 - val_mse: 534.3747\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 408.5975 - mae: 15.4055 - mse: 408.5975 - val_loss: 535.3683 - val_mae: 18.1083 - val_mse: 535.3683\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 403.6237 - mae: 15.2815 - mse: 403.6237 - val_loss: 553.8817 - val_mae: 17.7128 - val_mse: 553.8817\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 406.3804 - mae: 15.3107 - mse: 406.3804 - val_loss: 539.4963 - val_mae: 18.2008 - val_mse: 539.4963\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.1539 - mae: 15.2536 - mse: 404.1539 - val_loss: 544.6473 - val_mae: 18.5627 - val_mse: 544.6473\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.7103 - mae: 15.3133 - mse: 404.7103 - val_loss: 536.5920 - val_mae: 17.7851 - val_mse: 536.5920\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 401.8972 - mae: 15.2333 - mse: 401.8972 - val_loss: 544.3119 - val_mae: 17.5540 - val_mse: 544.3119\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 402.5469 - mae: 15.2146 - mse: 402.5469 - val_loss: 545.3627 - val_mae: 18.5923 - val_mse: 545.3627\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 401.5199 - mae: 15.2562 - mse: 401.5199 - val_loss: 540.8864 - val_mae: 17.9527 - val_mse: 540.8864\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.3398 - mae: 15.2059 - mse: 399.3398 - val_loss: 539.9683 - val_mae: 17.8942 - val_mse: 539.9683\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.4442 - mae: 15.1829 - mse: 399.4442 - val_loss: 546.2709 - val_mae: 18.4876 - val_mse: 546.2709\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 396.7794 - mae: 15.1454 - mse: 396.7794 - val_loss: 546.1042 - val_mae: 17.6583 - val_mse: 546.1042\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.6897 - mae: 15.1226 - mse: 395.6897 - val_loss: 550.5509 - val_mae: 18.8062 - val_mse: 550.5509\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 394.7950 - mae: 15.0754 - mse: 394.7950 - val_loss: 547.8023 - val_mae: 17.8473 - val_mse: 547.8023\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 389.0561 - mae: 14.9609 - mse: 389.0561 - val_loss: 549.7271 - val_mae: 18.2232 - val_mse: 549.7271\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.2055 - mae: 15.1064 - mse: 395.2055 - val_loss: 551.6607 - val_mae: 18.5804 - val_mse: 551.6607\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 390.7107 - mae: 15.0091 - mse: 390.7107 - val_loss: 561.3011 - val_mae: 18.7556 - val_mse: 561.3011\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 390.1767 - mae: 14.9912 - mse: 390.1767 - val_loss: 556.2134 - val_mae: 17.8476 - val_mse: 556.2134\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.8936 - mae: 15.0683 - mse: 392.8936 - val_loss: 549.7885 - val_mae: 18.0609 - val_mse: 549.7885\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 387.0791 - mae: 14.9649 - mse: 387.0791 - val_loss: 555.2167 - val_mae: 18.0514 - val_mse: 555.2167\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 385.3638 - mae: 14.9097 - mse: 385.3638 - val_loss: 550.8810 - val_mae: 17.7572 - val_mse: 550.8810\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 386.1971 - mae: 14.9128 - mse: 386.1971 - val_loss: 554.2172 - val_mae: 17.9531 - val_mse: 554.2172\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 385.8140 - mae: 14.9211 - mse: 385.8140 - val_loss: 564.4811 - val_mae: 18.5372 - val_mse: 564.4811\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 388.5844 - mae: 14.9259 - mse: 388.5844 - val_loss: 556.2178 - val_mae: 18.2214 - val_mse: 556.2178\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 381.2921 - mae: 14.7669 - mse: 381.2921 - val_loss: 569.2985 - val_mae: 18.9717 - val_mse: 569.2985\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 381.9758 - mae: 14.8298 - mse: 381.9758 - val_loss: 580.5042 - val_mae: 18.1858 - val_mse: 580.5042\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 381.8006 - mae: 14.8299 - mse: 381.8006 - val_loss: 594.4549 - val_mae: 17.8656 - val_mse: 594.4549\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 382.8683 - mae: 14.8499 - mse: 382.8683 - val_loss: 573.5894 - val_mae: 18.4865 - val_mse: 573.5894\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 379.3577 - mae: 14.8252 - mse: 379.3577 - val_loss: 598.6591 - val_mae: 18.1306 - val_mse: 598.6591\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 380.4925 - mae: 14.7939 - mse: 380.4925 - val_loss: 562.8003 - val_mae: 18.6473 - val_mse: 562.8003\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 378.3046 - mae: 14.8418 - mse: 378.3046 - val_loss: 581.5457 - val_mae: 18.1977 - val_mse: 581.5457\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 377.9459 - mae: 14.8233 - mse: 377.9459 - val_loss: 559.7964 - val_mae: 18.2243 - val_mse: 559.7964\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 372.1995 - mae: 14.6470 - mse: 372.1995 - val_loss: 573.3127 - val_mae: 18.6078 - val_mse: 573.3127\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5931.9980 - mae: 73.1585 - mse: 5931.9980 - val_loss: 5702.1641 - val_mae: 71.8102 - val_mse: 5702.1641\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4786.9932 - mae: 65.2343 - mse: 4786.9932 - val_loss: 3915.7493 - val_mae: 58.6695 - val_mse: 3915.7493\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3230.7505 - mae: 52.7308 - mse: 3230.7505 - val_loss: 2432.2144 - val_mae: 45.4152 - val_mse: 2432.2144\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1762.9460 - mae: 38.1016 - mse: 1762.9460 - val_loss: 1257.4479 - val_mae: 31.9857 - val_mse: 1257.4479\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 895.5186 - mae: 26.1274 - mse: 895.5186 - val_loss: 715.8124 - val_mae: 23.3197 - val_mse: 715.8124\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 594.2126 - mae: 20.2108 - mse: 594.2126 - val_loss: 522.2668 - val_mae: 18.9217 - val_mse: 522.2668\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 516.7600 - mae: 17.9717 - mse: 516.7600 - val_loss: 482.6864 - val_mae: 17.3351 - val_mse: 482.6864\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 509.8145 - mae: 17.5567 - mse: 509.8145 - val_loss: 481.8167 - val_mae: 17.2302 - val_mse: 481.8167\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 490.5380 - mae: 17.0077 - mse: 490.5380 - val_loss: 488.4101 - val_mae: 17.4061 - val_mse: 488.4101\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 495.5963 - mae: 17.0562 - mse: 495.5963 - val_loss: 494.5155 - val_mae: 17.1327 - val_mse: 494.5155\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 494.5826 - mae: 17.1209 - mse: 494.5826 - val_loss: 479.6342 - val_mae: 17.1293 - val_mse: 479.6342\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 487.4727 - mae: 16.9236 - mse: 487.4727 - val_loss: 486.2724 - val_mae: 17.2897 - val_mse: 486.2724\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 484.9589 - mae: 16.8217 - mse: 484.9589 - val_loss: 485.6905 - val_mae: 17.2524 - val_mse: 485.6905\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 479.9127 - mae: 16.7432 - mse: 479.9127 - val_loss: 491.9400 - val_mae: 17.0672 - val_mse: 491.9400\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 477.7688 - mae: 16.7120 - mse: 477.7688 - val_loss: 493.0016 - val_mae: 17.4025 - val_mse: 493.0016\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 476.0127 - mae: 16.5726 - mse: 476.0127 - val_loss: 485.2846 - val_mae: 17.2155 - val_mse: 485.2846\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 473.3752 - mae: 16.5548 - mse: 473.3752 - val_loss: 491.1880 - val_mae: 17.3664 - val_mse: 491.1880\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 468.9427 - mae: 16.5314 - mse: 468.9427 - val_loss: 483.8092 - val_mae: 17.2084 - val_mse: 483.8092\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 464.8211 - mae: 16.5159 - mse: 464.8211 - val_loss: 493.4429 - val_mae: 17.1602 - val_mse: 493.4429\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 469.8130 - mae: 16.3725 - mse: 469.8130 - val_loss: 492.1271 - val_mae: 17.2153 - val_mse: 492.1271\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 464.7998 - mae: 16.5645 - mse: 464.7998 - val_loss: 492.8002 - val_mae: 17.0331 - val_mse: 492.8002\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 461.4866 - mae: 16.3591 - mse: 461.4866 - val_loss: 492.7585 - val_mae: 17.3907 - val_mse: 492.7585\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.8232 - mae: 16.1669 - mse: 451.8232 - val_loss: 499.7457 - val_mae: 17.2378 - val_mse: 499.7457\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 455.5193 - mae: 16.2785 - mse: 455.5193 - val_loss: 495.8898 - val_mae: 17.4851 - val_mse: 495.8898\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 454.1225 - mae: 16.3429 - mse: 454.1225 - val_loss: 501.8672 - val_mae: 17.6001 - val_mse: 501.8672\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 446.0796 - mae: 16.0426 - mse: 446.0796 - val_loss: 512.3834 - val_mae: 17.7191 - val_mse: 512.3834\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 446.2498 - mae: 16.1336 - mse: 446.2498 - val_loss: 510.9290 - val_mae: 17.3359 - val_mse: 510.9290\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 441.5580 - mae: 16.0831 - mse: 441.5580 - val_loss: 507.9067 - val_mae: 17.5126 - val_mse: 507.9067\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 439.9460 - mae: 15.8849 - mse: 439.9460 - val_loss: 499.2601 - val_mae: 17.4045 - val_mse: 499.2601\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 442.0616 - mae: 16.0581 - mse: 442.0616 - val_loss: 508.0962 - val_mae: 17.4345 - val_mse: 508.0962\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 430.0239 - mae: 15.7123 - mse: 430.0239 - val_loss: 512.8926 - val_mae: 17.3395 - val_mse: 512.8926\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 437.0900 - mae: 15.9507 - mse: 437.0900 - val_loss: 515.6238 - val_mae: 17.8626 - val_mse: 515.6238\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 433.3152 - mae: 15.9605 - mse: 433.3152 - val_loss: 519.9049 - val_mae: 17.5046 - val_mse: 519.9049\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.1882 - mae: 15.7661 - mse: 426.1882 - val_loss: 517.6273 - val_mae: 17.5885 - val_mse: 517.6273\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.0895 - mae: 15.6989 - mse: 426.0895 - val_loss: 516.0448 - val_mae: 17.5528 - val_mse: 516.0448\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 428.2613 - mae: 15.7902 - mse: 428.2613 - val_loss: 525.8519 - val_mae: 17.8303 - val_mse: 525.8519\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 416.1374 - mae: 15.5786 - mse: 416.1374 - val_loss: 516.8428 - val_mae: 17.8402 - val_mse: 516.8428\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 421.1989 - mae: 15.6605 - mse: 421.1989 - val_loss: 520.7802 - val_mae: 18.0005 - val_mse: 520.7802\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.0493 - mae: 15.6158 - mse: 419.0493 - val_loss: 525.5323 - val_mae: 17.2928 - val_mse: 525.5323\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 416.3421 - mae: 15.5242 - mse: 416.3421 - val_loss: 521.0945 - val_mae: 17.7754 - val_mse: 521.0945\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 418.4395 - mae: 15.6311 - mse: 418.4395 - val_loss: 517.9205 - val_mae: 17.6145 - val_mse: 517.9205\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.7640 - mae: 15.3948 - mse: 409.7640 - val_loss: 524.4852 - val_mae: 17.7564 - val_mse: 524.4852\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 417.6532 - mae: 15.6195 - mse: 417.6532 - val_loss: 534.3660 - val_mae: 17.3852 - val_mse: 534.3660\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 410.5145 - mae: 15.3736 - mse: 410.5145 - val_loss: 525.4379 - val_mae: 17.7113 - val_mse: 525.4379\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 406.7996 - mae: 15.4191 - mse: 406.7996 - val_loss: 528.3398 - val_mae: 17.9254 - val_mse: 528.3398\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 401.5766 - mae: 15.2710 - mse: 401.5766 - val_loss: 518.8046 - val_mae: 17.5355 - val_mse: 518.8046\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.3571 - mae: 15.1501 - mse: 397.3571 - val_loss: 528.3182 - val_mae: 17.6740 - val_mse: 528.3182\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.3591 - mae: 15.3595 - mse: 409.3591 - val_loss: 527.6775 - val_mae: 17.7248 - val_mse: 527.6775\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 396.5363 - mae: 15.1449 - mse: 396.5363 - val_loss: 529.7886 - val_mae: 17.7621 - val_mse: 529.7886\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 399.6139 - mae: 15.2140 - mse: 399.6139 - val_loss: 532.4957 - val_mae: 18.0636 - val_mse: 532.4957\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 392.2844 - mae: 15.0520 - mse: 392.2844 - val_loss: 530.7830 - val_mae: 17.9730 - val_mse: 530.7830\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 403.9636 - mae: 15.2783 - mse: 403.9636 - val_loss: 543.7781 - val_mae: 17.7166 - val_mse: 543.7781\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 391.7389 - mae: 15.0533 - mse: 391.7389 - val_loss: 543.3358 - val_mae: 18.1435 - val_mse: 543.3358\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 390.3477 - mae: 15.0470 - mse: 390.3477 - val_loss: 538.2166 - val_mae: 18.2124 - val_mse: 538.2166\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 396.1538 - mae: 15.2580 - mse: 396.1538 - val_loss: 528.6210 - val_mae: 17.6905 - val_mse: 528.6210\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 387.9380 - mae: 14.9103 - mse: 387.9380 - val_loss: 538.2198 - val_mae: 17.8138 - val_mse: 538.2198\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 392.6754 - mae: 15.1234 - mse: 392.6754 - val_loss: 532.9286 - val_mae: 17.6644 - val_mse: 532.9286\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 378.8836 - mae: 14.7789 - mse: 378.8836 - val_loss: 542.1904 - val_mae: 17.9530 - val_mse: 542.1904\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.8271 - mae: 14.7047 - mse: 379.8271 - val_loss: 543.8846 - val_mae: 17.9054 - val_mse: 543.8846\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.6634 - mae: 14.8822 - mse: 381.6634 - val_loss: 547.7799 - val_mae: 17.8353 - val_mse: 547.7799\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.1134 - mae: 14.9055 - mse: 385.1134 - val_loss: 542.1042 - val_mae: 18.0645 - val_mse: 542.1042\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.6185 - mae: 14.8819 - mse: 383.6185 - val_loss: 541.3295 - val_mae: 17.7693 - val_mse: 541.3295\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 374.4981 - mae: 14.7868 - mse: 374.4981 - val_loss: 532.5433 - val_mae: 17.7204 - val_mse: 532.5433\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 375.7826 - mae: 14.7102 - mse: 375.7826 - val_loss: 551.9798 - val_mae: 18.0914 - val_mse: 551.9798\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.9211 - mae: 14.8714 - mse: 383.9211 - val_loss: 535.8361 - val_mae: 17.9972 - val_mse: 535.8361\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 382.2835 - mae: 14.8944 - mse: 382.2835 - val_loss: 537.6213 - val_mae: 17.9341 - val_mse: 537.6213\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 377.5918 - mae: 14.7073 - mse: 377.5918 - val_loss: 545.9606 - val_mae: 17.9519 - val_mse: 545.9606\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.2710 - mae: 14.7219 - mse: 373.2710 - val_loss: 539.5323 - val_mae: 18.0890 - val_mse: 539.5323\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 382.2792 - mae: 14.8905 - mse: 382.2792 - val_loss: 538.0212 - val_mae: 17.9158 - val_mse: 538.0212\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 370.1562 - mae: 14.6732 - mse: 370.1562 - val_loss: 536.2764 - val_mae: 17.8114 - val_mse: 536.2764\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 374.6016 - mae: 14.7503 - mse: 374.6016 - val_loss: 536.0980 - val_mae: 17.7305 - val_mse: 536.0980\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 377.9025 - mae: 14.7603 - mse: 377.9025 - val_loss: 539.8731 - val_mae: 17.9776 - val_mse: 539.8731\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 374.8433 - mae: 14.7058 - mse: 374.8433 - val_loss: 546.9469 - val_mae: 17.8321 - val_mse: 546.9469\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 370.6503 - mae: 14.6299 - mse: 370.6503 - val_loss: 545.1583 - val_mae: 17.9706 - val_mse: 545.1583\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 368.7176 - mae: 14.5516 - mse: 368.7176 - val_loss: 549.3287 - val_mae: 18.2536 - val_mse: 549.3287\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 369.2544 - mae: 14.6960 - mse: 369.2544 - val_loss: 551.1977 - val_mae: 18.0094 - val_mse: 551.1977\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 352.4943 - mae: 14.3267 - mse: 352.4943 - val_loss: 553.9220 - val_mae: 18.5249 - val_mse: 553.9220\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 369.8455 - mae: 14.5535 - mse: 369.8455 - val_loss: 548.3957 - val_mae: 18.1291 - val_mse: 548.3957\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.9182 - mae: 14.5272 - mse: 365.9182 - val_loss: 561.0125 - val_mae: 17.9636 - val_mse: 561.0125\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.9917 - mae: 14.5260 - mse: 365.9917 - val_loss: 562.1940 - val_mae: 18.2759 - val_mse: 562.1940\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 367.6235 - mae: 14.6715 - mse: 367.6235 - val_loss: 547.8807 - val_mae: 18.1753 - val_mse: 547.8807\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.0948 - mae: 14.5225 - mse: 365.0948 - val_loss: 567.4048 - val_mae: 18.2784 - val_mse: 567.4048\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.5602 - mae: 14.6120 - mse: 365.5602 - val_loss: 553.0934 - val_mae: 18.2933 - val_mse: 553.0934\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.0873 - mae: 14.2696 - mse: 354.0873 - val_loss: 547.8487 - val_mae: 17.9905 - val_mse: 547.8487\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.8150 - mae: 14.5363 - mse: 365.8150 - val_loss: 547.8832 - val_mae: 18.0715 - val_mse: 547.8832\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 351.5053 - mae: 14.2620 - mse: 351.5053 - val_loss: 554.7718 - val_mae: 18.0203 - val_mse: 554.7718\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 356.7432 - mae: 14.3191 - mse: 356.7432 - val_loss: 552.2071 - val_mae: 17.9469 - val_mse: 552.2071\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.8262 - mae: 14.2187 - mse: 354.8262 - val_loss: 558.9462 - val_mae: 18.5157 - val_mse: 558.9462\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.8906 - mae: 14.5200 - mse: 364.8906 - val_loss: 549.1179 - val_mae: 18.1949 - val_mse: 549.1179\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.2833 - mae: 14.4012 - mse: 355.2833 - val_loss: 545.1365 - val_mae: 18.0686 - val_mse: 545.1365\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 359.9155 - mae: 14.3931 - mse: 359.9155 - val_loss: 545.8161 - val_mae: 18.2846 - val_mse: 545.8161\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 357.6033 - mae: 14.2870 - mse: 357.6033 - val_loss: 558.7176 - val_mae: 18.3159 - val_mse: 558.7176\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 345.3906 - mae: 14.1056 - mse: 345.3906 - val_loss: 550.9021 - val_mae: 18.4433 - val_mse: 550.9021\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.6960 - mae: 14.4419 - mse: 355.6960 - val_loss: 553.0715 - val_mae: 18.3268 - val_mse: 553.0715\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.4996 - mae: 14.2371 - mse: 354.4996 - val_loss: 554.4630 - val_mae: 18.2414 - val_mse: 554.4630\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 359.7314 - mae: 14.3702 - mse: 359.7314 - val_loss: 549.6223 - val_mae: 18.0064 - val_mse: 549.6223\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.8668 - mae: 14.2054 - mse: 349.8668 - val_loss: 550.3535 - val_mae: 18.1198 - val_mse: 550.3535\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 347.6233 - mae: 14.1416 - mse: 347.6233 - val_loss: 555.2708 - val_mae: 18.4927 - val_mse: 555.2708\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 344.3596 - mae: 14.0790 - mse: 344.3596 - val_loss: 560.9141 - val_mae: 18.2190 - val_mse: 560.9141\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 341.7326 - mae: 13.9469 - mse: 341.7326 - val_loss: 570.5337 - val_mae: 18.5169 - val_mse: 570.5337\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2749.8577 - mae: 43.8581 - mse: 2749.8396 - val_loss: 773.7640 - val_mae: 23.4626 - val_mse: 773.7443\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1008.9476 - mae: 26.0395 - mse: 1008.9277 - val_loss: 658.5781 - val_mae: 21.3193 - val_mse: 658.5582\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 941.6064 - mae: 25.0359 - mse: 941.5869 - val_loss: 632.4376 - val_mae: 20.9965 - val_mse: 632.4180\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 883.1840 - mae: 23.9630 - mse: 883.1645 - val_loss: 576.8870 - val_mae: 19.6753 - val_mse: 576.8675\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 831.3453 - mae: 23.3315 - mse: 831.3253 - val_loss: 565.0843 - val_mae: 19.5343 - val_mse: 565.0648\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 828.0806 - mae: 23.2504 - mse: 828.0613 - val_loss: 561.9296 - val_mae: 19.6629 - val_mse: 561.9104\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 801.8204 - mae: 22.9343 - mse: 801.8011 - val_loss: 541.6521 - val_mae: 19.0960 - val_mse: 541.6329\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 781.5252 - mae: 22.4177 - mse: 781.5065 - val_loss: 545.5428 - val_mae: 19.3281 - val_mse: 545.5239\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 780.5395 - mae: 22.6052 - mse: 780.5203 - val_loss: 524.0717 - val_mae: 18.5758 - val_mse: 524.0526\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 764.6198 - mae: 22.1325 - mse: 764.6011 - val_loss: 567.4656 - val_mae: 20.0533 - val_mse: 567.4468\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 755.5993 - mae: 22.1620 - mse: 755.5803 - val_loss: 530.3950 - val_mae: 19.0932 - val_mse: 530.3763\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 751.6729 - mae: 21.8101 - mse: 751.6541 - val_loss: 514.7905 - val_mae: 18.5568 - val_mse: 514.7719\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 744.5913 - mae: 21.8896 - mse: 744.5726 - val_loss: 526.1418 - val_mae: 18.9712 - val_mse: 526.1235\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 736.8621 - mae: 21.7254 - mse: 736.8438 - val_loss: 521.5617 - val_mae: 18.8941 - val_mse: 521.5434\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 744.5638 - mae: 21.8132 - mse: 744.5453 - val_loss: 507.3040 - val_mae: 18.3369 - val_mse: 507.2856\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 701.6450 - mae: 21.2463 - mse: 701.6267 - val_loss: 508.0612 - val_mae: 18.4091 - val_mse: 508.0429\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 695.3365 - mae: 21.0269 - mse: 695.3185 - val_loss: 526.5679 - val_mae: 19.0963 - val_mse: 526.5499\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 710.8309 - mae: 21.3328 - mse: 710.8131 - val_loss: 518.5482 - val_mae: 18.8565 - val_mse: 518.5300\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 690.3646 - mae: 20.9829 - mse: 690.3463 - val_loss: 495.0914 - val_mae: 17.4253 - val_mse: 495.0733\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 693.1651 - mae: 20.8842 - mse: 693.1469 - val_loss: 506.1373 - val_mae: 18.4540 - val_mse: 506.1195\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 699.0087 - mae: 20.9688 - mse: 698.9907 - val_loss: 508.3813 - val_mae: 18.5221 - val_mse: 508.3635\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 686.6878 - mae: 20.8058 - mse: 686.6703 - val_loss: 517.4893 - val_mae: 18.8079 - val_mse: 517.4718\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 673.1516 - mae: 20.4810 - mse: 673.1339 - val_loss: 490.3933 - val_mae: 17.5445 - val_mse: 490.3757\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 691.8044 - mae: 20.9007 - mse: 691.7869 - val_loss: 488.6587 - val_mae: 17.6178 - val_mse: 488.6412\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 675.9564 - mae: 20.6743 - mse: 675.9390 - val_loss: 513.0001 - val_mae: 18.6875 - val_mse: 512.9829\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 676.1370 - mae: 20.6510 - mse: 676.1198 - val_loss: 498.2366 - val_mae: 18.1388 - val_mse: 498.2197\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 687.7100 - mae: 20.8397 - mse: 687.6928 - val_loss: 501.3711 - val_mae: 18.2077 - val_mse: 501.3543\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.2351 - mae: 20.6669 - mse: 673.2184 - val_loss: 493.8353 - val_mae: 17.8463 - val_mse: 493.8187\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 661.6321 - mae: 20.3575 - mse: 661.6154 - val_loss: 492.1310 - val_mae: 17.8216 - val_mse: 492.1143\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.5602 - mae: 20.5196 - mse: 672.5436 - val_loss: 503.1978 - val_mae: 18.3441 - val_mse: 503.1814\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.1528 - mae: 20.4740 - mse: 672.1367 - val_loss: 515.4133 - val_mae: 18.7430 - val_mse: 515.3970\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 659.0249 - mae: 20.3232 - mse: 659.0085 - val_loss: 493.4456 - val_mae: 17.9325 - val_mse: 493.4294\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.4331 - mae: 20.4086 - mse: 677.4170 - val_loss: 505.6134 - val_mae: 18.4939 - val_mse: 505.5974\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 664.4479 - mae: 20.3396 - mse: 664.4320 - val_loss: 486.8419 - val_mae: 17.5393 - val_mse: 486.8259\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.7628 - mae: 20.4622 - mse: 662.7469 - val_loss: 487.5172 - val_mae: 17.7942 - val_mse: 487.5015\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 669.9211 - mae: 20.3352 - mse: 669.9055 - val_loss: 488.4868 - val_mae: 17.8259 - val_mse: 488.4712\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.6624 - mae: 20.4534 - mse: 663.6469 - val_loss: 482.9872 - val_mae: 17.4100 - val_mse: 482.9716\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 662.3915 - mae: 20.2931 - mse: 662.3761 - val_loss: 485.3155 - val_mae: 17.5685 - val_mse: 485.3000\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 639.3493 - mae: 19.9598 - mse: 639.3340 - val_loss: 492.0002 - val_mae: 17.9941 - val_mse: 491.9848\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 643.9698 - mae: 20.0423 - mse: 643.9545 - val_loss: 489.5099 - val_mae: 17.8893 - val_mse: 489.4948\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.9674 - mae: 20.2921 - mse: 663.9523 - val_loss: 484.9317 - val_mae: 17.6185 - val_mse: 484.9166\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.1064 - mae: 20.2784 - mse: 659.0914 - val_loss: 482.5031 - val_mae: 17.1827 - val_mse: 482.4881\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.8064 - mae: 19.9616 - mse: 640.7914 - val_loss: 483.8096 - val_mae: 17.5201 - val_mse: 483.7947\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 655.8671 - mae: 20.2163 - mse: 655.8525 - val_loss: 494.7123 - val_mae: 18.0818 - val_mse: 494.6976\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.7888 - mae: 19.9542 - mse: 638.7744 - val_loss: 493.6978 - val_mae: 18.0759 - val_mse: 493.6832\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 637.6837 - mae: 19.8978 - mse: 637.6691 - val_loss: 491.5515 - val_mae: 17.9849 - val_mse: 491.5369\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 652.6545 - mae: 20.2014 - mse: 652.6398 - val_loss: 487.1930 - val_mae: 17.7786 - val_mse: 487.1786\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.5435 - mae: 19.8206 - mse: 638.5292 - val_loss: 497.4506 - val_mae: 18.2057 - val_mse: 497.4364\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.7270 - mae: 19.8675 - mse: 638.7125 - val_loss: 503.5759 - val_mae: 18.4631 - val_mse: 503.5618\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 644.8062 - mae: 20.1381 - mse: 644.7921 - val_loss: 487.4024 - val_mae: 17.8617 - val_mse: 487.3883\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.8113 - mae: 20.0151 - mse: 639.7974 - val_loss: 482.4729 - val_mae: 17.5148 - val_mse: 482.4590\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 643.6385 - mae: 19.8784 - mse: 643.6246 - val_loss: 479.7968 - val_mae: 17.4519 - val_mse: 479.7831\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 644.3761 - mae: 19.9568 - mse: 644.3621 - val_loss: 483.0709 - val_mae: 17.6827 - val_mse: 483.0572\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 638.1962 - mae: 19.8823 - mse: 638.1824 - val_loss: 487.5952 - val_mae: 17.8865 - val_mse: 487.5817\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.9206 - mae: 19.7290 - mse: 634.9073 - val_loss: 478.5944 - val_mae: 17.3647 - val_mse: 478.5808\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 639.3317 - mae: 19.9127 - mse: 639.3177 - val_loss: 496.0078 - val_mae: 18.2137 - val_mse: 495.9943\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.2209 - mae: 19.9684 - mse: 642.2070 - val_loss: 482.4488 - val_mae: 17.6649 - val_mse: 482.4356\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 634.3889 - mae: 19.8569 - mse: 634.3759 - val_loss: 482.2305 - val_mae: 17.6575 - val_mse: 482.2173\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.8660 - mae: 19.5686 - mse: 620.8528 - val_loss: 480.6808 - val_mae: 17.5674 - val_mse: 480.6678\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.7095 - mae: 19.5833 - mse: 619.6965 - val_loss: 485.5414 - val_mae: 17.7858 - val_mse: 485.5284\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.4918 - mae: 19.5119 - mse: 620.4789 - val_loss: 477.9732 - val_mae: 17.3771 - val_mse: 477.9601\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.1462 - mae: 19.7368 - mse: 630.1336 - val_loss: 491.5282 - val_mae: 17.9956 - val_mse: 491.5154\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 639.6391 - mae: 19.8415 - mse: 639.6263 - val_loss: 479.8945 - val_mae: 17.3651 - val_mse: 479.8816\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 619.7146 - mae: 19.5969 - mse: 619.7020 - val_loss: 486.0790 - val_mae: 17.7474 - val_mse: 486.0664\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.4631 - mae: 19.5877 - mse: 622.4503 - val_loss: 493.1613 - val_mae: 18.0505 - val_mse: 493.1487\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.4993 - mae: 19.4470 - mse: 616.4867 - val_loss: 484.4717 - val_mae: 17.6398 - val_mse: 484.4591\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.0298 - mae: 19.5868 - mse: 618.0170 - val_loss: 482.2854 - val_mae: 17.4697 - val_mse: 482.2728\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 614.3897 - mae: 19.3758 - mse: 614.3770 - val_loss: 478.6635 - val_mae: 17.3615 - val_mse: 478.6508\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 611.2604 - mae: 19.3658 - mse: 611.2479 - val_loss: 487.2647 - val_mae: 17.8429 - val_mse: 487.2522\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.8392 - mae: 19.4945 - mse: 621.8266 - val_loss: 482.4033 - val_mae: 17.5168 - val_mse: 482.3907\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 609.2070 - mae: 19.3550 - mse: 609.1943 - val_loss: 479.1348 - val_mae: 17.3792 - val_mse: 479.1224\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.8419 - mae: 19.5936 - mse: 620.8292 - val_loss: 487.4937 - val_mae: 17.8334 - val_mse: 487.4815\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.1115 - mae: 19.5143 - mse: 617.0990 - val_loss: 492.9579 - val_mae: 18.1019 - val_mse: 492.9458\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.4415 - mae: 19.3996 - mse: 611.4293 - val_loss: 481.8757 - val_mae: 17.6081 - val_mse: 481.8637\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.5388 - mae: 19.4066 - mse: 614.5265 - val_loss: 480.2392 - val_mae: 17.4232 - val_mse: 480.2270\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.6699 - mae: 19.5919 - mse: 619.6580 - val_loss: 480.9851 - val_mae: 17.5131 - val_mse: 480.9732\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.9426 - mae: 19.5271 - mse: 621.9308 - val_loss: 479.4653 - val_mae: 17.3443 - val_mse: 479.4532\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 598.5895 - mae: 19.2029 - mse: 598.5773 - val_loss: 483.4846 - val_mae: 17.6625 - val_mse: 483.4725\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.3304 - mae: 19.2490 - mse: 603.3183 - val_loss: 506.8968 - val_mae: 18.6095 - val_mse: 506.8849\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.2646 - mae: 19.1877 - mse: 600.2525 - val_loss: 483.9536 - val_mae: 17.7491 - val_mse: 483.9416\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 607.4686 - mae: 19.3024 - mse: 607.4567 - val_loss: 488.1273 - val_mae: 17.9203 - val_mse: 488.1154\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 614.8172 - mae: 19.3544 - mse: 614.8051 - val_loss: 497.6012 - val_mae: 18.2647 - val_mse: 497.5893\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.5198 - mae: 19.3433 - mse: 611.5082 - val_loss: 493.8315 - val_mae: 18.1398 - val_mse: 493.8196\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 605.4789 - mae: 19.2339 - mse: 605.4670 - val_loss: 487.5648 - val_mae: 17.8972 - val_mse: 487.5530\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.4693 - mae: 19.3640 - mse: 612.4577 - val_loss: 482.8064 - val_mae: 17.6992 - val_mse: 482.7946\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.1218 - mae: 19.3952 - mse: 609.1101 - val_loss: 477.3317 - val_mae: 17.0357 - val_mse: 477.3197\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.4288 - mae: 19.3117 - mse: 610.4170 - val_loss: 481.0812 - val_mae: 17.5764 - val_mse: 481.0694\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 604.5342 - mae: 19.3094 - mse: 604.5221 - val_loss: 482.2755 - val_mae: 17.6347 - val_mse: 482.2636\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.6187 - mae: 19.1385 - mse: 601.6069 - val_loss: 483.3733 - val_mae: 17.7411 - val_mse: 483.3613\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.1630 - mae: 19.3684 - mse: 610.1511 - val_loss: 482.9475 - val_mae: 17.7331 - val_mse: 482.9356\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.1411 - mae: 19.1939 - mse: 599.1290 - val_loss: 478.8399 - val_mae: 17.5684 - val_mse: 478.8280\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.6439 - mae: 19.0381 - mse: 593.6320 - val_loss: 477.3271 - val_mae: 17.4137 - val_mse: 477.3151\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 605.1656 - mae: 19.1927 - mse: 605.1536 - val_loss: 488.1133 - val_mae: 17.9795 - val_mse: 488.1013\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.4560 - mae: 19.2791 - mse: 603.4441 - val_loss: 476.1304 - val_mae: 17.3921 - val_mse: 476.1183\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.1156 - mae: 19.0410 - mse: 594.1035 - val_loss: 477.6432 - val_mae: 17.1958 - val_mse: 477.6311\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.4249 - mae: 19.2370 - mse: 600.4127 - val_loss: 490.7837 - val_mae: 18.0753 - val_mse: 490.7716\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.5600 - mae: 19.0373 - mse: 586.5477 - val_loss: 479.5840 - val_mae: 17.5053 - val_mse: 479.5719\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.8273 - mae: 19.0252 - mse: 591.8152 - val_loss: 485.9686 - val_mae: 17.8924 - val_mse: 485.9564\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.7815 - mae: 18.7993 - mse: 585.7693 - val_loss: 478.1638 - val_mae: 17.4960 - val_mse: 478.1517\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.9401 - mae: 19.1489 - mse: 595.9281 - val_loss: 477.6399 - val_mae: 17.4303 - val_mse: 477.6276\n"
     ]
    }
   ],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 14.5376, Train MSE: 370.5193\n",
      "Val   MAE: 18.6078, Val   MSE: 573.3127\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 12.2986, Train MSE: 265.4861\n",
      "Val   MAE: 18.5169, Val   MSE: 570.5337\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 17.0686, Train MSE: 485.9677\n",
      "Val   MAE: 17.4303, Val   MSE: 477.6276\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2182.0486 - mae: 36.5080 - mse: 2182.0486 - val_loss: 503.8967 - val_mae: 17.5305 - val_mse: 503.8967\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.8253 - mae: 16.8225 - mse: 478.8253 - val_loss: 465.0204 - val_mae: 17.0770 - val_mse: 465.0204\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 449.9460 - mae: 16.1127 - mse: 449.9460 - val_loss: 455.7939 - val_mae: 16.6261 - val_mse: 455.7939\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.5515 - mae: 15.9133 - mse: 438.5515 - val_loss: 450.6855 - val_mae: 16.4608 - val_mse: 450.6855\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 430.0503 - mae: 15.7003 - mse: 430.0503 - val_loss: 458.0494 - val_mae: 16.9044 - val_mse: 458.0494\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 422.6493 - mae: 15.5085 - mse: 422.6493 - val_loss: 450.8271 - val_mae: 16.5872 - val_mse: 450.8271\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 417.0813 - mae: 15.3571 - mse: 417.0813 - val_loss: 462.0924 - val_mae: 17.2140 - val_mse: 462.0924\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 411.3799 - mae: 15.2375 - mse: 411.3799 - val_loss: 447.6820 - val_mae: 16.0381 - val_mse: 447.6820\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 407.9619 - mae: 15.1560 - mse: 407.9619 - val_loss: 446.7441 - val_mae: 16.4749 - val_mse: 446.7441\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.8859 - mae: 15.1433 - mse: 405.8859 - val_loss: 446.0486 - val_mae: 16.3550 - val_mse: 446.0486\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 401.7272 - mae: 15.0531 - mse: 401.7272 - val_loss: 455.2699 - val_mae: 16.8135 - val_mse: 455.2699\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 398.3764 - mae: 14.9852 - mse: 398.3764 - val_loss: 454.5956 - val_mae: 16.5702 - val_mse: 454.5956\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 394.2083 - mae: 14.8984 - mse: 394.2083 - val_loss: 460.2808 - val_mae: 16.6995 - val_mse: 460.2808\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.0089 - mae: 14.8433 - mse: 392.0089 - val_loss: 450.9623 - val_mae: 16.3443 - val_mse: 450.9623\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 389.0811 - mae: 14.7340 - mse: 389.0811 - val_loss: 457.3499 - val_mae: 16.7967 - val_mse: 457.3499\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 387.6061 - mae: 14.7598 - mse: 387.6061 - val_loss: 448.2758 - val_mae: 16.1683 - val_mse: 448.2758\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 382.2968 - mae: 14.5585 - mse: 382.2968 - val_loss: 457.7800 - val_mae: 16.7263 - val_mse: 457.7800\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 377.6824 - mae: 14.5070 - mse: 377.6824 - val_loss: 454.7602 - val_mae: 16.3979 - val_mse: 454.7602\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 374.1512 - mae: 14.3560 - mse: 374.1512 - val_loss: 467.7038 - val_mae: 16.1171 - val_mse: 467.7038\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 375.6022 - mae: 14.4764 - mse: 375.6022 - val_loss: 461.0530 - val_mae: 16.5441 - val_mse: 461.0530\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 368.7956 - mae: 14.2736 - mse: 368.7956 - val_loss: 452.6031 - val_mae: 16.2520 - val_mse: 452.6031\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 366.7427 - mae: 14.2204 - mse: 366.7427 - val_loss: 463.0335 - val_mae: 16.8505 - val_mse: 463.0335\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 363.5916 - mae: 14.1837 - mse: 363.5916 - val_loss: 456.8402 - val_mae: 16.3879 - val_mse: 456.8402\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 359.8424 - mae: 14.0842 - mse: 359.8424 - val_loss: 470.1529 - val_mae: 16.9366 - val_mse: 470.1529\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 356.3500 - mae: 14.0714 - mse: 356.3500 - val_loss: 460.6620 - val_mae: 16.3431 - val_mse: 460.6620\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 357.2215 - mae: 14.0752 - mse: 357.2215 - val_loss: 462.4825 - val_mae: 16.6133 - val_mse: 462.4825\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 350.0069 - mae: 13.8684 - mse: 350.0069 - val_loss: 467.7684 - val_mae: 16.3879 - val_mse: 467.7684\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 345.7206 - mae: 13.8076 - mse: 345.7206 - val_loss: 459.1255 - val_mae: 16.4373 - val_mse: 459.1255\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 343.3484 - mae: 13.6927 - mse: 343.3484 - val_loss: 471.9368 - val_mae: 16.3071 - val_mse: 471.9368\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 338.2164 - mae: 13.6209 - mse: 338.2164 - val_loss: 466.1423 - val_mae: 16.7004 - val_mse: 466.1423\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 337.3854 - mae: 13.6231 - mse: 337.3854 - val_loss: 467.3150 - val_mae: 16.6807 - val_mse: 467.3150\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 332.1196 - mae: 13.4844 - mse: 332.1196 - val_loss: 464.1647 - val_mae: 16.3502 - val_mse: 464.1647\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 326.8225 - mae: 13.3540 - mse: 326.8225 - val_loss: 470.5904 - val_mae: 16.4051 - val_mse: 470.5904\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 325.4725 - mae: 13.3161 - mse: 325.4725 - val_loss: 474.5262 - val_mae: 16.6574 - val_mse: 474.5262\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 320.7762 - mae: 13.1956 - mse: 320.7762 - val_loss: 467.4740 - val_mae: 16.8613 - val_mse: 467.4740\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 318.1464 - mae: 13.1962 - mse: 318.1464 - val_loss: 475.8732 - val_mae: 16.5345 - val_mse: 475.8732\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 311.9218 - mae: 12.9515 - mse: 311.9218 - val_loss: 484.1225 - val_mae: 16.5455 - val_mse: 484.1225\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 308.5804 - mae: 12.9237 - mse: 308.5804 - val_loss: 483.2421 - val_mae: 16.7137 - val_mse: 483.2421\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 307.0122 - mae: 12.8515 - mse: 307.0122 - val_loss: 482.1918 - val_mae: 16.8282 - val_mse: 482.1918\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 300.1913 - mae: 12.7224 - mse: 300.1913 - val_loss: 499.6524 - val_mae: 17.3077 - val_mse: 499.6524\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 295.6700 - mae: 12.6444 - mse: 295.6700 - val_loss: 475.2884 - val_mae: 16.4197 - val_mse: 475.2884\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 294.1011 - mae: 12.5530 - mse: 294.1011 - val_loss: 492.5365 - val_mae: 17.1069 - val_mse: 492.5365\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 289.0869 - mae: 12.4902 - mse: 289.0869 - val_loss: 495.6072 - val_mae: 17.0883 - val_mse: 495.6072\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 281.4869 - mae: 12.2226 - mse: 281.4869 - val_loss: 502.0886 - val_mae: 16.8766 - val_mse: 502.0886\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 275.2623 - mae: 12.1093 - mse: 275.2623 - val_loss: 493.7740 - val_mae: 17.2847 - val_mse: 493.7740\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 275.3431 - mae: 12.1218 - mse: 275.3431 - val_loss: 519.9848 - val_mae: 17.0628 - val_mse: 519.9848\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 270.5727 - mae: 11.9975 - mse: 270.5727 - val_loss: 503.2643 - val_mae: 17.2766 - val_mse: 503.2643\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 264.2309 - mae: 11.9231 - mse: 264.2309 - val_loss: 514.9157 - val_mae: 17.3118 - val_mse: 514.9157\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 260.5845 - mae: 11.7176 - mse: 260.5845 - val_loss: 497.9745 - val_mae: 17.1129 - val_mse: 497.9745\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 257.8734 - mae: 11.7118 - mse: 257.8734 - val_loss: 527.7698 - val_mae: 17.6789 - val_mse: 527.7698\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 258.3547 - mae: 11.7739 - mse: 258.3547 - val_loss: 520.1775 - val_mae: 17.2346 - val_mse: 520.1775\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 246.8440 - mae: 11.3836 - mse: 246.8440 - val_loss: 534.7247 - val_mae: 17.7021 - val_mse: 534.7247\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 248.7887 - mae: 11.4928 - mse: 248.7887 - val_loss: 539.5551 - val_mae: 17.4603 - val_mse: 539.5551\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 242.5840 - mae: 11.2957 - mse: 242.5840 - val_loss: 537.4976 - val_mae: 17.4191 - val_mse: 537.4976\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 237.1108 - mae: 11.1829 - mse: 237.1108 - val_loss: 540.4225 - val_mae: 17.8767 - val_mse: 540.4225\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 234.8671 - mae: 11.1605 - mse: 234.8671 - val_loss: 556.7119 - val_mae: 17.3952 - val_mse: 556.7119\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 232.6680 - mae: 11.0474 - mse: 232.6680 - val_loss: 548.8168 - val_mae: 18.0209 - val_mse: 548.8168\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 226.6045 - mae: 10.9112 - mse: 226.6045 - val_loss: 561.0750 - val_mae: 17.5779 - val_mse: 561.0750\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 219.3235 - mae: 10.7481 - mse: 219.3235 - val_loss: 565.0891 - val_mae: 17.5828 - val_mse: 565.0891\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 219.5102 - mae: 10.6817 - mse: 219.5102 - val_loss: 563.0347 - val_mae: 17.7340 - val_mse: 563.0347\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 212.5804 - mae: 10.5518 - mse: 212.5804 - val_loss: 567.4607 - val_mae: 17.7369 - val_mse: 567.4607\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 211.5391 - mae: 10.4985 - mse: 211.5391 - val_loss: 583.0466 - val_mae: 17.9926 - val_mse: 583.0466\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 207.7875 - mae: 10.4401 - mse: 207.7875 - val_loss: 589.8658 - val_mae: 18.1586 - val_mse: 589.8658\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 207.7125 - mae: 10.4259 - mse: 207.7125 - val_loss: 582.3315 - val_mae: 18.3671 - val_mse: 582.3315\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 204.8923 - mae: 10.3004 - mse: 204.8923 - val_loss: 591.8841 - val_mae: 18.1321 - val_mse: 591.8841\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 199.8574 - mae: 10.1380 - mse: 199.8574 - val_loss: 586.7256 - val_mae: 18.1135 - val_mse: 586.7256\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 198.0101 - mae: 10.2189 - mse: 198.0101 - val_loss: 603.4646 - val_mae: 18.2896 - val_mse: 603.4646\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 194.1347 - mae: 10.0266 - mse: 194.1347 - val_loss: 598.2072 - val_mae: 18.3391 - val_mse: 598.2072\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 192.5365 - mae: 10.0777 - mse: 192.5365 - val_loss: 592.7697 - val_mae: 18.1380 - val_mse: 592.7697\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 188.9149 - mae: 9.8540 - mse: 188.9149 - val_loss: 589.6067 - val_mae: 18.1586 - val_mse: 589.6067\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 187.4955 - mae: 9.8753 - mse: 187.4955 - val_loss: 592.5135 - val_mae: 18.0809 - val_mse: 592.5135\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 183.7305 - mae: 9.7948 - mse: 183.7305 - val_loss: 601.9413 - val_mae: 18.4558 - val_mse: 601.9413\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 179.5937 - mae: 9.6711 - mse: 179.5937 - val_loss: 599.6772 - val_mae: 18.3401 - val_mse: 599.6772\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 177.4096 - mae: 9.5706 - mse: 177.4096 - val_loss: 604.5439 - val_mae: 18.4841 - val_mse: 604.5439\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 177.6160 - mae: 9.6266 - mse: 177.6160 - val_loss: 610.2143 - val_mae: 18.4424 - val_mse: 610.2143\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.1244 - mae: 9.4747 - mse: 171.1244 - val_loss: 615.2416 - val_mae: 18.4683 - val_mse: 615.2416\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.3242 - mae: 9.4615 - mse: 171.3242 - val_loss: 619.2242 - val_mae: 18.4950 - val_mse: 619.2242\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 168.5711 - mae: 9.3972 - mse: 168.5711 - val_loss: 626.0148 - val_mae: 18.7113 - val_mse: 626.0148\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 162.3230 - mae: 9.1055 - mse: 162.3230 - val_loss: 631.7012 - val_mae: 18.6045 - val_mse: 631.7012\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 161.1008 - mae: 9.1546 - mse: 161.1008 - val_loss: 614.9329 - val_mae: 18.8268 - val_mse: 614.9329\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 168.4247 - mae: 9.3996 - mse: 168.4247 - val_loss: 632.6860 - val_mae: 18.9776 - val_mse: 632.6860\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 163.0216 - mae: 9.2096 - mse: 163.0216 - val_loss: 615.6657 - val_mae: 18.5964 - val_mse: 615.6657\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 161.3979 - mae: 9.1717 - mse: 161.3979 - val_loss: 628.0312 - val_mae: 18.8156 - val_mse: 628.0312\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 155.7774 - mae: 8.9921 - mse: 155.7774 - val_loss: 630.7628 - val_mae: 18.7585 - val_mse: 630.7628\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 153.5428 - mae: 8.9736 - mse: 153.5428 - val_loss: 634.0084 - val_mae: 18.7926 - val_mse: 634.0084\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 149.1538 - mae: 8.8285 - mse: 149.1538 - val_loss: 636.2896 - val_mae: 18.7935 - val_mse: 636.2896\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 148.7450 - mae: 8.7901 - mse: 148.7450 - val_loss: 638.6161 - val_mae: 18.9251 - val_mse: 638.6161\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 147.7160 - mae: 8.8412 - mse: 147.7160 - val_loss: 662.8938 - val_mae: 19.1538 - val_mse: 662.8938\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 141.6107 - mae: 8.5383 - mse: 141.6107 - val_loss: 687.8277 - val_mae: 19.4982 - val_mse: 687.8277\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 145.9809 - mae: 8.7533 - mse: 145.9809 - val_loss: 640.5172 - val_mae: 18.8588 - val_mse: 640.5172\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.7252 - mae: 8.5412 - mse: 139.7252 - val_loss: 646.3626 - val_mae: 19.1831 - val_mse: 646.3626\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.5842 - mae: 8.4094 - mse: 137.5842 - val_loss: 663.4178 - val_mae: 19.1374 - val_mse: 663.4178\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5147 - mae: 8.3720 - mse: 135.5147 - val_loss: 669.8881 - val_mae: 19.4661 - val_mse: 669.8881\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1139 - mae: 8.4206 - mse: 137.1139 - val_loss: 657.3629 - val_mae: 19.2217 - val_mse: 657.3629\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.4714 - mae: 8.5063 - mse: 139.4714 - val_loss: 681.9637 - val_mae: 19.8340 - val_mse: 681.9637\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.7861 - mae: 8.4778 - mse: 135.7861 - val_loss: 701.9506 - val_mae: 19.7071 - val_mse: 701.9506\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.6648 - mae: 8.1816 - mse: 130.6648 - val_loss: 674.8962 - val_mae: 19.4825 - val_mse: 674.8962\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.1769 - mae: 8.1388 - mse: 129.1769 - val_loss: 688.6134 - val_mae: 19.7622 - val_mse: 688.6134\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.7073 - mae: 8.0457 - mse: 125.7073 - val_loss: 674.6932 - val_mae: 19.4789 - val_mse: 674.6932\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.7255 - mae: 8.0222 - mse: 125.7255 - val_loss: 680.3669 - val_mae: 19.5009 - val_mse: 680.3669\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5902.7656 - mae: 72.9645 - mse: 5902.7656 - val_loss: 5482.9775 - val_mae: 70.2731 - val_mse: 5482.9775\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4735.9858 - mae: 64.9505 - mse: 4735.9858 - val_loss: 3955.4973 - val_mae: 59.0720 - val_mse: 3955.4973\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3195.2764 - mae: 52.6563 - mse: 3195.2764 - val_loss: 2561.8228 - val_mae: 46.8941 - val_mse: 2561.8228\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1741.4871 - mae: 37.8545 - mse: 1741.4871 - val_loss: 1173.1561 - val_mae: 30.6199 - val_mse: 1173.1561\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 847.8584 - mae: 25.2598 - mse: 847.8584 - val_loss: 636.9456 - val_mae: 21.5026 - val_mse: 636.9456\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 527.5419 - mae: 18.7061 - mse: 527.5419 - val_loss: 495.8960 - val_mae: 18.2017 - val_mse: 495.8960\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 442.9458 - mae: 16.1915 - mse: 442.9458 - val_loss: 461.8093 - val_mae: 16.6039 - val_mse: 461.8093\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 427.2349 - mae: 15.5730 - mse: 427.2349 - val_loss: 445.5039 - val_mae: 16.2768 - val_mse: 445.5039\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.5034 - mae: 15.5930 - mse: 426.5034 - val_loss: 440.7376 - val_mae: 16.3942 - val_mse: 440.7376\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 410.1455 - mae: 15.1362 - mse: 410.1455 - val_loss: 446.2204 - val_mae: 16.3906 - val_mse: 446.2204\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.9039 - mae: 15.0709 - mse: 405.9039 - val_loss: 454.2081 - val_mae: 16.5787 - val_mse: 454.2081\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.4290 - mae: 14.9370 - mse: 405.4290 - val_loss: 446.0792 - val_mae: 16.1766 - val_mse: 446.0792\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 393.2164 - mae: 14.8383 - mse: 393.2164 - val_loss: 450.1557 - val_mae: 15.8809 - val_mse: 450.1557\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 389.6019 - mae: 14.6949 - mse: 389.6019 - val_loss: 449.2742 - val_mae: 16.2579 - val_mse: 449.2742\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 387.7713 - mae: 14.6350 - mse: 387.7713 - val_loss: 456.1447 - val_mae: 16.6037 - val_mse: 456.1447\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.6028 - mae: 14.5148 - mse: 381.6028 - val_loss: 438.1975 - val_mae: 15.6155 - val_mse: 438.1975\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 375.0681 - mae: 14.3785 - mse: 375.0681 - val_loss: 447.2342 - val_mae: 16.2238 - val_mse: 447.2342\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 370.2361 - mae: 14.2515 - mse: 370.2361 - val_loss: 456.0312 - val_mae: 16.2500 - val_mse: 456.0312\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.1023 - mae: 14.3788 - mse: 373.1023 - val_loss: 455.6874 - val_mae: 16.1723 - val_mse: 455.6874\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.8015 - mae: 14.1489 - mse: 364.8015 - val_loss: 458.5157 - val_mae: 16.4502 - val_mse: 458.5157\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 361.7460 - mae: 14.0773 - mse: 361.7460 - val_loss: 465.9282 - val_mae: 16.2707 - val_mse: 465.9282\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 356.7310 - mae: 13.8796 - mse: 356.7310 - val_loss: 455.7344 - val_mae: 16.2875 - val_mse: 455.7344\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.4084 - mae: 13.8444 - mse: 354.4084 - val_loss: 451.7862 - val_mae: 16.0790 - val_mse: 451.7862\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 348.2405 - mae: 13.8334 - mse: 348.2405 - val_loss: 454.9632 - val_mae: 16.2034 - val_mse: 454.9632\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 345.1452 - mae: 13.6843 - mse: 345.1452 - val_loss: 456.5693 - val_mae: 16.3168 - val_mse: 456.5693\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 340.2329 - mae: 13.6035 - mse: 340.2329 - val_loss: 477.6111 - val_mae: 16.3917 - val_mse: 477.6111\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 342.1311 - mae: 13.7365 - mse: 342.1311 - val_loss: 462.0365 - val_mae: 16.3140 - val_mse: 462.0365\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 338.7262 - mae: 13.6131 - mse: 338.7262 - val_loss: 463.7752 - val_mae: 16.3749 - val_mse: 463.7752\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 326.3851 - mae: 13.3116 - mse: 326.3851 - val_loss: 475.9017 - val_mae: 16.8327 - val_mse: 475.9017\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 325.7029 - mae: 13.2330 - mse: 325.7029 - val_loss: 472.0941 - val_mae: 16.3871 - val_mse: 472.0941\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 320.9798 - mae: 13.1558 - mse: 320.9798 - val_loss: 463.0893 - val_mae: 16.2611 - val_mse: 463.0893\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 318.7703 - mae: 13.1208 - mse: 318.7703 - val_loss: 482.1319 - val_mae: 16.7613 - val_mse: 482.1319\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 317.0847 - mae: 13.1042 - mse: 317.0847 - val_loss: 475.4051 - val_mae: 16.5055 - val_mse: 475.4051\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 317.7228 - mae: 13.0634 - mse: 317.7228 - val_loss: 488.4119 - val_mae: 16.5242 - val_mse: 488.4119\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 313.0230 - mae: 13.0378 - mse: 313.0230 - val_loss: 489.0173 - val_mae: 16.8357 - val_mse: 489.0173\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 309.5345 - mae: 12.9162 - mse: 309.5345 - val_loss: 483.5293 - val_mae: 16.6691 - val_mse: 483.5293\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 307.4996 - mae: 12.9781 - mse: 307.4996 - val_loss: 481.6039 - val_mae: 16.4216 - val_mse: 481.6039\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 306.5565 - mae: 12.8001 - mse: 306.5565 - val_loss: 490.8680 - val_mae: 16.6873 - val_mse: 490.8680\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 295.6597 - mae: 12.5865 - mse: 295.6597 - val_loss: 491.9912 - val_mae: 16.7488 - val_mse: 491.9912\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 292.4172 - mae: 12.6009 - mse: 292.4172 - val_loss: 486.8381 - val_mae: 16.7077 - val_mse: 486.8381\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 289.9767 - mae: 12.5012 - mse: 289.9767 - val_loss: 502.6448 - val_mae: 17.0374 - val_mse: 502.6448\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 287.4213 - mae: 12.4447 - mse: 287.4213 - val_loss: 481.7454 - val_mae: 16.6528 - val_mse: 481.7454\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 287.5920 - mae: 12.4328 - mse: 287.5920 - val_loss: 500.2602 - val_mae: 17.1420 - val_mse: 500.2602\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 289.5584 - mae: 12.5063 - mse: 289.5584 - val_loss: 501.9330 - val_mae: 16.8643 - val_mse: 501.9330\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 282.9669 - mae: 12.2549 - mse: 282.9669 - val_loss: 493.2559 - val_mae: 16.7257 - val_mse: 493.2559\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 279.7593 - mae: 12.2716 - mse: 279.7593 - val_loss: 500.9142 - val_mae: 16.9293 - val_mse: 500.9142\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 283.6273 - mae: 12.2960 - mse: 283.6273 - val_loss: 501.5281 - val_mae: 17.0035 - val_mse: 501.5281\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 271.6082 - mae: 12.1296 - mse: 271.6082 - val_loss: 509.3244 - val_mae: 16.9236 - val_mse: 509.3244\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 276.1896 - mae: 12.1881 - mse: 276.1896 - val_loss: 506.3791 - val_mae: 16.9292 - val_mse: 506.3791\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 277.3553 - mae: 12.2087 - mse: 277.3553 - val_loss: 510.4866 - val_mae: 17.1711 - val_mse: 510.4866\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 271.5810 - mae: 12.0986 - mse: 271.5810 - val_loss: 511.1380 - val_mae: 17.1459 - val_mse: 511.1380\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 269.5236 - mae: 12.0611 - mse: 269.5236 - val_loss: 515.8716 - val_mae: 17.1709 - val_mse: 515.8716\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 264.4379 - mae: 11.8200 - mse: 264.4379 - val_loss: 536.7704 - val_mae: 17.4682 - val_mse: 536.7704\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 258.9196 - mae: 11.7697 - mse: 258.9196 - val_loss: 522.8270 - val_mae: 17.3974 - val_mse: 522.8270\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 251.0571 - mae: 11.4887 - mse: 251.0571 - val_loss: 515.0964 - val_mae: 17.1148 - val_mse: 515.0964\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 255.9046 - mae: 11.7352 - mse: 255.9046 - val_loss: 523.8419 - val_mae: 17.3770 - val_mse: 523.8419\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 255.0076 - mae: 11.7133 - mse: 255.0076 - val_loss: 524.6684 - val_mae: 17.4140 - val_mse: 524.6684\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 248.7589 - mae: 11.5255 - mse: 248.7589 - val_loss: 537.4061 - val_mae: 17.3571 - val_mse: 537.4061\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 257.5624 - mae: 11.7036 - mse: 257.5624 - val_loss: 528.7615 - val_mae: 17.4194 - val_mse: 528.7615\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 251.6019 - mae: 11.6038 - mse: 251.6019 - val_loss: 532.3174 - val_mae: 17.5353 - val_mse: 532.3174\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 244.9750 - mae: 11.4713 - mse: 244.9750 - val_loss: 528.8391 - val_mae: 17.4156 - val_mse: 528.8391\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 245.9471 - mae: 11.5511 - mse: 245.9471 - val_loss: 526.9182 - val_mae: 17.3556 - val_mse: 526.9182\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 252.0598 - mae: 11.5197 - mse: 252.0598 - val_loss: 520.9149 - val_mae: 17.1034 - val_mse: 520.9149\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 242.7649 - mae: 11.3335 - mse: 242.7649 - val_loss: 537.5942 - val_mae: 17.4775 - val_mse: 537.5942\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 239.1239 - mae: 11.3220 - mse: 239.1239 - val_loss: 536.0482 - val_mae: 17.5657 - val_mse: 536.0482\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 239.6431 - mae: 11.3243 - mse: 239.6431 - val_loss: 547.6025 - val_mae: 17.5852 - val_mse: 547.6025\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 240.7364 - mae: 11.3880 - mse: 240.7364 - val_loss: 539.9077 - val_mae: 17.3881 - val_mse: 539.9077\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 231.5078 - mae: 11.1499 - mse: 231.5078 - val_loss: 531.0476 - val_mae: 17.2880 - val_mse: 531.0476\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 239.6638 - mae: 11.4184 - mse: 239.6638 - val_loss: 547.6461 - val_mae: 17.4205 - val_mse: 547.6461\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.5455 - mae: 11.2372 - mse: 234.5455 - val_loss: 535.1789 - val_mae: 17.3900 - val_mse: 535.1789\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.3461 - mae: 11.1484 - mse: 234.3461 - val_loss: 549.1049 - val_mae: 17.8023 - val_mse: 549.1049\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 228.8127 - mae: 11.0630 - mse: 228.8127 - val_loss: 546.2689 - val_mae: 17.4151 - val_mse: 546.2689\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 226.1972 - mae: 11.0590 - mse: 226.1972 - val_loss: 548.4529 - val_mae: 17.7159 - val_mse: 548.4529\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 231.4372 - mae: 11.1669 - mse: 231.4372 - val_loss: 543.5405 - val_mae: 17.5066 - val_mse: 543.5405\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 225.9484 - mae: 10.9924 - mse: 225.9484 - val_loss: 536.0966 - val_mae: 17.5071 - val_mse: 536.0966\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 222.4546 - mae: 10.8949 - mse: 222.4546 - val_loss: 537.2721 - val_mae: 17.3664 - val_mse: 537.2721\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 227.7451 - mae: 11.0399 - mse: 227.7451 - val_loss: 544.5289 - val_mae: 17.4305 - val_mse: 544.5289\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 217.1973 - mae: 10.7748 - mse: 217.1973 - val_loss: 551.8670 - val_mae: 17.5357 - val_mse: 551.8670\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 216.2971 - mae: 10.8681 - mse: 216.2971 - val_loss: 548.8179 - val_mae: 17.4920 - val_mse: 548.8179\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 224.7097 - mae: 10.9171 - mse: 224.7097 - val_loss: 541.4147 - val_mae: 17.3465 - val_mse: 541.4147\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 225.3084 - mae: 10.9579 - mse: 225.3084 - val_loss: 536.1412 - val_mae: 17.5490 - val_mse: 536.1412\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 221.7300 - mae: 10.9260 - mse: 221.7300 - val_loss: 539.9158 - val_mae: 17.2835 - val_mse: 539.9158\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.6993 - mae: 10.8298 - mse: 215.6993 - val_loss: 555.4696 - val_mae: 17.7185 - val_mse: 555.4696\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 213.4668 - mae: 10.7094 - mse: 213.4668 - val_loss: 550.5697 - val_mae: 17.8327 - val_mse: 550.5697\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 214.8804 - mae: 10.8226 - mse: 214.8804 - val_loss: 560.4873 - val_mae: 17.9050 - val_mse: 560.4873\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 216.3777 - mae: 10.8458 - mse: 216.3777 - val_loss: 565.3141 - val_mae: 17.8571 - val_mse: 565.3141\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 210.9505 - mae: 10.6878 - mse: 210.9505 - val_loss: 565.2214 - val_mae: 17.6487 - val_mse: 565.2214\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.0813 - mae: 10.6862 - mse: 215.0813 - val_loss: 569.4180 - val_mae: 17.8941 - val_mse: 569.4180\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 214.5651 - mae: 10.7421 - mse: 214.5651 - val_loss: 551.8315 - val_mae: 17.6140 - val_mse: 551.8315\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 213.8872 - mae: 10.6884 - mse: 213.8872 - val_loss: 564.7035 - val_mae: 17.8402 - val_mse: 564.7035\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 206.1817 - mae: 10.5628 - mse: 206.1817 - val_loss: 549.8301 - val_mae: 17.7566 - val_mse: 549.8301\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 205.4922 - mae: 10.5485 - mse: 205.4922 - val_loss: 554.8794 - val_mae: 17.7821 - val_mse: 554.8794\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 204.0298 - mae: 10.5163 - mse: 204.0298 - val_loss: 570.0151 - val_mae: 17.8715 - val_mse: 570.0151\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 199.5368 - mae: 10.3751 - mse: 199.5368 - val_loss: 560.6179 - val_mae: 17.8346 - val_mse: 560.6179\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.1385 - mae: 10.4066 - mse: 202.1385 - val_loss: 560.2350 - val_mae: 17.8060 - val_mse: 560.2350\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.5389 - mae: 10.3886 - mse: 203.5389 - val_loss: 569.2051 - val_mae: 17.9860 - val_mse: 569.2051\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.7766 - mae: 10.5209 - mse: 202.7766 - val_loss: 569.6832 - val_mae: 17.6736 - val_mse: 569.6832\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 204.4994 - mae: 10.5869 - mse: 204.4994 - val_loss: 570.2509 - val_mae: 17.9751 - val_mse: 570.2509\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 201.1834 - mae: 10.4448 - mse: 201.1834 - val_loss: 559.6537 - val_mae: 17.8469 - val_mse: 559.6537\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.7669 - mae: 10.4514 - mse: 203.7669 - val_loss: 578.7206 - val_mae: 18.1137 - val_mse: 578.7206\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2561.1311 - mae: 41.6426 - mse: 2561.1023 - val_loss: 628.7026 - val_mae: 21.0269 - val_mse: 628.6701\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 889.4348 - mae: 24.2678 - mse: 889.4014 - val_loss: 518.2886 - val_mae: 18.5074 - val_mse: 518.2542\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 828.0317 - mae: 23.2819 - mse: 827.9969 - val_loss: 542.7605 - val_mae: 19.4807 - val_mse: 542.7253\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 786.1314 - mae: 22.7033 - mse: 786.0959 - val_loss: 483.8418 - val_mae: 17.8034 - val_mse: 483.8052\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 770.2985 - mae: 22.3831 - mse: 770.2617 - val_loss: 499.8347 - val_mae: 18.2805 - val_mse: 499.7976\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 742.5986 - mae: 21.8940 - mse: 742.5611 - val_loss: 501.3590 - val_mae: 18.3959 - val_mse: 501.3213\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 730.9299 - mae: 21.7682 - mse: 730.8920 - val_loss: 473.0716 - val_mae: 17.6708 - val_mse: 473.0333\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 710.1210 - mae: 21.5326 - mse: 710.0826 - val_loss: 463.7256 - val_mae: 17.2490 - val_mse: 463.6868\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 712.5085 - mae: 21.4800 - mse: 712.4696 - val_loss: 512.3224 - val_mae: 18.8537 - val_mse: 512.2835\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.0503 - mae: 21.5259 - mse: 709.0112 - val_loss: 496.6535 - val_mae: 18.5359 - val_mse: 496.6142\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 691.1590 - mae: 21.0867 - mse: 691.1196 - val_loss: 480.0706 - val_mae: 18.0204 - val_mse: 480.0310\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 691.4133 - mae: 21.0164 - mse: 691.3737 - val_loss: 463.1276 - val_mae: 17.3549 - val_mse: 463.0880\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.9160 - mae: 20.7341 - mse: 666.8765 - val_loss: 487.2556 - val_mae: 18.0220 - val_mse: 487.2163\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.1520 - mae: 20.7005 - mse: 667.1125 - val_loss: 547.2728 - val_mae: 19.7818 - val_mse: 547.2336\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 680.7238 - mae: 20.9470 - mse: 680.6841 - val_loss: 506.4449 - val_mae: 18.7349 - val_mse: 506.4056\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 654.9788 - mae: 20.4531 - mse: 654.9393 - val_loss: 487.3206 - val_mae: 18.2202 - val_mse: 487.2812\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 652.9431 - mae: 20.5482 - mse: 652.9034 - val_loss: 485.4628 - val_mae: 18.0617 - val_mse: 485.4234\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.2375 - mae: 20.2872 - mse: 647.1982 - val_loss: 472.9494 - val_mae: 17.7016 - val_mse: 472.9102\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.0198 - mae: 20.3449 - mse: 642.9808 - val_loss: 502.0686 - val_mae: 18.6596 - val_mse: 502.0298\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.6451 - mae: 20.3097 - mse: 643.6064 - val_loss: 493.8661 - val_mae: 18.4318 - val_mse: 493.8277\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.9607 - mae: 19.9987 - mse: 622.9224 - val_loss: 482.6420 - val_mae: 18.0653 - val_mse: 482.6039\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.1292 - mae: 19.9317 - mse: 623.0913 - val_loss: 458.4967 - val_mae: 16.9930 - val_mse: 458.4588\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.7161 - mae: 20.0716 - mse: 631.6782 - val_loss: 500.8308 - val_mae: 18.6656 - val_mse: 500.7935\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.2188 - mae: 19.9503 - mse: 627.1816 - val_loss: 603.1238 - val_mae: 21.2501 - val_mse: 603.0870\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 606.8828 - mae: 19.6992 - mse: 606.8459 - val_loss: 453.5468 - val_mae: 17.0463 - val_mse: 453.5101\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.3848 - mae: 19.8158 - mse: 613.3480 - val_loss: 451.1295 - val_mae: 16.8660 - val_mse: 451.0930\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.2043 - mae: 19.8615 - mse: 612.1683 - val_loss: 479.2374 - val_mae: 17.9557 - val_mse: 479.2014\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.3369 - mae: 19.8168 - mse: 618.3011 - val_loss: 472.5398 - val_mae: 17.8069 - val_mse: 472.5042\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8207 - mae: 19.7430 - mse: 615.7850 - val_loss: 481.2527 - val_mae: 18.1139 - val_mse: 481.2176\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.3398 - mae: 19.6289 - mse: 602.3050 - val_loss: 461.3817 - val_mae: 17.3810 - val_mse: 461.3471\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.0411 - mae: 19.6182 - mse: 597.0063 - val_loss: 492.8215 - val_mae: 18.4880 - val_mse: 492.7872\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.9500 - mae: 19.7041 - mse: 611.9159 - val_loss: 461.0556 - val_mae: 17.4195 - val_mse: 461.0217\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.0320 - mae: 19.4880 - mse: 597.9981 - val_loss: 474.4476 - val_mae: 17.9102 - val_mse: 474.4142\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0012 - mae: 19.4798 - mse: 595.9676 - val_loss: 483.0919 - val_mae: 18.0996 - val_mse: 483.0589\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.5812 - mae: 19.5063 - mse: 593.5483 - val_loss: 480.0828 - val_mae: 18.0509 - val_mse: 480.0501\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.5002 - mae: 19.3116 - mse: 593.4677 - val_loss: 457.4789 - val_mae: 17.2098 - val_mse: 457.4462\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.8199 - mae: 19.4893 - mse: 595.7874 - val_loss: 458.3059 - val_mae: 17.1911 - val_mse: 458.2735\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.9100 - mae: 19.4697 - mse: 590.8779 - val_loss: 446.2241 - val_mae: 16.6896 - val_mse: 446.1918\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.3126 - mae: 19.4080 - mse: 585.2807 - val_loss: 476.8203 - val_mae: 17.9003 - val_mse: 476.7888\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 576.7161 - mae: 19.0699 - mse: 576.6846 - val_loss: 455.9740 - val_mae: 17.2049 - val_mse: 455.9429\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 578.7926 - mae: 19.2468 - mse: 578.7617 - val_loss: 493.8838 - val_mae: 18.4820 - val_mse: 493.8531\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.2783 - mae: 19.3894 - mse: 588.2473 - val_loss: 459.3521 - val_mae: 17.3118 - val_mse: 459.3213\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.6266 - mae: 19.2417 - mse: 588.5962 - val_loss: 456.9565 - val_mae: 17.1510 - val_mse: 456.9261\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.6975 - mae: 19.2751 - mse: 585.6671 - val_loss: 471.4781 - val_mae: 17.7797 - val_mse: 471.4479\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 568.9533 - mae: 18.9758 - mse: 568.9232 - val_loss: 478.9620 - val_mae: 17.9683 - val_mse: 478.9320\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 576.9432 - mae: 19.3410 - mse: 576.9135 - val_loss: 460.2525 - val_mae: 17.3737 - val_mse: 460.2227\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.6431 - mae: 18.9870 - mse: 563.6133 - val_loss: 462.2648 - val_mae: 17.4666 - val_mse: 462.2354\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 564.6555 - mae: 19.0232 - mse: 564.6262 - val_loss: 445.1971 - val_mae: 16.6604 - val_mse: 445.1678\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 560.0280 - mae: 18.8372 - mse: 559.9988 - val_loss: 469.9112 - val_mae: 17.7330 - val_mse: 469.8826\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.6318 - mae: 18.9873 - mse: 563.6030 - val_loss: 464.4495 - val_mae: 17.4914 - val_mse: 464.4207\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 564.5607 - mae: 18.9265 - mse: 564.5319 - val_loss: 448.4793 - val_mae: 16.6435 - val_mse: 448.4504\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 570.7716 - mae: 18.9994 - mse: 570.7427 - val_loss: 449.7896 - val_mae: 16.8082 - val_mse: 449.7607\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.4003 - mae: 18.7221 - mse: 554.3718 - val_loss: 463.3341 - val_mae: 17.5119 - val_mse: 463.3055\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.8386 - mae: 18.7815 - mse: 557.8105 - val_loss: 446.2419 - val_mae: 16.6446 - val_mse: 446.2135\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 561.3138 - mae: 18.7471 - mse: 561.2852 - val_loss: 468.3584 - val_mae: 17.6523 - val_mse: 468.3301\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.9447 - mae: 18.6952 - mse: 554.9161 - val_loss: 444.3348 - val_mae: 16.5813 - val_mse: 444.3066\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.3765 - mae: 18.7454 - mse: 552.3483 - val_loss: 443.8726 - val_mae: 16.3019 - val_mse: 443.8443\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 554.6514 - mae: 18.8307 - mse: 554.6232 - val_loss: 445.9528 - val_mae: 16.6265 - val_mse: 445.9247\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.6981 - mae: 18.5037 - mse: 541.6702 - val_loss: 447.9279 - val_mae: 16.6896 - val_mse: 447.8999\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.2111 - mae: 18.9971 - mse: 563.1832 - val_loss: 461.0129 - val_mae: 17.3163 - val_mse: 460.9850\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.2296 - mae: 18.4802 - mse: 537.2016 - val_loss: 451.9321 - val_mae: 16.7779 - val_mse: 451.9043\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 545.4102 - mae: 18.5167 - mse: 545.3823 - val_loss: 450.6618 - val_mae: 16.8879 - val_mse: 450.6340\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.9033 - mae: 18.5972 - mse: 549.8754 - val_loss: 453.6886 - val_mae: 17.0070 - val_mse: 453.6610\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 540.7123 - mae: 18.5822 - mse: 540.6848 - val_loss: 449.1878 - val_mae: 16.9019 - val_mse: 449.1602\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.3462 - mae: 18.7006 - mse: 554.3185 - val_loss: 455.4106 - val_mae: 17.2079 - val_mse: 455.3833\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 532.4962 - mae: 18.4343 - mse: 532.4689 - val_loss: 450.0901 - val_mae: 16.8251 - val_mse: 450.0627\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.2378 - mae: 18.6675 - mse: 554.2100 - val_loss: 448.7404 - val_mae: 16.8534 - val_mse: 448.7130\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.6873 - mae: 18.6759 - mse: 551.6598 - val_loss: 448.8308 - val_mae: 16.6610 - val_mse: 448.8034\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.1431 - mae: 18.4528 - mse: 541.1159 - val_loss: 445.1296 - val_mae: 16.4802 - val_mse: 445.1022\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 540.7128 - mae: 18.4707 - mse: 540.6853 - val_loss: 445.0198 - val_mae: 16.6658 - val_mse: 444.9926\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.7227 - mae: 18.3567 - mse: 531.6957 - val_loss: 448.6711 - val_mae: 16.5588 - val_mse: 448.6440\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 546.0388 - mae: 18.6130 - mse: 546.0120 - val_loss: 450.4486 - val_mae: 16.8334 - val_mse: 450.4214\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.3688 - mae: 18.6023 - mse: 541.3419 - val_loss: 463.1728 - val_mae: 17.3695 - val_mse: 463.1460\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.6602 - mae: 18.3375 - mse: 531.6332 - val_loss: 453.2433 - val_mae: 16.7403 - val_mse: 453.2162\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.2695 - mae: 18.3542 - mse: 531.2424 - val_loss: 449.1778 - val_mae: 16.6752 - val_mse: 449.1509\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.4647 - mae: 18.2417 - mse: 523.4377 - val_loss: 451.6989 - val_mae: 16.9575 - val_mse: 451.6720\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 532.8738 - mae: 18.4115 - mse: 532.8469 - val_loss: 446.2023 - val_mae: 16.2796 - val_mse: 446.1753\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 535.1946 - mae: 18.3831 - mse: 535.1680 - val_loss: 462.7093 - val_mae: 17.3011 - val_mse: 462.6824\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 532.1636 - mae: 18.3388 - mse: 532.1364 - val_loss: 463.3605 - val_mae: 17.3013 - val_mse: 463.3337\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.8857 - mae: 18.1734 - mse: 523.8590 - val_loss: 444.7223 - val_mae: 16.3756 - val_mse: 444.6953\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.1050 - mae: 18.3985 - mse: 537.0780 - val_loss: 447.8806 - val_mae: 16.5097 - val_mse: 447.8539\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 520.1198 - mae: 18.1191 - mse: 520.0930 - val_loss: 449.3444 - val_mae: 16.8910 - val_mse: 449.3176\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 521.0432 - mae: 18.2171 - mse: 521.0162 - val_loss: 449.4455 - val_mae: 16.7898 - val_mse: 449.4187\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 513.4701 - mae: 18.0318 - mse: 513.4432 - val_loss: 442.2904 - val_mae: 16.3462 - val_mse: 442.2635\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 529.1407 - mae: 18.2482 - mse: 529.1139 - val_loss: 449.9357 - val_mae: 16.5094 - val_mse: 449.9088\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 511.7293 - mae: 17.9082 - mse: 511.7026 - val_loss: 452.4222 - val_mae: 16.6759 - val_mse: 452.3953\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.7408 - mae: 18.6112 - mse: 537.7142 - val_loss: 450.1567 - val_mae: 16.6173 - val_mse: 450.1298\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 519.2513 - mae: 18.1215 - mse: 519.2245 - val_loss: 453.3589 - val_mae: 16.9219 - val_mse: 453.3321\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 516.0361 - mae: 18.0082 - mse: 516.0093 - val_loss: 462.5218 - val_mae: 16.2331 - val_mse: 462.4947\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 522.7048 - mae: 18.1404 - mse: 522.6782 - val_loss: 451.0509 - val_mae: 16.6907 - val_mse: 451.0240\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 520.3738 - mae: 18.1436 - mse: 520.3472 - val_loss: 451.1312 - val_mae: 16.2449 - val_mse: 451.1043\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 518.8555 - mae: 18.0666 - mse: 518.8288 - val_loss: 444.8595 - val_mae: 16.4452 - val_mse: 444.8326\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 516.8596 - mae: 18.0048 - mse: 516.8328 - val_loss: 448.9940 - val_mae: 16.3061 - val_mse: 448.9671\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.5149 - mae: 17.9648 - mse: 512.4880 - val_loss: 447.1774 - val_mae: 16.5732 - val_mse: 447.1505\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 515.7722 - mae: 18.0081 - mse: 515.7451 - val_loss: 455.8147 - val_mae: 16.6678 - val_mse: 455.7877\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 522.4598 - mae: 18.1815 - mse: 522.4326 - val_loss: 443.8064 - val_mae: 16.4174 - val_mse: 443.7794\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 522.7246 - mae: 18.1501 - mse: 522.6976 - val_loss: 454.3087 - val_mae: 16.9292 - val_mse: 454.2819\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 524.6341 - mae: 18.2694 - mse: 524.6072 - val_loss: 450.9086 - val_mae: 16.7809 - val_mse: 450.8817\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 517.8517 - mae: 18.1772 - mse: 517.8246 - val_loss: 451.1758 - val_mae: 16.5276 - val_mse: 451.1487\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 503.9037 - mae: 17.9018 - mse: 503.8766 - val_loss: 460.1456 - val_mae: 16.4281 - val_mse: 460.1183\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 7.4308, Train MSE: 108.3524\n",
      "Val   MAE: 19.5009, Val   MSE: 680.3669\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 8.1341, Train MSE: 127.4221\n",
      "Val   MAE: 18.1137, Val   MSE: 578.7206\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 14.0596, Train MSE: 361.2997\n",
      "Val   MAE: 16.4281, Val   MSE: 460.1183\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 173.0271 - mae: 9.5167 - mse: 173.0271 - val_loss: 143.0209 - val_mae: 8.7535 - val_mse: 143.0209\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.3795 - mae: 8.5948 - mse: 140.3795 - val_loss: 140.3466 - val_mae: 8.7081 - val_mse: 140.3466\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.5347 - mae: 8.5605 - mse: 138.5347 - val_loss: 140.5827 - val_mae: 8.6551 - val_mse: 140.5827\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1174 - mae: 8.5069 - mse: 137.1174 - val_loss: 139.1542 - val_mae: 8.7391 - val_mse: 139.1542\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.1940 - mae: 8.5012 - mse: 136.1940 - val_loss: 140.8932 - val_mae: 8.5937 - val_mse: 140.8932\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.5046 - mae: 8.4423 - mse: 135.5046 - val_loss: 141.6653 - val_mae: 8.6074 - val_mse: 141.6653\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.2277 - mae: 8.4313 - mse: 135.2277 - val_loss: 139.9702 - val_mae: 8.6792 - val_mse: 139.9702\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4435 - mae: 8.4150 - mse: 134.4435 - val_loss: 140.3248 - val_mae: 8.6876 - val_mse: 140.3248\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6505 - mae: 8.3805 - mse: 133.6505 - val_loss: 140.2516 - val_mae: 8.6995 - val_mse: 140.2516\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.3628 - mae: 8.3747 - mse: 133.3628 - val_loss: 140.9760 - val_mae: 8.8482 - val_mse: 140.9760\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.2348 - mae: 8.3669 - mse: 132.2348 - val_loss: 141.5553 - val_mae: 8.7321 - val_mse: 141.5553\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.8664 - mae: 8.3648 - mse: 131.8664 - val_loss: 144.3671 - val_mae: 8.6369 - val_mse: 144.3671\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.1719 - mae: 8.3041 - mse: 131.1719 - val_loss: 142.2281 - val_mae: 8.6888 - val_mse: 142.2281\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.6427 - mae: 8.3091 - mse: 130.6427 - val_loss: 143.5618 - val_mae: 8.6687 - val_mse: 143.5618\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.4802 - mae: 8.2825 - mse: 129.4802 - val_loss: 144.1240 - val_mae: 8.7187 - val_mse: 144.1240\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.8703 - mae: 8.2211 - mse: 128.8703 - val_loss: 142.5458 - val_mae: 8.8611 - val_mse: 142.5458\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6128 - mae: 8.2024 - mse: 127.6128 - val_loss: 143.9353 - val_mae: 8.6715 - val_mse: 143.9353\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.6689 - mae: 8.1635 - mse: 126.6689 - val_loss: 143.0770 - val_mae: 8.7687 - val_mse: 143.0770\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.1148 - mae: 8.1277 - mse: 125.1148 - val_loss: 148.3418 - val_mae: 8.7692 - val_mse: 148.3418\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.8875 - mae: 8.1299 - mse: 124.8875 - val_loss: 147.3224 - val_mae: 8.7296 - val_mse: 147.3224\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.7552 - mae: 8.0963 - mse: 123.7552 - val_loss: 144.8333 - val_mae: 8.9474 - val_mse: 144.8333\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.2169 - mae: 8.0429 - mse: 122.2169 - val_loss: 144.2256 - val_mae: 8.8608 - val_mse: 144.2256\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.7229 - mae: 8.0247 - mse: 121.7229 - val_loss: 146.0359 - val_mae: 8.9247 - val_mse: 146.0359\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4500 - mae: 7.9993 - mse: 120.4500 - val_loss: 146.4178 - val_mae: 8.9385 - val_mse: 146.4178\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.3331 - mae: 7.9316 - mse: 118.3331 - val_loss: 149.2853 - val_mae: 8.9466 - val_mse: 149.2853\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.9796 - mae: 7.9446 - mse: 117.9796 - val_loss: 147.6824 - val_mae: 8.9046 - val_mse: 147.6824\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.8316 - mae: 7.8873 - mse: 116.8316 - val_loss: 153.8340 - val_mae: 8.9967 - val_mse: 153.8340\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.7855 - mae: 7.8243 - mse: 114.7855 - val_loss: 154.0764 - val_mae: 9.0998 - val_mse: 154.0764\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.2461 - mae: 7.8134 - mse: 114.2461 - val_loss: 150.5782 - val_mae: 8.9447 - val_mse: 150.5782\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.7212 - mae: 7.7607 - mse: 112.7212 - val_loss: 152.8512 - val_mae: 9.0534 - val_mse: 152.8512\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.1877 - mae: 7.7777 - mse: 112.1877 - val_loss: 154.9390 - val_mae: 9.1683 - val_mse: 154.9390\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4278 - mae: 7.7202 - mse: 110.4278 - val_loss: 153.0996 - val_mae: 9.0824 - val_mse: 153.0996\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.8467 - mae: 7.6580 - mse: 108.8467 - val_loss: 150.4980 - val_mae: 9.0292 - val_mse: 150.4980\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7494 - mae: 7.6468 - mse: 108.7494 - val_loss: 155.9817 - val_mae: 9.1583 - val_mse: 155.9817\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 106.8264 - mae: 7.5951 - mse: 106.8264 - val_loss: 159.8390 - val_mae: 9.2094 - val_mse: 159.8390\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 105.8573 - mae: 7.5698 - mse: 105.8573 - val_loss: 158.1673 - val_mae: 9.2044 - val_mse: 158.1673\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 105.3326 - mae: 7.5732 - mse: 105.3326 - val_loss: 159.3315 - val_mae: 9.3108 - val_mse: 159.3315\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 102.7592 - mae: 7.4529 - mse: 102.7592 - val_loss: 161.3333 - val_mae: 9.2858 - val_mse: 161.3333\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 102.2407 - mae: 7.4730 - mse: 102.2407 - val_loss: 160.4015 - val_mae: 9.3878 - val_mse: 160.4015\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 101.3043 - mae: 7.4186 - mse: 101.3043 - val_loss: 162.8857 - val_mae: 9.5650 - val_mse: 162.8857\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.6136 - mae: 7.3549 - mse: 99.6136 - val_loss: 162.4040 - val_mae: 9.4338 - val_mse: 162.4040\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.5985 - mae: 7.3424 - mse: 98.5985 - val_loss: 163.0058 - val_mae: 9.4077 - val_mse: 163.0058\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 98.0591 - mae: 7.3139 - mse: 98.0591 - val_loss: 164.1699 - val_mae: 9.5729 - val_mse: 164.1699\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 96.8811 - mae: 7.2610 - mse: 96.8811 - val_loss: 169.1530 - val_mae: 9.4563 - val_mse: 169.1530\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 96.0285 - mae: 7.2514 - mse: 96.0285 - val_loss: 167.4295 - val_mae: 9.6078 - val_mse: 167.4295\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.6136 - mae: 7.2222 - mse: 95.6136 - val_loss: 165.9786 - val_mae: 9.6688 - val_mse: 165.9786\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.7367 - mae: 7.1814 - mse: 93.7367 - val_loss: 167.3526 - val_mae: 9.5989 - val_mse: 167.3526\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 92.9152 - mae: 7.0993 - mse: 92.9152 - val_loss: 171.3071 - val_mae: 9.9107 - val_mse: 171.3071\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 92.4303 - mae: 7.1572 - mse: 92.4303 - val_loss: 166.0521 - val_mae: 9.6467 - val_mse: 166.0521\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 90.8870 - mae: 7.0670 - mse: 90.8870 - val_loss: 170.7605 - val_mae: 9.7544 - val_mse: 170.7605\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 90.4413 - mae: 7.0279 - mse: 90.4413 - val_loss: 171.5852 - val_mae: 9.8947 - val_mse: 171.5852\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.2523 - mae: 7.0053 - mse: 89.2523 - val_loss: 172.7643 - val_mae: 9.7486 - val_mse: 172.7643\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 88.4860 - mae: 7.0047 - mse: 88.4860 - val_loss: 172.2258 - val_mae: 9.6063 - val_mse: 172.2258\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 87.5900 - mae: 6.9551 - mse: 87.5900 - val_loss: 172.7149 - val_mae: 9.7244 - val_mse: 172.7149\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 86.3949 - mae: 6.9006 - mse: 86.3949 - val_loss: 177.4499 - val_mae: 9.7664 - val_mse: 177.4499\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 87.8041 - mae: 6.9594 - mse: 87.8041 - val_loss: 173.9659 - val_mae: 9.7900 - val_mse: 173.9659\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 85.2173 - mae: 6.8451 - mse: 85.2173 - val_loss: 176.2816 - val_mae: 10.0280 - val_mse: 176.2816\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 84.4084 - mae: 6.8136 - mse: 84.4084 - val_loss: 176.9351 - val_mae: 9.8086 - val_mse: 176.9351\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 83.8992 - mae: 6.8282 - mse: 83.8992 - val_loss: 179.2179 - val_mae: 10.0108 - val_mse: 179.2179\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.8393 - mae: 6.7637 - mse: 82.8393 - val_loss: 176.6071 - val_mae: 9.9789 - val_mse: 176.6071\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 81.2527 - mae: 6.7089 - mse: 81.2527 - val_loss: 184.3745 - val_mae: 9.9541 - val_mse: 184.3745\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.6619 - mae: 6.6745 - mse: 80.6619 - val_loss: 180.0178 - val_mae: 10.1151 - val_mse: 180.0178\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 79.6751 - mae: 6.6708 - mse: 79.6751 - val_loss: 182.8744 - val_mae: 10.1175 - val_mse: 182.8744\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 79.3098 - mae: 6.6668 - mse: 79.3098 - val_loss: 184.3798 - val_mae: 10.1510 - val_mse: 184.3798\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 77.9786 - mae: 6.5830 - mse: 77.9786 - val_loss: 179.7539 - val_mae: 9.9218 - val_mse: 179.7539\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 77.9072 - mae: 6.5605 - mse: 77.9072 - val_loss: 184.5873 - val_mae: 10.1276 - val_mse: 184.5873\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 77.7325 - mae: 6.5800 - mse: 77.7325 - val_loss: 191.8681 - val_mae: 10.1499 - val_mse: 191.8681\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 76.4044 - mae: 6.5091 - mse: 76.4044 - val_loss: 192.9452 - val_mae: 10.3327 - val_mse: 192.9452\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 76.1604 - mae: 6.4935 - mse: 76.1604 - val_loss: 193.3440 - val_mae: 10.5964 - val_mse: 193.3440\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 74.3494 - mae: 6.4563 - mse: 74.3494 - val_loss: 195.2757 - val_mae: 10.2592 - val_mse: 195.2757\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 74.6789 - mae: 6.4524 - mse: 74.6789 - val_loss: 189.0131 - val_mae: 10.3401 - val_mse: 189.0131\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 74.3994 - mae: 6.4249 - mse: 74.3994 - val_loss: 188.0287 - val_mae: 10.2762 - val_mse: 188.0287\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.7613 - mae: 6.4364 - mse: 73.7613 - val_loss: 185.1457 - val_mae: 10.1285 - val_mse: 185.1457\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.5493 - mae: 6.4098 - mse: 73.5493 - val_loss: 186.9588 - val_mae: 10.1350 - val_mse: 186.9588\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.1219 - mae: 6.3201 - mse: 71.1219 - val_loss: 189.9112 - val_mae: 10.2397 - val_mse: 189.9112\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.2353 - mae: 6.3046 - mse: 71.2353 - val_loss: 196.5649 - val_mae: 10.6127 - val_mse: 196.5649\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 69.7137 - mae: 6.2314 - mse: 69.7137 - val_loss: 195.5271 - val_mae: 10.4921 - val_mse: 195.5271\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 69.4945 - mae: 6.2460 - mse: 69.4945 - val_loss: 196.8690 - val_mae: 10.5811 - val_mse: 196.8690\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 70.9643 - mae: 6.3329 - mse: 70.9643 - val_loss: 196.3426 - val_mae: 10.4862 - val_mse: 196.3426\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.4150 - mae: 6.1559 - mse: 67.4150 - val_loss: 192.0762 - val_mae: 10.2998 - val_mse: 192.0762\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 66.5680 - mae: 6.1380 - mse: 66.5680 - val_loss: 192.5975 - val_mae: 10.2151 - val_mse: 192.5975\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.1228 - mae: 6.0763 - mse: 66.1228 - val_loss: 197.6806 - val_mae: 10.5936 - val_mse: 197.6806\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 65.6923 - mae: 6.1098 - mse: 65.6923 - val_loss: 197.1839 - val_mae: 10.4056 - val_mse: 197.1839\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.1066 - mae: 6.1024 - mse: 66.1066 - val_loss: 197.6765 - val_mae: 10.5084 - val_mse: 197.6765\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.4368 - mae: 6.0431 - mse: 64.4368 - val_loss: 195.0496 - val_mae: 10.3643 - val_mse: 195.0496\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.0991 - mae: 6.0230 - mse: 64.0991 - val_loss: 196.8986 - val_mae: 10.5272 - val_mse: 196.8986\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.9921 - mae: 6.0332 - mse: 63.9921 - val_loss: 193.0467 - val_mae: 10.4602 - val_mse: 193.0467\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 62.8610 - mae: 5.9817 - mse: 62.8610 - val_loss: 196.9140 - val_mae: 10.4610 - val_mse: 196.9140\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.0276 - mae: 5.9872 - mse: 63.0276 - val_loss: 199.2178 - val_mae: 10.5045 - val_mse: 199.2178\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 61.1565 - mae: 5.9106 - mse: 61.1565 - val_loss: 196.0818 - val_mae: 10.3881 - val_mse: 196.0818\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.6505 - mae: 5.8773 - mse: 60.6505 - val_loss: 197.7093 - val_mae: 10.5435 - val_mse: 197.7093\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.9590 - mae: 5.8861 - mse: 60.9590 - val_loss: 199.3451 - val_mae: 10.5369 - val_mse: 199.3451\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 59.7455 - mae: 5.8188 - mse: 59.7455 - val_loss: 198.0278 - val_mae: 10.5172 - val_mse: 198.0278\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.0412 - mae: 5.8076 - mse: 60.0412 - val_loss: 212.0776 - val_mae: 11.0094 - val_mse: 212.0776\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.6573 - mae: 5.7863 - mse: 58.6573 - val_loss: 200.0991 - val_mae: 10.6127 - val_mse: 200.0991\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.1178 - mae: 5.7647 - mse: 58.1178 - val_loss: 208.2575 - val_mae: 10.8694 - val_mse: 208.2575\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.8605 - mae: 5.8091 - mse: 58.8605 - val_loss: 207.5718 - val_mae: 10.8667 - val_mse: 207.5718\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.1065 - mae: 5.7437 - mse: 58.1065 - val_loss: 214.0897 - val_mae: 11.0243 - val_mse: 214.0897\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.5711 - mae: 5.7714 - mse: 58.5711 - val_loss: 196.9500 - val_mae: 10.5143 - val_mse: 196.9500\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.9940 - mae: 5.6879 - mse: 56.9940 - val_loss: 200.4673 - val_mae: 10.5364 - val_mse: 200.4673\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 230.9191 - mae: 10.9147 - mse: 230.9191 - val_loss: 223.2374 - val_mae: 10.8990 - val_mse: 223.2374\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 156.9523 - mae: 8.7983 - mse: 156.9523 - val_loss: 151.2193 - val_mae: 8.7141 - val_mse: 151.2193\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9824 - mae: 8.3225 - mse: 135.9824 - val_loss: 143.6171 - val_mae: 8.5982 - val_mse: 143.6171\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.1593 - mae: 8.3107 - mse: 132.1593 - val_loss: 140.5015 - val_mae: 8.6541 - val_mse: 140.5015\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.8289 - mae: 8.3266 - mse: 130.8289 - val_loss: 139.0056 - val_mae: 8.6276 - val_mse: 139.0056\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9548 - mae: 8.3062 - mse: 129.9548 - val_loss: 139.6539 - val_mae: 8.7659 - val_mse: 139.6539\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 128.7274 - mae: 8.3052 - mse: 128.7274 - val_loss: 140.8854 - val_mae: 8.7436 - val_mse: 140.8854\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6976 - mae: 8.2370 - mse: 127.6976 - val_loss: 140.3816 - val_mae: 8.7771 - val_mse: 140.3816\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4793 - mae: 8.2327 - mse: 127.4793 - val_loss: 139.4449 - val_mae: 8.7999 - val_mse: 139.4449\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.0869 - mae: 8.2182 - mse: 126.0869 - val_loss: 141.1482 - val_mae: 8.8174 - val_mse: 141.1482\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.6170 - mae: 8.1961 - mse: 125.6170 - val_loss: 140.7751 - val_mae: 8.8186 - val_mse: 140.7751\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1001 - mae: 8.2003 - mse: 124.1001 - val_loss: 142.2218 - val_mae: 8.8130 - val_mse: 142.2218\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.0058 - mae: 8.1470 - mse: 123.0058 - val_loss: 141.2489 - val_mae: 8.8312 - val_mse: 141.2489\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.5712 - mae: 8.0944 - mse: 122.5712 - val_loss: 141.7751 - val_mae: 8.7846 - val_mse: 141.7751\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.5482 - mae: 8.1169 - mse: 121.5482 - val_loss: 144.6199 - val_mae: 8.9325 - val_mse: 144.6199\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.3829 - mae: 8.1020 - mse: 121.3829 - val_loss: 142.6219 - val_mae: 8.9208 - val_mse: 142.6219\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6953 - mae: 8.0962 - mse: 120.6953 - val_loss: 144.3347 - val_mae: 8.8466 - val_mse: 144.3347\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.0872 - mae: 8.0569 - mse: 120.0872 - val_loss: 143.1688 - val_mae: 8.7883 - val_mse: 143.1688\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.0471 - mae: 8.0329 - mse: 119.0471 - val_loss: 145.0483 - val_mae: 8.9680 - val_mse: 145.0483\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.7465 - mae: 8.0076 - mse: 118.7465 - val_loss: 144.7223 - val_mae: 8.9347 - val_mse: 144.7223\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.2935 - mae: 8.0132 - mse: 117.2935 - val_loss: 147.2758 - val_mae: 9.0655 - val_mse: 147.2758\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.7014 - mae: 7.9607 - mse: 117.7014 - val_loss: 147.3468 - val_mae: 8.9969 - val_mse: 147.3468\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.9925 - mae: 7.9340 - mse: 116.9925 - val_loss: 147.8961 - val_mae: 9.0784 - val_mse: 147.8961\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.3493 - mae: 7.9500 - mse: 115.3493 - val_loss: 149.2041 - val_mae: 8.9155 - val_mse: 149.2041\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.7431 - mae: 7.9085 - mse: 115.7431 - val_loss: 149.1875 - val_mae: 8.9411 - val_mse: 149.1875\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6854 - mae: 7.9036 - mse: 113.6854 - val_loss: 152.4071 - val_mae: 9.0895 - val_mse: 152.4071\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6900 - mae: 7.8692 - mse: 113.6900 - val_loss: 151.4642 - val_mae: 9.0725 - val_mse: 151.4642\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.6623 - mae: 7.8339 - mse: 112.6623 - val_loss: 147.6328 - val_mae: 8.9634 - val_mse: 147.6328\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.7088 - mae: 7.7983 - mse: 111.7088 - val_loss: 149.1687 - val_mae: 9.1448 - val_mse: 149.1687\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.4418 - mae: 7.8164 - mse: 111.4418 - val_loss: 152.0806 - val_mae: 9.0834 - val_mse: 152.0806\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.5406 - mae: 7.7780 - mse: 111.5406 - val_loss: 150.5909 - val_mae: 8.9750 - val_mse: 150.5909\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.4422 - mae: 7.7529 - mse: 109.4422 - val_loss: 155.7330 - val_mae: 9.2821 - val_mse: 155.7330\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.6630 - mae: 7.7478 - mse: 109.6630 - val_loss: 152.1289 - val_mae: 9.2332 - val_mse: 152.1289\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.8417 - mae: 7.7630 - mse: 109.8417 - val_loss: 152.2545 - val_mae: 9.1043 - val_mse: 152.2545\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.6134 - mae: 7.7626 - mse: 109.6134 - val_loss: 153.6743 - val_mae: 9.0572 - val_mse: 153.6743\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.1941 - mae: 7.6799 - mse: 108.1941 - val_loss: 154.6873 - val_mae: 9.0587 - val_mse: 154.6873\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9760 - mae: 7.7526 - mse: 107.9760 - val_loss: 153.9307 - val_mae: 9.2719 - val_mse: 153.9307\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.8381 - mae: 7.6886 - mse: 105.8381 - val_loss: 150.2641 - val_mae: 9.1121 - val_mse: 150.2641\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.5571 - mae: 7.6613 - mse: 106.5571 - val_loss: 155.7375 - val_mae: 9.2488 - val_mse: 155.7375\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.6983 - mae: 7.6461 - mse: 105.6983 - val_loss: 155.9593 - val_mae: 9.1546 - val_mse: 155.9593\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.3185 - mae: 7.6656 - mse: 105.3185 - val_loss: 155.7313 - val_mae: 9.2134 - val_mse: 155.7313\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.7082 - mae: 7.6372 - mse: 105.7082 - val_loss: 156.7016 - val_mae: 9.1528 - val_mse: 156.7016\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6741 - mae: 7.5448 - mse: 103.6741 - val_loss: 156.1550 - val_mae: 9.2150 - val_mse: 156.1550\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.7333 - mae: 7.5568 - mse: 103.7333 - val_loss: 155.8993 - val_mae: 9.3328 - val_mse: 155.8993\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.1514 - mae: 7.5828 - mse: 104.1514 - val_loss: 157.5819 - val_mae: 9.3330 - val_mse: 157.5819\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.5943 - mae: 7.5205 - mse: 102.5943 - val_loss: 159.6867 - val_mae: 9.3584 - val_mse: 159.6867\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.3104 - mae: 7.5315 - mse: 101.3104 - val_loss: 160.2397 - val_mae: 9.5580 - val_mse: 160.2397\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.5483 - mae: 7.5195 - mse: 103.5483 - val_loss: 159.2206 - val_mae: 9.3518 - val_mse: 159.2206\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.6424 - mae: 7.4154 - mse: 99.6424 - val_loss: 162.6225 - val_mae: 9.3321 - val_mse: 162.6225\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.2913 - mae: 7.5009 - mse: 102.2913 - val_loss: 159.8032 - val_mae: 9.2934 - val_mse: 159.8032\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.5160 - mae: 7.4190 - mse: 99.5160 - val_loss: 163.7370 - val_mae: 9.5359 - val_mse: 163.7370\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.5725 - mae: 7.4719 - mse: 100.5725 - val_loss: 161.2011 - val_mae: 9.4444 - val_mse: 161.2011\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.2629 - mae: 7.4230 - mse: 99.2629 - val_loss: 161.6209 - val_mae: 9.4630 - val_mse: 161.6209\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.9980 - mae: 7.4086 - mse: 97.9980 - val_loss: 163.8878 - val_mae: 9.3915 - val_mse: 163.8878\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.8078 - mae: 7.3603 - mse: 97.8078 - val_loss: 163.9520 - val_mae: 9.3770 - val_mse: 163.9520\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.0503 - mae: 7.3573 - mse: 98.0503 - val_loss: 160.0842 - val_mae: 9.2473 - val_mse: 160.0842\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.7150 - mae: 7.4074 - mse: 98.7150 - val_loss: 160.9028 - val_mae: 9.3023 - val_mse: 160.9028\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.2335 - mae: 7.3337 - mse: 97.2335 - val_loss: 160.2729 - val_mae: 9.3443 - val_mse: 160.2729\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.0151 - mae: 7.4031 - mse: 98.0151 - val_loss: 160.8282 - val_mae: 9.3792 - val_mse: 160.8282\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.5282 - mae: 7.4474 - mse: 99.5282 - val_loss: 162.8835 - val_mae: 9.3677 - val_mse: 162.8835\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.7630 - mae: 7.2914 - mse: 95.7630 - val_loss: 161.8911 - val_mae: 9.3099 - val_mse: 161.8911\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.5101 - mae: 7.3462 - mse: 96.5101 - val_loss: 165.5258 - val_mae: 9.3946 - val_mse: 165.5258\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.9296 - mae: 7.2347 - mse: 94.9296 - val_loss: 161.2593 - val_mae: 9.3169 - val_mse: 161.2593\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.5293 - mae: 7.2796 - mse: 95.5293 - val_loss: 166.5282 - val_mae: 9.5288 - val_mse: 166.5282\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.4981 - mae: 7.2742 - mse: 95.4981 - val_loss: 165.7332 - val_mae: 9.5225 - val_mse: 165.7332\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.5163 - mae: 7.3167 - mse: 95.5163 - val_loss: 167.5124 - val_mae: 9.4417 - val_mse: 167.5124\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.9689 - mae: 7.2535 - mse: 93.9689 - val_loss: 164.0140 - val_mae: 9.3322 - val_mse: 164.0140\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.8254 - mae: 7.2661 - mse: 93.8254 - val_loss: 167.6078 - val_mae: 9.4992 - val_mse: 167.6078\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.5991 - mae: 7.2010 - mse: 92.5991 - val_loss: 172.5078 - val_mae: 9.6968 - val_mse: 172.5078\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.1350 - mae: 7.2743 - mse: 94.1350 - val_loss: 169.0264 - val_mae: 9.5347 - val_mse: 169.0264\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.2172 - mae: 7.1845 - mse: 93.2172 - val_loss: 165.6279 - val_mae: 9.4453 - val_mse: 165.6279\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.6309 - mae: 7.2045 - mse: 91.6309 - val_loss: 164.9106 - val_mae: 9.5167 - val_mse: 164.9106\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.8382 - mae: 7.1684 - mse: 91.8382 - val_loss: 165.8899 - val_mae: 9.4985 - val_mse: 165.8899\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.6512 - mae: 7.2072 - mse: 92.6512 - val_loss: 165.4980 - val_mae: 9.5069 - val_mse: 165.4980\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.6241 - mae: 7.2088 - mse: 92.6241 - val_loss: 167.2214 - val_mae: 9.5109 - val_mse: 167.2214\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.7593 - mae: 7.1630 - mse: 90.7593 - val_loss: 166.8267 - val_mae: 9.5293 - val_mse: 166.8267\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.0104 - mae: 7.2027 - mse: 93.0104 - val_loss: 165.2652 - val_mae: 9.4648 - val_mse: 165.2652\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.7983 - mae: 7.1838 - mse: 91.7983 - val_loss: 168.3728 - val_mae: 9.5887 - val_mse: 168.3728\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.0396 - mae: 7.1177 - mse: 90.0396 - val_loss: 168.0331 - val_mae: 9.5192 - val_mse: 168.0331\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.7027 - mae: 7.1684 - mse: 90.7027 - val_loss: 168.3611 - val_mae: 9.5546 - val_mse: 168.3611\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.3769 - mae: 7.0426 - mse: 88.3769 - val_loss: 166.8371 - val_mae: 9.5726 - val_mse: 166.8371\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.7913 - mae: 7.1001 - mse: 88.7913 - val_loss: 172.4809 - val_mae: 9.8325 - val_mse: 172.4809\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.9188 - mae: 7.1039 - mse: 89.9188 - val_loss: 172.6153 - val_mae: 9.5696 - val_mse: 172.6153\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.1882 - mae: 7.0504 - mse: 89.1882 - val_loss: 174.9415 - val_mae: 9.7317 - val_mse: 174.9415\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.7410 - mae: 7.0781 - mse: 89.7410 - val_loss: 174.9747 - val_mae: 9.6555 - val_mse: 174.9747\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.1890 - mae: 7.1128 - mse: 89.1890 - val_loss: 174.4090 - val_mae: 9.7352 - val_mse: 174.4090\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.3940 - mae: 7.0817 - mse: 88.3940 - val_loss: 173.3007 - val_mae: 9.6323 - val_mse: 173.3007\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.3134 - mae: 7.0404 - mse: 88.3134 - val_loss: 172.0357 - val_mae: 9.6821 - val_mse: 172.0357\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.7001 - mae: 7.0245 - mse: 86.7001 - val_loss: 174.4039 - val_mae: 9.7777 - val_mse: 174.4039\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 85.1833 - mae: 6.9169 - mse: 85.1833 - val_loss: 173.4770 - val_mae: 9.6087 - val_mse: 173.4770\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.3186 - mae: 7.0617 - mse: 88.3186 - val_loss: 171.4287 - val_mae: 9.5650 - val_mse: 171.4287\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.2140 - mae: 7.0358 - mse: 87.2140 - val_loss: 172.7524 - val_mae: 9.8007 - val_mse: 172.7524\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.6764 - mae: 6.9648 - mse: 86.6764 - val_loss: 171.4194 - val_mae: 9.6121 - val_mse: 171.4194\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.0094 - mae: 6.9828 - mse: 87.0094 - val_loss: 176.3474 - val_mae: 9.8071 - val_mse: 176.3474\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.2426 - mae: 6.9685 - mse: 86.2426 - val_loss: 174.0805 - val_mae: 9.7737 - val_mse: 174.0805\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 85.8132 - mae: 6.9552 - mse: 85.8132 - val_loss: 171.1558 - val_mae: 9.5964 - val_mse: 171.1558\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.7195 - mae: 6.9142 - mse: 84.7195 - val_loss: 171.2355 - val_mae: 9.5994 - val_mse: 171.2355\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.8916 - mae: 7.0856 - mse: 87.8916 - val_loss: 177.7919 - val_mae: 9.7368 - val_mse: 177.7919\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.5992 - mae: 6.9824 - mse: 87.5992 - val_loss: 174.5731 - val_mae: 9.6634 - val_mse: 174.5731\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.3826 - mae: 7.0148 - mse: 87.3826 - val_loss: 173.9546 - val_mae: 9.6900 - val_mse: 173.9546\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 183.3255 - mae: 9.8088 - mse: 183.3093 - val_loss: 149.9002 - val_mae: 8.7986 - val_mse: 149.8839\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 152.8785 - mae: 8.9936 - mse: 152.8620 - val_loss: 144.4063 - val_mae: 8.6695 - val_mse: 144.3899\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 148.9977 - mae: 8.8929 - mse: 148.9812 - val_loss: 146.3103 - val_mae: 8.6466 - val_mse: 146.2940\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 147.1469 - mae: 8.8556 - mse: 147.1305 - val_loss: 143.1593 - val_mae: 8.6153 - val_mse: 143.1429\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.1480 - mae: 8.8299 - mse: 147.1316 - val_loss: 142.7018 - val_mae: 8.5824 - val_mse: 142.6854\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 146.5077 - mae: 8.7636 - mse: 146.4913 - val_loss: 140.3496 - val_mae: 8.5752 - val_mse: 140.3332\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 143.7296 - mae: 8.6707 - mse: 143.7134 - val_loss: 140.7499 - val_mae: 8.5553 - val_mse: 140.7336\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.3887 - mae: 8.6979 - mse: 142.3723 - val_loss: 143.5829 - val_mae: 8.5600 - val_mse: 143.5665\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 141.4878 - mae: 8.6812 - mse: 141.4715 - val_loss: 143.0710 - val_mae: 8.5490 - val_mse: 143.0547\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.0772 - mae: 8.6191 - mse: 141.0608 - val_loss: 140.2198 - val_mae: 8.5665 - val_mse: 140.2035\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5509 - mae: 8.5890 - mse: 140.5346 - val_loss: 139.7846 - val_mae: 8.5628 - val_mse: 139.7681\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.9668 - mae: 8.6904 - mse: 142.9505 - val_loss: 146.7622 - val_mae: 8.5963 - val_mse: 146.7460\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.2543 - mae: 8.5703 - mse: 140.2382 - val_loss: 139.5465 - val_mae: 8.5491 - val_mse: 139.5303\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.7357 - mae: 8.6516 - mse: 141.7195 - val_loss: 139.7637 - val_mae: 8.5401 - val_mse: 139.7475\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 138.8007 - mae: 8.5689 - mse: 138.7844 - val_loss: 141.1973 - val_mae: 8.5405 - val_mse: 141.1811\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.2412 - mae: 8.5786 - mse: 140.2250 - val_loss: 142.7302 - val_mae: 8.5608 - val_mse: 142.7140\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.2189 - mae: 8.5447 - mse: 139.2027 - val_loss: 141.3053 - val_mae: 8.5626 - val_mse: 141.2891\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.9012 - mae: 8.5725 - mse: 138.8850 - val_loss: 140.3501 - val_mae: 8.5526 - val_mse: 140.3338\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.0336 - mae: 8.6291 - mse: 140.0175 - val_loss: 141.9281 - val_mae: 8.5452 - val_mse: 141.9119\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.8345 - mae: 8.5625 - mse: 139.8183 - val_loss: 138.6757 - val_mae: 8.5786 - val_mse: 138.6595\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2046 - mae: 8.5581 - mse: 139.1884 - val_loss: 144.1160 - val_mae: 8.5622 - val_mse: 144.0998\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.5835 - mae: 8.5685 - mse: 139.5673 - val_loss: 140.8939 - val_mae: 8.5493 - val_mse: 140.8778\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.8687 - mae: 8.6037 - mse: 139.8525 - val_loss: 139.0344 - val_mae: 8.5691 - val_mse: 139.0182\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2001 - mae: 8.5975 - mse: 139.1838 - val_loss: 143.0413 - val_mae: 8.5649 - val_mse: 143.0251\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1230 - mae: 8.5262 - mse: 138.1066 - val_loss: 139.7518 - val_mae: 8.5472 - val_mse: 139.7355\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.3162 - mae: 8.5743 - mse: 138.2998 - val_loss: 144.2964 - val_mae: 8.5731 - val_mse: 144.2802\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.5603 - mae: 8.5323 - mse: 137.5439 - val_loss: 142.6039 - val_mae: 8.5513 - val_mse: 142.5876\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1512 - mae: 8.4989 - mse: 137.1348 - val_loss: 142.3635 - val_mae: 8.5593 - val_mse: 142.3472\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.8510 - mae: 8.5034 - mse: 136.8345 - val_loss: 141.2697 - val_mae: 8.5497 - val_mse: 141.2533\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2336 - mae: 8.4901 - mse: 136.2172 - val_loss: 139.7577 - val_mae: 8.5622 - val_mse: 139.7412\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9258 - mae: 8.5554 - mse: 137.9093 - val_loss: 140.4651 - val_mae: 8.5361 - val_mse: 140.4486\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.5768 - mae: 8.4951 - mse: 137.5603 - val_loss: 141.3430 - val_mae: 8.5569 - val_mse: 141.3264\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.9620 - mae: 8.4920 - mse: 135.9456 - val_loss: 141.9067 - val_mae: 8.5757 - val_mse: 141.8902\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.0229 - mae: 8.4981 - mse: 137.0063 - val_loss: 141.9733 - val_mae: 8.5736 - val_mse: 141.9567\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1289 - mae: 8.5170 - mse: 137.1123 - val_loss: 142.7853 - val_mae: 8.5789 - val_mse: 142.7686\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.2704 - mae: 8.5096 - mse: 137.2537 - val_loss: 145.3314 - val_mae: 8.5974 - val_mse: 145.3148\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.9423 - mae: 8.4833 - mse: 136.9255 - val_loss: 140.9961 - val_mae: 8.5702 - val_mse: 140.9792\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.1815 - mae: 8.4486 - mse: 136.1647 - val_loss: 141.3851 - val_mae: 8.5724 - val_mse: 141.3683\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.6523 - mae: 8.4911 - mse: 135.6354 - val_loss: 142.5217 - val_mae: 8.5871 - val_mse: 142.5048\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5553 - mae: 8.4607 - mse: 135.5383 - val_loss: 144.4676 - val_mae: 8.5839 - val_mse: 144.4506\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1452 - mae: 8.4924 - mse: 135.1281 - val_loss: 143.6870 - val_mae: 8.5800 - val_mse: 143.6699\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.3440 - mae: 8.4919 - mse: 137.3268 - val_loss: 143.7261 - val_mae: 8.5643 - val_mse: 143.7090\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.9089 - mae: 8.4779 - mse: 135.8916 - val_loss: 144.4938 - val_mae: 8.5723 - val_mse: 144.4766\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.4301 - mae: 8.4351 - mse: 136.4128 - val_loss: 140.8745 - val_mae: 8.6274 - val_mse: 140.8571\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.3741 - mae: 8.4757 - mse: 135.3567 - val_loss: 142.6200 - val_mae: 8.5766 - val_mse: 142.6026\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.5674 - mae: 8.4643 - mse: 135.5499 - val_loss: 143.0523 - val_mae: 8.5628 - val_mse: 143.0348\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4725 - mae: 8.4624 - mse: 135.4550 - val_loss: 142.4402 - val_mae: 8.5650 - val_mse: 142.4226\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8279 - mae: 8.4882 - mse: 135.8103 - val_loss: 144.4825 - val_mae: 8.5773 - val_mse: 144.4649\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3378 - mae: 8.4238 - mse: 134.3201 - val_loss: 143.3517 - val_mae: 8.5869 - val_mse: 143.3340\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9702 - mae: 8.4109 - mse: 133.9524 - val_loss: 144.2868 - val_mae: 8.5773 - val_mse: 144.2689\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7982 - mae: 8.4290 - mse: 135.7802 - val_loss: 142.9539 - val_mae: 8.5885 - val_mse: 142.9359\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4635 - mae: 8.4576 - mse: 134.4454 - val_loss: 144.1449 - val_mae: 8.5900 - val_mse: 144.1267\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5332 - mae: 8.5074 - mse: 136.5150 - val_loss: 144.7964 - val_mae: 8.5860 - val_mse: 144.7782\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9383 - mae: 8.4066 - mse: 133.9200 - val_loss: 143.3301 - val_mae: 8.5834 - val_mse: 143.3117\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8320 - mae: 8.4548 - mse: 135.8136 - val_loss: 143.5281 - val_mae: 8.5884 - val_mse: 143.5095\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.5649 - mae: 8.3994 - mse: 133.5462 - val_loss: 144.1359 - val_mae: 8.5964 - val_mse: 144.1172\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.3186 - mae: 8.4295 - mse: 134.2998 - val_loss: 142.3152 - val_mae: 8.5940 - val_mse: 142.2964\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.9770 - mae: 8.4501 - mse: 133.9582 - val_loss: 142.3402 - val_mae: 8.6356 - val_mse: 142.3212\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.6334 - mae: 8.4768 - mse: 134.6145 - val_loss: 143.3130 - val_mae: 8.5781 - val_mse: 143.2940\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.1615 - mae: 8.4329 - mse: 134.1424 - val_loss: 144.7891 - val_mae: 8.5928 - val_mse: 144.7700\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.2147 - mae: 8.4683 - mse: 135.1956 - val_loss: 142.9332 - val_mae: 8.5946 - val_mse: 142.9140\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.9707 - mae: 8.4082 - mse: 132.9514 - val_loss: 143.5680 - val_mae: 8.5788 - val_mse: 143.5487\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.0841 - mae: 8.4565 - mse: 134.0646 - val_loss: 144.4871 - val_mae: 8.5872 - val_mse: 144.4677\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.6066 - mae: 8.4510 - mse: 134.5871 - val_loss: 141.8581 - val_mae: 8.5897 - val_mse: 141.8384\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6505 - mae: 8.4243 - mse: 133.6308 - val_loss: 142.4615 - val_mae: 8.5693 - val_mse: 142.4418\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.4928 - mae: 8.4474 - mse: 133.4730 - val_loss: 143.4640 - val_mae: 8.5807 - val_mse: 143.4441\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.2602 - mae: 8.4188 - mse: 133.2403 - val_loss: 142.7008 - val_mae: 8.5998 - val_mse: 142.6807\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.0688 - mae: 8.4399 - mse: 133.0487 - val_loss: 142.3438 - val_mae: 8.5992 - val_mse: 142.3235\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.0629 - mae: 8.4617 - mse: 133.0426 - val_loss: 142.8803 - val_mae: 8.5748 - val_mse: 142.8598\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.8190 - mae: 8.4313 - mse: 133.7985 - val_loss: 143.8816 - val_mae: 8.5972 - val_mse: 143.8610\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.1977 - mae: 8.4202 - mse: 132.1771 - val_loss: 142.9684 - val_mae: 8.5972 - val_mse: 142.9476\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.7330 - mae: 8.4272 - mse: 133.7122 - val_loss: 144.6925 - val_mae: 8.5914 - val_mse: 144.6716\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.2258 - mae: 8.4212 - mse: 132.2048 - val_loss: 142.9574 - val_mae: 8.5718 - val_mse: 142.9362\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5891 - mae: 8.3935 - mse: 131.5677 - val_loss: 143.5703 - val_mae: 8.5974 - val_mse: 143.5489\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.3946 - mae: 8.3996 - mse: 131.3732 - val_loss: 144.4570 - val_mae: 8.6005 - val_mse: 144.4355\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3014 - mae: 8.4428 - mse: 133.2798 - val_loss: 143.5554 - val_mae: 8.6062 - val_mse: 143.5337\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.6789 - mae: 8.4088 - mse: 132.6572 - val_loss: 144.6945 - val_mae: 8.5847 - val_mse: 144.6727\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.7029 - mae: 8.4031 - mse: 132.6809 - val_loss: 144.5328 - val_mae: 8.5932 - val_mse: 144.5107\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.9331 - mae: 8.4235 - mse: 133.9109 - val_loss: 144.4895 - val_mae: 8.5883 - val_mse: 144.4673\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.4503 - mae: 8.4359 - mse: 132.4280 - val_loss: 144.3537 - val_mae: 8.5855 - val_mse: 144.3313\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.8871 - mae: 8.3990 - mse: 131.8645 - val_loss: 144.5784 - val_mae: 8.5958 - val_mse: 144.5558\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.4138 - mae: 8.3745 - mse: 131.3909 - val_loss: 144.8569 - val_mae: 8.6136 - val_mse: 144.8340\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.7187 - mae: 8.3771 - mse: 131.6957 - val_loss: 144.2223 - val_mae: 8.6227 - val_mse: 144.1991\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3433 - mae: 8.4115 - mse: 132.3200 - val_loss: 146.0655 - val_mae: 8.6082 - val_mse: 146.0421\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.5187 - mae: 8.3760 - mse: 130.4951 - val_loss: 144.5741 - val_mae: 8.5998 - val_mse: 144.5504\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.2735 - mae: 8.3999 - mse: 132.2497 - val_loss: 144.7532 - val_mae: 8.5970 - val_mse: 144.7294\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7874 - mae: 8.3517 - mse: 131.7634 - val_loss: 144.1888 - val_mae: 8.6175 - val_mse: 144.1648\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.4880 - mae: 8.3569 - mse: 130.4638 - val_loss: 142.9981 - val_mae: 8.6884 - val_mse: 142.9736\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.8810 - mae: 8.3823 - mse: 130.8565 - val_loss: 143.8848 - val_mae: 8.6227 - val_mse: 143.8601\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.1907 - mae: 8.4222 - mse: 132.1661 - val_loss: 143.4673 - val_mae: 8.6020 - val_mse: 143.4425\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.6546 - mae: 8.3709 - mse: 129.6295 - val_loss: 144.6354 - val_mae: 8.6159 - val_mse: 144.6102\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.5262 - mae: 8.3476 - mse: 130.5009 - val_loss: 142.7194 - val_mae: 8.6453 - val_mse: 142.6938\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5811 - mae: 8.4110 - mse: 131.5555 - val_loss: 146.0856 - val_mae: 8.6047 - val_mse: 146.0600\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0515 - mae: 8.3196 - mse: 130.0257 - val_loss: 144.5776 - val_mae: 8.6076 - val_mse: 144.5517\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.7828 - mae: 8.3529 - mse: 130.7567 - val_loss: 144.8442 - val_mae: 8.6024 - val_mse: 144.8181\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.1658 - mae: 8.3667 - mse: 131.1394 - val_loss: 144.2976 - val_mae: 8.6036 - val_mse: 144.2711\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.0412 - mae: 8.3672 - mse: 130.0146 - val_loss: 145.1060 - val_mae: 8.5908 - val_mse: 145.0794\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.4825 - mae: 8.3833 - mse: 130.4557 - val_loss: 144.7029 - val_mae: 8.5866 - val_mse: 144.6760\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.5559 - mae: 8.3538 - mse: 130.5289 - val_loss: 144.0106 - val_mae: 8.6020 - val_mse: 143.9834\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.0035 - mae: 8.3674 - mse: 128.9762 - val_loss: 144.5912 - val_mae: 8.5943 - val_mse: 144.5637\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 5.2871, Train MSE: 50.2355\n",
      "Val   MAE: 10.5364, Val   MSE: 200.4673\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8791, Train MSE: 61.1933\n",
      "Val   MAE: 9.6900, Val   MSE: 173.9546\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 8.0337, Train MSE: 126.7191\n",
      "Val   MAE: 8.5943, Val   MSE: 144.5637\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 154.9848 - mae: 8.8368 - mse: 154.9848 - val_loss: 138.1960 - val_mae: 8.6468 - val_mse: 138.1960\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0099 - mae: 8.2058 - mse: 131.0099 - val_loss: 138.0397 - val_mae: 8.5955 - val_mse: 138.0397\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 127.7437 - mae: 8.0588 - mse: 127.7437 - val_loss: 138.4746 - val_mae: 8.7850 - val_mse: 138.4746\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 126.1602 - mae: 8.0246 - mse: 126.1602 - val_loss: 139.2280 - val_mae: 8.6638 - val_mse: 139.2280\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 124.8090 - mae: 7.9446 - mse: 124.8090 - val_loss: 139.9446 - val_mae: 8.7577 - val_mse: 139.9446\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.9915 - mae: 7.9374 - mse: 123.9915 - val_loss: 138.9343 - val_mae: 8.6341 - val_mse: 138.9343\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 122.7943 - mae: 7.8684 - mse: 122.7943 - val_loss: 140.1031 - val_mae: 8.6941 - val_mse: 140.1031\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9075 - mae: 7.8142 - mse: 120.9075 - val_loss: 141.4104 - val_mae: 8.6482 - val_mse: 141.4104\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.5926 - mae: 7.7358 - mse: 119.5926 - val_loss: 141.1548 - val_mae: 8.8480 - val_mse: 141.1548\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 118.2959 - mae: 7.7270 - mse: 118.2959 - val_loss: 141.6122 - val_mae: 8.6686 - val_mse: 141.6122\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.8341 - mae: 7.6611 - mse: 116.8341 - val_loss: 143.2398 - val_mae: 8.9122 - val_mse: 143.2398\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.8387 - mae: 7.6012 - mse: 114.8387 - val_loss: 142.6922 - val_mae: 8.7821 - val_mse: 142.6922\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.5507 - mae: 7.4904 - mse: 112.5507 - val_loss: 143.7836 - val_mae: 8.7780 - val_mse: 143.7836\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 110.1049 - mae: 7.4458 - mse: 110.1049 - val_loss: 146.1590 - val_mae: 8.8264 - val_mse: 146.1590\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.5135 - mae: 7.3486 - mse: 108.5135 - val_loss: 146.3651 - val_mae: 9.0257 - val_mse: 146.3651\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 105.8868 - mae: 7.2635 - mse: 105.8868 - val_loss: 148.4701 - val_mae: 9.0840 - val_mse: 148.4701\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.4712 - mae: 7.1829 - mse: 103.4712 - val_loss: 150.9601 - val_mae: 9.0594 - val_mse: 150.9601\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 100.6117 - mae: 7.0263 - mse: 100.6117 - val_loss: 154.7659 - val_mae: 9.2626 - val_mse: 154.7659\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 98.4238 - mae: 7.0086 - mse: 98.4238 - val_loss: 152.8541 - val_mae: 9.1802 - val_mse: 152.8541\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 96.1863 - mae: 6.8776 - mse: 96.1863 - val_loss: 155.4202 - val_mae: 9.1501 - val_mse: 155.4202\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.0062 - mae: 6.7440 - mse: 92.0062 - val_loss: 157.9956 - val_mae: 9.2337 - val_mse: 157.9956\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.5868 - mae: 6.6565 - mse: 89.5868 - val_loss: 160.2663 - val_mae: 9.3793 - val_mse: 160.2663\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 86.4016 - mae: 6.5448 - mse: 86.4016 - val_loss: 163.9035 - val_mae: 9.4612 - val_mse: 163.9035\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 83.8888 - mae: 6.4260 - mse: 83.8888 - val_loss: 165.3537 - val_mae: 9.2658 - val_mse: 165.3537\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.9701 - mae: 6.3687 - mse: 82.9701 - val_loss: 173.1596 - val_mae: 9.6302 - val_mse: 173.1596\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 79.8920 - mae: 6.2190 - mse: 79.8920 - val_loss: 164.7123 - val_mae: 9.5618 - val_mse: 164.7123\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 78.8314 - mae: 6.1861 - mse: 78.8314 - val_loss: 166.9779 - val_mae: 9.4804 - val_mse: 166.9779\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 74.7701 - mae: 6.0532 - mse: 74.7701 - val_loss: 175.2322 - val_mae: 9.6466 - val_mse: 175.2322\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 72.5830 - mae: 5.9245 - mse: 72.5830 - val_loss: 170.5549 - val_mae: 9.5434 - val_mse: 170.5549\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.6110 - mae: 5.8721 - mse: 71.6110 - val_loss: 169.5111 - val_mae: 9.4069 - val_mse: 169.5111\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 68.6674 - mae: 5.7682 - mse: 68.6674 - val_loss: 172.9179 - val_mae: 9.4310 - val_mse: 172.9179\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 65.6279 - mae: 5.6227 - mse: 65.6279 - val_loss: 177.3636 - val_mae: 9.6564 - val_mse: 177.3636\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.7512 - mae: 5.6766 - mse: 67.7512 - val_loss: 180.5681 - val_mae: 9.8166 - val_mse: 180.5681\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.2232 - mae: 5.4576 - mse: 62.2232 - val_loss: 178.2605 - val_mae: 9.6303 - val_mse: 178.2605\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 61.5600 - mae: 5.3900 - mse: 61.5600 - val_loss: 179.8342 - val_mae: 9.8221 - val_mse: 179.8342\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.0929 - mae: 5.2283 - mse: 58.0929 - val_loss: 185.6378 - val_mae: 9.9818 - val_mse: 185.6378\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 55.9187 - mae: 5.1853 - mse: 55.9187 - val_loss: 185.1926 - val_mae: 9.8088 - val_mse: 185.1926\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 53.5720 - mae: 5.0622 - mse: 53.5720 - val_loss: 180.6849 - val_mae: 9.6983 - val_mse: 180.6849\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 53.5282 - mae: 5.0141 - mse: 53.5282 - val_loss: 187.0270 - val_mae: 9.9688 - val_mse: 187.0270\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 51.8128 - mae: 4.9875 - mse: 51.8128 - val_loss: 185.1358 - val_mae: 9.7756 - val_mse: 185.1358\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 50.1927 - mae: 4.9389 - mse: 50.1927 - val_loss: 182.8357 - val_mae: 9.6230 - val_mse: 182.8357\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 49.0078 - mae: 4.8184 - mse: 49.0078 - val_loss: 195.2571 - val_mae: 10.0621 - val_mse: 195.2571\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 46.2402 - mae: 4.6963 - mse: 46.2402 - val_loss: 198.3261 - val_mae: 10.2442 - val_mse: 198.3261\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 45.5424 - mae: 4.6336 - mse: 45.5424 - val_loss: 190.7889 - val_mae: 10.0027 - val_mse: 190.7889\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 44.5083 - mae: 4.5897 - mse: 44.5083 - val_loss: 186.6897 - val_mae: 9.8250 - val_mse: 186.6897\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.2718 - mae: 4.4848 - mse: 42.2718 - val_loss: 195.5838 - val_mae: 10.0723 - val_mse: 195.5838\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.9050 - mae: 4.5071 - mse: 42.9050 - val_loss: 198.8718 - val_mae: 10.1752 - val_mse: 198.8718\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 43.0858 - mae: 4.5396 - mse: 43.0858 - val_loss: 198.0504 - val_mae: 9.9744 - val_mse: 198.0504\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 38.5245 - mae: 4.2757 - mse: 38.5245 - val_loss: 202.5983 - val_mae: 10.3022 - val_mse: 202.5983\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 39.4653 - mae: 4.2567 - mse: 39.4653 - val_loss: 212.5082 - val_mae: 10.4910 - val_mse: 212.5082\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 35.9051 - mae: 4.1213 - mse: 35.9051 - val_loss: 204.3475 - val_mae: 10.2602 - val_mse: 204.3475\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 36.6266 - mae: 4.2153 - mse: 36.6266 - val_loss: 209.6196 - val_mae: 10.3635 - val_mse: 209.6196\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 34.9968 - mae: 4.0582 - mse: 34.9968 - val_loss: 209.6265 - val_mae: 10.4073 - val_mse: 209.6265\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 34.5476 - mae: 4.0871 - mse: 34.5476 - val_loss: 202.3706 - val_mae: 10.2421 - val_mse: 202.3706\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 32.4530 - mae: 3.9029 - mse: 32.4530 - val_loss: 200.6513 - val_mae: 10.2324 - val_mse: 200.6513\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 33.1771 - mae: 3.9595 - mse: 33.1771 - val_loss: 206.4464 - val_mae: 10.2970 - val_mse: 206.4464\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 32.8712 - mae: 3.9298 - mse: 32.8712 - val_loss: 202.6764 - val_mae: 10.3042 - val_mse: 202.6764\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 31.6389 - mae: 3.9216 - mse: 31.6389 - val_loss: 214.2000 - val_mae: 10.5517 - val_mse: 214.2000\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 29.3602 - mae: 3.7521 - mse: 29.3602 - val_loss: 215.4597 - val_mae: 10.6156 - val_mse: 215.4597\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 29.3808 - mae: 3.7323 - mse: 29.3808 - val_loss: 213.7932 - val_mae: 10.4291 - val_mse: 213.7932\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 28.6263 - mae: 3.6632 - mse: 28.6263 - val_loss: 207.4640 - val_mae: 10.2865 - val_mse: 207.4640\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 28.4198 - mae: 3.6848 - mse: 28.4198 - val_loss: 226.1633 - val_mae: 10.8309 - val_mse: 226.1633\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.9458 - mae: 3.6115 - mse: 26.9458 - val_loss: 207.0650 - val_mae: 10.2568 - val_mse: 207.0650\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.9420 - mae: 3.5926 - mse: 26.9420 - val_loss: 213.8491 - val_mae: 10.3457 - val_mse: 213.8491\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 25.0327 - mae: 3.4644 - mse: 25.0327 - val_loss: 219.8501 - val_mae: 10.6576 - val_mse: 219.8501\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.2991 - mae: 3.5671 - mse: 26.2991 - val_loss: 212.7595 - val_mae: 10.3850 - val_mse: 212.7595\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.0862 - mae: 3.5203 - mse: 26.0862 - val_loss: 215.2376 - val_mae: 10.6081 - val_mse: 215.2376\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 23.9053 - mae: 3.3842 - mse: 23.9053 - val_loss: 211.2601 - val_mae: 10.4673 - val_mse: 211.2601\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 24.0901 - mae: 3.3895 - mse: 24.0901 - val_loss: 220.9115 - val_mae: 10.6001 - val_mse: 220.9115\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 23.0655 - mae: 3.3244 - mse: 23.0655 - val_loss: 212.3203 - val_mae: 10.4179 - val_mse: 212.3203\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.6144 - mae: 3.2177 - mse: 21.6144 - val_loss: 223.1977 - val_mae: 10.6627 - val_mse: 223.1977\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.7762 - mae: 3.1707 - mse: 21.7762 - val_loss: 218.5349 - val_mae: 10.6137 - val_mse: 218.5349\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.4904 - mae: 3.2674 - mse: 22.4904 - val_loss: 216.8880 - val_mae: 10.4016 - val_mse: 216.8880\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.9804 - mae: 3.2462 - mse: 21.9804 - val_loss: 216.3953 - val_mae: 10.5732 - val_mse: 216.3953\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 20.7445 - mae: 3.1494 - mse: 20.7445 - val_loss: 217.6079 - val_mae: 10.5199 - val_mse: 217.6079\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.5373 - mae: 3.1633 - mse: 21.5373 - val_loss: 216.2381 - val_mae: 10.5172 - val_mse: 216.2381\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 20.4013 - mae: 3.0980 - mse: 20.4013 - val_loss: 221.8059 - val_mae: 10.6018 - val_mse: 221.8059\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.1231 - mae: 3.0086 - mse: 19.1231 - val_loss: 218.4679 - val_mae: 10.5866 - val_mse: 218.4679\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 18.4506 - mae: 2.9845 - mse: 18.4506 - val_loss: 232.0055 - val_mae: 10.8793 - val_mse: 232.0055\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 19.3010 - mae: 3.0727 - mse: 19.3010 - val_loss: 219.9297 - val_mae: 10.5755 - val_mse: 219.9297\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.7622 - mae: 3.0862 - mse: 19.7622 - val_loss: 223.9324 - val_mae: 10.6238 - val_mse: 223.9324\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.4244 - mae: 3.0072 - mse: 19.4244 - val_loss: 230.3828 - val_mae: 10.8779 - val_mse: 230.3828\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.3206 - mae: 2.9248 - mse: 18.3206 - val_loss: 222.9341 - val_mae: 10.7485 - val_mse: 222.9341\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.5612 - mae: 2.9565 - mse: 18.5612 - val_loss: 220.3819 - val_mae: 10.5935 - val_mse: 220.3819\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.3959 - mae: 2.8647 - mse: 17.3959 - val_loss: 218.7614 - val_mae: 10.5767 - val_mse: 218.7614\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.4074 - mae: 2.8662 - mse: 17.4074 - val_loss: 227.3249 - val_mae: 10.7395 - val_mse: 227.3249\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 15.8813 - mae: 2.7316 - mse: 15.8813 - val_loss: 231.9523 - val_mae: 10.8992 - val_mse: 231.9523\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.3184 - mae: 2.7995 - mse: 16.3184 - val_loss: 222.5830 - val_mae: 10.6596 - val_mse: 222.5830\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 16.1865 - mae: 2.7630 - mse: 16.1865 - val_loss: 225.9023 - val_mae: 10.8086 - val_mse: 225.9023\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.0837 - mae: 2.8558 - mse: 17.0837 - val_loss: 229.3857 - val_mae: 10.8437 - val_mse: 229.3857\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.5680 - mae: 2.8315 - mse: 16.5680 - val_loss: 221.5206 - val_mae: 10.5678 - val_mse: 221.5206\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 15.7257 - mae: 2.7229 - mse: 15.7257 - val_loss: 227.9866 - val_mae: 10.6919 - val_mse: 227.9866\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.2157 - mae: 2.8346 - mse: 17.2157 - val_loss: 225.7415 - val_mae: 10.6729 - val_mse: 225.7415\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.2349 - mae: 2.7962 - mse: 16.2349 - val_loss: 225.5674 - val_mae: 10.7325 - val_mse: 225.5674\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.6313 - mae: 2.6235 - mse: 14.6313 - val_loss: 222.5823 - val_mae: 10.5951 - val_mse: 222.5823\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.4939 - mae: 2.6180 - mse: 14.4939 - val_loss: 232.4482 - val_mae: 10.9100 - val_mse: 232.4482\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.4943 - mae: 2.5989 - mse: 14.4943 - val_loss: 224.4429 - val_mae: 10.6919 - val_mse: 224.4429\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.2583 - mae: 2.5565 - mse: 14.2583 - val_loss: 227.6956 - val_mae: 10.7596 - val_mse: 227.6956\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 15.2240 - mae: 2.6805 - mse: 15.2240 - val_loss: 229.3468 - val_mae: 10.9351 - val_mse: 229.3468\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.2607 - mae: 2.5612 - mse: 14.2607 - val_loss: 219.6110 - val_mae: 10.6082 - val_mse: 219.6110\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 227.8885 - mae: 10.7955 - mse: 227.8885 - val_loss: 219.6735 - val_mae: 10.7090 - val_mse: 219.6735\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 150.3945 - mae: 8.4885 - mse: 150.3945 - val_loss: 149.6552 - val_mae: 8.7005 - val_mse: 149.6552\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6347 - mae: 7.9994 - mse: 129.6347 - val_loss: 140.3612 - val_mae: 8.6412 - val_mse: 140.3612\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.7956 - mae: 7.9791 - mse: 125.7956 - val_loss: 139.8844 - val_mae: 8.6394 - val_mse: 139.8844\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.8026 - mae: 7.8993 - mse: 123.8026 - val_loss: 139.2763 - val_mae: 8.6081 - val_mse: 139.2763\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.7854 - mae: 7.8571 - mse: 121.7854 - val_loss: 140.7435 - val_mae: 8.7702 - val_mse: 140.7435\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.4885 - mae: 7.8128 - mse: 119.4885 - val_loss: 142.3179 - val_mae: 8.7472 - val_mse: 142.3179\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.7685 - mae: 7.7623 - mse: 118.7685 - val_loss: 141.2466 - val_mae: 8.7040 - val_mse: 141.2466\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.5200 - mae: 7.7220 - mse: 116.5200 - val_loss: 141.1274 - val_mae: 8.6655 - val_mse: 141.1274\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.8963 - mae: 7.6884 - mse: 115.8963 - val_loss: 142.5742 - val_mae: 8.7843 - val_mse: 142.5742\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.3778 - mae: 7.6031 - mse: 114.3778 - val_loss: 143.6867 - val_mae: 8.7270 - val_mse: 143.6867\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.5716 - mae: 7.5268 - mse: 111.5716 - val_loss: 145.4806 - val_mae: 8.9353 - val_mse: 145.4806\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.2506 - mae: 7.5067 - mse: 110.2506 - val_loss: 145.1964 - val_mae: 8.8583 - val_mse: 145.1964\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.2318 - mae: 7.5022 - mse: 109.2318 - val_loss: 146.5817 - val_mae: 8.8076 - val_mse: 146.5817\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.4247 - mae: 7.3653 - mse: 107.4247 - val_loss: 142.7118 - val_mae: 8.6929 - val_mse: 142.7118\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.7827 - mae: 7.3423 - mse: 106.7827 - val_loss: 147.0810 - val_mae: 8.9444 - val_mse: 147.0810\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.5917 - mae: 7.3016 - mse: 104.5917 - val_loss: 145.5023 - val_mae: 8.8039 - val_mse: 145.5023\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.8888 - mae: 7.1962 - mse: 101.8888 - val_loss: 145.0894 - val_mae: 8.9839 - val_mse: 145.0894\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.1894 - mae: 7.2237 - mse: 102.1894 - val_loss: 152.6931 - val_mae: 9.0228 - val_mse: 152.6931\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.7724 - mae: 7.1083 - mse: 100.7724 - val_loss: 146.3107 - val_mae: 8.8944 - val_mse: 146.3107\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.7297 - mae: 7.1033 - mse: 98.7297 - val_loss: 145.7811 - val_mae: 8.7434 - val_mse: 145.7811\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.6656 - mae: 6.9969 - mse: 96.6656 - val_loss: 150.5262 - val_mae: 8.9299 - val_mse: 150.5262\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.7278 - mae: 6.9188 - mse: 94.7278 - val_loss: 156.2894 - val_mae: 9.1648 - val_mse: 156.2894\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.5428 - mae: 6.9441 - mse: 94.5428 - val_loss: 151.6901 - val_mae: 8.9429 - val_mse: 151.6901\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.3990 - mae: 6.8843 - mse: 93.3990 - val_loss: 148.5452 - val_mae: 8.8718 - val_mse: 148.5452\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.1576 - mae: 6.7913 - mse: 91.1576 - val_loss: 153.6394 - val_mae: 8.9640 - val_mse: 153.6394\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.8913 - mae: 6.7761 - mse: 90.8913 - val_loss: 156.9099 - val_mae: 9.2478 - val_mse: 156.9099\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8550 - mae: 6.7280 - mse: 88.8550 - val_loss: 154.4890 - val_mae: 8.9729 - val_mse: 154.4890\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.7961 - mae: 6.6686 - mse: 87.7961 - val_loss: 155.9176 - val_mae: 9.0456 - val_mse: 155.9176\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.4914 - mae: 6.6712 - mse: 87.4914 - val_loss: 156.5973 - val_mae: 9.0112 - val_mse: 156.5973\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 85.2690 - mae: 6.6481 - mse: 85.2690 - val_loss: 155.8816 - val_mae: 9.0016 - val_mse: 155.8816\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.0026 - mae: 6.4987 - mse: 83.0026 - val_loss: 161.7435 - val_mae: 9.2109 - val_mse: 161.7435\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 82.3510 - mae: 6.5138 - mse: 82.3510 - val_loss: 163.0137 - val_mae: 9.2074 - val_mse: 163.0137\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 81.9572 - mae: 6.4620 - mse: 81.9572 - val_loss: 164.4895 - val_mae: 9.3534 - val_mse: 164.4895\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 81.6322 - mae: 6.4234 - mse: 81.6322 - val_loss: 162.3862 - val_mae: 9.3246 - val_mse: 162.3862\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.1567 - mae: 6.4172 - mse: 80.1567 - val_loss: 164.5874 - val_mae: 9.3755 - val_mse: 164.5874\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 79.6461 - mae: 6.3546 - mse: 79.6461 - val_loss: 162.5681 - val_mae: 9.2734 - val_mse: 162.5681\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 79.5432 - mae: 6.3671 - mse: 79.5432 - val_loss: 161.6340 - val_mae: 9.3127 - val_mse: 161.6340\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 76.4755 - mae: 6.2355 - mse: 76.4755 - val_loss: 163.6764 - val_mae: 9.2730 - val_mse: 163.6764\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 75.5189 - mae: 6.1677 - mse: 75.5189 - val_loss: 165.2307 - val_mae: 9.3100 - val_mse: 165.2307\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 75.9564 - mae: 6.2067 - mse: 75.9564 - val_loss: 163.5557 - val_mae: 9.2438 - val_mse: 163.5557\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.5464 - mae: 6.1384 - mse: 73.5464 - val_loss: 171.5630 - val_mae: 9.4631 - val_mse: 171.5630\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 74.3611 - mae: 6.2022 - mse: 74.3611 - val_loss: 166.5519 - val_mae: 9.3920 - val_mse: 166.5519\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.7272 - mae: 6.1206 - mse: 73.7272 - val_loss: 167.9423 - val_mae: 9.4788 - val_mse: 167.9423\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.6757 - mae: 6.1689 - mse: 73.6757 - val_loss: 168.5956 - val_mae: 9.5173 - val_mse: 168.5956\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.4061 - mae: 6.1153 - mse: 73.4061 - val_loss: 166.8846 - val_mae: 9.3604 - val_mse: 166.8846\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 70.1965 - mae: 5.9456 - mse: 70.1965 - val_loss: 168.1162 - val_mae: 9.3344 - val_mse: 168.1162\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 69.4730 - mae: 5.9819 - mse: 69.4730 - val_loss: 164.2535 - val_mae: 9.1322 - val_mse: 164.2535\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 69.1859 - mae: 5.9681 - mse: 69.1859 - val_loss: 166.6625 - val_mae: 9.3460 - val_mse: 166.6625\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 69.4261 - mae: 5.9993 - mse: 69.4261 - val_loss: 164.3622 - val_mae: 9.2596 - val_mse: 164.3622\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 67.8609 - mae: 5.9003 - mse: 67.8609 - val_loss: 164.4780 - val_mae: 9.2826 - val_mse: 164.4780\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.3078 - mae: 5.9015 - mse: 68.3078 - val_loss: 165.4382 - val_mae: 9.3876 - val_mse: 165.4382\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.3706 - mae: 5.8221 - mse: 65.3706 - val_loss: 169.9686 - val_mae: 9.6094 - val_mse: 169.9686\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.1987 - mae: 5.8113 - mse: 65.1987 - val_loss: 171.3141 - val_mae: 9.4360 - val_mse: 171.3141\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 67.1512 - mae: 5.8579 - mse: 67.1512 - val_loss: 165.0600 - val_mae: 9.4145 - val_mse: 165.0600\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.2494 - mae: 5.6796 - mse: 62.2494 - val_loss: 167.4719 - val_mae: 9.4109 - val_mse: 167.4719\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 63.7923 - mae: 5.6761 - mse: 63.7923 - val_loss: 170.7481 - val_mae: 9.5162 - val_mse: 170.7481\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.6566 - mae: 5.6368 - mse: 62.6566 - val_loss: 175.4212 - val_mae: 9.6493 - val_mse: 175.4212\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.3042 - mae: 5.6776 - mse: 62.3042 - val_loss: 168.5213 - val_mae: 9.4207 - val_mse: 168.5213\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 61.5122 - mae: 5.5916 - mse: 61.5122 - val_loss: 171.1292 - val_mae: 9.6467 - val_mse: 171.1292\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.8043 - mae: 5.5459 - mse: 59.8043 - val_loss: 178.9437 - val_mae: 9.7685 - val_mse: 178.9437\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 61.6307 - mae: 5.6587 - mse: 61.6307 - val_loss: 166.6191 - val_mae: 9.3054 - val_mse: 166.6191\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.7729 - mae: 5.5533 - mse: 59.7729 - val_loss: 167.1918 - val_mae: 9.3534 - val_mse: 167.1918\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.1132 - mae: 5.5329 - mse: 59.1132 - val_loss: 173.2427 - val_mae: 9.5008 - val_mse: 173.2427\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.2699 - mae: 5.5275 - mse: 59.2699 - val_loss: 171.5818 - val_mae: 9.4371 - val_mse: 171.5818\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.1327 - mae: 5.5223 - mse: 59.1327 - val_loss: 172.3989 - val_mae: 9.5585 - val_mse: 172.3989\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.5068 - mae: 5.6765 - mse: 62.5068 - val_loss: 169.8676 - val_mae: 9.4871 - val_mse: 169.8676\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.8071 - mae: 5.4069 - mse: 56.8071 - val_loss: 172.6810 - val_mae: 9.4311 - val_mse: 172.6810\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.0720 - mae: 5.5598 - mse: 59.0720 - val_loss: 176.1606 - val_mae: 9.5688 - val_mse: 176.1606\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 57.8528 - mae: 5.4842 - mse: 57.8528 - val_loss: 169.5488 - val_mae: 9.3504 - val_mse: 169.5488\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.1705 - mae: 5.4171 - mse: 56.1705 - val_loss: 179.6496 - val_mae: 9.7275 - val_mse: 179.6496\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.3805 - mae: 5.3693 - mse: 56.3805 - val_loss: 175.9501 - val_mae: 9.6618 - val_mse: 175.9501\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 57.7786 - mae: 5.4180 - mse: 57.7786 - val_loss: 168.0920 - val_mae: 9.4084 - val_mse: 168.0920\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.3867 - mae: 5.4317 - mse: 56.3867 - val_loss: 172.5616 - val_mae: 9.6058 - val_mse: 172.5616\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.3958 - mae: 5.3221 - mse: 54.3958 - val_loss: 169.9511 - val_mae: 9.4092 - val_mse: 169.9511\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.9668 - mae: 5.3394 - mse: 54.9668 - val_loss: 176.1924 - val_mae: 9.5093 - val_mse: 176.1924\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.6256 - mae: 5.3280 - mse: 54.6256 - val_loss: 174.4692 - val_mae: 9.7427 - val_mse: 174.4692\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 55.1545 - mae: 5.4000 - mse: 55.1545 - val_loss: 173.4189 - val_mae: 9.4916 - val_mse: 173.4189\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.4160 - mae: 5.2831 - mse: 53.4160 - val_loss: 174.1491 - val_mae: 9.5566 - val_mse: 174.1491\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.7891 - mae: 5.2153 - mse: 52.7891 - val_loss: 172.9953 - val_mae: 9.5240 - val_mse: 172.9953\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.0826 - mae: 5.1785 - mse: 51.0826 - val_loss: 172.4391 - val_mae: 9.5245 - val_mse: 172.4391\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.8422 - mae: 5.1762 - mse: 50.8422 - val_loss: 178.0777 - val_mae: 9.8061 - val_mse: 178.0777\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.2068 - mae: 5.1834 - mse: 52.2068 - val_loss: 173.6156 - val_mae: 9.5702 - val_mse: 173.6156\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.1167 - mae: 5.2117 - mse: 52.1167 - val_loss: 166.9050 - val_mae: 9.3709 - val_mse: 166.9050\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.7689 - mae: 5.1791 - mse: 51.7689 - val_loss: 174.1576 - val_mae: 9.7223 - val_mse: 174.1576\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.2008 - mae: 5.1502 - mse: 51.2008 - val_loss: 182.1694 - val_mae: 9.8782 - val_mse: 182.1694\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.9871 - mae: 5.1937 - mse: 50.9871 - val_loss: 168.5618 - val_mae: 9.4031 - val_mse: 168.5618\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.9648 - mae: 5.2089 - mse: 52.9648 - val_loss: 170.5045 - val_mae: 9.4234 - val_mse: 170.5045\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.8791 - mae: 5.1037 - mse: 49.8791 - val_loss: 174.1838 - val_mae: 9.5453 - val_mse: 174.1838\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.3074 - mae: 5.1635 - mse: 50.3074 - val_loss: 172.8823 - val_mae: 9.6028 - val_mse: 172.8823\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.9183 - mae: 5.0858 - mse: 49.9183 - val_loss: 172.9340 - val_mae: 9.4966 - val_mse: 172.9340\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 48.6383 - mae: 5.0105 - mse: 48.6383 - val_loss: 180.2252 - val_mae: 9.7755 - val_mse: 180.2252\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.4695 - mae: 5.1280 - mse: 50.4695 - val_loss: 172.7402 - val_mae: 9.4084 - val_mse: 172.7402\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 46.7043 - mae: 4.9124 - mse: 46.7043 - val_loss: 169.8661 - val_mae: 9.4399 - val_mse: 169.8661\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.5046 - mae: 4.9890 - mse: 47.5046 - val_loss: 176.2451 - val_mae: 9.6012 - val_mse: 176.2451\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.9639 - mae: 5.0240 - mse: 47.9639 - val_loss: 172.3439 - val_mae: 9.4551 - val_mse: 172.3439\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 48.2821 - mae: 5.0645 - mse: 48.2821 - val_loss: 174.4822 - val_mae: 9.6043 - val_mse: 174.4822\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.5543 - mae: 5.0039 - mse: 47.5543 - val_loss: 171.7526 - val_mae: 9.5675 - val_mse: 171.7526\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.6888 - mae: 4.9795 - mse: 47.6888 - val_loss: 183.2466 - val_mae: 9.8844 - val_mse: 183.2466\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 45.7609 - mae: 4.9306 - mse: 45.7609 - val_loss: 173.4950 - val_mae: 9.6203 - val_mse: 173.4950\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 187.8355 - mae: 9.7742 - mse: 187.8106 - val_loss: 144.3890 - val_mae: 8.4949 - val_mse: 144.3627\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 145.8791 - mae: 8.6639 - mse: 145.8523 - val_loss: 138.9953 - val_mae: 8.4993 - val_mse: 138.9680\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.9660 - mae: 8.5497 - mse: 140.9386 - val_loss: 145.6071 - val_mae: 8.5743 - val_mse: 145.5797\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.9796 - mae: 8.5254 - mse: 140.9519 - val_loss: 144.6924 - val_mae: 8.5317 - val_mse: 144.6647\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.4547 - mae: 8.4134 - mse: 138.4270 - val_loss: 142.0000 - val_mae: 8.5083 - val_mse: 141.9722\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6833 - mae: 8.4395 - mse: 137.6553 - val_loss: 148.3880 - val_mae: 8.6446 - val_mse: 148.3602\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2641 - mae: 8.4718 - mse: 139.2362 - val_loss: 141.4469 - val_mae: 8.5385 - val_mse: 141.4188\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2157 - mae: 8.3029 - mse: 134.1877 - val_loss: 143.5502 - val_mae: 8.5677 - val_mse: 143.5223\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3025 - mae: 8.3463 - mse: 134.2746 - val_loss: 143.5366 - val_mae: 8.6203 - val_mse: 143.5087\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.2895 - mae: 8.2570 - mse: 133.2618 - val_loss: 142.6046 - val_mae: 8.6295 - val_mse: 142.5768\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.7383 - mae: 8.2248 - mse: 130.7105 - val_loss: 145.1000 - val_mae: 8.6504 - val_mse: 145.0721\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5477 - mae: 8.2108 - mse: 132.5199 - val_loss: 145.0675 - val_mae: 8.5863 - val_mse: 145.0399\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.5916 - mae: 8.2944 - mse: 133.5640 - val_loss: 142.3833 - val_mae: 8.5863 - val_mse: 142.3556\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7920 - mae: 8.2516 - mse: 131.7644 - val_loss: 140.8563 - val_mae: 8.5919 - val_mse: 140.8286\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7092 - mae: 8.1425 - mse: 128.6816 - val_loss: 146.3273 - val_mae: 8.6495 - val_mse: 146.2998\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0600 - mae: 8.1548 - mse: 130.0324 - val_loss: 145.4575 - val_mae: 8.6289 - val_mse: 145.4299\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.5158 - mae: 8.0987 - mse: 128.4882 - val_loss: 143.4370 - val_mae: 8.5748 - val_mse: 143.4093\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9121 - mae: 8.1597 - mse: 129.8845 - val_loss: 143.5996 - val_mae: 8.6566 - val_mse: 143.5719\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4414 - mae: 8.0760 - mse: 128.4137 - val_loss: 143.7253 - val_mae: 8.6248 - val_mse: 143.6975\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.1494 - mae: 8.0810 - mse: 128.1217 - val_loss: 143.2903 - val_mae: 8.6041 - val_mse: 143.2624\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9519 - mae: 8.1519 - mse: 127.9239 - val_loss: 142.2705 - val_mae: 8.5364 - val_mse: 142.2425\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9589 - mae: 8.1106 - mse: 127.9307 - val_loss: 145.4341 - val_mae: 8.6216 - val_mse: 145.4060\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.5522 - mae: 8.0614 - mse: 127.5241 - val_loss: 141.7295 - val_mae: 8.5988 - val_mse: 141.7012\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.6201 - mae: 8.0226 - mse: 125.5919 - val_loss: 144.6646 - val_mae: 8.5766 - val_mse: 144.6364\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.7587 - mae: 8.0585 - mse: 125.7304 - val_loss: 145.3211 - val_mae: 8.6257 - val_mse: 145.2928\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.0367 - mae: 8.0081 - mse: 128.0083 - val_loss: 140.8302 - val_mae: 8.5384 - val_mse: 140.8016\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.5427 - mae: 8.0364 - mse: 126.5141 - val_loss: 140.4296 - val_mae: 8.5703 - val_mse: 140.4009\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.4347 - mae: 7.9596 - mse: 122.4060 - val_loss: 141.4919 - val_mae: 8.5618 - val_mse: 141.4632\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.8020 - mae: 8.0141 - mse: 124.7733 - val_loss: 144.5244 - val_mae: 8.6239 - val_mse: 144.4956\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.9939 - mae: 7.9828 - mse: 122.9649 - val_loss: 141.6579 - val_mae: 8.5350 - val_mse: 141.6288\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.9516 - mae: 8.0463 - mse: 124.9224 - val_loss: 146.2155 - val_mae: 8.6625 - val_mse: 146.1863\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.6050 - mae: 8.0145 - mse: 123.5757 - val_loss: 144.5411 - val_mae: 8.5958 - val_mse: 144.5118\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.7589 - mae: 7.9535 - mse: 122.7294 - val_loss: 140.8428 - val_mae: 8.5199 - val_mse: 140.8132\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9466 - mae: 7.9298 - mse: 120.9170 - val_loss: 141.2559 - val_mae: 8.5742 - val_mse: 141.2261\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.2861 - mae: 7.9395 - mse: 121.2563 - val_loss: 141.8452 - val_mae: 8.5500 - val_mse: 141.8152\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.1437 - mae: 7.8855 - mse: 120.1137 - val_loss: 143.0919 - val_mae: 8.5923 - val_mse: 143.0618\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.5289 - mae: 7.9017 - mse: 120.4986 - val_loss: 141.7600 - val_mae: 8.5300 - val_mse: 141.7296\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.5872 - mae: 7.8093 - mse: 118.5566 - val_loss: 143.1664 - val_mae: 8.5528 - val_mse: 143.1359\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.5960 - mae: 7.8959 - mse: 120.5653 - val_loss: 144.4003 - val_mae: 8.6045 - val_mse: 144.3694\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.5142 - mae: 7.8708 - mse: 118.4831 - val_loss: 143.5090 - val_mae: 8.6209 - val_mse: 143.4779\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.5918 - mae: 7.8725 - mse: 118.5605 - val_loss: 141.7980 - val_mae: 8.5681 - val_mse: 141.7666\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2666 - mae: 7.7729 - mse: 118.2351 - val_loss: 140.6412 - val_mae: 8.5339 - val_mse: 140.6094\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.5555 - mae: 7.8346 - mse: 117.5237 - val_loss: 145.4902 - val_mae: 8.6073 - val_mse: 145.4583\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.8500 - mae: 7.8251 - mse: 116.8179 - val_loss: 144.6864 - val_mae: 8.5737 - val_mse: 144.6542\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.5803 - mae: 7.8034 - mse: 116.5478 - val_loss: 143.2716 - val_mae: 8.5843 - val_mse: 143.2390\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.0675 - mae: 7.7651 - mse: 116.0348 - val_loss: 141.5832 - val_mae: 8.5545 - val_mse: 141.5503\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.2681 - mae: 7.8011 - mse: 117.2351 - val_loss: 146.5845 - val_mae: 8.6515 - val_mse: 146.5514\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.9442 - mae: 7.7420 - mse: 114.9109 - val_loss: 145.9900 - val_mae: 8.6461 - val_mse: 145.9566\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.3758 - mae: 7.7482 - mse: 115.3422 - val_loss: 143.6714 - val_mae: 8.6365 - val_mse: 143.6375\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.9513 - mae: 7.6823 - mse: 112.9174 - val_loss: 144.3694 - val_mae: 8.6396 - val_mse: 144.3352\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6083 - mae: 7.7575 - mse: 113.5740 - val_loss: 142.3385 - val_mae: 8.6238 - val_mse: 142.3039\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.8184 - mae: 7.7382 - mse: 113.7837 - val_loss: 150.6640 - val_mae: 8.7939 - val_mse: 150.6292\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.0223 - mae: 7.6109 - mse: 109.9872 - val_loss: 143.1660 - val_mae: 8.6267 - val_mse: 143.1307\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.4472 - mae: 7.7016 - mse: 113.4118 - val_loss: 145.0606 - val_mae: 8.6717 - val_mse: 145.0251\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.8669 - mae: 7.7533 - mse: 113.8313 - val_loss: 144.1842 - val_mae: 8.6701 - val_mse: 144.1483\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.9166 - mae: 7.6758 - mse: 110.8805 - val_loss: 147.7301 - val_mae: 8.6732 - val_mse: 147.6940\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.4985 - mae: 7.6751 - mse: 112.4622 - val_loss: 144.5995 - val_mae: 8.6282 - val_mse: 144.5630\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.7466 - mae: 7.6688 - mse: 111.7099 - val_loss: 145.6452 - val_mae: 8.6336 - val_mse: 145.6083\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.2792 - mae: 7.6334 - mse: 109.2421 - val_loss: 145.4759 - val_mae: 8.6517 - val_mse: 145.4386\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.2314 - mae: 7.6623 - mse: 111.1940 - val_loss: 146.4841 - val_mae: 8.6340 - val_mse: 146.4467\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.5111 - mae: 7.6212 - mse: 109.4734 - val_loss: 146.8966 - val_mae: 8.6645 - val_mse: 146.8587\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7263 - mae: 7.6036 - mse: 108.6883 - val_loss: 146.5126 - val_mae: 8.6476 - val_mse: 146.4744\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.6345 - mae: 7.6153 - mse: 109.5961 - val_loss: 146.8902 - val_mae: 8.6872 - val_mse: 146.8516\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.4399 - mae: 7.5796 - mse: 108.4011 - val_loss: 144.9064 - val_mae: 8.5961 - val_mse: 144.8674\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7846 - mae: 7.5731 - mse: 108.7454 - val_loss: 145.3032 - val_mae: 8.6097 - val_mse: 145.2637\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9155 - mae: 7.5685 - mse: 107.8759 - val_loss: 144.4389 - val_mae: 8.6083 - val_mse: 144.3990\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.7056 - mae: 7.6586 - mse: 111.6656 - val_loss: 148.2445 - val_mae: 8.6626 - val_mse: 148.2044\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.0976 - mae: 7.5504 - mse: 107.0573 - val_loss: 146.2753 - val_mae: 8.6179 - val_mse: 146.2350\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.0013 - mae: 7.5076 - mse: 105.9607 - val_loss: 144.3508 - val_mae: 8.5972 - val_mse: 144.3099\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9229 - mae: 7.5747 - mse: 107.8819 - val_loss: 147.6190 - val_mae: 8.7097 - val_mse: 147.5778\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.3694 - mae: 7.5251 - mse: 104.3280 - val_loss: 147.9387 - val_mae: 8.7117 - val_mse: 147.8972\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.6539 - mae: 7.5739 - mse: 108.6121 - val_loss: 147.1353 - val_mae: 8.6993 - val_mse: 147.0933\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.4648 - mae: 7.5093 - mse: 105.4226 - val_loss: 146.0907 - val_mae: 8.6521 - val_mse: 146.0483\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.4720 - mae: 7.5794 - mse: 107.4294 - val_loss: 147.5302 - val_mae: 8.7115 - val_mse: 147.4876\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.0241 - mae: 7.4980 - mse: 105.9812 - val_loss: 143.9575 - val_mae: 8.6069 - val_mse: 143.9144\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.7858 - mae: 7.5030 - mse: 106.7427 - val_loss: 147.1285 - val_mae: 8.6612 - val_mse: 147.0851\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.8343 - mae: 7.5030 - mse: 105.7907 - val_loss: 148.1801 - val_mae: 8.6699 - val_mse: 148.1364\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.4948 - mae: 7.5235 - mse: 105.4508 - val_loss: 149.5109 - val_mae: 8.7039 - val_mse: 149.4670\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6562 - mae: 7.4187 - mse: 103.6119 - val_loss: 146.7872 - val_mae: 8.6479 - val_mse: 146.7427\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6643 - mae: 7.4763 - mse: 103.6195 - val_loss: 150.5200 - val_mae: 8.7662 - val_mse: 150.4751\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.4768 - mae: 7.3917 - mse: 101.4316 - val_loss: 146.1327 - val_mae: 8.6811 - val_mse: 146.0873\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.8870 - mae: 7.5180 - mse: 105.8414 - val_loss: 146.6223 - val_mae: 8.6715 - val_mse: 146.5764\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.4192 - mae: 7.5068 - mse: 104.3731 - val_loss: 149.0723 - val_mae: 8.7377 - val_mse: 149.0260\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.2334 - mae: 7.4625 - mse: 104.1870 - val_loss: 148.4072 - val_mae: 8.7033 - val_mse: 148.3606\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.1145 - mae: 7.5033 - mse: 105.0678 - val_loss: 149.1321 - val_mae: 8.7204 - val_mse: 149.0853\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.6398 - mae: 7.4210 - mse: 102.5928 - val_loss: 148.0076 - val_mae: 8.7056 - val_mse: 147.9604\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.8765 - mae: 7.3921 - mse: 101.8290 - val_loss: 148.3654 - val_mae: 8.7254 - val_mse: 148.3176\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.6749 - mae: 7.3744 - mse: 100.6270 - val_loss: 150.5261 - val_mae: 8.7554 - val_mse: 150.4780\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6832 - mae: 7.4077 - mse: 103.6349 - val_loss: 150.7799 - val_mae: 8.7778 - val_mse: 150.7315\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.7610 - mae: 7.3945 - mse: 102.7124 - val_loss: 150.1767 - val_mae: 8.7285 - val_mse: 150.1280\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6710 - mae: 7.3390 - mse: 101.6220 - val_loss: 146.3770 - val_mae: 8.6541 - val_mse: 146.3279\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.4993 - mae: 7.3361 - mse: 99.4498 - val_loss: 150.8656 - val_mae: 8.7587 - val_mse: 150.8159\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.8228 - mae: 7.3342 - mse: 100.7728 - val_loss: 150.3237 - val_mae: 8.7535 - val_mse: 150.2735\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.0027 - mae: 7.3049 - mse: 98.9522 - val_loss: 149.9853 - val_mae: 8.7299 - val_mse: 149.9346\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.6890 - mae: 7.3342 - mse: 100.6381 - val_loss: 152.6665 - val_mae: 8.7975 - val_mse: 152.6154\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.5082 - mae: 7.3159 - mse: 99.4567 - val_loss: 153.4423 - val_mae: 8.8239 - val_mse: 153.3908\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.3598 - mae: 7.3956 - mse: 102.3080 - val_loss: 151.2717 - val_mae: 8.7754 - val_mse: 151.2196\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.1640 - mae: 7.3277 - mse: 100.1116 - val_loss: 148.7606 - val_mae: 8.6995 - val_mse: 148.7080\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.1388 - mae: 7.3379 - mse: 100.0859 - val_loss: 149.3358 - val_mae: 8.7292 - val_mse: 149.2827\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.5131 - mae: 7.3185 - mse: 100.4598 - val_loss: 149.8845 - val_mae: 8.7305 - val_mse: 149.8310\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 2.3837, Train MSE: 12.2793\n",
      "Val   MAE: 10.6082, Val   MSE: 219.6110\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8791, Train MSE: 61.1933\n",
      "Val   MAE: 9.6900, Val   MSE: 173.9546\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.2130, Train MSE: 98.2133\n",
      "Val   MAE: 8.7305, Val   MSE: 149.8310\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all versions of the deep learning model, the regularized model has the best combination of low MAE (accuracy) and small difference between training and validation (least overfitting). \n",
    "\n",
    "Additionally, the max peak position model with genre data had a lower MAE score. For max rank change, the two models had essentially the same MAE.\n",
    "\n",
    "Next I'll optimize the regularized, max peak position model with genre and the regularized, max rank change model without genre (in the interest of maximizing model efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position (regularized, with genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2550.5391 - mae: 41.4492 - mse: 2550.5146 - val_loss: 740.9598 - val_mae: 23.0691 - val_mse: 740.9349\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1155.8129 - mae: 27.9100 - mse: 1155.7880 - val_loss: 702.9857 - val_mae: 22.6556 - val_mse: 702.9611\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1036.5747 - mae: 26.2058 - mse: 1036.5500 - val_loss: 679.6901 - val_mae: 22.3479 - val_mse: 679.6654\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 932.4151 - mae: 24.7938 - mse: 932.3902 - val_loss: 585.0630 - val_mae: 20.2962 - val_mse: 585.0381\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 918.3151 - mae: 24.4859 - mse: 918.2909 - val_loss: 594.4334 - val_mae: 20.6051 - val_mse: 594.4087\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 888.9482 - mae: 24.1876 - mse: 888.9235 - val_loss: 565.6799 - val_mae: 19.9815 - val_mse: 565.6554\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 834.1050 - mae: 23.4916 - mse: 834.0804 - val_loss: 560.8658 - val_mae: 19.8122 - val_mse: 560.8414\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 836.3242 - mae: 23.4111 - mse: 836.2998 - val_loss: 565.4625 - val_mae: 20.0786 - val_mse: 565.4384\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 799.1129 - mae: 22.9278 - mse: 799.0887 - val_loss: 525.2540 - val_mae: 18.8881 - val_mse: 525.2299\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 806.3350 - mae: 22.8305 - mse: 806.3108 - val_loss: 652.4023 - val_mae: 22.2738 - val_mse: 652.3784\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 784.1400 - mae: 22.4104 - mse: 784.1160 - val_loss: 570.8513 - val_mae: 20.3866 - val_mse: 570.8275\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 757.8657 - mae: 22.0246 - mse: 757.8420 - val_loss: 596.0571 - val_mae: 20.9241 - val_mse: 596.0337\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 786.1470 - mae: 22.6922 - mse: 786.1234 - val_loss: 529.1861 - val_mae: 19.1352 - val_mse: 529.1625\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 766.8898 - mae: 22.2443 - mse: 766.8663 - val_loss: 510.2804 - val_mae: 18.5876 - val_mse: 510.2569\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 753.2427 - mae: 22.0180 - mse: 753.2194 - val_loss: 525.5469 - val_mae: 19.0496 - val_mse: 525.5236\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 730.8320 - mae: 21.5695 - mse: 730.8087 - val_loss: 565.3273 - val_mae: 20.2059 - val_mse: 565.3044\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 739.7444 - mae: 21.6744 - mse: 739.7213 - val_loss: 528.5656 - val_mae: 19.2217 - val_mse: 528.5425\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 714.4052 - mae: 21.3837 - mse: 714.3823 - val_loss: 498.2834 - val_mae: 18.1939 - val_mse: 498.2605\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 721.2151 - mae: 21.4387 - mse: 721.1923 - val_loss: 511.8896 - val_mae: 18.6991 - val_mse: 511.8666\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.5150 - mae: 21.2263 - mse: 709.4924 - val_loss: 503.8450 - val_mae: 18.4372 - val_mse: 503.8223\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 722.8815 - mae: 21.5868 - mse: 722.8588 - val_loss: 524.2426 - val_mae: 19.1458 - val_mse: 524.2200\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 722.9645 - mae: 21.5811 - mse: 722.9418 - val_loss: 484.1558 - val_mae: 17.4672 - val_mse: 484.1331\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 701.8588 - mae: 21.1898 - mse: 701.8365 - val_loss: 508.4922 - val_mae: 18.6273 - val_mse: 508.4698\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 707.0107 - mae: 21.1946 - mse: 706.9884 - val_loss: 517.1035 - val_mae: 18.9431 - val_mse: 517.0811\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 695.5546 - mae: 21.1564 - mse: 695.5324 - val_loss: 483.6995 - val_mae: 17.4206 - val_mse: 483.6772\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 711.7127 - mae: 21.0893 - mse: 711.6906 - val_loss: 534.2538 - val_mae: 19.4332 - val_mse: 534.2317\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.2241 - mae: 20.8063 - mse: 687.2019 - val_loss: 492.8662 - val_mae: 17.9549 - val_mse: 492.8442\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 685.1694 - mae: 20.7509 - mse: 685.1471 - val_loss: 494.3452 - val_mae: 18.1496 - val_mse: 494.3230\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 683.4014 - mae: 20.6977 - mse: 683.3792 - val_loss: 515.0099 - val_mae: 18.8532 - val_mse: 514.9879\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 686.4190 - mae: 20.8577 - mse: 686.3970 - val_loss: 491.3023 - val_mae: 18.0075 - val_mse: 491.2802\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 684.1968 - mae: 20.7283 - mse: 684.1747 - val_loss: 516.0704 - val_mae: 18.8858 - val_mse: 516.0485\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 693.7278 - mae: 20.9328 - mse: 693.7061 - val_loss: 493.1169 - val_mae: 18.1141 - val_mse: 493.0950\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 682.1000 - mae: 20.6658 - mse: 682.0782 - val_loss: 494.8199 - val_mae: 18.1618 - val_mse: 494.7980\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.0580 - mae: 20.5358 - mse: 672.0363 - val_loss: 523.8546 - val_mae: 19.1590 - val_mse: 523.8331\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.3615 - mae: 20.4365 - mse: 671.3397 - val_loss: 501.2729 - val_mae: 18.4225 - val_mse: 501.2513\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.7885 - mae: 20.5133 - mse: 664.7672 - val_loss: 486.2704 - val_mae: 17.7910 - val_mse: 486.2490\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 670.3580 - mae: 20.6490 - mse: 670.3367 - val_loss: 496.4927 - val_mae: 18.2367 - val_mse: 496.4714\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.4993 - mae: 20.4315 - mse: 664.4778 - val_loss: 490.3100 - val_mae: 17.9950 - val_mse: 490.2887\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.3170 - mae: 20.5427 - mse: 677.2957 - val_loss: 488.5288 - val_mae: 17.8719 - val_mse: 488.5076\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.8015 - mae: 20.4765 - mse: 665.7805 - val_loss: 495.7144 - val_mae: 18.2026 - val_mse: 495.6934\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.9492 - mae: 20.5097 - mse: 671.9282 - val_loss: 481.3233 - val_mae: 17.2561 - val_mse: 481.3022\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.2343 - mae: 20.3878 - mse: 658.2131 - val_loss: 487.2592 - val_mae: 17.8426 - val_mse: 487.2383\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.7134 - mae: 20.2272 - mse: 659.6929 - val_loss: 492.9971 - val_mae: 18.0999 - val_mse: 492.9761\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.1669 - mae: 20.4447 - mse: 668.1460 - val_loss: 489.1465 - val_mae: 17.9422 - val_mse: 489.1257\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.6862 - mae: 20.0544 - mse: 645.6659 - val_loss: 506.8859 - val_mae: 18.5916 - val_mse: 506.8653\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.6431 - mae: 20.3296 - mse: 658.6225 - val_loss: 496.0126 - val_mae: 18.2150 - val_mse: 495.9920\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 656.2781 - mae: 20.2799 - mse: 656.2572 - val_loss: 523.6943 - val_mae: 19.1239 - val_mse: 523.6737\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.0532 - mae: 20.2836 - mse: 655.0327 - val_loss: 500.0995 - val_mae: 18.3917 - val_mse: 500.0789\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.4176 - mae: 20.3040 - mse: 663.3971 - val_loss: 491.1298 - val_mae: 18.0647 - val_mse: 491.1093\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.3411 - mae: 20.3717 - mse: 657.3207 - val_loss: 502.4334 - val_mae: 18.4791 - val_mse: 502.4131\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.4357 - mae: 19.7668 - mse: 628.4152 - val_loss: 500.6351 - val_mae: 18.4164 - val_mse: 500.6148\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.5756 - mae: 20.2909 - mse: 655.5554 - val_loss: 482.4966 - val_mae: 17.7354 - val_mse: 482.4763\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.8384 - mae: 20.1676 - mse: 645.8182 - val_loss: 508.8675 - val_mae: 18.7030 - val_mse: 508.8473\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.5884 - mae: 20.0684 - mse: 642.5679 - val_loss: 484.1640 - val_mae: 17.7971 - val_mse: 484.1436\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.4424 - mae: 20.0438 - mse: 651.4221 - val_loss: 497.4085 - val_mae: 18.3289 - val_mse: 497.3883\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.3356 - mae: 20.1667 - mse: 645.3156 - val_loss: 496.1659 - val_mae: 18.2897 - val_mse: 496.1457\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 641.0388 - mae: 19.9946 - mse: 641.0186 - val_loss: 487.5426 - val_mae: 17.9770 - val_mse: 487.5224\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.0013 - mae: 19.8813 - mse: 639.9810 - val_loss: 504.9963 - val_mae: 18.5386 - val_mse: 504.9761\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.3517 - mae: 20.0695 - mse: 648.3310 - val_loss: 508.8680 - val_mae: 18.6948 - val_mse: 508.8477\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.3317 - mae: 19.6873 - mse: 631.3113 - val_loss: 495.8689 - val_mae: 18.2423 - val_mse: 495.8489\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.7930 - mae: 19.9684 - mse: 641.7726 - val_loss: 490.4263 - val_mae: 18.0638 - val_mse: 490.4061\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.4383 - mae: 19.6888 - mse: 631.4182 - val_loss: 476.6729 - val_mae: 17.3915 - val_mse: 476.6527\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.7372 - mae: 19.6725 - mse: 627.7173 - val_loss: 499.3804 - val_mae: 18.3885 - val_mse: 499.3604\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.3793 - mae: 19.7479 - mse: 635.3593 - val_loss: 492.2260 - val_mae: 18.1469 - val_mse: 492.2060\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.0147 - mae: 19.7863 - mse: 640.9945 - val_loss: 499.8416 - val_mae: 18.4007 - val_mse: 499.8216\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.5507 - mae: 19.7689 - mse: 628.5306 - val_loss: 487.6898 - val_mae: 17.9184 - val_mse: 487.6698\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5074 - mae: 19.8996 - mse: 636.4875 - val_loss: 491.5178 - val_mae: 18.0879 - val_mse: 491.4977\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.7731 - mae: 19.7361 - mse: 626.7531 - val_loss: 477.2408 - val_mae: 17.3297 - val_mse: 477.2207\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.7603 - mae: 19.7435 - mse: 629.7402 - val_loss: 495.5009 - val_mae: 18.2366 - val_mse: 495.4808\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.2747 - mae: 19.6029 - mse: 616.2545 - val_loss: 507.8150 - val_mae: 18.6539 - val_mse: 507.7950\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.8223 - mae: 19.4152 - mse: 610.8024 - val_loss: 500.2597 - val_mae: 18.4064 - val_mse: 500.2396\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.1820 - mae: 19.6886 - mse: 627.1618 - val_loss: 474.7615 - val_mae: 17.2021 - val_mse: 474.7414\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.7963 - mae: 19.8866 - mse: 629.7762 - val_loss: 508.8105 - val_mae: 18.7000 - val_mse: 508.7905\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.2652 - mae: 19.5000 - mse: 619.2449 - val_loss: 517.9775 - val_mae: 18.9664 - val_mse: 517.9574\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 637.0454 - mae: 19.9037 - mse: 637.0253 - val_loss: 478.0966 - val_mae: 17.2741 - val_mse: 478.0764\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.6863 - mae: 19.7272 - mse: 625.6662 - val_loss: 495.0882 - val_mae: 18.2747 - val_mse: 495.0681\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.4999 - mae: 19.3451 - mse: 610.4797 - val_loss: 497.6088 - val_mae: 18.3195 - val_mse: 497.5886\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.1476 - mae: 19.6358 - mse: 626.1275 - val_loss: 491.6377 - val_mae: 18.0917 - val_mse: 491.6176\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.5822 - mae: 19.7416 - mse: 622.5624 - val_loss: 493.4169 - val_mae: 18.1188 - val_mse: 493.3967\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 617.1932 - mae: 19.5536 - mse: 617.1733 - val_loss: 477.6815 - val_mae: 17.3184 - val_mse: 477.6613\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.5979 - mae: 19.4383 - mse: 616.5777 - val_loss: 503.5620 - val_mae: 18.4623 - val_mse: 503.5419\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.9920 - mae: 19.8164 - mse: 629.9723 - val_loss: 484.6902 - val_mae: 17.6717 - val_mse: 484.6702\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.1978 - mae: 19.5906 - mse: 621.1776 - val_loss: 494.1782 - val_mae: 18.1427 - val_mse: 494.1581\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.9108 - mae: 19.4078 - mse: 613.8903 - val_loss: 495.1786 - val_mae: 18.1896 - val_mse: 495.1585\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.7232 - mae: 19.4594 - mse: 606.7029 - val_loss: 492.1408 - val_mae: 18.0852 - val_mse: 492.1205\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.5325 - mae: 19.3691 - mse: 607.5121 - val_loss: 509.1317 - val_mae: 18.6649 - val_mse: 509.1115\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.6436 - mae: 19.5878 - mse: 617.6231 - val_loss: 498.4043 - val_mae: 18.3066 - val_mse: 498.3839\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.5381 - mae: 19.2770 - mse: 606.5180 - val_loss: 487.5840 - val_mae: 17.8779 - val_mse: 487.5635\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.5746 - mae: 19.3901 - mse: 606.5545 - val_loss: 488.8189 - val_mae: 17.8906 - val_mse: 488.7984\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 628.6603 - mae: 19.7652 - mse: 628.6398 - val_loss: 514.6236 - val_mae: 18.8389 - val_mse: 514.6033\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.0532 - mae: 19.5554 - mse: 615.0328 - val_loss: 479.2493 - val_mae: 17.1063 - val_mse: 479.2287\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.2244 - mae: 19.2362 - mse: 598.2039 - val_loss: 493.3948 - val_mae: 18.0828 - val_mse: 493.3741\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.3721 - mae: 19.2377 - mse: 602.3514 - val_loss: 495.0116 - val_mae: 18.1675 - val_mse: 494.9910\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.3805 - mae: 19.2582 - mse: 597.3596 - val_loss: 490.4711 - val_mae: 17.9966 - val_mse: 490.4505\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.7319 - mae: 19.3306 - mse: 605.7114 - val_loss: 490.3131 - val_mae: 18.0128 - val_mse: 490.2925\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.2212 - mae: 19.5407 - mse: 610.2005 - val_loss: 483.0766 - val_mae: 17.5718 - val_mse: 483.0558\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.4686 - mae: 19.3910 - mse: 609.4478 - val_loss: 506.0747 - val_mae: 18.5199 - val_mse: 506.0540\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.8585 - mae: 19.4816 - mse: 612.8376 - val_loss: 516.8599 - val_mae: 18.8600 - val_mse: 516.8393\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.9306 - mae: 19.3383 - mse: 602.9100 - val_loss: 509.7747 - val_mae: 18.6421 - val_mse: 509.7538\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.6806 - mae: 19.3327 - mse: 600.6598 - val_loss: 513.7763 - val_mae: 18.7383 - val_mse: 513.7554\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_1.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding an additional deep layer\n",
      "Train MAE: 18.4079, Train MSE: 502.4922\n",
      "Val   MAE: 18.7383, Val   MSE: 513.7554\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1947.6418 - mae: 35.1718 - mse: 1947.6106 - val_loss: 655.0114 - val_mae: 21.1287 - val_mse: 654.9781\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 811.7910 - mae: 22.7635 - mse: 811.7576 - val_loss: 661.7117 - val_mae: 21.6889 - val_mse: 661.6786\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 777.1511 - mae: 22.5260 - mse: 777.1180 - val_loss: 579.3759 - val_mae: 19.7862 - val_mse: 579.3428\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 747.2487 - mae: 22.0153 - mse: 747.2158 - val_loss: 555.0302 - val_mae: 19.1920 - val_mse: 554.9969\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 716.0401 - mae: 21.5011 - mse: 716.0067 - val_loss: 547.3139 - val_mae: 19.0778 - val_mse: 547.2806\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.6259 - mae: 21.2120 - mse: 708.5923 - val_loss: 545.7036 - val_mae: 19.2439 - val_mse: 545.6703\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 693.3495 - mae: 21.0516 - mse: 693.3165 - val_loss: 528.3761 - val_mae: 18.5187 - val_mse: 528.3427\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.7571 - mae: 20.6789 - mse: 677.7236 - val_loss: 558.9370 - val_mae: 19.7317 - val_mse: 558.9038\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.5981 - mae: 20.6303 - mse: 667.5648 - val_loss: 538.8868 - val_mae: 19.1370 - val_mse: 538.8536\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.6769 - mae: 20.3503 - mse: 655.6436 - val_loss: 526.3064 - val_mae: 18.6891 - val_mse: 526.2731\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.1416 - mae: 20.1541 - mse: 649.1082 - val_loss: 516.6729 - val_mae: 17.5477 - val_mse: 516.6394\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.1287 - mae: 20.3282 - mse: 664.0956 - val_loss: 555.5757 - val_mae: 19.8125 - val_mse: 555.5428\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 652.3121 - mae: 20.1621 - mse: 652.2789 - val_loss: 524.4755 - val_mae: 18.8786 - val_mse: 524.4424\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.3555 - mae: 20.0370 - mse: 643.3226 - val_loss: 512.6190 - val_mae: 18.4717 - val_mse: 512.5861\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.1259 - mae: 20.0505 - mse: 645.0928 - val_loss: 513.7154 - val_mae: 18.5092 - val_mse: 513.6827\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.4424 - mae: 19.8908 - mse: 639.4095 - val_loss: 502.9912 - val_mae: 18.1213 - val_mse: 502.9583\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.5529 - mae: 20.1305 - mse: 649.5200 - val_loss: 561.1951 - val_mae: 20.0527 - val_mse: 561.1623\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 637.4250 - mae: 19.9691 - mse: 637.3922 - val_loss: 498.3020 - val_mae: 17.9869 - val_mse: 498.2691\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.0121 - mae: 19.3374 - mse: 608.9789 - val_loss: 497.0277 - val_mae: 17.9858 - val_mse: 496.9948\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.6761 - mae: 19.7176 - mse: 630.6432 - val_loss: 493.3241 - val_mae: 17.7289 - val_mse: 493.2912\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.8081 - mae: 19.4012 - mse: 617.7751 - val_loss: 504.0964 - val_mae: 18.3859 - val_mse: 504.0637\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.8976 - mae: 19.5183 - mse: 618.8649 - val_loss: 492.1415 - val_mae: 17.8454 - val_mse: 492.1089\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.0225 - mae: 19.6808 - mse: 623.9897 - val_loss: 493.3885 - val_mae: 17.8293 - val_mse: 493.3559\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.5745 - mae: 19.4288 - mse: 607.5417 - val_loss: 502.1556 - val_mae: 18.3113 - val_mse: 502.1230\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.8170 - mae: 19.3275 - mse: 606.7844 - val_loss: 490.6751 - val_mae: 17.7674 - val_mse: 490.6422\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.6923 - mae: 19.4358 - mse: 614.6598 - val_loss: 499.6507 - val_mae: 18.2612 - val_mse: 499.6180\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.0151 - mae: 19.2924 - mse: 612.9824 - val_loss: 502.2760 - val_mae: 18.2906 - val_mse: 502.2433\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.9226 - mae: 19.4548 - mse: 611.8900 - val_loss: 510.8444 - val_mae: 18.6010 - val_mse: 510.8119\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.6877 - mae: 19.3834 - mse: 613.6551 - val_loss: 488.3429 - val_mae: 17.7294 - val_mse: 488.3103\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.5862 - mae: 19.2099 - mse: 605.5535 - val_loss: 488.5855 - val_mae: 17.7758 - val_mse: 488.5529\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.9570 - mae: 19.1162 - mse: 594.9246 - val_loss: 494.7089 - val_mae: 18.1202 - val_mse: 494.6765\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.0259 - mae: 19.0429 - mse: 594.9933 - val_loss: 482.5582 - val_mae: 17.5338 - val_mse: 482.5256\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.3554 - mae: 19.2020 - mse: 603.3234 - val_loss: 494.1681 - val_mae: 18.0056 - val_mse: 494.1357\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.8419 - mae: 19.0757 - mse: 598.8091 - val_loss: 490.4492 - val_mae: 17.8727 - val_mse: 490.4169\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0695 - mae: 19.0699 - mse: 596.0375 - val_loss: 482.9086 - val_mae: 17.4668 - val_mse: 482.8762\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 589.5054 - mae: 18.9253 - mse: 589.4729 - val_loss: 489.0319 - val_mae: 17.6772 - val_mse: 488.9996\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.2675 - mae: 19.2040 - mse: 604.2353 - val_loss: 493.1058 - val_mae: 17.9637 - val_mse: 493.0738\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.0959 - mae: 19.0777 - mse: 599.0640 - val_loss: 481.9203 - val_mae: 17.1160 - val_mse: 481.8881\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.4242 - mae: 19.0042 - mse: 599.3920 - val_loss: 483.6495 - val_mae: 16.9985 - val_mse: 483.6175\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.2266 - mae: 18.9888 - mse: 595.1948 - val_loss: 488.0965 - val_mae: 17.7563 - val_mse: 488.0646\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.9180 - mae: 18.9963 - mse: 590.8864 - val_loss: 484.5071 - val_mae: 17.0852 - val_mse: 484.4753\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.9288 - mae: 19.0260 - mse: 593.8968 - val_loss: 491.0038 - val_mae: 17.8799 - val_mse: 490.9721\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.3633 - mae: 18.8013 - mse: 583.3317 - val_loss: 482.9609 - val_mae: 17.2798 - val_mse: 482.9290\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.9064 - mae: 19.0761 - mse: 590.8746 - val_loss: 489.4505 - val_mae: 17.7698 - val_mse: 489.4188\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.6490 - mae: 18.8248 - mse: 583.6171 - val_loss: 485.6766 - val_mae: 17.7122 - val_mse: 485.6449\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.5479 - mae: 18.9778 - mse: 593.5164 - val_loss: 484.7975 - val_mae: 17.6200 - val_mse: 484.7660\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 584.7568 - mae: 18.7926 - mse: 584.7252 - val_loss: 496.0199 - val_mae: 18.1231 - val_mse: 495.9885\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.9570 - mae: 18.8553 - mse: 583.9255 - val_loss: 479.3408 - val_mae: 17.0430 - val_mse: 479.3093\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.5997 - mae: 18.8107 - mse: 585.5683 - val_loss: 482.5511 - val_mae: 17.5251 - val_mse: 482.5198\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.0825 - mae: 18.7841 - mse: 582.0511 - val_loss: 481.9988 - val_mae: 17.4332 - val_mse: 481.9676\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 584.6132 - mae: 18.7997 - mse: 584.5820 - val_loss: 478.3465 - val_mae: 17.0048 - val_mse: 478.3151\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.9492 - mae: 18.8363 - mse: 586.9179 - val_loss: 485.7816 - val_mae: 17.7058 - val_mse: 485.7506\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 581.3176 - mae: 18.7783 - mse: 581.2865 - val_loss: 481.3427 - val_mae: 17.4790 - val_mse: 481.3116\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.6002 - mae: 18.7339 - mse: 583.5691 - val_loss: 490.8839 - val_mae: 17.9680 - val_mse: 490.8529\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.7771 - mae: 18.9066 - mse: 588.7461 - val_loss: 494.7249 - val_mae: 18.1516 - val_mse: 494.6940\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.7141 - mae: 19.0030 - mse: 591.6830 - val_loss: 480.7027 - val_mae: 17.5219 - val_mse: 480.6715\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 581.0287 - mae: 18.7429 - mse: 580.9977 - val_loss: 477.4633 - val_mae: 17.0114 - val_mse: 477.4323\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.4630 - mae: 18.7475 - mse: 580.4322 - val_loss: 479.6716 - val_mae: 17.2332 - val_mse: 479.6404\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.8371 - mae: 18.8817 - mse: 585.8061 - val_loss: 477.1076 - val_mae: 17.3566 - val_mse: 477.0766\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.5665 - mae: 18.8337 - mse: 585.5355 - val_loss: 493.6339 - val_mae: 18.1996 - val_mse: 493.6030\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 574.6594 - mae: 18.5438 - mse: 574.6284 - val_loss: 475.4626 - val_mae: 17.3363 - val_mse: 475.4317\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 573.0937 - mae: 18.6508 - mse: 573.0626 - val_loss: 476.8803 - val_mae: 17.2690 - val_mse: 476.8494\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 573.8896 - mae: 18.5744 - mse: 573.8586 - val_loss: 480.7321 - val_mae: 17.6630 - val_mse: 480.7010\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 558.4229 - mae: 18.3049 - mse: 558.3918 - val_loss: 475.1917 - val_mae: 17.2932 - val_mse: 475.1606\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 573.3161 - mae: 18.6024 - mse: 573.2852 - val_loss: 472.0461 - val_mae: 16.8801 - val_mse: 472.0150\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.4663 - mae: 18.3435 - mse: 563.4352 - val_loss: 473.9614 - val_mae: 17.0779 - val_mse: 473.9302\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.6741 - mae: 18.5198 - mse: 571.6429 - val_loss: 474.5257 - val_mae: 16.9890 - val_mse: 474.4944\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 576.7927 - mae: 18.6594 - mse: 576.7615 - val_loss: 474.1399 - val_mae: 17.4415 - val_mse: 474.1088\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.0494 - mae: 18.5744 - mse: 566.0184 - val_loss: 470.6408 - val_mae: 16.9859 - val_mse: 470.6096\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 570.6231 - mae: 18.4967 - mse: 570.5919 - val_loss: 481.0966 - val_mae: 17.6868 - val_mse: 481.0656\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.8993 - mae: 18.2853 - mse: 557.8680 - val_loss: 475.6786 - val_mae: 17.5701 - val_mse: 475.6474\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 573.7385 - mae: 18.5748 - mse: 573.7072 - val_loss: 491.3752 - val_mae: 18.1696 - val_mse: 491.3441\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 570.7053 - mae: 18.5207 - mse: 570.6740 - val_loss: 472.5103 - val_mae: 17.3014 - val_mse: 472.4790\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.5162 - mae: 18.4530 - mse: 566.4849 - val_loss: 470.0221 - val_mae: 16.8332 - val_mse: 469.9904\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.6735 - mae: 18.2882 - mse: 557.6420 - val_loss: 476.8047 - val_mae: 17.4428 - val_mse: 476.7733\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 562.2261 - mae: 18.4502 - mse: 562.1946 - val_loss: 483.9417 - val_mae: 17.8248 - val_mse: 483.9102\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 560.9247 - mae: 18.3725 - mse: 560.8930 - val_loss: 475.7039 - val_mae: 17.3353 - val_mse: 475.6723\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.2550 - mae: 18.2825 - mse: 557.2232 - val_loss: 476.4225 - val_mae: 17.2851 - val_mse: 476.3907\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 572.4376 - mae: 18.6288 - mse: 572.4057 - val_loss: 477.3485 - val_mae: 17.5429 - val_mse: 477.3166\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 560.3908 - mae: 18.3922 - mse: 560.3586 - val_loss: 474.2136 - val_mae: 17.1586 - val_mse: 474.1815\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 561.6982 - mae: 18.4426 - mse: 561.6658 - val_loss: 473.9059 - val_mae: 17.2012 - val_mse: 473.8735\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.7866 - mae: 18.1748 - mse: 549.7542 - val_loss: 475.0828 - val_mae: 17.2518 - val_mse: 475.0504\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.0781 - mae: 18.2799 - mse: 554.0460 - val_loss: 476.5026 - val_mae: 16.9433 - val_mse: 476.4700\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 562.0466 - mae: 18.4127 - mse: 562.0141 - val_loss: 479.1758 - val_mae: 17.5542 - val_mse: 479.1433\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.7370 - mae: 18.1100 - mse: 549.7042 - val_loss: 480.2450 - val_mae: 17.7136 - val_mse: 480.2123\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 560.6952 - mae: 18.3094 - mse: 560.6626 - val_loss: 494.1032 - val_mae: 18.2051 - val_mse: 494.0705\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 556.4014 - mae: 18.4008 - mse: 556.3684 - val_loss: 471.7414 - val_mae: 17.0913 - val_mse: 471.7083\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.6288 - mae: 18.1612 - mse: 554.5956 - val_loss: 477.7794 - val_mae: 17.5516 - val_mse: 477.7462\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 550.4943 - mae: 18.2538 - mse: 550.4611 - val_loss: 475.2423 - val_mae: 17.3897 - val_mse: 475.2090\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.7153 - mae: 18.1432 - mse: 552.6820 - val_loss: 481.1389 - val_mae: 17.5074 - val_mse: 481.1054\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 534.4479 - mae: 17.9184 - mse: 534.4140 - val_loss: 480.5096 - val_mae: 17.5702 - val_mse: 480.4760\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.9672 - mae: 18.0922 - mse: 541.9336 - val_loss: 495.3867 - val_mae: 18.2055 - val_mse: 495.3529\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 547.3016 - mae: 18.0461 - mse: 547.2676 - val_loss: 473.7418 - val_mae: 17.1506 - val_mse: 473.7074\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 547.2070 - mae: 18.1712 - mse: 547.1727 - val_loss: 494.7887 - val_mae: 18.2204 - val_mse: 494.7543\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.9669 - mae: 18.2283 - mse: 551.9323 - val_loss: 475.0000 - val_mae: 17.3996 - val_mse: 474.9654\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.6801 - mae: 18.2092 - mse: 551.6451 - val_loss: 471.9733 - val_mae: 17.1965 - val_mse: 471.9385\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.9735 - mae: 18.1078 - mse: 544.9385 - val_loss: 474.2457 - val_mae: 17.2975 - val_mse: 474.2104\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 546.9900 - mae: 18.1082 - mse: 546.9548 - val_loss: 478.4219 - val_mae: 17.4139 - val_mse: 478.3865\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 539.7834 - mae: 18.0308 - mse: 539.7480 - val_loss: 484.8378 - val_mae: 17.7456 - val_mse: 484.8021\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 542.6056 - mae: 18.0773 - mse: 542.5696 - val_loss: 473.3631 - val_mae: 17.1504 - val_mse: 473.3271\n"
     ]
    }
   ],
   "source": [
    "reg_model_maxpeak_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_2.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    3 deep layers, 128 kernels per layer\n",
      "Train MAE: 16.4419, Train MSE: 458.0215\n",
      "Val   MAE: 17.1504, Val   MSE: 473.3271\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model but increased overfitting a little bit. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2478.4563 - mae: 41.0362 - mse: 2478.4243 - val_loss: 688.8759 - val_mae: 21.8672 - val_mse: 688.8425\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1096.3026 - mae: 27.2055 - mse: 1096.2690 - val_loss: 625.9619 - val_mae: 20.8098 - val_mse: 625.9283\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 994.9009 - mae: 25.5603 - mse: 994.8668 - val_loss: 627.5157 - val_mae: 21.0581 - val_mse: 627.4822\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 928.9902 - mae: 24.8961 - mse: 928.9564 - val_loss: 576.8396 - val_mae: 19.8430 - val_mse: 576.8060\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 915.4586 - mae: 24.5118 - mse: 915.4252 - val_loss: 561.7078 - val_mae: 19.6096 - val_mse: 561.6743\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 891.7353 - mae: 24.4100 - mse: 891.7018 - val_loss: 589.1294 - val_mae: 20.4652 - val_mse: 589.0963\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 860.2171 - mae: 23.8213 - mse: 860.1838 - val_loss: 565.7035 - val_mae: 19.7964 - val_mse: 565.6703\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 852.5818 - mae: 23.5688 - mse: 852.5491 - val_loss: 568.8915 - val_mae: 19.9438 - val_mse: 568.8585\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 829.4464 - mae: 23.2769 - mse: 829.4132 - val_loss: 579.9208 - val_mae: 20.4331 - val_mse: 579.8879\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 811.2747 - mae: 23.0195 - mse: 811.2419 - val_loss: 534.5608 - val_mae: 19.1898 - val_mse: 534.5280\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 802.0970 - mae: 22.9190 - mse: 802.0641 - val_loss: 516.0900 - val_mae: 18.5245 - val_mse: 516.0573\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 792.2016 - mae: 22.6350 - mse: 792.1690 - val_loss: 566.6472 - val_mae: 20.1046 - val_mse: 566.6146\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 788.8181 - mae: 22.5517 - mse: 788.7854 - val_loss: 526.8708 - val_mae: 18.9638 - val_mse: 526.8380\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 765.4383 - mae: 22.4468 - mse: 765.4056 - val_loss: 500.5483 - val_mae: 17.8038 - val_mse: 500.5157\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 756.5424 - mae: 22.2666 - mse: 756.5099 - val_loss: 519.1866 - val_mae: 18.7940 - val_mse: 519.1542\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 758.8331 - mae: 22.1527 - mse: 758.8005 - val_loss: 536.5627 - val_mae: 19.3514 - val_mse: 536.5303\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 777.5232 - mae: 22.4699 - mse: 777.4909 - val_loss: 494.8387 - val_mae: 17.5974 - val_mse: 494.8063\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 757.5643 - mae: 22.0101 - mse: 757.5318 - val_loss: 542.9224 - val_mae: 19.6194 - val_mse: 542.8903\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 753.8101 - mae: 22.0114 - mse: 753.7780 - val_loss: 519.9173 - val_mae: 18.8263 - val_mse: 519.8853\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 733.3951 - mae: 21.6467 - mse: 733.3630 - val_loss: 524.7614 - val_mae: 18.9796 - val_mse: 524.7294\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 720.9318 - mae: 21.5050 - mse: 720.8997 - val_loss: 560.3332 - val_mae: 20.0744 - val_mse: 560.3014\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 738.3373 - mae: 21.7403 - mse: 738.3057 - val_loss: 521.1794 - val_mae: 18.9460 - val_mse: 521.1477\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 726.1182 - mae: 21.5970 - mse: 726.0865 - val_loss: 523.7363 - val_mae: 19.0252 - val_mse: 523.7048\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 736.1784 - mae: 21.6533 - mse: 736.1469 - val_loss: 547.8981 - val_mae: 19.7418 - val_mse: 547.8666\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 735.3216 - mae: 21.8403 - mse: 735.2903 - val_loss: 489.6526 - val_mae: 17.5839 - val_mse: 489.6210\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 724.7808 - mae: 21.2213 - mse: 724.7495 - val_loss: 497.5825 - val_mae: 18.0333 - val_mse: 497.5511\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 717.6314 - mae: 21.4437 - mse: 717.6002 - val_loss: 492.2149 - val_mae: 17.8647 - val_mse: 492.1837\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 705.3210 - mae: 21.1197 - mse: 705.2899 - val_loss: 522.6177 - val_mae: 19.0191 - val_mse: 522.5867\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.2439 - mae: 20.8431 - mse: 690.2129 - val_loss: 507.2786 - val_mae: 18.4603 - val_mse: 507.2478\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 719.2338 - mae: 21.3417 - mse: 719.2030 - val_loss: 495.6480 - val_mae: 18.0548 - val_mse: 495.6172\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 700.3171 - mae: 21.1153 - mse: 700.2864 - val_loss: 486.0881 - val_mae: 17.4873 - val_mse: 486.0574\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 712.0428 - mae: 21.2049 - mse: 712.0122 - val_loss: 485.0630 - val_mae: 17.4677 - val_mse: 485.0323\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 701.3853 - mae: 21.0071 - mse: 701.3550 - val_loss: 504.3051 - val_mae: 18.4410 - val_mse: 504.2747\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 682.5002 - mae: 20.6946 - mse: 682.4699 - val_loss: 496.1633 - val_mae: 18.1032 - val_mse: 496.1330\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.1106 - mae: 20.9223 - mse: 690.0804 - val_loss: 498.7805 - val_mae: 18.1698 - val_mse: 498.7503\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.8868 - mae: 20.4827 - mse: 668.8566 - val_loss: 500.9088 - val_mae: 18.2860 - val_mse: 500.8787\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 686.5294 - mae: 20.7973 - mse: 686.4995 - val_loss: 499.2328 - val_mae: 18.2388 - val_mse: 499.2029\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 703.3476 - mae: 21.0112 - mse: 703.3174 - val_loss: 496.2560 - val_mae: 18.0851 - val_mse: 496.2260\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.5924 - mae: 20.8341 - mse: 687.5628 - val_loss: 493.8365 - val_mae: 17.9932 - val_mse: 493.8067\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 676.4921 - mae: 20.6900 - mse: 676.4623 - val_loss: 491.3937 - val_mae: 17.8480 - val_mse: 491.3642\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.1445 - mae: 20.8832 - mse: 690.1149 - val_loss: 490.3721 - val_mae: 17.8587 - val_mse: 490.3425\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 684.2949 - mae: 20.7633 - mse: 684.2653 - val_loss: 487.1249 - val_mae: 17.7177 - val_mse: 487.0954\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.9189 - mae: 20.4843 - mse: 668.8893 - val_loss: 495.0521 - val_mae: 18.0695 - val_mse: 495.0228\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.7018 - mae: 20.8098 - mse: 687.6724 - val_loss: 499.3498 - val_mae: 18.2399 - val_mse: 499.3206\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 682.7531 - mae: 20.6478 - mse: 682.7239 - val_loss: 495.9377 - val_mae: 18.0392 - val_mse: 495.9085\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 686.1223 - mae: 20.6839 - mse: 686.0932 - val_loss: 491.4380 - val_mae: 17.9509 - val_mse: 491.4090\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 684.5901 - mae: 20.7069 - mse: 684.5607 - val_loss: 494.3822 - val_mae: 18.0656 - val_mse: 494.3532\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.0345 - mae: 20.5921 - mse: 677.0055 - val_loss: 500.5854 - val_mae: 18.2500 - val_mse: 500.5565\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.1715 - mae: 20.4136 - mse: 673.1421 - val_loss: 485.9848 - val_mae: 17.7099 - val_mse: 485.9560\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.6024 - mae: 20.3238 - mse: 660.5735 - val_loss: 498.6259 - val_mae: 18.2086 - val_mse: 498.5974\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.4070 - mae: 20.4671 - mse: 677.3787 - val_loss: 485.6252 - val_mae: 17.5975 - val_mse: 485.5963\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.6506 - mae: 20.2072 - mse: 653.6224 - val_loss: 485.1877 - val_mae: 17.6638 - val_mse: 485.1588\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.4370 - mae: 20.5077 - mse: 673.4086 - val_loss: 502.8574 - val_mae: 18.3921 - val_mse: 502.8289\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.0224 - mae: 20.3676 - mse: 664.9940 - val_loss: 482.2259 - val_mae: 17.5766 - val_mse: 482.1973\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.3185 - mae: 20.2311 - mse: 658.2900 - val_loss: 493.5415 - val_mae: 18.0692 - val_mse: 493.5133\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.7979 - mae: 20.3775 - mse: 666.7692 - val_loss: 483.5042 - val_mae: 17.5700 - val_mse: 483.4758\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.7431 - mae: 20.1230 - mse: 653.7148 - val_loss: 482.3199 - val_mae: 17.4392 - val_mse: 482.2915\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 670.0886 - mae: 20.4485 - mse: 670.0605 - val_loss: 502.0713 - val_mae: 18.3602 - val_mse: 502.0431\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.7696 - mae: 20.3370 - mse: 663.7415 - val_loss: 483.5248 - val_mae: 17.5228 - val_mse: 483.4963\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.3087 - mae: 20.3571 - mse: 662.2805 - val_loss: 485.4273 - val_mae: 17.7922 - val_mse: 485.3989\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.9410 - mae: 20.1644 - mse: 651.9124 - val_loss: 491.6492 - val_mae: 18.0051 - val_mse: 491.6209\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.6440 - mae: 20.1311 - mse: 649.6153 - val_loss: 479.3885 - val_mae: 17.5010 - val_mse: 479.3601\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.6992 - mae: 19.8304 - mse: 633.6708 - val_loss: 477.8202 - val_mae: 17.4112 - val_mse: 477.7918\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 656.1469 - mae: 20.2269 - mse: 656.1182 - val_loss: 477.7863 - val_mae: 17.2894 - val_mse: 477.7579\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.6312 - mae: 20.2213 - mse: 659.6028 - val_loss: 487.8446 - val_mae: 17.8939 - val_mse: 487.8163\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.8828 - mae: 20.1693 - mse: 648.8545 - val_loss: 479.8856 - val_mae: 17.5885 - val_mse: 479.8573\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.1058 - mae: 20.0992 - mse: 646.0777 - val_loss: 476.3389 - val_mae: 17.3619 - val_mse: 476.3106\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.8201 - mae: 20.1197 - mse: 655.7918 - val_loss: 486.8254 - val_mae: 17.8201 - val_mse: 486.7970\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.6218 - mae: 19.8811 - mse: 630.5935 - val_loss: 479.2194 - val_mae: 17.5087 - val_mse: 479.1911\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.5709 - mae: 20.0134 - mse: 642.5427 - val_loss: 490.5251 - val_mae: 18.0358 - val_mse: 490.4969\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.4320 - mae: 19.8762 - mse: 638.4039 - val_loss: 482.9916 - val_mae: 17.7136 - val_mse: 482.9632\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.8569 - mae: 19.6569 - mse: 629.8286 - val_loss: 477.0014 - val_mae: 17.3919 - val_mse: 476.9731\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.0268 - mae: 19.8620 - mse: 639.9981 - val_loss: 488.7954 - val_mae: 18.0242 - val_mse: 488.7668\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.3289 - mae: 19.8351 - mse: 638.3000 - val_loss: 475.9368 - val_mae: 17.3351 - val_mse: 475.9083\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.1443 - mae: 19.9036 - mse: 639.1156 - val_loss: 477.2393 - val_mae: 17.4700 - val_mse: 477.2108\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.4490 - mae: 20.1064 - mse: 648.4210 - val_loss: 474.1721 - val_mae: 17.3201 - val_mse: 474.1436\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.6848 - mae: 19.6568 - mse: 634.6564 - val_loss: 474.4938 - val_mae: 17.2552 - val_mse: 474.4652\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.7017 - mae: 19.7469 - mse: 631.6727 - val_loss: 477.5831 - val_mae: 17.3956 - val_mse: 477.5543\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.7173 - mae: 19.5826 - mse: 621.6886 - val_loss: 515.7998 - val_mae: 18.8591 - val_mse: 515.7714\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.3489 - mae: 19.7985 - mse: 625.3203 - val_loss: 475.1909 - val_mae: 17.1527 - val_mse: 475.1620\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.2001 - mae: 19.9523 - mse: 639.1713 - val_loss: 476.8680 - val_mae: 17.4651 - val_mse: 476.8391\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.0585 - mae: 19.4518 - mse: 619.0295 - val_loss: 480.7815 - val_mae: 17.6980 - val_mse: 480.7526\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 625.2151 - mae: 19.6333 - mse: 625.1863 - val_loss: 473.2425 - val_mae: 17.2956 - val_mse: 473.2134\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.0717 - mae: 19.9242 - mse: 645.0425 - val_loss: 472.3946 - val_mae: 17.0140 - val_mse: 472.3653\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.7697 - mae: 19.5434 - mse: 613.7403 - val_loss: 478.8974 - val_mae: 17.5562 - val_mse: 478.8680\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.9986 - mae: 19.7270 - mse: 623.9692 - val_loss: 473.8050 - val_mae: 17.1988 - val_mse: 473.7756\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.1812 - mae: 19.4977 - mse: 621.1520 - val_loss: 484.6533 - val_mae: 17.8342 - val_mse: 484.6239\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.0715 - mae: 19.5466 - mse: 615.0419 - val_loss: 473.2037 - val_mae: 17.1475 - val_mse: 473.1738\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 625.1141 - mae: 19.5554 - mse: 625.0842 - val_loss: 490.3676 - val_mae: 18.0069 - val_mse: 490.3377\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.0599 - mae: 19.4145 - mse: 619.0303 - val_loss: 481.3433 - val_mae: 17.6867 - val_mse: 481.3134\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8957 - mae: 19.4432 - mse: 615.8661 - val_loss: 498.4056 - val_mae: 18.3314 - val_mse: 498.3759\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.1568 - mae: 19.2768 - mse: 604.1268 - val_loss: 494.6874 - val_mae: 18.1755 - val_mse: 494.6572\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.1085 - mae: 19.6061 - mse: 618.0781 - val_loss: 473.8729 - val_mae: 17.1094 - val_mse: 473.8422\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.2227 - mae: 19.3860 - mse: 614.1921 - val_loss: 475.6330 - val_mae: 17.2428 - val_mse: 475.6022\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.8269 - mae: 19.4244 - mse: 612.7960 - val_loss: 492.9090 - val_mae: 18.1359 - val_mse: 492.8779\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.8717 - mae: 19.5517 - mse: 618.8406 - val_loss: 487.7396 - val_mae: 17.9320 - val_mse: 487.7084\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.4332 - mae: 19.3294 - mse: 612.4018 - val_loss: 482.0693 - val_mae: 17.6184 - val_mse: 482.0380\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.1392 - mae: 19.3370 - mse: 608.1077 - val_loss: 478.5480 - val_mae: 17.4536 - val_mse: 478.5165\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.0007 - mae: 19.5646 - mse: 616.9690 - val_loss: 498.1702 - val_mae: 18.2845 - val_mse: 498.1383\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.6151 - mae: 19.2996 - mse: 606.5831 - val_loss: 485.9403 - val_mae: 17.8200 - val_mse: 485.9081\n"
     ]
    }
   ],
   "source": [
    "# increasing dropout rate\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_3 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_3.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "   3 deep layers, 128 kernels per layer\n",
      "Train MAE: 17.5323, Train MSE: 488.9498\n",
      "Val   MAE: 17.8200, Val   MSE: 485.9081\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"   3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the dropout rate brought the training and validation MAEs closer together but the result is similar to the model before optimization. Trying another layer again, keeping the increased kernels and higher dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2260.2783 - mae: 38.9985 - mse: 2260.2344 - val_loss: 892.2157 - val_mae: 26.0777 - val_mse: 892.1715\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1165.7540 - mae: 28.1469 - mse: 1165.7098 - val_loss: 664.5418 - val_mae: 21.8370 - val_mse: 664.4970\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1041.1562 - mae: 26.4527 - mse: 1041.1118 - val_loss: 735.1785 - val_mae: 23.5140 - val_mse: 735.1337\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 976.5663 - mae: 25.5190 - mse: 976.5210 - val_loss: 753.2098 - val_mae: 24.0159 - val_mse: 753.1648\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 926.5393 - mae: 24.7539 - mse: 926.4940 - val_loss: 736.9257 - val_mae: 23.7986 - val_mse: 736.8807\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 915.1553 - mae: 24.6918 - mse: 915.1100 - val_loss: 768.8854 - val_mae: 24.4699 - val_mse: 768.8405\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 866.8342 - mae: 24.0129 - mse: 866.7891 - val_loss: 736.0831 - val_mae: 23.9431 - val_mse: 736.0383\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 840.6263 - mae: 23.6072 - mse: 840.5812 - val_loss: 711.0715 - val_mae: 23.4406 - val_mse: 711.0267\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 836.6950 - mae: 23.5311 - mse: 836.6506 - val_loss: 799.3163 - val_mae: 25.2317 - val_mse: 799.2717\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 820.9029 - mae: 23.0700 - mse: 820.8581 - val_loss: 648.2452 - val_mae: 22.1670 - val_mse: 648.2004\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 796.8235 - mae: 22.7436 - mse: 796.7787 - val_loss: 655.4661 - val_mae: 22.3058 - val_mse: 655.4214\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 791.0940 - mae: 22.7491 - mse: 791.0492 - val_loss: 631.7515 - val_mae: 21.7795 - val_mse: 631.7068\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 796.2056 - mae: 22.7986 - mse: 796.1614 - val_loss: 651.4388 - val_mae: 22.2931 - val_mse: 651.3943\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 766.9386 - mae: 22.2266 - mse: 766.8946 - val_loss: 582.6080 - val_mae: 20.6737 - val_mse: 582.5634\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 749.9867 - mae: 22.0031 - mse: 749.9427 - val_loss: 595.9649 - val_mae: 21.0290 - val_mse: 595.9204\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 737.2410 - mae: 21.9034 - mse: 737.1964 - val_loss: 619.9849 - val_mae: 21.5867 - val_mse: 619.9406\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 753.0820 - mae: 22.0207 - mse: 753.0380 - val_loss: 672.1492 - val_mae: 22.7704 - val_mse: 672.1050\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 731.3133 - mae: 21.7598 - mse: 731.2690 - val_loss: 577.0895 - val_mae: 20.5370 - val_mse: 577.0452\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 728.9861 - mae: 21.6846 - mse: 728.9417 - val_loss: 583.3571 - val_mae: 20.6875 - val_mse: 583.3127\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 701.5103 - mae: 21.2071 - mse: 701.4661 - val_loss: 608.7086 - val_mae: 21.3645 - val_mse: 608.6645\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 713.2109 - mae: 21.5037 - mse: 713.1669 - val_loss: 576.5927 - val_mae: 20.5227 - val_mse: 576.5487\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 709.9100 - mae: 21.2417 - mse: 709.8659 - val_loss: 582.9475 - val_mae: 20.6988 - val_mse: 582.9034\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 715.9779 - mae: 21.4241 - mse: 715.9340 - val_loss: 543.1458 - val_mae: 19.6399 - val_mse: 543.1016\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 720.7066 - mae: 21.4072 - mse: 720.6630 - val_loss: 574.0916 - val_mae: 20.5002 - val_mse: 574.0477\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 700.0049 - mae: 21.0990 - mse: 699.9610 - val_loss: 539.9302 - val_mae: 19.5932 - val_mse: 539.8863\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 704.3446 - mae: 21.1646 - mse: 704.3008 - val_loss: 569.1740 - val_mae: 20.3814 - val_mse: 569.1302\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 705.7264 - mae: 21.0766 - mse: 705.6826 - val_loss: 505.8164 - val_mae: 18.4899 - val_mse: 505.7724\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 707.5071 - mae: 21.2143 - mse: 707.4630 - val_loss: 559.4184 - val_mae: 20.1259 - val_mse: 559.3746\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 680.6580 - mae: 20.7973 - mse: 680.6147 - val_loss: 555.8986 - val_mae: 20.0296 - val_mse: 555.8549\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 694.8723 - mae: 20.9669 - mse: 694.8285 - val_loss: 595.6931 - val_mae: 21.0660 - val_mse: 595.6497\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 684.5768 - mae: 20.8673 - mse: 684.5334 - val_loss: 571.6312 - val_mae: 20.4601 - val_mse: 571.5876\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 686.3846 - mae: 20.9110 - mse: 686.3410 - val_loss: 530.6046 - val_mae: 19.3161 - val_mse: 530.5608\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 694.0100 - mae: 20.9976 - mse: 693.9664 - val_loss: 544.2440 - val_mae: 19.6936 - val_mse: 544.2002\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 662.2266 - mae: 20.5896 - mse: 662.1829 - val_loss: 511.4301 - val_mae: 18.7523 - val_mse: 511.3865\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 698.4640 - mae: 20.9591 - mse: 698.4200 - val_loss: 529.0475 - val_mae: 19.3192 - val_mse: 529.0040\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 684.8284 - mae: 20.8552 - mse: 684.7849 - val_loss: 515.9723 - val_mae: 18.8849 - val_mse: 515.9285\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 671.6181 - mae: 20.5611 - mse: 671.5740 - val_loss: 518.4438 - val_mae: 18.9405 - val_mse: 518.4000\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 674.9081 - mae: 20.6264 - mse: 674.8644 - val_loss: 504.1011 - val_mae: 18.4799 - val_mse: 504.0573\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.4896 - mae: 20.6070 - mse: 672.4459 - val_loss: 551.5110 - val_mae: 19.9167 - val_mse: 551.4673\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.5237 - mae: 20.6483 - mse: 674.4803 - val_loss: 505.3721 - val_mae: 18.5165 - val_mse: 505.3281\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 665.4890 - mae: 20.4063 - mse: 665.4449 - val_loss: 523.5003 - val_mae: 19.1046 - val_mse: 523.4562\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 669.4604 - mae: 20.5323 - mse: 669.4164 - val_loss: 494.3637 - val_mae: 18.1712 - val_mse: 494.3195\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 657.3191 - mae: 20.2736 - mse: 657.2753 - val_loss: 509.8946 - val_mae: 18.6618 - val_mse: 509.8504\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 652.6611 - mae: 20.1918 - mse: 652.6166 - val_loss: 503.8331 - val_mae: 18.4532 - val_mse: 503.7886\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.6560 - mae: 20.2754 - mse: 660.6117 - val_loss: 492.0482 - val_mae: 17.9912 - val_mse: 492.0037\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 671.4720 - mae: 20.4771 - mse: 671.4273 - val_loss: 521.2620 - val_mae: 19.0095 - val_mse: 521.2177\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.3011 - mae: 20.1976 - mse: 659.2567 - val_loss: 504.6066 - val_mae: 18.4803 - val_mse: 504.5620\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.4183 - mae: 20.0287 - mse: 641.3737 - val_loss: 497.7297 - val_mae: 18.2308 - val_mse: 497.6851\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 670.9264 - mae: 20.4418 - mse: 670.8818 - val_loss: 508.7089 - val_mae: 18.6174 - val_mse: 508.6642\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 660.6922 - mae: 20.2284 - mse: 660.6474 - val_loss: 494.0539 - val_mae: 18.0824 - val_mse: 494.0089\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.2308 - mae: 20.3000 - mse: 659.1860 - val_loss: 497.3497 - val_mae: 18.1853 - val_mse: 497.3046\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 646.9742 - mae: 20.1752 - mse: 646.9291 - val_loss: 494.1869 - val_mae: 18.0023 - val_mse: 494.1416\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 657.2301 - mae: 20.3846 - mse: 657.1852 - val_loss: 516.4979 - val_mae: 18.8169 - val_mse: 516.4525\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 645.7410 - mae: 20.1121 - mse: 645.6956 - val_loss: 485.3294 - val_mae: 17.5602 - val_mse: 485.2838\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 656.4642 - mae: 20.1771 - mse: 656.4186 - val_loss: 522.3662 - val_mae: 18.9677 - val_mse: 522.3206\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.0648 - mae: 20.1618 - mse: 653.0187 - val_loss: 512.1999 - val_mae: 18.6280 - val_mse: 512.1541\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.6245 - mae: 19.9749 - mse: 641.5782 - val_loss: 488.7241 - val_mae: 17.7736 - val_mse: 488.6779\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 656.3311 - mae: 20.1086 - mse: 656.2848 - val_loss: 493.7908 - val_mae: 17.9799 - val_mse: 493.7443\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 633.6789 - mae: 19.9036 - mse: 633.6323 - val_loss: 488.2796 - val_mae: 17.7106 - val_mse: 488.2328\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 644.1697 - mae: 19.9474 - mse: 644.1230 - val_loss: 500.3830 - val_mae: 18.2886 - val_mse: 500.3361\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5522 - mae: 19.9450 - mse: 636.5053 - val_loss: 512.4265 - val_mae: 18.6985 - val_mse: 512.3797\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.5363 - mae: 20.0361 - mse: 648.4893 - val_loss: 481.1758 - val_mae: 17.3935 - val_mse: 481.1285\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.3122 - mae: 19.8000 - mse: 634.2650 - val_loss: 501.4670 - val_mae: 18.2725 - val_mse: 501.4197\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 650.8057 - mae: 19.9307 - mse: 650.7582 - val_loss: 492.8241 - val_mae: 18.0023 - val_mse: 492.7763\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 636.2818 - mae: 19.9192 - mse: 636.2338 - val_loss: 514.9262 - val_mae: 18.7729 - val_mse: 514.8785\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.9224 - mae: 19.9163 - mse: 636.8744 - val_loss: 495.1886 - val_mae: 18.0935 - val_mse: 495.1404\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.6268 - mae: 19.6556 - mse: 623.5784 - val_loss: 510.7019 - val_mae: 18.6424 - val_mse: 510.6537\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.2888 - mae: 19.5972 - mse: 625.2399 - val_loss: 521.2504 - val_mae: 18.9539 - val_mse: 521.2021\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.2126 - mae: 19.6564 - mse: 617.1640 - val_loss: 505.8094 - val_mae: 18.4510 - val_mse: 505.7607\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 631.6982 - mae: 19.7742 - mse: 631.6493 - val_loss: 504.4453 - val_mae: 18.4250 - val_mse: 504.3963\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.2637 - mae: 19.5922 - mse: 625.2140 - val_loss: 497.9720 - val_mae: 18.1244 - val_mse: 497.9230\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 623.3605 - mae: 19.6617 - mse: 623.3110 - val_loss: 492.3997 - val_mae: 17.9087 - val_mse: 492.3502\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 619.8915 - mae: 19.6552 - mse: 619.8424 - val_loss: 497.1014 - val_mae: 18.0711 - val_mse: 497.0521\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.3320 - mae: 19.7044 - mse: 627.2820 - val_loss: 490.8898 - val_mae: 17.7958 - val_mse: 490.8401\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 637.0105 - mae: 19.8008 - mse: 636.9607 - val_loss: 489.6224 - val_mae: 17.7649 - val_mse: 489.5724\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 620.5110 - mae: 19.5791 - mse: 620.4608 - val_loss: 510.0927 - val_mae: 18.5552 - val_mse: 510.0426\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.0084 - mae: 19.4758 - mse: 621.9581 - val_loss: 506.1238 - val_mae: 18.3869 - val_mse: 506.0735\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 615.2650 - mae: 19.4554 - mse: 615.2142 - val_loss: 525.0690 - val_mae: 19.0134 - val_mse: 525.0184\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.1758 - mae: 19.7512 - mse: 628.1250 - val_loss: 510.1824 - val_mae: 18.5474 - val_mse: 510.1316\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.6378 - mae: 19.6320 - mse: 620.5870 - val_loss: 494.1287 - val_mae: 17.8872 - val_mse: 494.0775\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 619.1696 - mae: 19.4652 - mse: 619.1182 - val_loss: 509.3361 - val_mae: 18.5191 - val_mse: 509.2846\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.9091 - mae: 19.6177 - mse: 617.8578 - val_loss: 494.2689 - val_mae: 17.8958 - val_mse: 494.2173\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.5320 - mae: 19.6401 - mse: 618.4801 - val_loss: 509.0492 - val_mae: 18.4973 - val_mse: 508.9973\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.3395 - mae: 19.6195 - mse: 619.2875 - val_loss: 507.0007 - val_mae: 18.4208 - val_mse: 506.9484\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.9060 - mae: 19.4612 - mse: 615.8537 - val_loss: 514.4423 - val_mae: 18.6437 - val_mse: 514.3897\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 615.7711 - mae: 19.5142 - mse: 615.7181 - val_loss: 514.0265 - val_mae: 18.6385 - val_mse: 513.9738\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.0441 - mae: 19.5945 - mse: 621.9914 - val_loss: 503.9153 - val_mae: 18.2176 - val_mse: 503.8623\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.9501 - mae: 19.6290 - mse: 622.8966 - val_loss: 502.6047 - val_mae: 18.1933 - val_mse: 502.5512\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 606.0505 - mae: 19.2540 - mse: 605.9969 - val_loss: 497.3097 - val_mae: 17.9605 - val_mse: 497.2559\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 601.8676 - mae: 19.2962 - mse: 601.8137 - val_loss: 495.3577 - val_mae: 17.8204 - val_mse: 495.3036\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 606.7151 - mae: 19.2875 - mse: 606.6609 - val_loss: 492.4668 - val_mae: 17.7261 - val_mse: 492.4124\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.9197 - mae: 19.2355 - mse: 601.8652 - val_loss: 506.0706 - val_mae: 18.3305 - val_mse: 506.0161\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.1295 - mae: 19.2350 - mse: 602.0746 - val_loss: 492.6207 - val_mae: 17.7311 - val_mse: 492.5656\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.6532 - mae: 19.3960 - mse: 605.5978 - val_loss: 532.1761 - val_mae: 19.1733 - val_mse: 532.1208\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.0135 - mae: 19.4414 - mse: 613.9580 - val_loss: 513.3737 - val_mae: 18.5248 - val_mse: 513.3181\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 608.1764 - mae: 19.3674 - mse: 608.1205 - val_loss: 501.4567 - val_mae: 17.9775 - val_mse: 501.4007\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.3032 - mae: 19.2121 - mse: 604.2469 - val_loss: 500.6130 - val_mae: 17.8932 - val_mse: 500.5566\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.7468 - mae: 19.1167 - mse: 596.6901 - val_loss: 505.1090 - val_mae: 18.1973 - val_mse: 505.0525\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 599.4329 - mae: 19.2887 - mse: 599.3760 - val_loss: 509.0491 - val_mae: 18.3779 - val_mse: 508.9921\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.5031 - mae: 19.4569 - mse: 612.4459 - val_loss: 511.9264 - val_mae: 18.5028 - val_mse: 511.8691\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_4 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_4.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_4.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "  4 deep layers, 128 kernels per layer\n",
      "Train MAE: 18.2605, Train MSE: 506.6980\n",
      "Val   MAE: 18.5028, Val   MSE: 511.8691\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"  4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model continues to have the best combination of train/val MAE and limited overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_maxpeak_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4),\n",
    "                 input_shape=(X_nogenre_train.shape[1],)),\n",
    "    layers.Dropout(0.4),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "final_maxpeak_model = reg_model_maxpeak_nogenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change (regularized, no genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 179.9257 - mae: 9.6881 - mse: 179.9037 - val_loss: 150.1309 - val_mae: 8.7702 - val_mse: 150.1087\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 154.3066 - mae: 9.0212 - mse: 154.2844 - val_loss: 149.9547 - val_mae: 8.7279 - val_mse: 149.9326\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 148.8666 - mae: 8.8780 - mse: 148.8444 - val_loss: 149.3153 - val_mae: 8.7046 - val_mse: 149.2931\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.4923 - mae: 8.8583 - mse: 147.4701 - val_loss: 143.9539 - val_mae: 8.5822 - val_mse: 143.9317\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 145.6524 - mae: 8.7344 - mse: 145.6302 - val_loss: 143.7356 - val_mae: 8.6012 - val_mse: 143.7134\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.5453 - mae: 8.7338 - mse: 144.5232 - val_loss: 143.2408 - val_mae: 8.5816 - val_mse: 143.2186\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 145.5588 - mae: 8.7699 - mse: 145.5367 - val_loss: 148.5334 - val_mae: 8.6450 - val_mse: 148.5113\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5407 - mae: 8.6716 - mse: 142.5186 - val_loss: 143.3226 - val_mae: 8.5688 - val_mse: 143.3004\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.1665 - mae: 8.6336 - mse: 141.1444 - val_loss: 143.7142 - val_mae: 8.5770 - val_mse: 143.6921\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.3748 - mae: 8.6201 - mse: 141.3527 - val_loss: 143.6967 - val_mae: 8.5545 - val_mse: 143.6746\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.1322 - mae: 8.6770 - mse: 142.1100 - val_loss: 146.4403 - val_mae: 8.6145 - val_mse: 146.4182\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.2470 - mae: 8.5966 - mse: 140.2250 - val_loss: 141.8609 - val_mae: 8.5424 - val_mse: 141.8388\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.9384 - mae: 8.6083 - mse: 139.9163 - val_loss: 141.7517 - val_mae: 8.5603 - val_mse: 141.7295\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.7850 - mae: 8.5766 - mse: 139.7629 - val_loss: 144.5035 - val_mae: 8.5542 - val_mse: 144.4813\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.7662 - mae: 8.5708 - mse: 139.7441 - val_loss: 141.1284 - val_mae: 8.5737 - val_mse: 141.1062\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.3112 - mae: 8.5860 - mse: 140.2890 - val_loss: 143.6412 - val_mae: 8.5422 - val_mse: 143.6190\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5871 - mae: 8.5998 - mse: 140.5649 - val_loss: 140.8666 - val_mae: 8.5408 - val_mse: 140.8443\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2934 - mae: 8.6117 - mse: 139.2712 - val_loss: 141.4128 - val_mae: 8.5291 - val_mse: 141.3905\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7510 - mae: 8.5433 - mse: 138.7286 - val_loss: 140.9887 - val_mae: 8.5423 - val_mse: 140.9663\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.8279 - mae: 8.5906 - mse: 139.8055 - val_loss: 141.2791 - val_mae: 8.5295 - val_mse: 141.2566\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.8559 - mae: 8.5458 - mse: 137.8335 - val_loss: 139.8175 - val_mae: 8.5221 - val_mse: 139.7950\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.2089 - mae: 8.5441 - mse: 138.1863 - val_loss: 140.3846 - val_mae: 8.5085 - val_mse: 140.3620\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6810 - mae: 8.5437 - mse: 137.6584 - val_loss: 140.1291 - val_mae: 8.5236 - val_mse: 140.1064\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9186 - mae: 8.5553 - mse: 137.8959 - val_loss: 140.8592 - val_mae: 8.5165 - val_mse: 140.8365\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.5473 - mae: 8.5306 - mse: 137.5246 - val_loss: 140.2355 - val_mae: 8.5078 - val_mse: 140.2128\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2921 - mae: 8.4859 - mse: 136.2693 - val_loss: 139.3021 - val_mae: 8.5444 - val_mse: 139.2792\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9933 - mae: 8.5547 - mse: 137.9704 - val_loss: 140.4440 - val_mae: 8.5327 - val_mse: 140.4210\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.4727 - mae: 8.5183 - mse: 137.4496 - val_loss: 140.5037 - val_mae: 8.5203 - val_mse: 140.4807\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6075 - mae: 8.5285 - mse: 136.5843 - val_loss: 140.9895 - val_mae: 8.5274 - val_mse: 140.9663\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6729 - mae: 8.5417 - mse: 137.6495 - val_loss: 140.1876 - val_mae: 8.5350 - val_mse: 140.1642\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6262 - mae: 8.4649 - mse: 136.6027 - val_loss: 140.0060 - val_mae: 8.5457 - val_mse: 139.9824\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5799 - mae: 8.5608 - mse: 136.5562 - val_loss: 140.1067 - val_mae: 8.5186 - val_mse: 140.0830\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.0833 - mae: 8.4965 - mse: 137.0597 - val_loss: 141.9907 - val_mae: 8.5243 - val_mse: 141.9670\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.1014 - mae: 8.4624 - mse: 136.0776 - val_loss: 140.4296 - val_mae: 8.5369 - val_mse: 140.4057\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6776 - mae: 8.5283 - mse: 137.6536 - val_loss: 139.6714 - val_mae: 8.5281 - val_mse: 139.6474\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.3259 - mae: 8.5148 - mse: 137.3017 - val_loss: 139.9314 - val_mae: 8.5087 - val_mse: 139.9072\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3339 - mae: 8.4979 - mse: 136.3096 - val_loss: 139.8838 - val_mae: 8.5288 - val_mse: 139.8594\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7704 - mae: 8.4482 - mse: 135.7459 - val_loss: 141.2675 - val_mae: 8.5371 - val_mse: 141.2430\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3442 - mae: 8.5066 - mse: 136.3195 - val_loss: 140.5552 - val_mae: 8.5198 - val_mse: 140.5306\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4135 - mae: 8.4819 - mse: 135.3887 - val_loss: 140.6830 - val_mae: 8.5230 - val_mse: 140.6581\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6470 - mae: 8.4475 - mse: 136.6220 - val_loss: 138.7043 - val_mae: 8.5880 - val_mse: 138.6793\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.1255 - mae: 8.5088 - mse: 136.1005 - val_loss: 138.5046 - val_mae: 8.5432 - val_mse: 138.4794\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6242 - mae: 8.4920 - mse: 134.5990 - val_loss: 140.9669 - val_mae: 8.4941 - val_mse: 140.9416\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.8159 - mae: 8.5129 - mse: 135.7904 - val_loss: 141.7645 - val_mae: 8.4922 - val_mse: 141.7390\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5319 - mae: 8.4548 - mse: 135.5063 - val_loss: 140.3289 - val_mae: 8.5015 - val_mse: 140.3032\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.8893 - mae: 8.4912 - mse: 135.8636 - val_loss: 139.3492 - val_mae: 8.5128 - val_mse: 139.3234\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.8498 - mae: 8.4247 - mse: 134.8239 - val_loss: 139.2587 - val_mae: 8.5248 - val_mse: 139.2328\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.8295 - mae: 8.4445 - mse: 133.8033 - val_loss: 140.5046 - val_mae: 8.5129 - val_mse: 140.4782\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6736 - mae: 8.4194 - mse: 134.6472 - val_loss: 139.7899 - val_mae: 8.5114 - val_mse: 139.7634\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.4412 - mae: 8.4475 - mse: 133.4144 - val_loss: 140.5143 - val_mae: 8.5218 - val_mse: 140.4874\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.9449 - mae: 8.4454 - mse: 134.9179 - val_loss: 140.9058 - val_mae: 8.5334 - val_mse: 140.8788\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.7866 - mae: 8.4277 - mse: 133.7594 - val_loss: 142.0295 - val_mae: 8.5238 - val_mse: 142.0023\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.6430 - mae: 8.4401 - mse: 133.6157 - val_loss: 140.1405 - val_mae: 8.5515 - val_mse: 140.1131\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.6923 - mae: 8.4779 - mse: 135.6647 - val_loss: 141.8494 - val_mae: 8.5038 - val_mse: 141.8219\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.7075 - mae: 8.4000 - mse: 133.6800 - val_loss: 140.8833 - val_mae: 8.5278 - val_mse: 140.8555\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.1004 - mae: 8.4641 - mse: 134.0725 - val_loss: 139.9828 - val_mae: 8.5483 - val_mse: 139.9547\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.7828 - mae: 8.4493 - mse: 134.7547 - val_loss: 140.2826 - val_mae: 8.5286 - val_mse: 140.2545\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3067 - mae: 8.4678 - mse: 133.2784 - val_loss: 142.3734 - val_mae: 8.5327 - val_mse: 142.3450\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.6570 - mae: 8.4000 - mse: 132.6285 - val_loss: 140.7145 - val_mae: 8.5670 - val_mse: 140.6858\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4568 - mae: 8.4861 - mse: 135.4280 - val_loss: 140.6827 - val_mae: 8.5327 - val_mse: 140.6539\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.0091 - mae: 8.4168 - mse: 132.9802 - val_loss: 139.8720 - val_mae: 8.5623 - val_mse: 139.8429\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0176 - mae: 8.4415 - mse: 133.9884 - val_loss: 141.0247 - val_mae: 8.5164 - val_mse: 140.9955\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.2913 - mae: 8.4065 - mse: 132.2618 - val_loss: 142.9775 - val_mae: 8.5430 - val_mse: 142.9480\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0639 - mae: 8.4205 - mse: 134.0341 - val_loss: 142.0345 - val_mae: 8.5487 - val_mse: 142.0047\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.2321 - mae: 8.4450 - mse: 133.2021 - val_loss: 142.6472 - val_mae: 8.5374 - val_mse: 142.6172\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.7036 - mae: 8.3871 - mse: 132.6733 - val_loss: 139.3901 - val_mae: 8.6041 - val_mse: 139.3597\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.1305 - mae: 8.4160 - mse: 132.1001 - val_loss: 142.0072 - val_mae: 8.5636 - val_mse: 141.9766\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.1038 - mae: 8.3700 - mse: 131.0731 - val_loss: 140.1910 - val_mae: 8.5614 - val_mse: 140.1602\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.1055 - mae: 8.3834 - mse: 131.0745 - val_loss: 141.6427 - val_mae: 8.5416 - val_mse: 141.6116\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0861 - mae: 8.3830 - mse: 132.0549 - val_loss: 141.5463 - val_mae: 8.5564 - val_mse: 141.5150\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.6911 - mae: 8.3275 - mse: 131.6597 - val_loss: 141.5451 - val_mae: 8.5300 - val_mse: 141.5135\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.6841 - mae: 8.3795 - mse: 131.6523 - val_loss: 139.8889 - val_mae: 8.5549 - val_mse: 139.8570\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.9963 - mae: 8.3774 - mse: 130.9644 - val_loss: 139.7649 - val_mae: 8.5558 - val_mse: 139.7328\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.7151 - mae: 8.4200 - mse: 132.6829 - val_loss: 141.5986 - val_mae: 8.5157 - val_mse: 141.5664\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.4794 - mae: 8.3664 - mse: 131.4471 - val_loss: 140.5154 - val_mae: 8.5405 - val_mse: 140.4829\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.4400 - mae: 8.3927 - mse: 132.4074 - val_loss: 140.0415 - val_mae: 8.5532 - val_mse: 140.0087\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.8898 - mae: 8.3421 - mse: 131.8569 - val_loss: 139.0861 - val_mae: 8.5773 - val_mse: 139.0530\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0248 - mae: 8.4138 - mse: 130.9915 - val_loss: 140.5969 - val_mae: 8.5682 - val_mse: 140.5635\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.5002 - mae: 8.3672 - mse: 130.4665 - val_loss: 140.0323 - val_mae: 8.6142 - val_mse: 139.9985\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0138 - mae: 8.3911 - mse: 129.9798 - val_loss: 142.1735 - val_mae: 8.5465 - val_mse: 142.1394\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5734 - mae: 8.3931 - mse: 132.5391 - val_loss: 140.9383 - val_mae: 8.5775 - val_mse: 140.9039\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.8020 - mae: 8.3501 - mse: 130.7675 - val_loss: 141.1815 - val_mae: 8.5560 - val_mse: 141.1467\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.7207 - mae: 8.3913 - mse: 129.6857 - val_loss: 141.1506 - val_mae: 8.5467 - val_mse: 141.1155\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6156 - mae: 8.3523 - mse: 129.5802 - val_loss: 142.3693 - val_mae: 8.5515 - val_mse: 142.3339\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3439 - mae: 8.4095 - mse: 132.3085 - val_loss: 140.7166 - val_mae: 8.5600 - val_mse: 140.6811\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.9219 - mae: 8.4165 - mse: 132.8862 - val_loss: 140.5584 - val_mae: 8.5516 - val_mse: 140.5225\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.8582 - mae: 8.3982 - mse: 131.8222 - val_loss: 140.7042 - val_mae: 8.5389 - val_mse: 140.6681\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9353 - mae: 8.2843 - mse: 127.8989 - val_loss: 140.2428 - val_mae: 8.5793 - val_mse: 140.2061\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.5057 - mae: 8.3540 - mse: 130.4690 - val_loss: 141.2736 - val_mae: 8.5450 - val_mse: 141.2369\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8204 - mae: 8.3525 - mse: 129.7836 - val_loss: 140.5360 - val_mae: 8.5656 - val_mse: 140.4990\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.2243 - mae: 8.3586 - mse: 129.1871 - val_loss: 140.7296 - val_mae: 8.5650 - val_mse: 140.6924\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.9743 - mae: 8.3663 - mse: 128.9369 - val_loss: 141.4337 - val_mae: 8.5434 - val_mse: 141.3962\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.3716 - mae: 8.3398 - mse: 129.3338 - val_loss: 139.7036 - val_mae: 8.5782 - val_mse: 139.6657\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.5850 - mae: 8.3502 - mse: 129.5469 - val_loss: 141.2226 - val_mae: 8.5635 - val_mse: 141.1844\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.3087 - mae: 8.2801 - mse: 127.2703 - val_loss: 140.1176 - val_mae: 8.5802 - val_mse: 140.0789\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.3326 - mae: 8.2785 - mse: 127.2938 - val_loss: 141.3738 - val_mae: 8.6164 - val_mse: 141.3348\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2872 - mae: 8.3015 - mse: 128.2480 - val_loss: 139.9357 - val_mae: 8.6164 - val_mse: 139.8962\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8780 - mae: 8.3359 - mse: 129.8385 - val_loss: 139.6444 - val_mae: 8.5593 - val_mse: 139.6048\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4527 - mae: 8.3572 - mse: 128.4130 - val_loss: 142.3857 - val_mae: 8.5438 - val_mse: 142.3458\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6179 - mae: 8.3023 - mse: 127.5777 - val_loss: 141.2922 - val_mae: 8.5524 - val_mse: 141.2520\n"
     ]
    }
   ],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_1.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      "        Adding a layer\n",
      "Train MAE: 7.9913, Train MSE: 122.5016\n",
      "Val   MAE: 8.5524, Val   MSE: 141.2520\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"        Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very nearly identical to the initial configurations. Doubling the kernels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 163.0172 - mae: 9.2372 - mse: 162.9753 - val_loss: 149.2135 - val_mae: 8.7432 - val_mse: 149.1713\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.3550 - mae: 8.8254 - mse: 147.3125 - val_loss: 146.8706 - val_mae: 8.6685 - val_mse: 146.8281\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 145.7400 - mae: 8.7905 - mse: 145.6973 - val_loss: 143.9216 - val_mae: 8.5736 - val_mse: 143.8787\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.5578 - mae: 8.7554 - mse: 144.5147 - val_loss: 147.9919 - val_mae: 8.6472 - val_mse: 147.9487\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.6966 - mae: 8.6617 - mse: 142.6534 - val_loss: 144.6930 - val_mae: 8.6329 - val_mse: 144.6495\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.6469 - mae: 8.6761 - mse: 141.6032 - val_loss: 144.1739 - val_mae: 8.6085 - val_mse: 144.1302\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.6347 - mae: 8.6542 - mse: 142.5908 - val_loss: 147.8210 - val_mae: 8.6448 - val_mse: 147.7769\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.1204 - mae: 8.6485 - mse: 141.0761 - val_loss: 144.7388 - val_mae: 8.6073 - val_mse: 144.6944\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.7029 - mae: 8.5882 - mse: 139.6584 - val_loss: 141.3609 - val_mae: 8.6232 - val_mse: 141.3161\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.2896 - mae: 8.5820 - mse: 140.2448 - val_loss: 141.1873 - val_mae: 8.5701 - val_mse: 141.1422\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7545 - mae: 8.5604 - mse: 138.7094 - val_loss: 140.8930 - val_mae: 8.6007 - val_mse: 140.8477\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.1550 - mae: 8.5645 - mse: 139.1095 - val_loss: 145.0400 - val_mae: 8.6060 - val_mse: 144.9945\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6284 - mae: 8.5150 - mse: 137.5826 - val_loss: 139.3888 - val_mae: 8.6111 - val_mse: 139.3428\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 137.4926 - mae: 8.5369 - mse: 137.4464 - val_loss: 139.4924 - val_mae: 8.5649 - val_mse: 139.4460\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.9787 - mae: 8.5597 - mse: 136.9322 - val_loss: 144.4159 - val_mae: 8.5955 - val_mse: 144.3693\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.9413 - mae: 8.5063 - mse: 136.8945 - val_loss: 141.1071 - val_mae: 8.5652 - val_mse: 141.0601\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.1301 - mae: 8.5042 - mse: 137.0829 - val_loss: 144.0881 - val_mae: 8.5563 - val_mse: 144.0408\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7555 - mae: 8.4761 - mse: 135.7079 - val_loss: 139.7931 - val_mae: 8.5683 - val_mse: 139.7454\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.8818 - mae: 8.4769 - mse: 135.8339 - val_loss: 142.2652 - val_mae: 8.5415 - val_mse: 142.2172\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9672 - mae: 8.4726 - mse: 135.9189 - val_loss: 143.4031 - val_mae: 8.5483 - val_mse: 143.3545\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7347 - mae: 8.4529 - mse: 135.6859 - val_loss: 140.4522 - val_mae: 8.5497 - val_mse: 140.4032\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6033 - mae: 8.4448 - mse: 134.5539 - val_loss: 145.4696 - val_mae: 8.5564 - val_mse: 145.4201\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6459 - mae: 8.5131 - mse: 136.5961 - val_loss: 143.7178 - val_mae: 8.5404 - val_mse: 143.6679\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.1053 - mae: 8.4883 - mse: 136.0551 - val_loss: 141.5384 - val_mae: 8.5405 - val_mse: 141.4881\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6564 - mae: 8.4421 - mse: 134.6059 - val_loss: 140.4671 - val_mae: 8.5305 - val_mse: 140.4164\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2512 - mae: 8.4407 - mse: 134.2001 - val_loss: 142.0443 - val_mae: 8.5047 - val_mse: 141.9930\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.0369 - mae: 8.3900 - mse: 132.9852 - val_loss: 140.6564 - val_mae: 8.5651 - val_mse: 140.6044\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.8468 - mae: 8.4272 - mse: 133.7944 - val_loss: 141.6119 - val_mae: 8.5350 - val_mse: 141.5591\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7781 - mae: 8.3491 - mse: 131.7249 - val_loss: 142.9668 - val_mae: 8.5291 - val_mse: 142.9133\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.1023 - mae: 8.4157 - mse: 133.0484 - val_loss: 142.4453 - val_mae: 8.5264 - val_mse: 142.3912\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3851 - mae: 8.3680 - mse: 132.3306 - val_loss: 143.8435 - val_mae: 8.5795 - val_mse: 143.7887\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.1273 - mae: 8.3978 - mse: 133.0723 - val_loss: 142.6855 - val_mae: 8.5917 - val_mse: 142.6301\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.3878 - mae: 8.3836 - mse: 131.3319 - val_loss: 141.6529 - val_mae: 8.5701 - val_mse: 141.5967\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.8369 - mae: 8.3790 - mse: 131.7803 - val_loss: 141.9164 - val_mae: 8.5450 - val_mse: 141.8595\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.8163 - mae: 8.3258 - mse: 130.7590 - val_loss: 143.9431 - val_mae: 8.5593 - val_mse: 143.8855\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.1008 - mae: 8.3276 - mse: 131.0428 - val_loss: 142.0895 - val_mae: 8.6000 - val_mse: 142.0312\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.4648 - mae: 8.4231 - mse: 132.4063 - val_loss: 144.4855 - val_mae: 8.5531 - val_mse: 144.4266\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 131.5140 - mae: 8.3826 - mse: 131.4549 - val_loss: 145.3935 - val_mae: 8.5589 - val_mse: 145.3343\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 132.1589 - mae: 8.3377 - mse: 132.0993 - val_loss: 139.9239 - val_mae: 8.5625 - val_mse: 139.8639\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 129.3704 - mae: 8.3247 - mse: 129.3100 - val_loss: 142.8299 - val_mae: 8.5785 - val_mse: 142.7690\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 130.7510 - mae: 8.3595 - mse: 130.6898 - val_loss: 142.4441 - val_mae: 8.5721 - val_mse: 142.3826\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3714 - mae: 8.2870 - mse: 128.3094 - val_loss: 143.6836 - val_mae: 8.5821 - val_mse: 143.6212\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 128.6978 - mae: 8.3257 - mse: 128.6351 - val_loss: 142.7482 - val_mae: 8.5946 - val_mse: 142.6851\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.8978 - mae: 8.2231 - mse: 125.8341 - val_loss: 143.5394 - val_mae: 8.6066 - val_mse: 143.4754\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4314 - mae: 8.2993 - mse: 127.3669 - val_loss: 144.4043 - val_mae: 8.6111 - val_mse: 144.3396\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3006 - mae: 8.2544 - mse: 128.2355 - val_loss: 144.3361 - val_mae: 8.6205 - val_mse: 144.2706\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.5619 - mae: 8.3068 - mse: 129.4960 - val_loss: 141.5454 - val_mae: 8.5886 - val_mse: 141.4792\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.1195 - mae: 8.2938 - mse: 128.0528 - val_loss: 143.7775 - val_mae: 8.6074 - val_mse: 143.7104\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.3027 - mae: 8.2673 - mse: 124.2349 - val_loss: 144.3048 - val_mae: 8.5953 - val_mse: 144.2364\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6522 - mae: 8.2787 - mse: 127.5834 - val_loss: 143.3929 - val_mae: 8.5739 - val_mse: 143.3239\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.7504 - mae: 8.3239 - mse: 127.6810 - val_loss: 142.3537 - val_mae: 8.5683 - val_mse: 142.2839\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.4089 - mae: 8.2929 - mse: 126.3387 - val_loss: 146.1191 - val_mae: 8.6421 - val_mse: 146.0486\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.0203 - mae: 8.2449 - mse: 124.9494 - val_loss: 145.4918 - val_mae: 8.5686 - val_mse: 145.4206\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.8579 - mae: 8.2689 - mse: 126.7862 - val_loss: 147.0217 - val_mae: 8.5916 - val_mse: 146.9497\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.9162 - mae: 8.1485 - mse: 122.8436 - val_loss: 143.3550 - val_mae: 8.6153 - val_mse: 143.2818\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.6450 - mae: 8.2326 - mse: 125.5715 - val_loss: 142.8389 - val_mae: 8.5680 - val_mse: 142.7650\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.9312 - mae: 8.1985 - mse: 123.8567 - val_loss: 142.8941 - val_mae: 8.6328 - val_mse: 142.8192\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.1251 - mae: 8.1563 - mse: 123.0497 - val_loss: 142.4625 - val_mae: 8.5778 - val_mse: 142.3867\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.1895 - mae: 8.2396 - mse: 125.1131 - val_loss: 144.0088 - val_mae: 8.6227 - val_mse: 143.9319\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 124.7827 - mae: 8.2307 - mse: 124.7055 - val_loss: 143.9834 - val_mae: 8.6311 - val_mse: 143.9056\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.5944 - mae: 8.2052 - mse: 124.5164 - val_loss: 142.2643 - val_mae: 8.5859 - val_mse: 142.1861\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.3292 - mae: 8.1987 - mse: 123.2506 - val_loss: 142.6576 - val_mae: 8.6003 - val_mse: 142.5786\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.7634 - mae: 8.1770 - mse: 122.6838 - val_loss: 142.7128 - val_mae: 8.6147 - val_mse: 142.6328\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.9035 - mae: 8.2016 - mse: 123.8233 - val_loss: 146.1552 - val_mae: 8.5952 - val_mse: 146.0747\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.8032 - mae: 8.1640 - mse: 121.7222 - val_loss: 143.8247 - val_mae: 8.5971 - val_mse: 143.7431\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.7168 - mae: 8.1503 - mse: 121.6347 - val_loss: 145.3227 - val_mae: 8.5939 - val_mse: 145.2402\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.8129 - mae: 8.1654 - mse: 121.7300 - val_loss: 145.2671 - val_mae: 8.5865 - val_mse: 145.1839\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.4479 - mae: 8.1740 - mse: 121.3640 - val_loss: 144.4576 - val_mae: 8.5610 - val_mse: 144.3733\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6887 - mae: 8.1580 - mse: 120.6038 - val_loss: 146.6296 - val_mae: 8.5871 - val_mse: 146.5446\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.7033 - mae: 8.1400 - mse: 120.6178 - val_loss: 144.1723 - val_mae: 8.5895 - val_mse: 144.0863\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.0355 - mae: 8.0686 - mse: 117.9489 - val_loss: 144.3443 - val_mae: 8.6042 - val_mse: 144.2572\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.1850 - mae: 8.0856 - mse: 119.0974 - val_loss: 144.2402 - val_mae: 8.5786 - val_mse: 144.1524\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.4353 - mae: 8.0830 - mse: 119.3473 - val_loss: 144.9527 - val_mae: 8.6171 - val_mse: 144.8641\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.1244 - mae: 8.1315 - mse: 119.0351 - val_loss: 145.0289 - val_mae: 8.5993 - val_mse: 144.9394\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.6067 - mae: 8.1151 - mse: 119.5165 - val_loss: 146.6984 - val_mae: 8.5903 - val_mse: 146.6080\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.3561 - mae: 8.1306 - mse: 120.2654 - val_loss: 143.3135 - val_mae: 8.5765 - val_mse: 143.2223\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.7486 - mae: 8.0805 - mse: 118.6569 - val_loss: 143.0924 - val_mae: 8.5961 - val_mse: 143.0000\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.8492 - mae: 8.1544 - mse: 120.7566 - val_loss: 142.9733 - val_mae: 8.5662 - val_mse: 142.8803\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.4193 - mae: 8.0572 - mse: 117.3257 - val_loss: 145.9086 - val_mae: 8.6267 - val_mse: 145.8144\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4118 - mae: 8.1217 - mse: 120.3172 - val_loss: 144.6698 - val_mae: 8.5723 - val_mse: 144.5748\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.6655 - mae: 8.1028 - mse: 118.5697 - val_loss: 145.7580 - val_mae: 8.5727 - val_mse: 145.6618\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.0733 - mae: 8.0624 - mse: 116.9766 - val_loss: 145.0550 - val_mae: 8.5659 - val_mse: 144.9582\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.1496 - mae: 8.0273 - mse: 117.0524 - val_loss: 144.6649 - val_mae: 8.5374 - val_mse: 144.5671\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.1327 - mae: 7.9895 - mse: 117.0345 - val_loss: 144.1732 - val_mae: 8.5757 - val_mse: 144.0744\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.6637 - mae: 8.0275 - mse: 118.5646 - val_loss: 144.7095 - val_mae: 8.5587 - val_mse: 144.6102\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.4479 - mae: 7.9675 - mse: 115.3482 - val_loss: 144.7959 - val_mae: 8.5588 - val_mse: 144.6957\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.2200 - mae: 7.9417 - mse: 112.1191 - val_loss: 146.7483 - val_mae: 8.5983 - val_mse: 146.6468\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2982 - mae: 8.0327 - mse: 118.1966 - val_loss: 143.0739 - val_mae: 8.5745 - val_mse: 142.9722\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.7887 - mae: 8.0656 - mse: 116.6866 - val_loss: 145.6134 - val_mae: 8.5803 - val_mse: 145.5108\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.1133 - mae: 8.0009 - mse: 115.0101 - val_loss: 145.4687 - val_mae: 8.5653 - val_mse: 145.3654\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.1027 - mae: 7.9444 - mse: 114.9987 - val_loss: 144.5054 - val_mae: 8.5863 - val_mse: 144.4009\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.4230 - mae: 7.9709 - mse: 113.3179 - val_loss: 143.8007 - val_mae: 8.5967 - val_mse: 143.6947\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.2356 - mae: 7.9910 - mse: 115.1291 - val_loss: 147.2254 - val_mae: 8.5651 - val_mse: 147.1189\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.6184 - mae: 7.9700 - mse: 115.5111 - val_loss: 144.1750 - val_mae: 8.5542 - val_mse: 144.0673\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.3846 - mae: 7.9574 - mse: 115.2766 - val_loss: 142.5138 - val_mae: 8.5677 - val_mse: 142.4052\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.9203 - mae: 7.9371 - mse: 111.8110 - val_loss: 143.8436 - val_mae: 8.5828 - val_mse: 143.7338\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.0379 - mae: 7.9334 - mse: 112.9279 - val_loss: 143.5026 - val_mae: 8.6021 - val_mse: 143.3924\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.7762 - mae: 7.9466 - mse: 113.6655 - val_loss: 142.8156 - val_mae: 8.6214 - val_mse: 142.7042\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.4491 - mae: 7.9913 - mse: 115.3372 - val_loss: 145.5372 - val_mae: 8.6201 - val_mse: 145.4249\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.5366 - mae: 7.9656 - mse: 114.4239 - val_loss: 146.4503 - val_mae: 8.6096 - val_mse: 146.3371\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_2.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      " 4 deep layers, 128 kernels per layer\n",
      "Train MAE: 7.9913, Train MSE: 122.5016\n",
      "Val   MAE: 8.5524, Val   MSE: 141.2520\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\" 4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model has the best MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4),\n",
    "                 input_shape=(X_withgenre_train.shape[1],)),\n",
    "    layers.Dropout(0.4),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "final_maxrankchange_model = reg_model_maxrank_withgenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "The XGBoost model did not return meaningful results for this dataset. The highest r<sup>2</sup> value XGB achieved was 0.271, not high enough to indicate that the model strongly fits the underlying data.\n",
    "\n",
    " The best metrics for XGBoost were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- RMSE: 21.607\n",
    "- r<sup>2</sup>: 0.271\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- RMSE: 11.764\n",
    "- r<sup>2</sup>: 0.045\n",
    "\n",
    "**k-Nearest Neighbors**\n",
    "\n",
    "This model also did not perform well on this data set--its accuracy was not above 22% in any scenario.\n",
    "\n",
    "Its best metrics were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- Accuracy: 0.035\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- Accuracy: 0.217\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "The deep learning model returned the most promising results and are described in the next section.\n",
    "\n",
    "### Final Models\n",
    "\n",
    "The final models are both based on a deep learning architecture. \n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "The best model for the Maximum Peak Position is a 3-layer, regularized model with 64 kernels per layer. It was trained a song dataset with genre information.\n",
    "\n",
    "The final metrics for the best model were:\n",
    "\n",
    "- Training mean absolute error: 14.84\n",
    "- Validation mean absolute error: 16.91\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "The best model for the Maximum Rank Increase is named reg_model4_2. It is also a 3-layer, regularized model trained on the song dataset without genre information.\n",
    "Its final metrics:\n",
    "\n",
    "- Training mean absolute error: 7.96\n",
    "- Validation mean absolute error: 8.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://885f1ad0-2c85-4cd6-a58f-28b17bf7fc4c/assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ram://3adbc2ae-09ee-441a-97b2-cd1ae930a9aa/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_maxrankchange_model.sav']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickling the final models\n",
    "\n",
    "joblib.dump(final_maxpeak_model, 'final_maxpeak_model.sav')\n",
    "joblib.dump(final_maxrankchange_model, 'final_maxrankchange_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_music_data(hotlist_path, features_path, min_genre_count=100):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for Hot 100 music data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hotlist_path : str\n",
    "        Path to Hot Stuff.csv file\n",
    "    features_path : str\n",
    "        Path to Hot 100 Audio Features.csv file\n",
    "    min_genre_count : int, default=100\n",
    "        Minimum count threshold for including genres in one-hot encoding\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all prepared datasets and scalers:\n",
    "        - 'X_withgenre_train_scaled': Training features with genre (scaled)\n",
    "        - 'X_withgenre_val_scaled': Validation features with genre (scaled)\n",
    "        - 'X_withgenre_test_scaled': Test features with genre (scaled)\n",
    "        - 'y_withgenre_train': Training target for peak position\n",
    "        - 'y_withgenre_val': Validation target for peak position\n",
    "        - 'y_withgenre_test': Test target for peak position\n",
    "        - 'X_nogenre_train_scaled': Training features without genre (scaled)\n",
    "        - 'X_nogenre_val_scaled': Validation features without genre (scaled)\n",
    "        - 'X_nogenre_test_scaled': Test features without genre (scaled)\n",
    "        - 'y_nogenre_train': Training target for rank change\n",
    "        - 'y_nogenre_val': Validation target for rank change\n",
    "        - 'y_nogenre_test': Test target for rank change\n",
    "        - 'scaler_withgenre': Scaler fitted on training data with genre\n",
    "        - 'scaler_nogenre': Scaler fitted on training data without genre\n",
    "        - 'df_clean_withgenre': Clean dataframe with genre columns\n",
    "        - 'df_clean_nogenre': Clean dataframe without genre columns\n",
    "        - 'df_top_genres': Top 5 genres per week\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load CSVs into dataframes\n",
    "    df_hotlist = pd.read_csv(hotlist_path)\n",
    "    df_features = pd.read_csv(features_path)\n",
    "    \n",
    "    # Step 2: Initial data selection - drop unwanted columns\n",
    "    hotlist_drop_cols = ['index', 'url', 'Song', 'Performer', 'Instance']\n",
    "    features_drop_cols = ['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                          'spotify_track_id', 'spotify_track_preview_url', \n",
    "                          'spotify_track_explicit', 'spotify_track_popularity']\n",
    "    \n",
    "    # Drop columns that exist in the dataframes\n",
    "    df_hotlist = df_hotlist.drop(columns=[col for col in hotlist_drop_cols if col in df_hotlist.columns])\n",
    "    df_features = df_features.drop(columns=[col for col in features_drop_cols if col in df_features.columns])\n",
    "    \n",
    "    # Step 3: Combine dataframes using 'SongID' as key\n",
    "    df_combined = pd.merge(df_hotlist, df_features, on='SongID', how='inner')\n",
    "    \n",
    "    # Step 3a: Calculate Rank_Change (Week Position - Previous Week Position)\n",
    "    df_combined.loc[:, 'Rank_Change'] = df_combined['Week Position'] - df_combined['Previous Week Position']\n",
    "    df_combined.loc[:, 'Rank_Change'] = df_combined['Rank_Change'].fillna(0) # filling nans with 0s\n",
    "    \n",
    "    # Extract unique genres and count them\n",
    "    unique_genres = list(set(\n",
    "        genre \n",
    "        for genre_string in df_combined['spotify_genre'] \n",
    "        if pd.notna(genre_string)\n",
    "        for genre in ast.literal_eval(genre_string)\n",
    "    ))\n",
    "    \n",
    "    df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "    \n",
    "    # Count occurrences of each genre\n",
    "    all_genres_list = []\n",
    "    for genre_string in df_combined['spotify_genre']:\n",
    "        if pd.notna(genre_string):\n",
    "            genre_list = ast.literal_eval(genre_string)\n",
    "            all_genres_list.extend(genre_list)\n",
    "    \n",
    "    genre_counts = Counter(all_genres_list)\n",
    "    df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "    df_unique_genres = df_unique_genres.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Filter genres with count >= min_genre_count\n",
    "    df_genres_filtered = df_unique_genres[df_unique_genres['count'] >= min_genre_count]\n",
    "    final_genres_list = df_genres_filtered['genre'].tolist()\n",
    "    \n",
    "    # Step 4: Manually one-hot encode each genre\n",
    "    genre_data = []\n",
    "    for genre_string in df_combined['spotify_genre']:\n",
    "        if pd.notna(genre_string):\n",
    "            genre_list = ast.literal_eval(genre_string)\n",
    "            row_dict = {genre: (1 if genre in genre_list else 0) for genre in final_genres_list}\n",
    "        else:\n",
    "            row_dict = {genre: 0 for genre in final_genres_list}\n",
    "        genre_data.append(row_dict)\n",
    "    \n",
    "    genre_df = pd.DataFrame(genre_data)\n",
    "    df_combined = pd.concat([df_combined, genre_df], axis=1)\n",
    "    \n",
    "    # Step 5: Drop 'spotify_genre' column\n",
    "    df_combined = df_combined.drop(columns=['spotify_genre'])\n",
    "    \n",
    "    # Step 6: Create dataframe with 5 most popular genres per week\n",
    "    genre_start_idx = df_combined.columns.get_loc(final_genres_list[0])\n",
    "    genre_cols = df_combined.columns[genre_start_idx:].tolist()\n",
    "    \n",
    "    genre_counts = df_combined.groupby('WeekID')[genre_cols].sum()\n",
    "    \n",
    "    top_genres = []\n",
    "    for week_id in genre_counts.index:\n",
    "        week_genres = genre_counts.loc[week_id].sort_values(ascending=False)\n",
    "        top_5 = week_genres.head(5).index.tolist()\n",
    "        \n",
    "        while len(top_5) < 5:\n",
    "            top_5.append(None)\n",
    "        \n",
    "        top_genres.append({\n",
    "            'WeekID': week_id,\n",
    "            'Most_Popular_Genre': top_5[0],\n",
    "            '2nd_Most_Popular_Genre': top_5[1],\n",
    "            '3rd_Most_Popular_Genre': top_5[2],\n",
    "            '4th_Most_Popular_Genre': top_5[3],\n",
    "            '5th_Most_Popular_Genre': top_5[4]\n",
    "        })\n",
    "    \n",
    "    df_top_genres = pd.DataFrame(top_genres)\n",
    "    \n",
    "    # Step 7: Add column indicating whether each song is in the top 5 genres\n",
    "    def is_in_top5_genres(row, df_top_genres):\n",
    "        week = row['WeekID']\n",
    "        top_genres = df_top_genres[df_top_genres['WeekID'] == week]\n",
    "        \n",
    "        if len(top_genres) == 0:\n",
    "            return 0\n",
    "        \n",
    "        top_5 = [\n",
    "            top_genres.iloc[0]['Most_Popular_Genre'],\n",
    "            top_genres.iloc[0]['2nd_Most_Popular_Genre'],\n",
    "            top_genres.iloc[0]['3rd_Most_Popular_Genre'],\n",
    "            top_genres.iloc[0]['4th_Most_Popular_Genre'],\n",
    "            top_genres.iloc[0]['5th_Most_Popular_Genre'],\n",
    "        ]\n",
    "        \n",
    "        for genre in top_5:\n",
    "            if genre in row.index and row[genre] == 1:\n",
    "                return 1\n",
    "        \n",
    "        return 0\n",
    "    # Step 8: Create in_top5_genres column\n",
    "    df_combined['in_top5_genres'] = df_combined.apply(\n",
    "        lambda row: is_in_top5_genres(row, df_top_genres), axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 9: Calculate mean score of appearance in top 5 weekly genres\n",
    "    df_mean_genre_match = df_combined.groupby('SongID', as_index=False)['in_top5_genres'].mean()\n",
    "    df_mean_genre_match.rename(columns={'in_top5_genres': 'In_Top5genres_Mean'}, inplace=True)\n",
    "    df_mean_genre_match.set_index('SongID', inplace=True)\n",
    "    # Adding top5genres mean to main df\n",
    "    df_combined = df_combined.join(df_mean_genre_match, on='SongID')\n",
    "\n",
    "    # Step 10: Calculate max weekly rank change for each song\n",
    "    df_max_rank_change = df_combined.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "    df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "    df_max_rank_change.set_index('SongID', inplace=True)\n",
    "    # Add max weekly rank change to main df\n",
    "    df_combined = df_combined.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "    # Step 11: Calculate max peak position for each song\n",
    "    df_max_peak_pos = df_combined.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "    df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "    df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "    # Adding max peak position to main df\n",
    "    df_combined = df_combined.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "    # Step 12: Remove unneeded columns\n",
    "    cols_to_remove = ['WeekID', 'Week Position', 'Previous Week Position', 'Peak Position', 'Weeks on Chart', 'Rank_Change', 'in_top5_genres']\n",
    "    df_combined = df_combined.drop(columns=[col for col in cols_to_remove if col in df_combined.columns])\n",
    "    \n",
    "    # Step 13: Remove duplicate rows\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "\n",
    "    # Step 14: Remove rows with missing values and SongID\n",
    "    df_combined = df_combined.dropna()\n",
    "    df_combined = df_combined.drop(['SongID'], axis=1)\n",
    "\n",
    "    # Step 15: Create 2 copies of the df\n",
    "    df_clean_withgenre = df_combined.copy()\n",
    "    df_clean_nogenre = df_combined.copy()\n",
    "    \n",
    "    # Step 16: Drop genre columns from df_clean_nogenre\n",
    "    df_clean_nogenre = df_clean_nogenre.drop(columns=genre_cols)\n",
    "    \n",
    "    # Step 17: Prepare features and target for max peak position analysis\n",
    "    X_withgenre = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "    y_withgenre = df_clean_withgenre['Max_Peak_Position']\n",
    "    \n",
    "    X_withgenre_train, X_withgenre_test, y_withgenre_train, y_withgenre_test = train_test_split(\n",
    "        X_withgenre, y_withgenre, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_withgenre_train_final, X_withgenre_val, y_withgenre_train_final, y_withgenre_val = train_test_split(\n",
    "        X_withgenre_train, y_withgenre_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler_withgenre = StandardScaler()\n",
    "    scaler_withgenre.fit(X_withgenre_train_final)\n",
    "    X_withgenre_train_scaled = scaler_withgenre.transform(X_withgenre_train_final)\n",
    "    X_withgenre_val_scaled = scaler_withgenre.transform(X_withgenre_val)\n",
    "    X_withgenre_test_scaled = scaler_withgenre.transform(X_withgenre_test)\n",
    "    \n",
    "    # Step 16: Prepare features and target for max rank change analysis\n",
    "    X_nogenre = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "    y_nogenre = df_clean_nogenre['Max_Rank_Change']\n",
    "    \n",
    "    X_nogenre_train, X_nogenre_test, y_nogenre_train, y_nogenre_test = train_test_split(\n",
    "        X_nogenre, y_nogenre, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_nogenre_train_final, X_nogenre_val, y_nogenre_train_final, y_nogenre_val = train_test_split(\n",
    "        X_nogenre_train, y_nogenre_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler_nogenre = StandardScaler()\n",
    "    scaler_nogenre.fit(X_nogenre_train_final)\n",
    "    X_nogenre_train_scaled = scaler_nogenre.transform(X_nogenre_train_final)\n",
    "    X_nogenre_val_scaled = scaler_nogenre.transform(X_nogenre_val)\n",
    "    X_nogenre_test_scaled = scaler_nogenre.transform(X_nogenre_test)\n",
    "    \n",
    "    # Step 18: Return all prepared datasets\n",
    "    return {\n",
    "        'X_withgenre_train_scaled': X_withgenre_train_scaled,\n",
    "        'X_withgenre_val_scaled': X_withgenre_val_scaled,\n",
    "        'X_withgenre_test_scaled': X_withgenre_test_scaled,\n",
    "        'y_withgenre_train': y_withgenre_train_final,\n",
    "        'y_withgenre_val': y_withgenre_val,\n",
    "        'y_withgenre_test': y_withgenre_test,\n",
    "        'X_nogenre_train_scaled': X_nogenre_train_scaled,\n",
    "        'X_nogenre_val_scaled': X_nogenre_val_scaled,\n",
    "        'X_nogenre_test_scaled': X_nogenre_test_scaled,\n",
    "        'y_nogenre_train': y_nogenre_train_final,\n",
    "        'y_nogenre_val': y_nogenre_val,\n",
    "        'y_nogenre_test': y_nogenre_test,\n",
    "        'scaler_withgenre': scaler_withgenre,\n",
    "        'scaler_nogenre': scaler_nogenre,\n",
    "        'df_clean_withgenre': df_clean_withgenre,\n",
    "        'df_clean_nogenre': df_clean_nogenre,\n",
    "        'df_top_genres': df_top_genres\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_withgenre_train_scaled': array([[ 1.005965  ,  1.29795512, -0.37810755, ..., -0.01606283,\n",
       "          0.80937835,  1.23602015],\n",
       "        [-0.26449697, -0.4074845 , -0.02153011, ..., -0.01606283,\n",
       "         -1.36147781, -0.62688688],\n",
       "        [-1.06233792, -1.51406745, -1.32228443, ..., -0.01606283,\n",
       "         -1.36147781, -0.14091113],\n",
       "        ...,\n",
       "        [-0.79734075,  0.5493843 , -0.51872964, ..., -0.01606283,\n",
       "          0.80937835,  1.7219959 ],\n",
       "        [ 1.47186137, -0.08852823, -0.1069078 , ..., -0.01606283,\n",
       "          0.80937835,  3.01793123],\n",
       "        [ 1.53064515,  0.56240292,  0.16429194, ..., -0.01606283,\n",
       "         -1.36147781,  0.66904845]]),\n",
       " 'X_withgenre_val_scaled': array([[ 0.21331002, -0.68738489, -0.73468499, ..., -0.01606283,\n",
       "         -1.36147781, -1.19385859],\n",
       "        [ 1.0889832 , -0.34239138, -1.427751  , ..., -0.01606283,\n",
       "         -1.36147781, -1.19385859],\n",
       "        [ 0.48166952, -0.82408043,  0.26473629, ..., -0.01606283,\n",
       "          0.80937835, -1.19385859],\n",
       "        ...,\n",
       "        [-0.78215328, -1.61821643, -1.63366191, ..., -0.01606283,\n",
       "          0.80937835, -0.22190709],\n",
       "        [-1.22951409, -0.17314928, -1.38757326, ..., -0.01606283,\n",
       "         -1.36147781, -1.19385859],\n",
       "        [ 0.55955875, -0.6353104 , -1.08121799, ..., -0.01606283,\n",
       "          0.80937835, -0.62688688]]),\n",
       " 'X_withgenre_test_scaled': array([[ 0.90604967, -1.22765774,  1.41482409, ..., -0.01606283,\n",
       "          0.80937835, -0.30290305],\n",
       "        [ 0.08407404,  2.35897289, -0.40824085, ..., -0.01606283,\n",
       "          0.37520712, -1.19385859],\n",
       "        [-0.64481066, -2.12594273, -0.90544038, ..., -0.01606283,\n",
       "          0.80937835, -1.19385859],\n",
       "        ...,\n",
       "        [-1.095776  ,  0.07420456,  0.27980294, ..., -0.01606283,\n",
       "         -1.36147781, -0.70788284],\n",
       "        [ 0.02695717,  1.73407899, -0.90544038, ..., -0.01606283,\n",
       "          0.80937835,  0.1830727 ],\n",
       "        [-0.41457656,  0.69258915, -1.543262  , ..., -0.01606283,\n",
       "          0.80937835, -0.38389901]]),\n",
       " 'y_withgenre_train': 90009     82\n",
       " 288809    99\n",
       " 123342    95\n",
       " 241743    90\n",
       " 108136    91\n",
       "           ..\n",
       " 40802     98\n",
       " 51803     87\n",
       " 324557    88\n",
       " 60045     39\n",
       " 300503    87\n",
       " Name: Max_Peak_Position, Length: 15507, dtype: int64,\n",
       " 'y_withgenre_val': 269914    93\n",
       " 197784    99\n",
       " 120861    94\n",
       " 197104    86\n",
       " 132876    97\n",
       "           ..\n",
       " 148635    70\n",
       " 259423    74\n",
       " 16240     71\n",
       " 287546    99\n",
       " 120056    93\n",
       " Name: Max_Peak_Position, Length: 3877, dtype: int64,\n",
       " 'y_withgenre_test': 81546     54\n",
       " 124039    95\n",
       " 123640    95\n",
       " 245447    92\n",
       " 58785     35\n",
       "           ..\n",
       " 100863    90\n",
       " 288966    99\n",
       " 161745    77\n",
       " 326819    81\n",
       " 217497    85\n",
       " Name: Max_Peak_Position, Length: 4846, dtype: int64,\n",
       " 'X_nogenre_train_scaled': array([[ 1.005965  ,  1.29795512, -0.37810755, ..., -0.01917298,\n",
       "          0.21189413,  0.80937835],\n",
       "        [-0.26449697, -0.4074845 , -0.02153011, ..., -0.49913249,\n",
       "          0.21189413, -1.36147781],\n",
       "        [-1.06233792, -1.51406745, -1.32228443, ..., -0.75477699,\n",
       "         -9.00936589, -1.36147781],\n",
       "        ...,\n",
       "        [-0.79734075,  0.5493843 , -0.51872964, ..., -1.1209628 ,\n",
       "          0.21189413,  0.80937835],\n",
       "        [ 1.47186137, -0.08852823, -0.1069078 , ..., -0.7222753 ,\n",
       "          0.21189413,  0.80937835],\n",
       "        [ 1.53064515,  0.56240292,  0.16429194, ..., -0.01306337,\n",
       "          0.21189413, -1.36147781]]),\n",
       " 'X_nogenre_val_scaled': array([[ 0.21331002, -0.68738489, -0.73468499, ..., -0.32166966,\n",
       "          0.21189413, -1.36147781],\n",
       "        [ 1.0889832 , -0.34239138, -1.427751  , ...,  0.41727332,\n",
       "          0.21189413, -1.36147781],\n",
       "        [ 0.48166952, -0.82408043,  0.26473629, ...,  0.4136857 ,\n",
       "          0.21189413,  0.80937835],\n",
       "        ...,\n",
       "        [-0.78215328, -1.61821643, -1.63366191, ..., -0.84183892,\n",
       "          0.21189413,  0.80937835],\n",
       "        [-1.22951409, -0.17314928, -1.38757326, ..., -0.0848868 ,\n",
       "          0.21189413, -1.36147781],\n",
       "        [ 0.55955875, -0.6353104 , -1.08121799, ...,  0.43418131,\n",
       "         -2.86185921,  0.80937835]]),\n",
       " 'X_nogenre_test_scaled': array([[ 0.90604967, -1.22765774,  1.41482409, ...,  2.00967894,\n",
       "          0.21189413,  0.80937835],\n",
       "        [ 0.08407404,  2.35897289, -0.40824085, ..., -0.41057868,\n",
       "          0.21189413,  0.37520712],\n",
       "        [-0.64481066, -2.12594273, -0.90544038, ..., -0.62906824,\n",
       "         -2.86185921,  0.80937835],\n",
       "        ...,\n",
       "        [-1.095776  ,  0.07420456,  0.27980294, ..., -0.83860651,\n",
       "          0.21189413, -1.36147781],\n",
       "        [ 0.02695717,  1.73407899, -0.90544038, ...,  0.30737141,\n",
       "          0.21189413,  0.80937835],\n",
       "        [-0.41457656,  0.69258915, -1.543262  , ..., -0.89060923,\n",
       "          0.21189413,  0.80937835]]),\n",
       " 'y_nogenre_train': 90009     30.0\n",
       " 288809     7.0\n",
       " 123342    13.0\n",
       " 241743     0.0\n",
       " 108136    18.0\n",
       "           ... \n",
       " 40802     24.0\n",
       " 51803     23.0\n",
       " 324557    36.0\n",
       " 60045     52.0\n",
       " 300503    23.0\n",
       " Name: Max_Rank_Change, Length: 15507, dtype: float64,\n",
       " 'y_nogenre_val': 269914     0.0\n",
       " 197784     0.0\n",
       " 120861     0.0\n",
       " 197104    33.0\n",
       " 132876    22.0\n",
       "           ... \n",
       " 148635     8.0\n",
       " 259423     0.0\n",
       " 16240     12.0\n",
       " 287546     0.0\n",
       " 120056     7.0\n",
       " Name: Max_Rank_Change, Length: 3877, dtype: float64,\n",
       " 'y_nogenre_test': 81546     11.0\n",
       " 124039     0.0\n",
       " 123640     0.0\n",
       " 245447     0.0\n",
       " 58785     11.0\n",
       "           ... \n",
       " 100863    31.0\n",
       " 288966     0.0\n",
       " 161745     6.0\n",
       " 326819    17.0\n",
       " 217497    10.0\n",
       " Name: Max_Rank_Change, Length: 4846, dtype: float64,\n",
       " 'scaler_withgenre': StandardScaler(),\n",
       " 'scaler_nogenre': StandardScaler(),\n",
       " 'df_clean_withgenre':         spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       " 0                        163160.0         0.574   0.256   7.0   -15.044   1.0   \n",
       " 17                       136400.0         0.731   0.701   7.0    -8.722   0.0   \n",
       " 29                       143440.0         0.658   0.692   9.0    -8.380   0.0   \n",
       " 42                       211973.0         0.401   0.285  11.0   -11.005   1.0   \n",
       " 68                       354640.0         0.384   0.535   9.0    -9.939   1.0   \n",
       " ...                           ...           ...     ...   ...       ...   ...   \n",
       " 330276                   217000.0         0.289   0.716   5.0    -6.530   0.0   \n",
       " 330326                   228733.0         0.595   0.927   0.0    -4.643   1.0   \n",
       " 330336                   327040.0         0.761   0.851   4.0    -9.779   0.0   \n",
       " 330349                   133800.0         0.782   0.371  10.0   -14.501   1.0   \n",
       " 330354                   214733.0         0.521   0.656   6.0    -5.015   1.0   \n",
       " \n",
       "         speechiness  acousticness  instrumentalness  liveness  ...  \\\n",
       " 0            0.0298       0.61000          0.000077    0.1000  ...   \n",
       " 17           0.0287       0.15700          0.000007    0.0595  ...   \n",
       " 29           0.0467       0.17500          0.000016    0.0630  ...   \n",
       " 42           0.0327       0.65700          0.000011    0.1580  ...   \n",
       " 68           0.0289       0.23800          0.000450    0.0533  ...   \n",
       " ...             ...           ...               ...       ...  ...   \n",
       " 330276       0.1060       0.04070          0.001230    0.1350  ...   \n",
       " 330326       0.0350       0.00511          0.000000    0.1640  ...   \n",
       " 330336       0.0524       0.05460          0.122000    0.0360  ...   \n",
       " 330349       0.0639       0.59700          0.000000    0.3110  ...   \n",
       " 330354       0.0289       0.38700          0.000000    0.1430  ...   \n",
       " \n",
       "         filter house  contemporary vocal jazz  harmonica blues  gothic rock  \\\n",
       " 0                  0                        0                0            0   \n",
       " 17                 0                        0                0            0   \n",
       " 29                 0                        0                0            0   \n",
       " 42                 0                        0                0            0   \n",
       " 68                 0                        0                0            0   \n",
       " ...              ...                      ...              ...          ...   \n",
       " 330276             0                        0                0            0   \n",
       " 330326             0                        0                0            0   \n",
       " 330336             0                        0                0            0   \n",
       " 330349             0                        0                0            0   \n",
       " 330354             0                        0                0            0   \n",
       " \n",
       "         german metal  neo classical metal  moombahton  In_Top5genres_Mean  \\\n",
       " 0                  0                    0           0                0.00   \n",
       " 17                 0                    0           0                1.00   \n",
       " 29                 0                    0           0                1.00   \n",
       " 42                 0                    0           0                1.00   \n",
       " 68                 0                    0           0                1.00   \n",
       " ...              ...                  ...         ...                 ...   \n",
       " 330276             0                    0           0                0.00   \n",
       " 330326             0                    0           0                0.50   \n",
       " 330336             0                    0           0                0.00   \n",
       " 330349             0                    0           0                1.00   \n",
       " 330354             0                    0           0                0.95   \n",
       " \n",
       "         Max_Rank_Change  Max_Peak_Position  \n",
       " 0                  22.0                 79  \n",
       " 17                 11.0                 96  \n",
       " 29                 25.0                 90  \n",
       " 42                 20.0                 72  \n",
       " 68                 18.0                 74  \n",
       " ...                 ...                ...  \n",
       " 330276             12.0                 55  \n",
       " 330326             10.0                 78  \n",
       " 330336             20.0                 83  \n",
       " 330349              0.0                 88  \n",
       " 330354              8.0                 80  \n",
       " \n",
       " [24230 rows x 476 columns],\n",
       " 'df_clean_nogenre':         spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       " 0                        163160.0         0.574   0.256   7.0   -15.044   1.0   \n",
       " 17                       136400.0         0.731   0.701   7.0    -8.722   0.0   \n",
       " 29                       143440.0         0.658   0.692   9.0    -8.380   0.0   \n",
       " 42                       211973.0         0.401   0.285  11.0   -11.005   1.0   \n",
       " 68                       354640.0         0.384   0.535   9.0    -9.939   1.0   \n",
       " ...                           ...           ...     ...   ...       ...   ...   \n",
       " 330276                   217000.0         0.289   0.716   5.0    -6.530   0.0   \n",
       " 330326                   228733.0         0.595   0.927   0.0    -4.643   1.0   \n",
       " 330336                   327040.0         0.761   0.851   4.0    -9.779   0.0   \n",
       " 330349                   133800.0         0.782   0.371  10.0   -14.501   1.0   \n",
       " 330354                   214733.0         0.521   0.656   6.0    -5.015   1.0   \n",
       " \n",
       "         speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       " 0            0.0298       0.61000          0.000077    0.1000    0.568   \n",
       " 17           0.0287       0.15700          0.000007    0.0595    0.961   \n",
       " 29           0.0467       0.17500          0.000016    0.0630    0.910   \n",
       " 42           0.0327       0.65700          0.000011    0.1580    0.267   \n",
       " 68           0.0289       0.23800          0.000450    0.0533    0.232   \n",
       " ...             ...           ...               ...       ...      ...   \n",
       " 330276       0.1060       0.04070          0.001230    0.1350    0.344   \n",
       " 330326       0.0350       0.00511          0.000000    0.1640    0.905   \n",
       " 330336       0.0524       0.05460          0.122000    0.0360    0.752   \n",
       " 330349       0.0639       0.59700          0.000000    0.3110    0.828   \n",
       " 330354       0.0289       0.38700          0.000000    0.1430    0.598   \n",
       " \n",
       "           tempo  time_signature  In_Top5genres_Mean  Max_Rank_Change  \\\n",
       " 0        82.331             3.0                0.00             22.0   \n",
       " 17      107.521             4.0                1.00             11.0   \n",
       " 29      115.311             4.0                1.00             25.0   \n",
       " 42      130.465             4.0                1.00             20.0   \n",
       " 68      132.445             4.0                1.00             18.0   \n",
       " ...         ...             ...                 ...              ...   \n",
       " 330276  168.049             4.0                0.00             12.0   \n",
       " 330326  126.779             4.0                0.50             10.0   \n",
       " 330336  122.065             4.0                0.00             20.0   \n",
       " 330349  120.851             4.0                1.00              0.0   \n",
       " 330354  170.939             4.0                0.95              8.0   \n",
       " \n",
       "         Max_Peak_Position  \n",
       " 0                      79  \n",
       " 17                     96  \n",
       " 29                     90  \n",
       " 42                     72  \n",
       " 68                     74  \n",
       " ...                   ...  \n",
       " 330276                 55  \n",
       " 330326                 78  \n",
       " 330336                 83  \n",
       " 330349                 88  \n",
       " 330354                 80  \n",
       " \n",
       " [24230 rows x 16 columns],\n",
       " 'df_top_genres':         WeekID  Most_Popular_Genre 2nd_Most_Popular_Genre  \\\n",
       " 0     1/1/1966  brill building pop        adult standards   \n",
       " 1     1/1/1972                soul              folk rock   \n",
       " 2     1/1/1977           soft rock            mellow gold   \n",
       " 3     1/1/1983           soft rock            mellow gold   \n",
       " 4     1/1/1994           dance pop     urban contemporary   \n",
       " ...        ...                 ...                    ...   \n",
       " 3274  9/9/1989        new wave pop                   rock   \n",
       " 3275  9/9/1995  urban contemporary                hip hop   \n",
       " 3276  9/9/2000           dance pop                country   \n",
       " 3277  9/9/2006           dance pop                pop rap   \n",
       " 3278  9/9/2017                 pop                    rap   \n",
       " \n",
       "      3rd_Most_Popular_Genre 4th_Most_Popular_Genre 5th_Most_Popular_Genre  \n",
       " 0             bubblegum pop          rock-and-roll              folk rock  \n",
       " 1                    motown           classic soul           classic rock  \n",
       " 2              classic rock             album rock                   rock  \n",
       " 3                album rock                   rock           classic rock  \n",
       " 4                       r&b                hip hop                   rock  \n",
       " ...                     ...                    ...                    ...  \n",
       " 3274              soft rock             album rock            mellow gold  \n",
       " 3275                    r&b               pop rock         new jack swing  \n",
       " 3276                    r&b     urban contemporary               pop rock  \n",
       " 3277                    pop                    r&b     urban contemporary  \n",
       " 3278                pop rap              dance pop                   trap  \n",
       " \n",
       " [3279 rows x 6 columns]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_music_data('Data/Hot Stuff.csv', 'Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for maximum peak position provides a reasonable estimate of the highest position a song will reach on the Billboard Hot 100, giving FutureProduct Advisors consultants an easy way to predict a song's popularity.\n",
    "\n",
    "The mean absolute error (MAE) of the model is 17, which is less accurate than the initial target of 10. However, this MAE still means that the model will typically predict whether a song ends up in the top third, middle third, or bottom third of the Hot 100 list. This gives enough information about the predicted popularity to help FPA consultants estimate if the song will become more expensive to license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for maximum rank change gives FutureProduct Advisors consultants a way to estimate whether a song's popularity is likely to grow more or less quickly in the future: \n",
    "\n",
    "- If the estimated maximum rank change is greater than the largest rank change to date, the song is likely to increase in popularity in the future\n",
    "- If a song's historical maximum rank change is greater than the estimated maximum rank change, the song's popularity has either peaked or its increase is slowing.\n",
    "\n",
    "The mean absolute error of the model's predictions is 9. Since the mean value of the maximum rank change for the songs in the dataset is 13, this is notably above the target accuracy of 90% (at 90% accuracy the MAE should be less than 2). This means that estimates using this metric are less predictive than the maximum peak position.\n",
    "\n",
    "To account for this uncertainty, FPA consultants should be cautious in drawing conclusions based on the predicted values of maximum rank change, and should not base critical decisions on this metric alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has demonstrated the difficulty of predicting the popularity of music. Even though I started with a dataset of more than 20 features (and I engineered more), I was not able to create models that predicted the maximum position and maximum rank change with 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The models to predict the highest ranking on the Billboard Hot 100 and to predict largest week over week ranking increase are ready for initial deployment to FutureProduct Advisors. \n",
    "\n",
    "The high-level deployment plan would be as follows:\n",
    "\n",
    "1. I and my colleagues will build a data pipeline to ingest the Billboard Hot 100 list from Spotify each week and these two models will be run on that data\n",
    "2. Each week's predictions will be published to the FutureProduct Advisors internal message board for use by their consultants.\n",
    "3. We will provide a 30 minute training to orient the FPA consultants to the models and how to use their outputs.\n",
    "\n",
    "There is also an additional opportunity to built on and refine these models by indluding additional data and experimenting with other modeling approaches. If the FutureProduct Advisors consultants have positive feedback on these prototype tools, we will be happy to partner on future enhancements.\n",
    "\n",
    "Examples of future avenues of study include:\n",
    "\n",
    "1. Identifying alternate sources of genre classifications. The dataset used in this project includes genre data that is assigned by Spotify users and then aggregated; a data source with a different approach to assigning genres may produce a smaller and more thoroughly vetted set of genres.\n",
    "2. Incorporating sentiment data. Analyzing written review data (both from consumers and journalists) may provide a set of predictive signals that could improve these models.\n",
    "3. Exploring other approaches to modeling. As an example, it may be interesting to calculate similarity scores between the #1 song and all others each week, then use that similarity score to make predictions about future weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
