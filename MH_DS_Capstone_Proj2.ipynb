{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project seeks to build machine learning models to achieve two things:\n",
    "\n",
    "1. Predict the highest ranking a song will achieve on the Billboard Hot 100 list.\n",
    "2. Predict the largest week over week increase in a given song's ranking on the Hot 100 list.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "- A song's genre has a minimal impact on its ranking on the Billboard Hot 100 list.\n",
    "- Song characteristics such as tempo, danceability, etc. appear to be the strongest drivers of their performance on the Hot 100 list.\n",
    "- Analyzing music can quickly lead to an overwhelming number of features, so thoughtful and careful data selection is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 327895 entries, 0 to 327894\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   327895 non-null  int64  \n",
      " 1   url                     327895 non-null  object \n",
      " 2   WeekID                  327895 non-null  object \n",
      " 3   Week Position           327895 non-null  int64  \n",
      " 4   Song                    327895 non-null  object \n",
      " 5   Performer               327895 non-null  object \n",
      " 6   SongID                  327895 non-null  object \n",
      " 7   Instance                327895 non-null  int64  \n",
      " 8   Previous Week Position  295941 non-null  float64\n",
      " 9   Peak Position           327895 non-null  int64  \n",
      " 10  Weeks on Chart          327895 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29503 entries, 0 to 29502\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   index                      29503 non-null  int64  \n",
      " 1   SongID                     29503 non-null  object \n",
      " 2   Performer                  29503 non-null  object \n",
      " 3   Song                       29503 non-null  object \n",
      " 4   spotify_genre              27903 non-null  object \n",
      " 5   spotify_track_id           24397 non-null  object \n",
      " 6   spotify_track_preview_url  14491 non-null  object \n",
      " 7   spotify_track_duration_ms  24397 non-null  float64\n",
      " 8   spotify_track_explicit     24397 non-null  object \n",
      " 9   spotify_track_album        24391 non-null  object \n",
      " 10  danceability               24334 non-null  float64\n",
      " 11  energy                     24334 non-null  float64\n",
      " 12  key                        24334 non-null  float64\n",
      " 13  loudness                   24334 non-null  float64\n",
      " 14  mode                       24334 non-null  float64\n",
      " 15  speechiness                24334 non-null  float64\n",
      " 16  acousticness               24334 non-null  float64\n",
      " 17  instrumentalness           24334 non-null  float64\n",
      " 18  liveness                   24334 non-null  float64\n",
      " 19  valence                    24334 non-null  float64\n",
      " 20  tempo                      24334 non-null  float64\n",
      " 21  time_signature             24334 non-null  float64\n",
      " 22  spotify_track_popularity   24397 non-null  float64\n",
      "dtypes: float64(14), int64(1), object(8)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveness_dist = df_cleaned_genre['liveness'].value_counts()\n",
    "df_liveness_dist = pd.DataFrame(liveness_dist)\n",
    "df_liveness_dist = df_liveness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_liveness_dist['liveness'], df_liveness_dist['count'], color='orange')\n",
    "plt.xlabel('Liveness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Liveness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "danceability_dist = df_cleaned_genre['danceability'].value_counts()\n",
    "df_danceability_dist = pd.DataFrame(danceability_dist)\n",
    "df_danceability_dist = df_danceability_dist.reset_index()\n",
    "\n",
    "plt.bar(df_danceability_dist['danceability'], df_danceability_dist['count'], color='skyblue')\n",
    "plt.xlabel('Danceability Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Danceability Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acousticness_dist = df_cleaned_genre['acousticness'].value_counts()\n",
    "df_acousticness_dist = pd.DataFrame(acousticness_dist)\n",
    "df_acousticness_dist = df_acousticness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_acousticness_dist['acousticness'], df_acousticness_dist['count'], color='orange')\n",
    "plt.xlabel('Acousticness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Acousticness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Highest Ranking: 76\n",
      "Standard Deviation of Highest Ranking: 24.5\n"
     ]
    }
   ],
   "source": [
    "peak_pos_dist = df_cleaned_genre['Max_Peak_Position'].value_counts()\n",
    "df_peak_pos_dist = pd.DataFrame(peak_pos_dist)\n",
    "df_peak_pos_dist = df_peak_pos_dist.reset_index()\n",
    "print(f\"Mean Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Peak Rankings')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='skyblue')\n",
    "plt.xlabel('Highest Ranking')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Peak Rankings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Largest Week over Week Rank Change: 13\n",
      "Standard Deviation of Largest Week over Week Rank Change: 11.8\n"
     ]
    }
   ],
   "source": [
    "max_rank_change = df_cleaned_genre['Max_Rank_Change'].value_counts()\n",
    "df_max_rank_change = pd.DataFrame(max_rank_change)\n",
    "df_max_rank_change = df_max_rank_change.reset_index()\n",
    "df_max_rank_change = df_max_rank_change[df_max_rank_change['Max_Rank_Change'] > 0]\n",
    "print(f\"Mean Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Largest Week over Week Rank Change')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df_max_rank_change['Max_Rank_Change'], df_max_rank_change['count'], color='orange')\n",
    "plt.xlabel('Largest Week over Week Rank Change')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Largest Week over Week Rank Change')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\797317108.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\797317108.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "def diff(a, b):\n",
    "    return a - b\n",
    "\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_preview_url', 'spotify_track_explicit', \n",
    "                                        'spotify_track_popularity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new df with the max weekly rank change for each song in df_hotlist_2000s\n",
    "df_max_rank_change = df_hotlist_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song in df_hotlist_2000s\n",
    "df_max_peak_pos = df_hotlist_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n",
      "8664\n"
     ]
    }
   ],
   "source": [
    "# extracting full list of songs in the time period being studied\n",
    "songid_list = df_hotlist_2000s['SongID'].unique()\n",
    "\n",
    "# creating a features df with only songs in df_hotlist_2000s\n",
    "df_features_2000s = df_features_all[df_features_all['SongID'].isin(songid_list)]\n",
    "\n",
    "# checking for duplicates\n",
    "print(len(df_features_2000s))\n",
    "print(len(pd.unique(df_features_2000s['SongID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8664\n",
      "8664\n"
     ]
    }
   ],
   "source": [
    "# removing duplicates and rechecking\n",
    "df_features_2000s = df_features_2000s.drop_duplicates(subset='SongID')\n",
    "\n",
    "print(len(df_features_2000s))\n",
    "print(len(pd.unique(df_features_2000s['SongID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7798 entries, 5 to 29499\n",
      "Data columns (total 18 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   SongID                     7798 non-null   object \n",
      " 1   spotify_genre              7798 non-null   object \n",
      " 2   spotify_track_id           7798 non-null   object \n",
      " 3   spotify_track_duration_ms  7798 non-null   float64\n",
      " 4   danceability               7798 non-null   float64\n",
      " 5   energy                     7798 non-null   float64\n",
      " 6   key                        7798 non-null   float64\n",
      " 7   loudness                   7798 non-null   float64\n",
      " 8   mode                       7798 non-null   float64\n",
      " 9   speechiness                7798 non-null   float64\n",
      " 10  acousticness               7798 non-null   float64\n",
      " 11  instrumentalness           7798 non-null   float64\n",
      " 12  liveness                   7798 non-null   float64\n",
      " 13  valence                    7798 non-null   float64\n",
      " 14  tempo                      7798 non-null   float64\n",
      " 15  time_signature             7798 non-null   float64\n",
      " 16  Max_Peak_Position          7798 non-null   int64  \n",
      " 17  Max_Rank_Change            7798 non-null   float64\n",
      "dtypes: float64(14), int64(1), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# adding max peak position to features df\n",
    "df_2000s_data = df_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to features df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "# removing entries with missing values and defining as a new df\n",
    "df_cleaned = df_2000s_data[df_2000s_data.notna().all(axis=1)]\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Genre\n",
    "\n",
    "The dataset has genre in a single column; the entry for each song has a variety of genres listed in that single column. In order to explore genre, I'll need to break this field out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_cleaned['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_cleaned['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I created a new csv that contains genres which appear in 50 or more song entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned[genre] = 0\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_22852\\3783977166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned.at[idx, genre] = 1\n"
     ]
    }
   ],
   "source": [
    "# loading list of genres with 50 or more instances in df_cleaned\n",
    "df_genres_50_up = pd.read_csv('genre_counts_50+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_50_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating each new genre column and initializing to 0\n",
    "for genre in final_genres_list:\n",
    "    df_cleaned[genre] = 0\n",
    "\n",
    "# iterating through rows to set values to 1 when genre column appears in original spotify_genre column\n",
    "for idx, genre_string in enumerate(df_cleaned['spotify_genre']):\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        for genre in genre_list:\n",
    "            df_cleaned.at[idx, genre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SongID</th>\n",
       "      <th>spotify_genre</th>\n",
       "      <th>spotify_track_id</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>pop</th>\n",
       "      <th>rap</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>trap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country road</th>\n",
       "      <th>country</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>glee club</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>edm</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>country pop</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>latin</th>\n",
       "      <th>electropop</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>talent show</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>boy band</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>electro house</th>\n",
       "      <th>rock</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>australian country</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>dance rock</th>\n",
       "      <th>new romantic</th>\n",
       "      <th>new wave</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>synthpop</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>europop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>easy listening</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>plugg</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>art pop</th>\n",
       "      <th>canadian indie</th>\n",
       "      <th>chamber pop</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>slow core</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>slow game</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>new rave</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>comic</th>\n",
       "      <th>texas pop punk</th>\n",
       "      <th>bachata</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>tropical</th>\n",
       "      <th>crunk</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>brooklyn drill</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>punk</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>west coast trap</th>\n",
       "      <th>g funk</th>\n",
       "      <th>complextro</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indie r&amp;b</th>\n",
       "      <th>indie soul</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>british soul</th>\n",
       "      <th>danish pop</th>\n",
       "      <th>scandipop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>idol</th>\n",
       "      <th>rap kreyol</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>country rock</th>\n",
       "      <th>redneck</th>\n",
       "      <th>american folk revival</th>\n",
       "      <th>cantautor</th>\n",
       "      <th>latin arena pop</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>rock en espanol</th>\n",
       "      <th>spanish pop</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>emo</th>\n",
       "      <th>funk</th>\n",
       "      <th>soul</th>\n",
       "      <th>classic soul</th>\n",
       "      <th>disco</th>\n",
       "      <th>motown</th>\n",
       "      <th>post-disco</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>lilith</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>ny roots</th>\n",
       "      <th>ethiopian pop</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>canadian pop punk</th>\n",
       "      <th>canadian punk</th>\n",
       "      <th>pop reggaeton</th>\n",
       "      <th>downtempo</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>shiver pop</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>alaska indie</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>sertanejo</th>\n",
       "      <th>sertanejo pop</th>\n",
       "      <th>sertanejo universitario</th>\n",
       "      <th>pixie</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>big room</th>\n",
       "      <th>brostep</th>\n",
       "      <th>catstep</th>\n",
       "      <th>electra</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>lounge</th>\n",
       "      <th>girl group</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>belgian dance</th>\n",
       "      <th>belgian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>modern uplift</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>kentucky hip hop</th>\n",
       "      <th>folk</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>art rock</th>\n",
       "      <th>experimental</th>\n",
       "      <th>experimental rock</th>\n",
       "      <th>melancholia</th>\n",
       "      <th>post-punk</th>\n",
       "      <th>psychedelic rock</th>\n",
       "      <th>jam band</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>deep contemporary country</th>\n",
       "      <th>lds youth</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>ohio hip hop</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>small room</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>deep big room</th>\n",
       "      <th>dutch house</th>\n",
       "      <th>smooth jazz</th>\n",
       "      <th>smooth saxophone</th>\n",
       "      <th>christian music</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>reggaeton colombiano</th>\n",
       "      <th>rap latina</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>cartoon</th>\n",
       "      <th>children's music</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>bounce</th>\n",
       "      <th>electro</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian ccm</th>\n",
       "      <th>christian punk</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>ectofolk</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>anthem worship</th>\n",
       "      <th>ccm</th>\n",
       "      <th>christian pop</th>\n",
       "      <th>worship</th>\n",
       "      <th>bassline</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>norwegian hip hop</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>hawaiian hip hop</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>bhangra</th>\n",
       "      <th>desi hip hop</th>\n",
       "      <th>desi pop</th>\n",
       "      <th>scottish singer-songwriter</th>\n",
       "      <th>grunge</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>k-pop boy group</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>trancecore</th>\n",
       "      <th>album rock</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>glam rock</th>\n",
       "      <th>protopunk</th>\n",
       "      <th>north carolina hip hop</th>\n",
       "      <th>house</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>nu-metalcore</th>\n",
       "      <th>trap soul</th>\n",
       "      <th>italian pop</th>\n",
       "      <th>italo dance</th>\n",
       "      <th>rock-and-roll</th>\n",
       "      <th>rockabilly</th>\n",
       "      <th>groove metal</th>\n",
       "      <th>rap conscient</th>\n",
       "      <th>drill</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>uk contemporary r&amp;b</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>electronica</th>\n",
       "      <th>nu jazz</th>\n",
       "      <th>trip hop</th>\n",
       "      <th>bow pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>san diego rap</th>\n",
       "      <th>canadian trap</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>christian indie</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>neo-singer-songwriter</th>\n",
       "      <th>neo-synthpop</th>\n",
       "      <th>neo-traditional country</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>aussietronica</th>\n",
       "      <th>disney</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>a cappella</th>\n",
       "      <th>latin viral pop</th>\n",
       "      <th>antiviral pop</th>\n",
       "      <th>comedy rock</th>\n",
       "      <th>parody</th>\n",
       "      <th>viral rap</th>\n",
       "      <th>alternative pop rock</th>\n",
       "      <th>la indie</th>\n",
       "      <th>movie tunes</th>\n",
       "      <th>indie electropop</th>\n",
       "      <th>la pop</th>\n",
       "      <th>pop edm</th>\n",
       "      <th>portland hip hop</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>bubble trance</th>\n",
       "      <th>hopebeat</th>\n",
       "      <th>gospel r&amp;b</th>\n",
       "      <th>k-hop</th>\n",
       "      <th>teen pop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>nu gaze</th>\n",
       "      <th>ghanaian hip hop</th>\n",
       "      <th>new americana</th>\n",
       "      <th>southern soul</th>\n",
       "      <th>pop soul</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>classic country pop</th>\n",
       "      <th>nashville sound</th>\n",
       "      <th>sleaze rock</th>\n",
       "      <th>kids dance party</th>\n",
       "      <th>metal</th>\n",
       "      <th>old school thrash</th>\n",
       "      <th>speed metal</th>\n",
       "      <th>thrash metal</th>\n",
       "      <th>k-rap</th>\n",
       "      <th>folk rock</th>\n",
       "      <th>meme rap</th>\n",
       "      <th>lo-fi</th>\n",
       "      <th>washington indie</th>\n",
       "      <th>brooklyn indie</th>\n",
       "      <th>shimmer pop</th>\n",
       "      <th>big beat</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>nyc pop</th>\n",
       "      <th>sheffield indie</th>\n",
       "      <th>scottish rock</th>\n",
       "      <th>uk alternative pop</th>\n",
       "      <th>vocal house</th>\n",
       "      <th>contemporary vocal jazz</th>\n",
       "      <th>norwegian pop</th>\n",
       "      <th>alabama metal</th>\n",
       "      <th>yacht rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>experimental pop</th>\n",
       "      <th>icelandic experimental</th>\n",
       "      <th>icelandic pop</th>\n",
       "      <th>latin rock</th>\n",
       "      <th>mexican rock</th>\n",
       "      <th>melbourne bounce international</th>\n",
       "      <th>cyberpunk</th>\n",
       "      <th>electronic rock</th>\n",
       "      <th>industrial</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>industrial rock</th>\n",
       "      <th>alternative country</th>\n",
       "      <th>indie folk</th>\n",
       "      <th>roots rock</th>\n",
       "      <th>canadian singer-songwriter</th>\n",
       "      <th>comedy</th>\n",
       "      <th>screamo</th>\n",
       "      <th>j-pop</th>\n",
       "      <th>japanese singer-songwriter</th>\n",
       "      <th>post-metal</th>\n",
       "      <th>progressive metal</th>\n",
       "      <th>progressive rock</th>\n",
       "      <th>detroit trap</th>\n",
       "      <th>battle rap</th>\n",
       "      <th>balkan brass</th>\n",
       "      <th>transpop</th>\n",
       "      <th>roots americana</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>australian indie</th>\n",
       "      <th>filter house</th>\n",
       "      <th>neo r&amp;b</th>\n",
       "      <th>lovers rock</th>\n",
       "      <th>old school dancehall</th>\n",
       "      <th>riddim</th>\n",
       "      <th>anti-folk</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>world worship</th>\n",
       "      <th>cedm</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>philly soul</th>\n",
       "      <th>canadian folk</th>\n",
       "      <th>bass trap</th>\n",
       "      <th>vapor twitch</th>\n",
       "      <th>san marcos tx indie</th>\n",
       "      <th>swedish garage rock</th>\n",
       "      <th>swedish hard rock</th>\n",
       "      <th>swedish indie rock</th>\n",
       "      <th>alt z</th>\n",
       "      <th>bedroom pop</th>\n",
       "      <th>chicago hardcore</th>\n",
       "      <th>chicago punk</th>\n",
       "      <th>hardcore punk</th>\n",
       "      <th>southern rock</th>\n",
       "      <th>cowboy western</th>\n",
       "      <th>traditional country</th>\n",
       "      <th>yodeling</th>\n",
       "      <th>chicago indie</th>\n",
       "      <th>alabama indie</th>\n",
       "      <th>queer country</th>\n",
       "      <th>mexican hip hop</th>\n",
       "      <th>deep vocal house</th>\n",
       "      <th>k-pop girl group</th>\n",
       "      <th>cello</th>\n",
       "      <th>power metal</th>\n",
       "      <th>chicago drill</th>\n",
       "      <th>birmingham metal</th>\n",
       "      <th>operatic pop</th>\n",
       "      <th>uk funky</th>\n",
       "      <th>modern salsa</th>\n",
       "      <th>salsa</th>\n",
       "      <th>gospel</th>\n",
       "      <th>grunge pop</th>\n",
       "      <th>french pop</th>\n",
       "      <th>minneapolis sound</th>\n",
       "      <th>bubblegum pop</th>\n",
       "      <th>classic uk pop</th>\n",
       "      <th>boston hip hop</th>\n",
       "      <th>modern southern rock</th>\n",
       "      <th>turntablism</th>\n",
       "      <th>trance</th>\n",
       "      <th>swedish alternative rock</th>\n",
       "      <th>trap boricua</th>\n",
       "      <th>modern blues</th>\n",
       "      <th>champeta</th>\n",
       "      <th>vallenato</th>\n",
       "      <th>deep norteno</th>\n",
       "      <th>duranguense</th>\n",
       "      <th>musica potosina</th>\n",
       "      <th>norteno</th>\n",
       "      <th>norteno-sax</th>\n",
       "      <th>cumbia</th>\n",
       "      <th>liquid funk</th>\n",
       "      <th>chicago house</th>\n",
       "      <th>bluegrass gospel</th>\n",
       "      <th>wu fam</th>\n",
       "      <th>chinese hip hop</th>\n",
       "      <th>chinese idol pop</th>\n",
       "      <th>palm desert scene</th>\n",
       "      <th>stoner metal</th>\n",
       "      <th>stoner rock</th>\n",
       "      <th>virgin islands reggae</th>\n",
       "      <th>destroy techno</th>\n",
       "      <th>tennessee hip hop</th>\n",
       "      <th>french shoegaze</th>\n",
       "      <th>miami bass</th>\n",
       "      <th>romanian pop</th>\n",
       "      <th>electroclash</th>\n",
       "      <th>easycore</th>\n",
       "      <th>milwaukee hip hop</th>\n",
       "      <th>bluegrass</th>\n",
       "      <th>traditional folk</th>\n",
       "      <th>pop r&amp;b</th>\n",
       "      <th>deep progressive trance</th>\n",
       "      <th>australian electropop</th>\n",
       "      <th>israeli pop</th>\n",
       "      <th>rebel blues</th>\n",
       "      <th>banda</th>\n",
       "      <th>regional mexican pop</th>\n",
       "      <th>oxford indie</th>\n",
       "      <th>celtic</th>\n",
       "      <th>middle earth</th>\n",
       "      <th>panamanian pop</th>\n",
       "      <th>albuquerque indie</th>\n",
       "      <th>portland indie</th>\n",
       "      <th>australian rock</th>\n",
       "      <th>lo star</th>\n",
       "      <th>drum and bass</th>\n",
       "      <th>azonto</th>\n",
       "      <th>deep latin christian</th>\n",
       "      <th>grupera</th>\n",
       "      <th>canadian electronic</th>\n",
       "      <th>quebec indie</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>pop dance</th>\n",
       "      <th>slap house</th>\n",
       "      <th>broadway</th>\n",
       "      <th>alabama rap</th>\n",
       "      <th>finnish edm</th>\n",
       "      <th>uk garage</th>\n",
       "      <th>indie anthem-folk</th>\n",
       "      <th>charlottesville indie</th>\n",
       "      <th>progressive trance</th>\n",
       "      <th>uplifting trance</th>\n",
       "      <th>classic girl group</th>\n",
       "      <th>irish singer-songwriter</th>\n",
       "      <th>power pop</th>\n",
       "      <th>strut</th>\n",
       "      <th>ninja</th>\n",
       "      <th>indy indie</th>\n",
       "      <th>german pop</th>\n",
       "      <th>deep euro house</th>\n",
       "      <th>deep house</th>\n",
       "      <th>german dance</th>\n",
       "      <th>bedroom soul</th>\n",
       "      <th>chutney</th>\n",
       "      <th>scorecore</th>\n",
       "      <th>soundtrack</th>\n",
       "      <th>comic metal</th>\n",
       "      <th>britpop</th>\n",
       "      <th>madchester</th>\n",
       "      <th>el paso indie</th>\n",
       "      <th>grime</th>\n",
       "      <th>deep underground hip hop</th>\n",
       "      <th>vapor pop</th>\n",
       "      <th>pinoy hip hop</th>\n",
       "      <th>dutch hip hop</th>\n",
       "      <th>icelandic indie</th>\n",
       "      <th>icelandic rock</th>\n",
       "      <th>australian house</th>\n",
       "      <th>bass house</th>\n",
       "      <th>jazz rap</th>\n",
       "      <th>seattle indie</th>\n",
       "      <th>reggae</th>\n",
       "      <th>bmore</th>\n",
       "      <th>arkansas hip hop</th>\n",
       "      <th>brazilian death metal</th>\n",
       "      <th>brazilian metal</th>\n",
       "      <th>brazilian thrash metal</th>\n",
       "      <th>crossover thrash</th>\n",
       "      <th>new wave of thrash metal</th>\n",
       "      <th>fake</th>\n",
       "      <th>ann arbor indie</th>\n",
       "      <th>bahamian pop</th>\n",
       "      <th>alternative pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>...Ready For It?Taylor Swift</td>\n",
       "      <td>['pop', 'post-teen pop']</td>\n",
       "      <td>2yLa0QULdQr0qAIvVwN6B5</td>\n",
       "      <td>208186.0</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.764</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.417</td>\n",
       "      <td>160.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'Til Summer Comes AroundKeith Urban</td>\n",
       "      <td>['australian country', 'contemporary country',...</td>\n",
       "      <td>1CKmI1IQjVEVB3F7VmJmM3</td>\n",
       "      <td>331466.0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.629</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-7.608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.308</td>\n",
       "      <td>127.907</td>\n",
       "      <td>4.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'Tis The Damn SeasonTaylor Swift</td>\n",
       "      <td>['pop', 'post-teen pop']</td>\n",
       "      <td>7dW84mWkdWE5a6lFWxJCBG</td>\n",
       "      <td>229840.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.434</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.348</td>\n",
       "      <td>145.916</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 SongID  \\\n",
       "5          ...Ready For It?Taylor Swift   \n",
       "13  'Til Summer Comes AroundKeith Urban   \n",
       "16     'Tis The Damn SeasonTaylor Swift   \n",
       "\n",
       "                                        spotify_genre        spotify_track_id  \\\n",
       "5                            ['pop', 'post-teen pop']  2yLa0QULdQr0qAIvVwN6B5   \n",
       "13  ['australian country', 'contemporary country',...  1CKmI1IQjVEVB3F7VmJmM3   \n",
       "16                           ['pop', 'post-teen pop']  7dW84mWkdWE5a6lFWxJCBG   \n",
       "\n",
       "    spotify_track_duration_ms  danceability  energy  key  loudness  mode  \\\n",
       "5                    208186.0         0.613   0.764  2.0    -6.509   1.0   \n",
       "13                   331466.0         0.570   0.629  9.0    -7.608   0.0   \n",
       "16                   229840.0         0.575   0.434  5.0    -8.193   1.0   \n",
       "\n",
       "    speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "5        0.1360        0.0527          0.000000     0.197    0.417  160.015   \n",
       "13       0.0331        0.5930          0.000136     0.770    0.308  127.907   \n",
       "16       0.0312        0.7350          0.000066     0.105    0.348  145.916   \n",
       "\n",
       "    time_signature  Max_Peak_Position  Max_Rank_Change  pop  rap  pop rap  \\\n",
       "5              4.0                4.0             22.0  0.0  0.0      0.0   \n",
       "13             4.0               92.0             14.0  1.0  0.0      0.0   \n",
       "16             4.0               39.0              0.0  1.0  0.0      1.0   \n",
       "\n",
       "    dance pop  post-teen pop  hip hop  trap  contemporary country  \\\n",
       "5         0.0            1.0      0.0   0.0                   0.0   \n",
       "13        1.0            1.0      0.0   0.0                   0.0   \n",
       "16        1.0            0.0      0.0   0.0                   0.0   \n",
       "\n",
       "    country road  country  southern hip hop  modern country rock  atl hip hop  \\\n",
       "5            0.0      0.0               0.0                  0.0          0.0   \n",
       "13           0.0      0.0               0.0                  0.0          0.0   \n",
       "16           0.0      0.0               0.0                  0.0          0.0   \n",
       "\n",
       "    r&b  canadian pop  melodic rap  urban contemporary  pop rock  hollywood  \\\n",
       "5   0.0           0.0          0.0                 0.0       0.0        1.0   \n",
       "13  0.0           0.0          0.0                 0.0       0.0        0.0   \n",
       "16  1.0           0.0          0.0                 0.0       0.0        0.0   \n",
       "\n",
       "    glee club  neo mellow  canadian hip hop  toronto rap  edm  gangster rap  \\\n",
       "5         1.0         0.0               0.0          0.0  0.0           0.0   \n",
       "13        0.0         0.0               0.0          0.0  0.0           0.0   \n",
       "16        0.0         0.0               0.0          0.0  0.0           0.0   \n",
       "\n",
       "    country pop  tropical house  hip pop  modern rock  miami hip hop  latin  \\\n",
       "5           0.0             0.0      0.0          0.0            0.0    0.0   \n",
       "13          0.0             0.0      0.0          0.0            0.0    0.0   \n",
       "16          0.0             0.0      0.0          0.0            0.0    0.0   \n",
       "\n",
       "    electropop  chicago rap  conscious hip hop  viral pop  country dawn  \\\n",
       "5          0.0          0.0                0.0        0.0           0.0   \n",
       "13         0.0          0.0                0.0        0.0           0.0   \n",
       "16         0.0          0.0                0.0        0.0           0.0   \n",
       "\n",
       "    uk pop  dirty south rap  philly rap  detroit hip hop  alternative r&b  \\\n",
       "5      0.0              0.0         0.0              0.0              0.0   \n",
       "13     0.0              0.0         0.0              0.0              0.0   \n",
       "16     0.0              0.0         0.0              0.0              0.0   \n",
       "\n",
       "    post-grunge  oklahoma country  talent show  canadian contemporary r&b  \\\n",
       "5           0.0               0.0          0.0                        0.0   \n",
       "13          0.0               0.0          0.0                        0.0   \n",
       "16          0.0               0.0          0.0                        0.0   \n",
       "\n",
       "    neo soul  boy band  reggaeton  electro house  rock  atl trap  nc hip hop  \\\n",
       "5        0.0       0.0        0.0            0.0   0.0       0.0         0.0   \n",
       "13       0.0       0.0        0.0            0.0   0.0       0.0         0.0   \n",
       "16       0.0       0.0        0.0            0.0   0.0       0.0         0.0   \n",
       "\n",
       "    new orleans rap  emo rap  australian country  alternative metal  \\\n",
       "5               0.0      0.0                 NaN                NaN   \n",
       "13              0.0      0.0                 NaN                NaN   \n",
       "16              0.0      0.0                 NaN                NaN   \n",
       "\n",
       "    canadian metal  canadian rock  nu metal  alternative rock  permanent wave  \\\n",
       "5              NaN            NaN       NaN               NaN             NaN   \n",
       "13             NaN            NaN       NaN               NaN             NaN   \n",
       "16             NaN            NaN       NaN               NaN             NaN   \n",
       "\n",
       "    dance rock  new romantic  new wave  new wave pop  soft rock  synthpop  \\\n",
       "5          NaN           NaN       NaN           NaN        NaN       NaN   \n",
       "13         NaN           NaN       NaN           NaN        NaN       NaN   \n",
       "16         NaN           NaN       NaN           NaN        NaN       NaN   \n",
       "\n",
       "    candy pop  europop  adult standards  brill building pop  easy listening  \\\n",
       "5         NaN      NaN              NaN                 NaN             NaN   \n",
       "13        NaN      NaN              NaN                 NaN             NaN   \n",
       "16        NaN      NaN              NaN                 NaN             NaN   \n",
       "\n",
       "    vocal jazz  dancehall  glam metal  plugg  underground hip hop  \\\n",
       "5          NaN        NaN         NaN    NaN                  NaN   \n",
       "13         NaN        NaN         NaN    NaN                  NaN   \n",
       "16         NaN        NaN         NaN    NaN                  NaN   \n",
       "\n",
       "    deep pop r&b  pop punk  pittsburgh rap  acoustic pop  piano rock  art pop  \\\n",
       "5            NaN       NaN             NaN           NaN         NaN      NaN   \n",
       "13           NaN       NaN             NaN           NaN         NaN      NaN   \n",
       "16           NaN       NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "    canadian indie  chamber pop  indie pop  indie rock  slow core  \\\n",
       "5              NaN          NaN        NaN         NaN        NaN   \n",
       "13             NaN          NaN        NaN         NaN        NaN   \n",
       "16             NaN          NaN        NaN         NaN        NaN   \n",
       "\n",
       "    stomp and holler  cali rap  slow game  alternative dance  dance-punk  \\\n",
       "5                NaN       NaN        NaN                NaN         NaN   \n",
       "13               NaN       NaN        NaN                NaN         NaN   \n",
       "16               NaN       NaN        NaN                NaN         NaN   \n",
       "\n",
       "    indietronica  new rave  indie pop rap  comic  texas pop punk  bachata  \\\n",
       "5            NaN       NaN            NaN    NaN             NaN      NaN   \n",
       "13           NaN       NaN            NaN    NaN             NaN      NaN   \n",
       "16           NaN       NaN            NaN    NaN             NaN      NaN   \n",
       "\n",
       "    latin pop  tropical  crunk  metropopolis  baton rouge rap  brooklyn drill  \\\n",
       "5         NaN       NaN    NaN           NaN              NaN             NaN   \n",
       "13        NaN       NaN    NaN           NaN              NaN             NaN   \n",
       "16        NaN       NaN    NaN           NaN              NaN             NaN   \n",
       "\n",
       "    nyc rap  australian pop  punk  east coast hip hop  queens hip hop  \\\n",
       "5       NaN             NaN   NaN                 NaN             NaN   \n",
       "13      NaN             NaN   NaN                 NaN             NaN   \n",
       "16      NaN             NaN   NaN                 NaN             NaN   \n",
       "\n",
       "    west coast trap  g funk  complextro  german techno  new jack swing  \\\n",
       "5               NaN     NaN         NaN            NaN             NaN   \n",
       "13              NaN     NaN         NaN            NaN             NaN   \n",
       "16              NaN     NaN         NaN            NaN             NaN   \n",
       "\n",
       "    escape room  indie r&b  indie soul  dmv rap  memphis hip hop  \\\n",
       "5           NaN        NaN         NaN      NaN              NaN   \n",
       "13          NaN        NaN         NaN      NaN              NaN   \n",
       "16          NaN        NaN         NaN      NaN              NaN   \n",
       "\n",
       "    new jersey rap  british soul  danish pop  scandipop  texas country  idol  \\\n",
       "5              NaN           NaN         NaN        NaN            NaN   NaN   \n",
       "13             NaN           NaN         NaN        NaN            NaN   NaN   \n",
       "16             NaN           NaN         NaN        NaN            NaN   NaN   \n",
       "\n",
       "    rap kreyol  dfw rap  deep southern trap  deep talent show  country rock  \\\n",
       "5          NaN      NaN                 NaN               NaN           NaN   \n",
       "13         NaN      NaN                 NaN               NaN           NaN   \n",
       "16         NaN      NaN                 NaN               NaN           NaN   \n",
       "\n",
       "    redneck  american folk revival  cantautor  latin arena pop  mexican pop  \\\n",
       "5       NaN                    NaN        NaN              NaN          NaN   \n",
       "13      NaN                    NaN        NaN              NaN          NaN   \n",
       "16      NaN                    NaN        NaN              NaN          NaN   \n",
       "\n",
       "    rock en espanol  spanish pop  country gospel  alberta country  \\\n",
       "5               NaN          NaN             NaN              NaN   \n",
       "13              NaN          NaN             NaN              NaN   \n",
       "16              NaN          NaN             NaN              NaN   \n",
       "\n",
       "    canadian contemporary country  canadian country  emo  funk  soul  \\\n",
       "5                             NaN               NaN  NaN   NaN   NaN   \n",
       "13                            NaN               NaN  NaN   NaN   NaN   \n",
       "16                            NaN               NaN  NaN   NaN   NaN   \n",
       "\n",
       "    classic soul  disco  motown  post-disco  quiet storm  lilith  folk-pop  \\\n",
       "5            NaN    NaN     NaN         NaN          NaN     NaN       NaN   \n",
       "13           NaN    NaN     NaN         NaN          NaN     NaN       NaN   \n",
       "16           NaN    NaN     NaN         NaN          NaN     NaN       NaN   \n",
       "\n",
       "    ny roots  ethiopian pop  trap queen  canadian pop punk  canadian punk  \\\n",
       "5        NaN            NaN         NaN                NaN            NaN   \n",
       "13       NaN            NaN         NaN                NaN            NaN   \n",
       "16       NaN            NaN         NaN                NaN            NaN   \n",
       "\n",
       "    pop reggaeton  downtempo  electronic trap  shiver pop  latin hip hop  \\\n",
       "5             NaN        NaN              NaN         NaN            NaN   \n",
       "13            NaN        NaN              NaN         NaN            NaN   \n",
       "16            NaN        NaN              NaN         NaN            NaN   \n",
       "\n",
       "    reggaeton flow  rap metal  socal pop punk  alaska indie  \\\n",
       "5              NaN        NaN             NaN           NaN   \n",
       "13             NaN        NaN             NaN           NaN   \n",
       "16             NaN        NaN             NaN           NaN   \n",
       "\n",
       "    singer-songwriter  sertanejo  sertanejo pop  sertanejo universitario  \\\n",
       "5                 NaN        NaN            NaN                      NaN   \n",
       "13                NaN        NaN            NaN                      NaN   \n",
       "16                NaN        NaN            NaN                      NaN   \n",
       "\n",
       "    pixie  pop emo  swedish electropop  swedish pop  garage rock  punk blues  \\\n",
       "5     NaN      NaN                 NaN          NaN          NaN         NaN   \n",
       "13    NaN      NaN                 NaN          NaN          NaN         NaN   \n",
       "16    NaN      NaN                 NaN          NaN          NaN         NaN   \n",
       "\n",
       "    big room  brostep  catstep  electra  funk metal  rap rock  \\\n",
       "5        NaN      NaN      NaN      NaN         NaN       NaN   \n",
       "13       NaN      NaN      NaN      NaN         NaN       NaN   \n",
       "16       NaN      NaN      NaN      NaN         NaN       NaN   \n",
       "\n",
       "    australian dance  christian alternative rock  christian rock  \\\n",
       "5                NaN                         NaN             NaN   \n",
       "13               NaN                         NaN             NaN   \n",
       "16               NaN                         NaN             NaN   \n",
       "\n",
       "    canadian latin  bronx hip hop  hardcore hip hop  lounge  girl group  \\\n",
       "5              NaN            NaN               NaN     NaN         NaN   \n",
       "13             NaN            NaN               NaN     NaN         NaN   \n",
       "16             NaN            NaN               NaN     NaN         NaN   \n",
       "\n",
       "    wrestling  west coast rap  show tunes  etherpop  indie poptimism  \\\n",
       "5         NaN             NaN         NaN       NaN              NaN   \n",
       "13        NaN             NaN         NaN       NaN              NaN   \n",
       "16        NaN             NaN         NaN       NaN              NaN   \n",
       "\n",
       "    belgian dance  belgian pop  eurodance  progressive electro house  \\\n",
       "5             NaN          NaN        NaN                        NaN   \n",
       "13            NaN          NaN        NaN                        NaN   \n",
       "16            NaN          NaN        NaN                        NaN   \n",
       "\n",
       "    modern uplift  australian hip hop  kentucky hip hop  folk  mellow gold  \\\n",
       "5             NaN                 NaN               NaN   NaN          NaN   \n",
       "13            NaN                 NaN               NaN   NaN          NaN   \n",
       "16            NaN                 NaN               NaN   NaN          NaN   \n",
       "\n",
       "    heartland rock  art rock  experimental  experimental rock  melancholia  \\\n",
       "5              NaN       NaN           NaN                NaN          NaN   \n",
       "13             NaN       NaN           NaN                NaN          NaN   \n",
       "16             NaN       NaN           NaN                NaN          NaN   \n",
       "\n",
       "    post-punk  psychedelic rock  jam band  barbadian pop  puerto rican pop  \\\n",
       "5         NaN               NaN       NaN            NaN               NaN   \n",
       "13        NaN               NaN       NaN            NaN               NaN   \n",
       "16        NaN               NaN       NaN            NaN               NaN   \n",
       "\n",
       "    trap latino  deep contemporary country  lds youth  reggae fusion  \\\n",
       "5           NaN                        NaN        NaN            NaN   \n",
       "13          NaN                        NaN        NaN            NaN   \n",
       "16          NaN                        NaN        NaN            NaN   \n",
       "\n",
       "    progressive house  ohio hip hop  arkansas country  blues rock  \\\n",
       "5                 NaN           NaN               NaN         NaN   \n",
       "13                NaN           NaN               NaN         NaN   \n",
       "16                NaN           NaN               NaN         NaN   \n",
       "\n",
       "    modern blues rock  small room  bubblegum dance  deep big room  \\\n",
       "5                 NaN         NaN              NaN            NaN   \n",
       "13                NaN         NaN              NaN            NaN   \n",
       "16                NaN         NaN              NaN            NaN   \n",
       "\n",
       "    dutch house  smooth jazz  smooth saxophone  christian music  \\\n",
       "5           NaN          NaN               NaN              NaN   \n",
       "13          NaN          NaN               NaN              NaN   \n",
       "16          NaN          NaN               NaN              NaN   \n",
       "\n",
       "    lgbtq+ hip hop  reggaeton colombiano  rap latina  houston rap  \\\n",
       "5              NaN                   NaN         NaN          NaN   \n",
       "13             NaN                   NaN         NaN          NaN   \n",
       "16             NaN                   NaN         NaN          NaN   \n",
       "\n",
       "    modern folk rock  uk americana  alternative hip hop  chicano rap  cartoon  \\\n",
       "5                NaN           NaN                  NaN          NaN      NaN   \n",
       "13               NaN           NaN                  NaN          NaN      NaN   \n",
       "16               NaN           NaN                  NaN          NaN      NaN   \n",
       "\n",
       "    children's music  old school hip hop  bounce  electro  disco house  \\\n",
       "5                NaN                 NaN     NaN      NaN          NaN   \n",
       "13               NaN                 NaN     NaN      NaN          NaN   \n",
       "16               NaN                 NaN     NaN      NaN          NaN   \n",
       "\n",
       "    canadian ccm  christian punk  indiecoustica  ectofolk  irish rock  \\\n",
       "5            NaN             NaN            NaN       NaN         NaN   \n",
       "13           NaN             NaN            NaN       NaN         NaN   \n",
       "16           NaN             NaN            NaN       NaN         NaN   \n",
       "\n",
       "    anthem worship  ccm  christian pop  worship  bassline  social media pop  \\\n",
       "5              NaN  NaN            NaN      NaN       NaN               NaN   \n",
       "13             NaN  NaN            NaN      NaN       NaN               NaN   \n",
       "16             NaN  NaN            NaN      NaN       NaN               NaN   \n",
       "\n",
       "    norwegian hip hop  outlaw country  hawaiian hip hop  vapor trap  bhangra  \\\n",
       "5                 NaN             NaN               NaN         NaN      NaN   \n",
       "13                NaN             NaN               NaN         NaN      NaN   \n",
       "16                NaN             NaN               NaN         NaN      NaN   \n",
       "\n",
       "    desi hip hop  desi pop  scottish singer-songwriter  grunge  hard rock  \\\n",
       "5            NaN       NaN                         NaN     NaN        NaN   \n",
       "13           NaN       NaN                         NaN     NaN        NaN   \n",
       "16           NaN       NaN                         NaN     NaN        NaN   \n",
       "\n",
       "    k-pop  k-pop boy group  electropowerpop  neon pop punk  trancecore  \\\n",
       "5     NaN              NaN              NaN            NaN         NaN   \n",
       "13    NaN              NaN              NaN            NaN         NaN   \n",
       "16    NaN              NaN              NaN            NaN         NaN   \n",
       "\n",
       "    album rock  classic rock  glam rock  protopunk  north carolina hip hop  \\\n",
       "5          NaN           NaN        NaN        NaN                     NaN   \n",
       "13         NaN           NaN        NaN        NaN                     NaN   \n",
       "16         NaN           NaN        NaN        NaN                     NaN   \n",
       "\n",
       "    house  uk dance  nu-metalcore  trap soul  italian pop  italo dance  \\\n",
       "5     NaN       NaN           NaN        NaN          NaN          NaN   \n",
       "13    NaN       NaN           NaN        NaN          NaN          NaN   \n",
       "16    NaN       NaN           NaN        NaN          NaN          NaN   \n",
       "\n",
       "    rock-and-roll  rockabilly  groove metal  rap conscient  drill  \\\n",
       "5             NaN         NaN           NaN            NaN    NaN   \n",
       "13            NaN         NaN           NaN            NaN    NaN   \n",
       "16            NaN         NaN           NaN            NaN    NaN   \n",
       "\n",
       "    baroque pop  uk contemporary r&b  celtic rock  harlem hip hop  \\\n",
       "5           NaN                  NaN          NaN             NaN   \n",
       "13          NaN                  NaN          NaN             NaN   \n",
       "16          NaN                  NaN          NaN             NaN   \n",
       "\n",
       "    electronica  nu jazz  trip hop  bow pop  country rap  san diego rap  \\\n",
       "5           NaN      NaN       NaN      NaN          NaN            NaN   \n",
       "13          NaN      NaN       NaN      NaN          NaN            NaN   \n",
       "16          NaN      NaN       NaN      NaN          NaN            NaN   \n",
       "\n",
       "    canadian trap  south african rock  christian indie  moombahton  \\\n",
       "5             NaN                 NaN              NaN         NaN   \n",
       "13            NaN                 NaN              NaN         NaN   \n",
       "16            NaN                 NaN              NaN         NaN   \n",
       "\n",
       "    neo-singer-songwriter  neo-synthpop  neo-traditional country  funk rock  \\\n",
       "5                     NaN           NaN                      NaN        NaN   \n",
       "13                    NaN           NaN                      NaN        NaN   \n",
       "16                    NaN           NaN                      NaN        NaN   \n",
       "\n",
       "    aussietronica  disney  florida rap  colombian pop  a cappella  \\\n",
       "5             NaN     NaN          NaN            NaN         NaN   \n",
       "13            NaN     NaN          NaN            NaN         NaN   \n",
       "16            NaN     NaN          NaN            NaN         NaN   \n",
       "\n",
       "    latin viral pop  antiviral pop  comedy rock  parody  viral rap  \\\n",
       "5               NaN            NaN          NaN     NaN        NaN   \n",
       "13              NaN            NaN          NaN     NaN        NaN   \n",
       "16              NaN            NaN          NaN     NaN        NaN   \n",
       "\n",
       "    alternative pop rock  la indie  movie tunes  indie electropop  la pop  \\\n",
       "5                    NaN       NaN          NaN               NaN     NaN   \n",
       "13                   NaN       NaN          NaN               NaN     NaN   \n",
       "16                   NaN       NaN          NaN               NaN     NaN   \n",
       "\n",
       "    pop edm  portland hip hop  viral trap  bubble trance  hopebeat  \\\n",
       "5       NaN               NaN         NaN            NaN       NaN   \n",
       "13      NaN               NaN         NaN            NaN       NaN   \n",
       "16      NaN               NaN         NaN            NaN       NaN   \n",
       "\n",
       "    gospel r&b  k-hop  teen pop  modern alternative rock  nu gaze  \\\n",
       "5          NaN    NaN       NaN                      NaN      NaN   \n",
       "13         NaN    NaN       NaN                      NaN      NaN   \n",
       "16         NaN    NaN       NaN                      NaN      NaN   \n",
       "\n",
       "    ghanaian hip hop  new americana  southern soul  pop soul  \\\n",
       "5                NaN            NaN            NaN       NaN   \n",
       "13               NaN            NaN            NaN       NaN   \n",
       "16               NaN            NaN            NaN       NaN   \n",
       "\n",
       "    swedish synthpop  classic country pop  nashville sound  sleaze rock  \\\n",
       "5                NaN                  NaN              NaN          NaN   \n",
       "13               NaN                  NaN              NaN          NaN   \n",
       "16               NaN                  NaN              NaN          NaN   \n",
       "\n",
       "    kids dance party  metal  old school thrash  speed metal  thrash metal  \\\n",
       "5                NaN    NaN                NaN          NaN           NaN   \n",
       "13               NaN    NaN                NaN          NaN           NaN   \n",
       "16               NaN    NaN                NaN          NaN           NaN   \n",
       "\n",
       "    k-rap  folk rock  meme rap  lo-fi  washington indie  brooklyn indie  \\\n",
       "5     NaN        NaN       NaN    NaN               NaN             NaN   \n",
       "13    NaN        NaN       NaN    NaN               NaN             NaN   \n",
       "16    NaN        NaN       NaN    NaN               NaN             NaN   \n",
       "\n",
       "    shimmer pop  big beat  skate punk  nyc pop  sheffield indie  \\\n",
       "5           NaN       NaN         NaN      NaN              NaN   \n",
       "13          NaN       NaN         NaN      NaN              NaN   \n",
       "16          NaN       NaN         NaN      NaN              NaN   \n",
       "\n",
       "    scottish rock  uk alternative pop  vocal house  contemporary vocal jazz  \\\n",
       "5             NaN                 NaN          NaN                      NaN   \n",
       "13            NaN                 NaN          NaN                      NaN   \n",
       "16            NaN                 NaN          NaN                      NaN   \n",
       "\n",
       "    norwegian pop  alabama metal  yacht rock  soca  experimental pop  \\\n",
       "5             NaN            NaN         NaN   NaN               NaN   \n",
       "13            NaN            NaN         NaN   NaN               NaN   \n",
       "16            NaN            NaN         NaN   NaN               NaN   \n",
       "\n",
       "    icelandic experimental  icelandic pop  latin rock  mexican rock  \\\n",
       "5                      NaN            NaN         NaN           NaN   \n",
       "13                     NaN            NaN         NaN           NaN   \n",
       "16                     NaN            NaN         NaN           NaN   \n",
       "\n",
       "    melbourne bounce international  cyberpunk  electronic rock  industrial  \\\n",
       "5                              NaN        NaN              NaN         NaN   \n",
       "13                             NaN        NaN              NaN         NaN   \n",
       "16                             NaN        NaN              NaN         NaN   \n",
       "\n",
       "    industrial metal  industrial rock  alternative country  indie folk  \\\n",
       "5                NaN              NaN                  NaN         NaN   \n",
       "13               NaN              NaN                  NaN         NaN   \n",
       "16               NaN              NaN                  NaN         NaN   \n",
       "\n",
       "    roots rock  canadian singer-songwriter  comedy  screamo  j-pop  \\\n",
       "5          NaN                         NaN     NaN      NaN    NaN   \n",
       "13         NaN                         NaN     NaN      NaN    NaN   \n",
       "16         NaN                         NaN     NaN      NaN    NaN   \n",
       "\n",
       "    japanese singer-songwriter  post-metal  progressive metal  \\\n",
       "5                          NaN         NaN                NaN   \n",
       "13                         NaN         NaN                NaN   \n",
       "16                         NaN         NaN                NaN   \n",
       "\n",
       "    progressive rock  detroit trap  battle rap  balkan brass  transpop  \\\n",
       "5                NaN           NaN         NaN           NaN       NaN   \n",
       "13               NaN           NaN         NaN           NaN       NaN   \n",
       "16               NaN           NaN         NaN           NaN       NaN   \n",
       "\n",
       "    roots americana  hyphy  australian indie  filter house  neo r&b  \\\n",
       "5               NaN    NaN               NaN           NaN      NaN   \n",
       "13              NaN    NaN               NaN           NaN      NaN   \n",
       "16              NaN    NaN               NaN           NaN      NaN   \n",
       "\n",
       "    lovers rock  old school dancehall  riddim  anti-folk  nz pop  \\\n",
       "5           NaN                   NaN     NaN        NaN     NaN   \n",
       "13          NaN                   NaN     NaN        NaN     NaN   \n",
       "16          NaN                   NaN     NaN        NaN     NaN   \n",
       "\n",
       "    world worship  cedm  minnesota hip hop  philly soul  canadian folk  \\\n",
       "5             NaN   NaN                NaN          NaN            NaN   \n",
       "13            NaN   NaN                NaN          NaN            NaN   \n",
       "16            NaN   NaN                NaN          NaN            NaN   \n",
       "\n",
       "    bass trap  vapor twitch  san marcos tx indie  swedish garage rock  \\\n",
       "5         NaN           NaN                  NaN                  NaN   \n",
       "13        NaN           NaN                  NaN                  NaN   \n",
       "16        NaN           NaN                  NaN                  NaN   \n",
       "\n",
       "    swedish hard rock  swedish indie rock  alt z  bedroom pop  \\\n",
       "5                 NaN                 NaN    NaN          NaN   \n",
       "13                NaN                 NaN    NaN          NaN   \n",
       "16                NaN                 NaN    NaN          NaN   \n",
       "\n",
       "    chicago hardcore  chicago punk  hardcore punk  southern rock  \\\n",
       "5                NaN           NaN            NaN            NaN   \n",
       "13               NaN           NaN            NaN            NaN   \n",
       "16               NaN           NaN            NaN            NaN   \n",
       "\n",
       "    cowboy western  traditional country  yodeling  chicago indie  \\\n",
       "5              NaN                  NaN       NaN            NaN   \n",
       "13             NaN                  NaN       NaN            NaN   \n",
       "16             NaN                  NaN       NaN            NaN   \n",
       "\n",
       "    alabama indie  queer country  mexican hip hop  deep vocal house  \\\n",
       "5             NaN            NaN              NaN               NaN   \n",
       "13            NaN            NaN              NaN               NaN   \n",
       "16            NaN            NaN              NaN               NaN   \n",
       "\n",
       "    k-pop girl group  cello  power metal  chicago drill  birmingham metal  \\\n",
       "5                NaN    NaN          NaN            NaN               NaN   \n",
       "13               NaN    NaN          NaN            NaN               NaN   \n",
       "16               NaN    NaN          NaN            NaN               NaN   \n",
       "\n",
       "    operatic pop  uk funky  modern salsa  salsa  gospel  grunge pop  \\\n",
       "5            NaN       NaN           NaN    NaN     NaN         NaN   \n",
       "13           NaN       NaN           NaN    NaN     NaN         NaN   \n",
       "16           NaN       NaN           NaN    NaN     NaN         NaN   \n",
       "\n",
       "    french pop  minneapolis sound  bubblegum pop  classic uk pop  \\\n",
       "5          NaN                NaN            NaN             NaN   \n",
       "13         NaN                NaN            NaN             NaN   \n",
       "16         NaN                NaN            NaN             NaN   \n",
       "\n",
       "    boston hip hop  modern southern rock  turntablism  trance  \\\n",
       "5              NaN                   NaN          NaN     NaN   \n",
       "13             NaN                   NaN          NaN     NaN   \n",
       "16             NaN                   NaN          NaN     NaN   \n",
       "\n",
       "    swedish alternative rock  trap boricua  modern blues  champeta  vallenato  \\\n",
       "5                        NaN           NaN           NaN       NaN        NaN   \n",
       "13                       NaN           NaN           NaN       NaN        NaN   \n",
       "16                       NaN           NaN           NaN       NaN        NaN   \n",
       "\n",
       "    deep norteno  duranguense  musica potosina  norteno  norteno-sax  cumbia  \\\n",
       "5            NaN          NaN              NaN      NaN          NaN     NaN   \n",
       "13           NaN          NaN              NaN      NaN          NaN     NaN   \n",
       "16           NaN          NaN              NaN      NaN          NaN     NaN   \n",
       "\n",
       "    liquid funk  chicago house  bluegrass gospel  wu fam  chinese hip hop  \\\n",
       "5           NaN            NaN               NaN     NaN              NaN   \n",
       "13          NaN            NaN               NaN     NaN              NaN   \n",
       "16          NaN            NaN               NaN     NaN              NaN   \n",
       "\n",
       "    chinese idol pop  palm desert scene  stoner metal  stoner rock  \\\n",
       "5                NaN                NaN           NaN          NaN   \n",
       "13               NaN                NaN           NaN          NaN   \n",
       "16               NaN                NaN           NaN          NaN   \n",
       "\n",
       "    virgin islands reggae  destroy techno  tennessee hip hop  french shoegaze  \\\n",
       "5                     NaN             NaN                NaN              NaN   \n",
       "13                    NaN             NaN                NaN              NaN   \n",
       "16                    NaN             NaN                NaN              NaN   \n",
       "\n",
       "    miami bass  romanian pop  electroclash  easycore  milwaukee hip hop  \\\n",
       "5          NaN           NaN           NaN       NaN                NaN   \n",
       "13         NaN           NaN           NaN       NaN                NaN   \n",
       "16         NaN           NaN           NaN       NaN                NaN   \n",
       "\n",
       "    bluegrass  traditional folk  pop r&b  deep progressive trance  \\\n",
       "5         NaN               NaN      NaN                      NaN   \n",
       "13        NaN               NaN      NaN                      NaN   \n",
       "16        NaN               NaN      NaN                      NaN   \n",
       "\n",
       "    australian electropop  israeli pop  rebel blues  banda  \\\n",
       "5                     NaN          NaN          NaN    NaN   \n",
       "13                    NaN          NaN          NaN    NaN   \n",
       "16                    NaN          NaN          NaN    NaN   \n",
       "\n",
       "    regional mexican pop  oxford indie  celtic  middle earth  panamanian pop  \\\n",
       "5                    NaN           NaN     NaN           NaN             NaN   \n",
       "13                   NaN           NaN     NaN           NaN             NaN   \n",
       "16                   NaN           NaN     NaN           NaN             NaN   \n",
       "\n",
       "    albuquerque indie  portland indie  australian rock  lo star  \\\n",
       "5                 NaN             NaN              NaN      NaN   \n",
       "13                NaN             NaN              NaN      NaN   \n",
       "16                NaN             NaN              NaN      NaN   \n",
       "\n",
       "    drum and bass  azonto  deep latin christian  grupera  canadian electronic  \\\n",
       "5             NaN     NaN                   NaN      NaN                  NaN   \n",
       "13            NaN     NaN                   NaN      NaN                  NaN   \n",
       "16            NaN     NaN                   NaN      NaN                  NaN   \n",
       "\n",
       "    quebec indie  stomp pop  pop dance  slap house  broadway  alabama rap  \\\n",
       "5            NaN        NaN        NaN         NaN       NaN          NaN   \n",
       "13           NaN        NaN        NaN         NaN       NaN          NaN   \n",
       "16           NaN        NaN        NaN         NaN       NaN          NaN   \n",
       "\n",
       "    finnish edm  uk garage  indie anthem-folk  charlottesville indie  \\\n",
       "5           NaN        NaN                NaN                    NaN   \n",
       "13          NaN        NaN                NaN                    NaN   \n",
       "16          NaN        NaN                NaN                    NaN   \n",
       "\n",
       "    progressive trance  uplifting trance  classic girl group  \\\n",
       "5                  NaN               NaN                 NaN   \n",
       "13                 NaN               NaN                 NaN   \n",
       "16                 NaN               NaN                 NaN   \n",
       "\n",
       "    irish singer-songwriter  power pop  strut  ninja  indy indie  german pop  \\\n",
       "5                       NaN        NaN    NaN    NaN         NaN         NaN   \n",
       "13                      NaN        NaN    NaN    NaN         NaN         NaN   \n",
       "16                      NaN        NaN    NaN    NaN         NaN         NaN   \n",
       "\n",
       "    deep euro house  deep house  german dance  bedroom soul  chutney  \\\n",
       "5               NaN         NaN           NaN           NaN      NaN   \n",
       "13              NaN         NaN           NaN           NaN      NaN   \n",
       "16              NaN         NaN           NaN           NaN      NaN   \n",
       "\n",
       "    scorecore  soundtrack  comic metal  britpop  madchester  el paso indie  \\\n",
       "5         NaN         NaN          NaN      NaN         NaN            NaN   \n",
       "13        NaN         NaN          NaN      NaN         NaN            NaN   \n",
       "16        NaN         NaN          NaN      NaN         NaN            NaN   \n",
       "\n",
       "    grime  deep underground hip hop  vapor pop  pinoy hip hop  dutch hip hop  \\\n",
       "5     NaN                       NaN        NaN            NaN            NaN   \n",
       "13    NaN                       NaN        NaN            NaN            NaN   \n",
       "16    NaN                       NaN        NaN            NaN            NaN   \n",
       "\n",
       "    icelandic indie  icelandic rock  australian house  bass house  jazz rap  \\\n",
       "5               NaN             NaN               NaN         NaN       NaN   \n",
       "13              NaN             NaN               NaN         NaN       NaN   \n",
       "16              NaN             NaN               NaN         NaN       NaN   \n",
       "\n",
       "    seattle indie  reggae  bmore  arkansas hip hop  brazilian death metal  \\\n",
       "5             NaN     NaN    NaN               NaN                    NaN   \n",
       "13            NaN     NaN    NaN               NaN                    NaN   \n",
       "16            NaN     NaN    NaN               NaN                    NaN   \n",
       "\n",
       "    brazilian metal  brazilian thrash metal  crossover thrash  \\\n",
       "5               NaN                     NaN               NaN   \n",
       "13              NaN                     NaN               NaN   \n",
       "16              NaN                     NaN               NaN   \n",
       "\n",
       "    new wave of thrash metal  fake  ann arbor indie  bahamian pop  \\\n",
       "5                        NaN   NaN              NaN           NaN   \n",
       "13                       NaN   NaN              NaN           NaN   \n",
       "16                       NaN   NaN              NaN           NaN   \n",
       "\n",
       "    alternative pop  \n",
       "5               NaN  \n",
       "13              NaN  \n",
       "16              NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviewing full df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_cleaned.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7798 entries, 5 to 29499\n",
      "Data columns (total 69 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   spotify_track_duration_ms  7798 non-null   float64\n",
      " 1   danceability               7798 non-null   float64\n",
      " 2   energy                     7798 non-null   float64\n",
      " 3   key                        7798 non-null   float64\n",
      " 4   loudness                   7798 non-null   float64\n",
      " 5   mode                       7798 non-null   float64\n",
      " 6   speechiness                7798 non-null   float64\n",
      " 7   acousticness               7798 non-null   float64\n",
      " 8   instrumentalness           7798 non-null   float64\n",
      " 9   liveness                   7798 non-null   float64\n",
      " 10  valence                    7798 non-null   float64\n",
      " 11  tempo                      7798 non-null   float64\n",
      " 12  time_signature             7798 non-null   float64\n",
      " 13  Max_Peak_Position          7798 non-null   float64\n",
      " 14  Max_Rank_Change            7798 non-null   float64\n",
      " 15  pop                        7798 non-null   float64\n",
      " 16  rap                        7798 non-null   float64\n",
      " 17  pop rap                    7798 non-null   float64\n",
      " 18  dance pop                  7798 non-null   float64\n",
      " 19  post-teen pop              7798 non-null   float64\n",
      " 20  hip hop                    7798 non-null   float64\n",
      " 21  trap                       7798 non-null   float64\n",
      " 22  contemporary country       7798 non-null   float64\n",
      " 23  country road               7798 non-null   float64\n",
      " 24  country                    7798 non-null   float64\n",
      " 25  southern hip hop           7798 non-null   float64\n",
      " 26  modern country rock        7798 non-null   float64\n",
      " 27  atl hip hop                7798 non-null   float64\n",
      " 28  r&b                        7798 non-null   float64\n",
      " 29  canadian pop               7798 non-null   float64\n",
      " 30  melodic rap                7798 non-null   float64\n",
      " 31  urban contemporary         7798 non-null   float64\n",
      " 32  pop rock                   7798 non-null   float64\n",
      " 33  hollywood                  7798 non-null   float64\n",
      " 34  glee club                  7798 non-null   float64\n",
      " 35  neo mellow                 7798 non-null   float64\n",
      " 36  canadian hip hop           7798 non-null   float64\n",
      " 37  toronto rap                7798 non-null   float64\n",
      " 38  edm                        7798 non-null   float64\n",
      " 39  gangster rap               7798 non-null   float64\n",
      " 40  country pop                7798 non-null   float64\n",
      " 41  tropical house             7798 non-null   float64\n",
      " 42  hip pop                    7798 non-null   float64\n",
      " 43  modern rock                7798 non-null   float64\n",
      " 44  miami hip hop              7798 non-null   float64\n",
      " 45  latin                      7798 non-null   float64\n",
      " 46  electropop                 7798 non-null   float64\n",
      " 47  chicago rap                7798 non-null   float64\n",
      " 48  conscious hip hop          7798 non-null   float64\n",
      " 49  viral pop                  7798 non-null   float64\n",
      " 50  country dawn               7798 non-null   float64\n",
      " 51  uk pop                     7798 non-null   float64\n",
      " 52  dirty south rap            7798 non-null   float64\n",
      " 53  philly rap                 7798 non-null   float64\n",
      " 54  detroit hip hop            7798 non-null   float64\n",
      " 55  alternative r&b            7798 non-null   float64\n",
      " 56  post-grunge                7798 non-null   float64\n",
      " 57  oklahoma country           7798 non-null   float64\n",
      " 58  talent show                7798 non-null   float64\n",
      " 59  canadian contemporary r&b  7798 non-null   float64\n",
      " 60  neo soul                   7798 non-null   float64\n",
      " 61  boy band                   7798 non-null   float64\n",
      " 62  reggaeton                  7798 non-null   float64\n",
      " 63  electro house              7798 non-null   float64\n",
      " 64  rock                       7798 non-null   float64\n",
      " 65  atl trap                   7798 non-null   float64\n",
      " 66  nc hip hop                 7798 non-null   float64\n",
      " 67  new orleans rap            7798 non-null   float64\n",
      " 68  emo rap                    7798 non-null   float64\n",
      "dtypes: float64(69)\n",
      "memory usage: 4.2 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7798 entries, 5 to 29499\n",
      "Data columns (total 15 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   spotify_track_duration_ms  7798 non-null   float64\n",
      " 1   danceability               7798 non-null   float64\n",
      " 2   energy                     7798 non-null   float64\n",
      " 3   key                        7798 non-null   float64\n",
      " 4   loudness                   7798 non-null   float64\n",
      " 5   mode                       7798 non-null   float64\n",
      " 6   speechiness                7798 non-null   float64\n",
      " 7   acousticness               7798 non-null   float64\n",
      " 8   instrumentalness           7798 non-null   float64\n",
      " 9   liveness                   7798 non-null   float64\n",
      " 10  valence                    7798 non-null   float64\n",
      " 11  tempo                      7798 non-null   float64\n",
      " 12  time_signature             7798 non-null   float64\n",
      " 13  Max_Peak_Position          7798 non-null   float64\n",
      " 14  Max_Rank_Change            7798 non-null   float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 974.8 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing fields used for prep/cleaning but not needed for analysis\n",
    "df_cleaned = df_cleaned.drop(['SongID', 'spotify_genre', 'spotify_track_id'], axis=1)\n",
    "\n",
    "# my code added columns for all genres in spotify_genre, removing unwanted columns and creating a clean df with genre\n",
    "last_col_to_keep_genre = 'emo rap'\n",
    "df_cleaned_genre = df_cleaned.loc[:, :last_col_to_keep_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "\n",
    "# creating a clean df for analysis without genre\n",
    "last_col_to_keep_no_genre = 'Max_Rank_Change'\n",
    "df_cleaned_no_genre = df_cleaned.loc[:, :last_col_to_keep_no_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "df_cleaned_no_genre = df_cleaned_no_genre.dropna()\n",
    "\n",
    "df_cleaned_genre.info(), df_cleaned_no_genre.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (1__1 variables)\n",
    "2. Max Peak Position, with genre (1_2 variables)\n",
    "3. Max Rank Change, no genre (2_1 variables)\n",
    "4. Max Rank Change, with genre (2_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X1_1 = df_cleaned_no_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y1_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_1_train, X1_1_test, y1_1_train, y1_1_test = train_test_split(X1_1, y1_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_1_train_scaled = scaler.fit_transform(X1_1_train)\n",
    "X1_1_test_scaled = scaler.fit_transform(X1_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X1_2 = df_cleaned_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y1_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_2_train, X1_2_test, y1_2_train, y1_2_test = train_test_split(X1_2, y1_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_2_train_scaled = scaler.fit_transform(X1_2_train)\n",
    "X1_2_test_scaled = scaler.fit_transform(X1_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X2_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_1_train, X2_1_test, y2_1_train, y2_1_test = train_test_split(X2_1, y2_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_1_train_scaled = scaler.fit_transform(X2_1_train)\n",
    "X2_1_test_scaled = scaler.fit_transform(X2_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X2_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_2_train, X2_2_test, y2_2_train, y2_2_test = train_test_split(X2_2, y2_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_2_train_scaled = scaler.fit_transform(X2_2_train)\n",
    "X2_2_test_scaled = scaler.fit_transform(X2_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (3__1 variables)\n",
    "2. Max Peak Position, with genre (3_2 variables)\n",
    "3. Max Rank Change, no genre (4_1 variables)\n",
    "4. Max Rank Change, with genre (4_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X3_1 = df_cleaned_no_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y3_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_1_train, X3_1_test, y3_1_train, y3_1_test = train_test_split(X3_1, y3_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_1_train_final, X3_1_val, y3_1_train_final, y3_1_val = train_test_split(X3_1_train, y3_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X3_1_train_final)\n",
    "X3_1_train_scaled = scaler.transform(X3_1_train_final)\n",
    "X3_1_val_scaled = scaler.transform(X3_1_val)\n",
    "X3_1_test_scaled = scaler.transform(X3_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X3_2 = df_cleaned_genre.drop(['Max_Peak_Position'], axis=1)\n",
    "y3_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_2_train, X3_2_test, y3_2_train, y3_2_test = train_test_split(X3_2, y3_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_2_train_final, X3_2_val, y3_2_train_final, y3_2_val = train_test_split(X3_2_train, y3_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X3_2_train_final)\n",
    "X3_2_train_scaled = scaler.transform(X3_2_train_final)\n",
    "X3_2_val_scaled = scaler.transform(X3_2_val)\n",
    "X3_2_test_scaled = scaler.transform(X3_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X4_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_1_train, X4_1_test, y4_1_train, y4_1_test = train_test_split(X4_1, y4_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_1_train_final, X4_1_val, y4_1_train_final, y4_1_val = train_test_split(X4_1_train, y4_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_1_train_final)\n",
    "X4_1_train_scaled = scaler.transform(X4_1_train_final)\n",
    "X4_1_val_scaled = scaler.transform(X4_1_val)\n",
    "X4_1_test_scaled = scaler.transform(X4_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X4_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_2_train, X4_2_test, y4_2_train, y4_2_test = train_test_split(X4_2, y4_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_2_train_final, X4_2_val, y4_2_train_final, y4_2_val = train_test_split(X4_2_train, y4_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_2_train_final)\n",
    "X4_2_train_scaled = scaler.transform(X4_2_train_final)\n",
    "X4_2_val_scaled = scaler.transform(X4_2_val)\n",
    "X4_2_test_scaled = scaler.transform(X4_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 24.288\n",
      "R²: -0.019\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_model1_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_1.fit(X1_1_train, y1_1_train)\n",
    "y1_1_pred = xgb_model1_1.predict(X1_1_test)\n",
    "y1_1_pred = np.clip(np.round(y1_1_pred), 1, 100)\n",
    "\n",
    "rmse1_1 = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred))\n",
    "r2_1_1 = r2_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_1:.3f}')\n",
    "print(f'R²: {r2_1_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 4, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 4, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [4, 5],\n",
    "    'learning_rate': [0.03, 0.05, 0.07],\n",
    "    'subsample': [0.75],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.349\n",
      "R²: 0.138\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb1_1.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_1_pred_best = best_xgb1_1.predict(X1_1_test)\n",
    "y1_1_pred_best = np.clip(np.round(y1_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_1_best = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred_best))\n",
    "r2_1_1_best = r2_score(y1_1_test, y1_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_1_best:.3f}')\n",
    "print(f'R²: {r2_1_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 24.107\n",
      "R²: -0.004\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_model1_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_2.fit(X1_2_train, y1_2_train)\n",
    "y1_2_pred = xgb_model1_2.predict(X1_2_test)\n",
    "y1_2_pred = np.clip(np.round(y1_2_pred), 1, 100)\n",
    "\n",
    "rmse1_2 = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred))\n",
    "r2_1_2 = r2_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_2:.3f}')\n",
    "print(f'R²: {r2_1_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.14864488 0.1492893         nan 0.15366233 0.15241199        nan\n",
      " 0.15877152 0.15567255        nan 0.15262342 0.15408111        nan\n",
      " 0.15857928 0.15333329        nan 0.15424407 0.1543409         nan\n",
      " 0.15486221 0.1547039         nan 0.15100173 0.15248342        nan\n",
      " 0.14676159 0.1448122         nan 0.150255   0.15014126        nan\n",
      " 0.15566821 0.15282115        nan 0.15791661 0.15503548        nan\n",
      " 0.15418192 0.15376956        nan 0.15579943 0.15556656        nan\n",
      " 0.15373814 0.15326052        nan 0.15492229 0.15663169        nan\n",
      " 0.15367467 0.15149787        nan 0.14259349 0.14503801        nan\n",
      " 0.14900672 0.15112244        nan 0.1545911  0.15348651        nan\n",
      " 0.1579947  0.15502871        nan 0.15338628 0.15437253        nan\n",
      " 0.15555987 0.15656924        nan 0.15411057 0.15124144        nan\n",
      " 0.15695699 0.15648522        nan 0.14876992 0.15490879        nan\n",
      " 0.14032044 0.1444165         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.05, 'max_depth': 4, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'subsample': [0.9, 1.0, 1.1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [4, 5],\n",
    "    'learning_rate': [0.03, 0.05, 0.07],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.452\n",
      "R²: 0.130\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model \n",
    "best_xgb1_2 = grid_search_xgb1_2.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_2_pred_best = best_xgb1_2.predict(X1_2_test)\n",
    "y1_2_pred_best = np.clip(np.round(y1_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_2_best = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred_best))\n",
    "r2_1_2_best = r2_score(y1_2_test, y1_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_2_best:.3f}')\n",
    "print(f'R²: {r2_1_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.816\n",
      "R²: -0.191\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_1.fit(X2_1_train, y2_1_train)\n",
    "y2_1_pred = xgb_model2_1.predict(X2_1_test)\n",
    "y2_1_pred = np.clip(np.round(y2_1_pred), 1, 100)\n",
    "\n",
    "rmse2_1 = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred))\n",
    "r2_2_1 = r2_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_1:.3f}')\n",
    "print(f'R²: {r2_2_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.005, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "270 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.2 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [ 1.50197986e-05             nan             nan  1.14608918e-04\n",
      "             nan             nan -2.80468071e-04             nan\n",
      "             nan  1.86636909e-04             nan             nan\n",
      " -5.87010988e-05             nan             nan -1.18472068e-03\n",
      "             nan             nan -1.92167407e-05             nan\n",
      "             nan -9.77921559e-04             nan             nan\n",
      " -2.34972333e-03             nan             nan  7.53915468e-05\n",
      "             nan             nan  3.22513503e-04             nan\n",
      "             nan -3.13645963e-04             nan             nan\n",
      "  3.76430746e-04             nan             nan  1.11380241e-04\n",
      "             nan             nan -1.69801804e-03             nan\n",
      "             nan  1.59004858e-04             nan             nan\n",
      " -8.38832207e-04             nan             nan -2.75257297e-03\n",
      "             nan             nan  8.21582394e-05             nan\n",
      "             nan  4.24560896e-04             nan             nan\n",
      " -4.03494149e-04             nan             nan  8.29615217e-05\n",
      "             nan             nan  4.43043770e-05             nan\n",
      "             nan -1.99583015e-03             nan             nan\n",
      "  1.53492102e-04             nan             nan -8.72020929e-04\n",
      "             nan             nan -3.14512564e-03             nan\n",
      "             nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.005, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3 for max_rank_change\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.9],\n",
    "        }\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.733\n",
      "R²: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_1 = grid_search_xgb2_1.best_estimator_\n",
    "\n",
    "y2_1_pred_best = best_xgb2_1.predict(X2_1_test)\n",
    "y2_1_pred_best = np.clip(np.round(y2_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_1_best = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred_best))\n",
    "r2_2_1_best = r2_score(y2_1_test, y2_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_1_best:.3f}')\n",
    "print(f'R²: {r2_2_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.695\n",
      "R²: -0.169\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_2.fit(X2_2_train, y2_2_train)\n",
    "y2_2_pred = xgb_model2_2.predict(X2_2_test)\n",
    "y2_2_pred = np.clip(np.round(y2_2_pred), 1, 100)\n",
    "\n",
    "rmse2_2 = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred))\n",
    "r2_2_2 = r2_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_2:.3f}')\n",
    "print(f'R²: {r2_2_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.005, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "270 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.2 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [-2.19718912e-04             nan             nan -1.23255268e-04\n",
      "             nan             nan -8.18078143e-04             nan\n",
      "             nan -4.19445809e-05             nan             nan\n",
      " -5.56518825e-05             nan             nan -1.64440182e-03\n",
      "             nan             nan  8.77093966e-05             nan\n",
      "             nan -1.40484322e-03             nan             nan\n",
      " -3.29516819e-03             nan             nan -6.54048762e-05\n",
      "             nan             nan -2.60882093e-05             nan\n",
      "             nan -5.58370523e-04             nan             nan\n",
      " -2.16077396e-05             nan             nan -1.29577220e-04\n",
      "             nan             nan -1.04199081e-03             nan\n",
      "             nan -1.31758542e-04             nan             nan\n",
      " -6.92682356e-04             nan             nan -3.26357852e-03\n",
      "             nan             nan -2.94075652e-05             nan\n",
      "             nan  1.88609699e-04             nan             nan\n",
      " -3.13534643e-04             nan             nan  3.90859780e-05\n",
      "             nan             nan -1.40054613e-04             nan\n",
      "             nan -2.16103170e-03             nan             nan\n",
      " -2.40464511e-04             nan             nan -8.02876287e-04\n",
      "             nan             nan -3.59632583e-03             nan\n",
      "             nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.007, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.735\n",
      "R²: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_2 = grid_search_xgb2_2.best_estimator_\n",
    "\n",
    "y2_2_pred_best = best_xgb2_2.predict(X2_2_test)\n",
    "y2_2_pred_best = np.clip(np.round(y2_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_2_best = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred_best))\n",
    "r2_2_2_best = r2_score(y2_2_test, y2_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_2_best:.3f}')\n",
    "print(f'R²: {r2_2_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, the Max Rank Change models both had an r<sup>2</sup> value less than 0.001, essentially indicating no fit of the model to the test data. Max Peak Position performed better, but the highest r<sup>2</sup> value was 0.138 so their predictive value is low.\n",
    "\n",
    "Given the lack of predictive power in these outcomes, I'm shifting focus to the other two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 0.0393\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_1.fit(X1_1_train_scaled, y1_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_1 = grid_search1_1.best_params_\n",
    "standard_best_score1_1 = grid_search1_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0410\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model1_1 = grid_search1_1.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y1_1_pred = final_model1_1.predict(X1_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_1 = accuracy_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 0.0381\n"
     ]
    }
   ],
   "source": [
    "# arameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_2.fit(X1_2_train_scaled, y1_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_2 = grid_search1_2.best_params_\n",
    "standard_best_score1_2 = grid_search1_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0523\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model1_2 = grid_search1_2.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y1_2_pred = final_model1_2.predict(X1_2_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_2 = accuracy_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'chebyshev', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2295\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_1.fit(X2_1_train_scaled, y2_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_1 = grid_search2_1.best_params_\n",
    "standard_best_score2_1 = grid_search2_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2159\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model2_1 = grid_search2_1.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y2_1_pred = final_model2_1.predict(X2_1_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_1 = accuracy_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'chebyshev', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2320\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_2.fit(X2_2_train_scaled, y2_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_2 = grid_search2_2.best_params_\n",
    "standard_best_score2_2 = grid_search2_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2169\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model2_2 = grid_search2_2.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y2_2_pred = final_model2_2.predict(X2_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_2 = accuracy_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed poorly on the Max Peak Position data, with a maximum accuracy of 0.052. The performance on the Max Rank Change was better, but the maximum accuracy was still just 0.217.\n",
    "\n",
    "I will not explore k-NN further and instead focus on the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2530.9824 - mae: 40.6783 - mse: 2530.9824 - val_loss: 797.7865 - val_mae: 21.8751 - val_mse: 797.7865\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 694.9589 - mae: 21.1091 - mse: 694.9589 - val_loss: 685.6998 - val_mae: 21.0481 - val_mse: 685.6998\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 638.1129 - mae: 20.1019 - mse: 638.1129 - val_loss: 656.5057 - val_mae: 19.5229 - val_mse: 656.5057\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 604.6266 - mae: 19.4268 - mse: 604.6266 - val_loss: 619.3231 - val_mae: 19.2630 - val_mse: 619.3231\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 585.0200 - mae: 19.0466 - mse: 585.0200 - val_loss: 604.8651 - val_mae: 19.2081 - val_mse: 604.8651\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 569.3412 - mae: 18.7617 - mse: 569.3412 - val_loss: 598.3763 - val_mae: 18.8303 - val_mse: 598.3763\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 558.8304 - mae: 18.5293 - mse: 558.8304 - val_loss: 583.6070 - val_mae: 18.7153 - val_mse: 583.6070\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 550.5787 - mae: 18.3605 - mse: 550.5787 - val_loss: 577.8023 - val_mae: 18.5138 - val_mse: 577.8023\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 544.5220 - mae: 18.2619 - mse: 544.5220 - val_loss: 588.8312 - val_mae: 18.2198 - val_mse: 588.8312\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 539.4745 - mae: 18.0974 - mse: 539.4745 - val_loss: 579.7988 - val_mae: 19.0069 - val_mse: 579.7988\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 535.4174 - mae: 18.0533 - mse: 535.4174 - val_loss: 570.5456 - val_mae: 18.2588 - val_mse: 570.5456\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 530.5438 - mae: 17.9281 - mse: 530.5438 - val_loss: 564.3566 - val_mae: 18.2532 - val_mse: 564.3566\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 523.1648 - mae: 17.7254 - mse: 523.1648 - val_loss: 566.5308 - val_mae: 18.1534 - val_mse: 566.5308\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 520.4877 - mae: 17.7276 - mse: 520.4877 - val_loss: 569.9415 - val_mae: 18.5676 - val_mse: 569.9415\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 517.3599 - mae: 17.6192 - mse: 517.3599 - val_loss: 565.8568 - val_mae: 17.7463 - val_mse: 565.8568\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 513.2205 - mae: 17.5193 - mse: 513.2205 - val_loss: 557.9642 - val_mae: 17.8520 - val_mse: 557.9642\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 510.2812 - mae: 17.4987 - mse: 510.2812 - val_loss: 567.6061 - val_mae: 18.7706 - val_mse: 567.6061\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 506.3032 - mae: 17.3720 - mse: 506.3032 - val_loss: 562.6249 - val_mae: 17.7269 - val_mse: 562.6249\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 505.6087 - mae: 17.3204 - mse: 505.6087 - val_loss: 561.7828 - val_mae: 17.5942 - val_mse: 561.7828\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 500.0096 - mae: 17.1870 - mse: 500.0096 - val_loss: 559.5499 - val_mae: 18.2713 - val_mse: 559.5499\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 498.3163 - mae: 17.1877 - mse: 498.3163 - val_loss: 561.3179 - val_mae: 18.0193 - val_mse: 561.3179\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 498.0854 - mae: 17.1514 - mse: 498.0854 - val_loss: 556.8733 - val_mae: 18.2193 - val_mse: 556.8733\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 491.9727 - mae: 17.0120 - mse: 491.9727 - val_loss: 564.6458 - val_mae: 18.4866 - val_mse: 564.6458\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 494.9720 - mae: 17.1435 - mse: 494.9720 - val_loss: 572.4710 - val_mae: 18.8798 - val_mse: 572.4710\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 488.7962 - mae: 16.9560 - mse: 488.7962 - val_loss: 563.8442 - val_mae: 17.7764 - val_mse: 563.8442\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 489.0466 - mae: 16.9614 - mse: 489.0466 - val_loss: 566.9279 - val_mae: 17.6314 - val_mse: 566.9279\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 488.2595 - mae: 16.8887 - mse: 488.2595 - val_loss: 553.0042 - val_mae: 17.6623 - val_mse: 553.0042\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 486.5658 - mae: 16.9488 - mse: 486.5658 - val_loss: 566.0759 - val_mae: 17.9328 - val_mse: 566.0759\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 480.4557 - mae: 16.7616 - mse: 480.4557 - val_loss: 559.5264 - val_mae: 17.8428 - val_mse: 559.5264\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 481.2077 - mae: 16.7352 - mse: 481.2077 - val_loss: 573.0399 - val_mae: 17.7203 - val_mse: 573.0399\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 479.8270 - mae: 16.7659 - mse: 479.8270 - val_loss: 556.7506 - val_mae: 17.8709 - val_mse: 556.7506\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 474.9323 - mae: 16.6734 - mse: 474.9323 - val_loss: 566.7853 - val_mae: 17.5285 - val_mse: 566.7853\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 474.6105 - mae: 16.5852 - mse: 474.6105 - val_loss: 560.7004 - val_mae: 17.9267 - val_mse: 560.7004\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 471.4749 - mae: 16.6158 - mse: 471.4749 - val_loss: 562.6207 - val_mae: 17.6920 - val_mse: 562.6207\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 472.3826 - mae: 16.5606 - mse: 472.3826 - val_loss: 561.4929 - val_mae: 17.7836 - val_mse: 561.4929\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 472.6447 - mae: 16.6183 - mse: 472.6447 - val_loss: 573.6048 - val_mae: 17.6879 - val_mse: 573.6048\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 471.8081 - mae: 16.6256 - mse: 471.8081 - val_loss: 565.6828 - val_mae: 17.9223 - val_mse: 565.6828\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 467.4731 - mae: 16.5159 - mse: 467.4731 - val_loss: 560.4177 - val_mae: 17.7550 - val_mse: 560.4177\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 466.4958 - mae: 16.4467 - mse: 466.4958 - val_loss: 576.4180 - val_mae: 17.8924 - val_mse: 576.4180\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 465.8021 - mae: 16.4723 - mse: 465.8021 - val_loss: 562.1943 - val_mae: 18.2314 - val_mse: 562.1943\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 471.3253 - mae: 16.5427 - mse: 471.3253 - val_loss: 566.2860 - val_mae: 17.9707 - val_mse: 566.2860\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 463.4594 - mae: 16.4073 - mse: 463.4594 - val_loss: 612.4145 - val_mae: 17.9726 - val_mse: 612.4145\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 463.7887 - mae: 16.3731 - mse: 463.7887 - val_loss: 569.7554 - val_mae: 17.9586 - val_mse: 569.7554\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 459.3797 - mae: 16.3327 - mse: 459.3797 - val_loss: 574.1302 - val_mae: 18.4356 - val_mse: 574.1302\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 462.0514 - mae: 16.4638 - mse: 462.0514 - val_loss: 574.2860 - val_mae: 18.1462 - val_mse: 574.2860\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 462.7394 - mae: 16.4198 - mse: 462.7394 - val_loss: 574.7142 - val_mae: 17.8033 - val_mse: 574.7142\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 458.7877 - mae: 16.2985 - mse: 458.7877 - val_loss: 574.5422 - val_mae: 18.2098 - val_mse: 574.5422\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 456.9918 - mae: 16.2520 - mse: 456.9918 - val_loss: 574.8157 - val_mae: 17.9738 - val_mse: 574.8157\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 452.9034 - mae: 16.1928 - mse: 452.9034 - val_loss: 578.9062 - val_mae: 18.2227 - val_mse: 578.9062\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 457.8095 - mae: 16.3307 - mse: 457.8095 - val_loss: 571.2515 - val_mae: 18.2729 - val_mse: 571.2515\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 452.3851 - mae: 16.2515 - mse: 452.3851 - val_loss: 577.2748 - val_mae: 18.1240 - val_mse: 577.2748\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 452.5614 - mae: 16.2941 - mse: 452.5614 - val_loss: 574.9706 - val_mae: 17.9937 - val_mse: 574.9706\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 455.9218 - mae: 16.2670 - mse: 455.9218 - val_loss: 583.0463 - val_mae: 17.8491 - val_mse: 583.0463\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 451.4932 - mae: 16.1981 - mse: 451.4932 - val_loss: 587.5252 - val_mae: 18.0307 - val_mse: 587.5252\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 447.2812 - mae: 16.1005 - mse: 447.2812 - val_loss: 580.7109 - val_mae: 18.1004 - val_mse: 580.7109\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 450.2054 - mae: 16.1602 - mse: 450.2054 - val_loss: 583.9913 - val_mae: 18.3259 - val_mse: 583.9913\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 444.6590 - mae: 16.1098 - mse: 444.6590 - val_loss: 585.2741 - val_mae: 18.2920 - val_mse: 585.2741\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 446.2849 - mae: 16.0776 - mse: 446.2849 - val_loss: 582.2386 - val_mae: 18.0785 - val_mse: 582.2386\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 443.0036 - mae: 16.0060 - mse: 443.0036 - val_loss: 589.8950 - val_mae: 18.5541 - val_mse: 589.8950\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 445.1748 - mae: 16.0379 - mse: 445.1748 - val_loss: 585.0993 - val_mae: 18.5444 - val_mse: 585.0993\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 443.2169 - mae: 16.0309 - mse: 443.2169 - val_loss: 597.2256 - val_mae: 18.1918 - val_mse: 597.2256\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 441.7938 - mae: 16.0437 - mse: 441.7938 - val_loss: 589.4788 - val_mae: 18.1864 - val_mse: 589.4788\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 442.7608 - mae: 16.0562 - mse: 442.7608 - val_loss: 589.4975 - val_mae: 18.4022 - val_mse: 589.4975\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 443.2856 - mae: 16.0740 - mse: 443.2856 - val_loss: 593.9718 - val_mae: 18.4871 - val_mse: 593.9718\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 437.5551 - mae: 15.9615 - mse: 437.5551 - val_loss: 590.3318 - val_mae: 18.7636 - val_mse: 590.3318\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 435.5496 - mae: 15.8779 - mse: 435.5496 - val_loss: 604.9910 - val_mae: 18.4536 - val_mse: 604.9910\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 434.3795 - mae: 15.8716 - mse: 434.3795 - val_loss: 595.0862 - val_mae: 18.8244 - val_mse: 595.0862\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 435.3375 - mae: 15.8543 - mse: 435.3375 - val_loss: 619.0197 - val_mae: 18.2916 - val_mse: 619.0197\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 438.2822 - mae: 15.9292 - mse: 438.2822 - val_loss: 606.3074 - val_mae: 18.7133 - val_mse: 606.3074\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 433.0367 - mae: 15.8536 - mse: 433.0367 - val_loss: 610.9443 - val_mae: 18.2545 - val_mse: 610.9443\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 430.2880 - mae: 15.7482 - mse: 430.2880 - val_loss: 601.1061 - val_mae: 18.7039 - val_mse: 601.1061\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 428.2929 - mae: 15.7307 - mse: 428.2929 - val_loss: 601.3718 - val_mae: 18.7057 - val_mse: 601.3718\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 428.1909 - mae: 15.7323 - mse: 428.1909 - val_loss: 607.4091 - val_mae: 18.7989 - val_mse: 607.4091\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 428.6762 - mae: 15.7764 - mse: 428.6762 - val_loss: 602.1373 - val_mae: 18.3729 - val_mse: 602.1373\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 424.0606 - mae: 15.6186 - mse: 424.0606 - val_loss: 600.8302 - val_mae: 18.5664 - val_mse: 600.8302\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 424.1744 - mae: 15.6600 - mse: 424.1744 - val_loss: 613.9360 - val_mae: 18.4377 - val_mse: 613.9360\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 423.8001 - mae: 15.6366 - mse: 423.8001 - val_loss: 623.4398 - val_mae: 18.4566 - val_mse: 623.4398\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 422.3813 - mae: 15.6350 - mse: 422.3813 - val_loss: 613.3779 - val_mae: 19.2384 - val_mse: 613.3779\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 418.9632 - mae: 15.5361 - mse: 418.9632 - val_loss: 607.6252 - val_mae: 18.7075 - val_mse: 607.6252\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 421.2202 - mae: 15.5755 - mse: 421.2202 - val_loss: 616.6373 - val_mae: 18.8527 - val_mse: 616.6373\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 421.1088 - mae: 15.5620 - mse: 421.1088 - val_loss: 620.4882 - val_mae: 19.2615 - val_mse: 620.4882\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 420.1887 - mae: 15.5889 - mse: 420.1887 - val_loss: 615.4393 - val_mae: 19.3398 - val_mse: 615.4393\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 418.6445 - mae: 15.6018 - mse: 418.6445 - val_loss: 617.6261 - val_mae: 18.5993 - val_mse: 617.6261\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 415.4882 - mae: 15.4573 - mse: 415.4882 - val_loss: 617.7864 - val_mae: 19.0966 - val_mse: 617.7864\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 416.2369 - mae: 15.4978 - mse: 416.2369 - val_loss: 616.8356 - val_mae: 19.1166 - val_mse: 616.8356\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 416.4150 - mae: 15.4895 - mse: 416.4150 - val_loss: 619.1196 - val_mae: 19.3112 - val_mse: 619.1196\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 416.8024 - mae: 15.5635 - mse: 416.8024 - val_loss: 623.2914 - val_mae: 18.7830 - val_mse: 623.2914\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 411.5469 - mae: 15.4216 - mse: 411.5469 - val_loss: 610.1105 - val_mae: 18.7287 - val_mse: 610.1105\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 409.6529 - mae: 15.3382 - mse: 409.6529 - val_loss: 627.7401 - val_mae: 18.8972 - val_mse: 627.7401\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 416.1454 - mae: 15.4678 - mse: 416.1454 - val_loss: 625.6109 - val_mae: 19.6506 - val_mse: 625.6109\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 411.0524 - mae: 15.3792 - mse: 411.0524 - val_loss: 615.6661 - val_mae: 18.7554 - val_mse: 615.6661\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 409.9914 - mae: 15.3906 - mse: 409.9914 - val_loss: 619.1609 - val_mae: 18.7870 - val_mse: 619.1609\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 407.8818 - mae: 15.3185 - mse: 407.8818 - val_loss: 613.5400 - val_mae: 18.9608 - val_mse: 613.5400\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 408.7516 - mae: 15.3469 - mse: 408.7516 - val_loss: 630.8140 - val_mae: 19.5785 - val_mse: 630.8140\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 407.1563 - mae: 15.3499 - mse: 407.1563 - val_loss: 628.6628 - val_mae: 19.7075 - val_mse: 628.6628\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 407.4641 - mae: 15.3391 - mse: 407.4641 - val_loss: 626.8748 - val_mae: 18.9489 - val_mse: 626.8748\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 401.7947 - mae: 15.2015 - mse: 401.7947 - val_loss: 621.6541 - val_mae: 18.7635 - val_mse: 621.6541\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 405.0627 - mae: 15.3032 - mse: 405.0627 - val_loss: 621.7344 - val_mae: 18.6493 - val_mse: 621.7344\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 401.7390 - mae: 15.2075 - mse: 401.7390 - val_loss: 646.1885 - val_mae: 18.8421 - val_mse: 646.1885\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 403.7266 - mae: 15.3076 - mse: 403.7266 - val_loss: 620.7819 - val_mae: 18.9784 - val_mse: 620.7819\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 5972.7104 - mae: 73.4225 - mse: 5972.7104 - val_loss: 5671.6406 - val_mae: 71.3022 - val_mse: 5671.6406\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 4916.6826 - mae: 66.2089 - mse: 4916.6826 - val_loss: 4269.3696 - val_mae: 61.4120 - val_mse: 4269.3696\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 3552.9482 - mae: 55.6523 - mse: 3552.9482 - val_loss: 2767.3711 - val_mae: 48.9553 - val_mse: 2767.3711\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 2127.1165 - mae: 42.2377 - mse: 2127.1165 - val_loss: 1558.0217 - val_mae: 35.9289 - val_mse: 1558.0217\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1104.1471 - mae: 29.6456 - mse: 1104.1471 - val_loss: 867.3753 - val_mae: 25.5002 - val_mse: 867.3753\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.2227 - mae: 21.5026 - mse: 644.2227 - val_loss: 597.9647 - val_mae: 19.6143 - val_mse: 597.9647\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 532.7088 - mae: 18.4554 - mse: 532.7088 - val_loss: 589.8149 - val_mae: 19.3221 - val_mse: 589.8149\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 510.6275 - mae: 17.6172 - mse: 510.6275 - val_loss: 565.9005 - val_mae: 18.5294 - val_mse: 565.9005\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 499.8018 - mae: 17.1532 - mse: 499.8018 - val_loss: 560.6464 - val_mae: 18.0148 - val_mse: 560.6464\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 495.4029 - mae: 17.0100 - mse: 495.4029 - val_loss: 569.2227 - val_mae: 18.0085 - val_mse: 569.2227\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 488.9934 - mae: 16.9231 - mse: 488.9934 - val_loss: 572.2444 - val_mae: 18.3896 - val_mse: 572.2444\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 487.8731 - mae: 16.9195 - mse: 487.8731 - val_loss: 567.8831 - val_mae: 18.1520 - val_mse: 567.8831\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 481.0981 - mae: 16.7755 - mse: 481.0981 - val_loss: 572.3922 - val_mae: 17.9450 - val_mse: 572.3922\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 486.7523 - mae: 16.8291 - mse: 486.7523 - val_loss: 575.6348 - val_mae: 18.1438 - val_mse: 575.6348\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 477.9380 - mae: 16.7452 - mse: 477.9380 - val_loss: 580.7258 - val_mae: 18.1375 - val_mse: 580.7258\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 473.5262 - mae: 16.6455 - mse: 473.5262 - val_loss: 586.8523 - val_mae: 18.4025 - val_mse: 586.8523\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 471.8839 - mae: 16.5417 - mse: 471.8839 - val_loss: 589.0156 - val_mae: 18.9032 - val_mse: 589.0156\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 474.0023 - mae: 16.6851 - mse: 474.0023 - val_loss: 590.7778 - val_mae: 18.1328 - val_mse: 590.7778\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 467.8321 - mae: 16.4765 - mse: 467.8321 - val_loss: 588.0555 - val_mae: 18.4936 - val_mse: 588.0555\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 470.8451 - mae: 16.6381 - mse: 470.8451 - val_loss: 590.3759 - val_mae: 18.1763 - val_mse: 590.3759\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 460.0473 - mae: 16.3831 - mse: 460.0473 - val_loss: 599.6561 - val_mae: 18.3768 - val_mse: 599.6561\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 463.3984 - mae: 16.3709 - mse: 463.3984 - val_loss: 593.5034 - val_mae: 18.2149 - val_mse: 593.5034\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 465.3891 - mae: 16.5127 - mse: 465.3891 - val_loss: 595.4298 - val_mae: 18.3298 - val_mse: 595.4298\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 456.2391 - mae: 16.2405 - mse: 456.2391 - val_loss: 591.9114 - val_mae: 18.6711 - val_mse: 591.9114\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 450.2305 - mae: 16.2069 - mse: 450.2305 - val_loss: 589.9571 - val_mae: 18.3545 - val_mse: 589.9571\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 449.3304 - mae: 16.1135 - mse: 449.3304 - val_loss: 606.2886 - val_mae: 18.3263 - val_mse: 606.2886\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 455.1899 - mae: 16.3001 - mse: 455.1899 - val_loss: 613.5870 - val_mae: 18.5651 - val_mse: 613.5870\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 449.5728 - mae: 16.1360 - mse: 449.5728 - val_loss: 609.5199 - val_mae: 18.5564 - val_mse: 609.5199\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 447.2625 - mae: 16.1025 - mse: 447.2625 - val_loss: 612.5334 - val_mae: 18.4479 - val_mse: 612.5334\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 450.0401 - mae: 16.1661 - mse: 450.0401 - val_loss: 608.3971 - val_mae: 18.7690 - val_mse: 608.3971\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 445.9954 - mae: 16.0798 - mse: 445.9954 - val_loss: 609.1295 - val_mae: 18.5751 - val_mse: 609.1295\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 441.6493 - mae: 15.8903 - mse: 441.6493 - val_loss: 608.9907 - val_mae: 18.5750 - val_mse: 608.9907\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 439.7421 - mae: 15.9773 - mse: 439.7421 - val_loss: 615.0403 - val_mae: 18.5960 - val_mse: 615.0403\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 431.1949 - mae: 15.7435 - mse: 431.1949 - val_loss: 625.1329 - val_mae: 18.9644 - val_mse: 625.1329\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 434.4572 - mae: 15.8557 - mse: 434.4572 - val_loss: 614.4839 - val_mae: 18.5613 - val_mse: 614.4839\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 429.5322 - mae: 15.6972 - mse: 429.5322 - val_loss: 620.4547 - val_mae: 18.5986 - val_mse: 620.4547\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 430.2894 - mae: 15.7675 - mse: 430.2894 - val_loss: 621.1924 - val_mae: 18.5791 - val_mse: 621.1924\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 436.4014 - mae: 15.9240 - mse: 436.4014 - val_loss: 626.7543 - val_mae: 18.7531 - val_mse: 626.7543\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 433.1811 - mae: 15.7934 - mse: 433.1811 - val_loss: 624.9362 - val_mae: 18.7644 - val_mse: 624.9362\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 423.9046 - mae: 15.6691 - mse: 423.9046 - val_loss: 630.7518 - val_mae: 18.6303 - val_mse: 630.7518\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 422.2438 - mae: 15.6261 - mse: 422.2438 - val_loss: 638.2579 - val_mae: 19.1362 - val_mse: 638.2579\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 425.3517 - mae: 15.6120 - mse: 425.3517 - val_loss: 629.0947 - val_mae: 18.9677 - val_mse: 629.0947\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 422.1991 - mae: 15.6606 - mse: 422.1991 - val_loss: 636.7446 - val_mae: 18.5867 - val_mse: 636.7446\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 416.5389 - mae: 15.3741 - mse: 416.5389 - val_loss: 634.5366 - val_mae: 19.1576 - val_mse: 634.5366\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 418.5050 - mae: 15.5127 - mse: 418.5050 - val_loss: 636.7823 - val_mae: 19.1109 - val_mse: 636.7823\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 422.5831 - mae: 15.6290 - mse: 422.5831 - val_loss: 636.1489 - val_mae: 19.2077 - val_mse: 636.1489\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 416.6834 - mae: 15.4523 - mse: 416.6834 - val_loss: 641.7596 - val_mae: 18.9911 - val_mse: 641.7596\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 416.1373 - mae: 15.4444 - mse: 416.1373 - val_loss: 649.4799 - val_mae: 19.3209 - val_mse: 649.4799\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 416.2859 - mae: 15.4865 - mse: 416.2859 - val_loss: 635.4937 - val_mae: 19.1194 - val_mse: 635.4937\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 408.0337 - mae: 15.3403 - mse: 408.0337 - val_loss: 635.1645 - val_mae: 18.8837 - val_mse: 635.1645\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 421.7256 - mae: 15.5101 - mse: 421.7256 - val_loss: 639.5693 - val_mae: 18.9039 - val_mse: 639.5693\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 408.5255 - mae: 15.3681 - mse: 408.5255 - val_loss: 646.6036 - val_mae: 19.1883 - val_mse: 646.6036\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 415.2614 - mae: 15.4519 - mse: 415.2614 - val_loss: 653.5292 - val_mae: 19.3626 - val_mse: 653.5292\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 408.5764 - mae: 15.3752 - mse: 408.5764 - val_loss: 646.0066 - val_mae: 19.0563 - val_mse: 646.0066\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 403.7662 - mae: 15.2234 - mse: 403.7662 - val_loss: 645.2939 - val_mae: 19.1903 - val_mse: 645.2939\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 401.6416 - mae: 15.1465 - mse: 401.6416 - val_loss: 654.1207 - val_mae: 19.1968 - val_mse: 654.1207\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 409.0722 - mae: 15.3509 - mse: 409.0722 - val_loss: 647.0562 - val_mae: 19.1035 - val_mse: 647.0562\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 405.9894 - mae: 15.2268 - mse: 405.9894 - val_loss: 643.6894 - val_mae: 19.0165 - val_mse: 643.6894\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 407.4735 - mae: 15.3891 - mse: 407.4735 - val_loss: 647.7355 - val_mae: 19.2670 - val_mse: 647.7355\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 400.9138 - mae: 15.1825 - mse: 400.9138 - val_loss: 644.4033 - val_mae: 18.7902 - val_mse: 644.4033\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 407.0788 - mae: 15.3355 - mse: 407.0788 - val_loss: 656.7529 - val_mae: 18.9069 - val_mse: 656.7529\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 410.1648 - mae: 15.3392 - mse: 410.1648 - val_loss: 655.1675 - val_mae: 19.0886 - val_mse: 655.1675\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 400.0644 - mae: 15.2039 - mse: 400.0644 - val_loss: 662.4070 - val_mae: 19.4826 - val_mse: 662.4070\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 400.7485 - mae: 15.1074 - mse: 400.7485 - val_loss: 646.7939 - val_mae: 19.0938 - val_mse: 646.7939\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 392.8666 - mae: 15.0599 - mse: 392.8666 - val_loss: 655.1916 - val_mae: 19.0393 - val_mse: 655.1916\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 390.4651 - mae: 14.9707 - mse: 390.4651 - val_loss: 668.3179 - val_mae: 19.6339 - val_mse: 668.3179\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 398.7171 - mae: 15.2121 - mse: 398.7171 - val_loss: 673.6194 - val_mae: 19.2155 - val_mse: 673.6194\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 391.2224 - mae: 14.9230 - mse: 391.2224 - val_loss: 678.8500 - val_mae: 19.6752 - val_mse: 678.8500\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 391.3029 - mae: 15.0513 - mse: 391.3029 - val_loss: 662.6423 - val_mae: 19.4262 - val_mse: 662.6423\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 387.0668 - mae: 14.8734 - mse: 387.0668 - val_loss: 681.3126 - val_mae: 19.4668 - val_mse: 681.3126\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 394.5926 - mae: 15.0078 - mse: 394.5926 - val_loss: 657.4155 - val_mae: 19.4167 - val_mse: 657.4155\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 392.6453 - mae: 14.9875 - mse: 392.6453 - val_loss: 664.3505 - val_mae: 19.5345 - val_mse: 664.3505\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 384.8854 - mae: 14.8348 - mse: 384.8854 - val_loss: 669.0848 - val_mae: 19.8161 - val_mse: 669.0848\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 383.2418 - mae: 14.8988 - mse: 383.2418 - val_loss: 682.6046 - val_mae: 19.6025 - val_mse: 682.6046\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 388.3006 - mae: 14.9122 - mse: 388.3006 - val_loss: 674.7035 - val_mae: 19.7439 - val_mse: 674.7035\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 388.3610 - mae: 14.9567 - mse: 388.3610 - val_loss: 693.4173 - val_mae: 19.5487 - val_mse: 693.4173\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 390.8118 - mae: 15.0511 - mse: 390.8118 - val_loss: 674.8178 - val_mae: 19.8042 - val_mse: 674.8178\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 382.7668 - mae: 14.8145 - mse: 382.7668 - val_loss: 670.2932 - val_mae: 19.6115 - val_mse: 670.2932\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 383.7675 - mae: 14.9156 - mse: 383.7675 - val_loss: 671.3888 - val_mae: 19.3791 - val_mse: 671.3888\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 384.4990 - mae: 14.8912 - mse: 384.4990 - val_loss: 677.1148 - val_mae: 19.7346 - val_mse: 677.1148\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 380.0943 - mae: 14.8235 - mse: 380.0943 - val_loss: 683.3852 - val_mae: 19.6627 - val_mse: 683.3852\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 371.7566 - mae: 14.6289 - mse: 371.7566 - val_loss: 670.9137 - val_mae: 19.7083 - val_mse: 670.9137\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 378.7343 - mae: 14.7167 - mse: 378.7343 - val_loss: 674.4329 - val_mae: 19.5080 - val_mse: 674.4329\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 374.6105 - mae: 14.6242 - mse: 374.6105 - val_loss: 676.4079 - val_mae: 19.7149 - val_mse: 676.4079\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 370.8990 - mae: 14.6204 - mse: 370.8990 - val_loss: 676.7133 - val_mae: 19.6899 - val_mse: 676.7133\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 375.8349 - mae: 14.7616 - mse: 375.8349 - val_loss: 687.7112 - val_mae: 19.8670 - val_mse: 687.7112\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 380.7338 - mae: 14.7285 - mse: 380.7338 - val_loss: 674.9547 - val_mae: 19.5657 - val_mse: 674.9547\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 375.5447 - mae: 14.7236 - mse: 375.5447 - val_loss: 690.0629 - val_mae: 19.6189 - val_mse: 690.0629\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 370.4913 - mae: 14.5928 - mse: 370.4913 - val_loss: 685.1900 - val_mae: 19.9010 - val_mse: 685.1900\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 376.4669 - mae: 14.7392 - mse: 376.4669 - val_loss: 674.5004 - val_mae: 19.5160 - val_mse: 674.5004\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 360.9502 - mae: 14.3910 - mse: 360.9502 - val_loss: 674.7232 - val_mae: 19.8008 - val_mse: 674.7232\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 367.6606 - mae: 14.4496 - mse: 367.6606 - val_loss: 681.4532 - val_mae: 19.4749 - val_mse: 681.4532\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 379.5930 - mae: 14.7574 - mse: 379.5930 - val_loss: 682.0385 - val_mae: 19.6412 - val_mse: 682.0385\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 373.7168 - mae: 14.6552 - mse: 373.7168 - val_loss: 680.1695 - val_mae: 19.4733 - val_mse: 680.1695\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 369.9330 - mae: 14.4558 - mse: 369.9330 - val_loss: 683.2800 - val_mae: 19.8354 - val_mse: 683.2800\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 370.0988 - mae: 14.6297 - mse: 370.0988 - val_loss: 669.4846 - val_mae: 19.2213 - val_mse: 669.4846\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 370.4027 - mae: 14.5997 - mse: 370.4027 - val_loss: 684.0035 - val_mae: 19.8686 - val_mse: 684.0035\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 368.2084 - mae: 14.4809 - mse: 368.2084 - val_loss: 686.7247 - val_mae: 19.5920 - val_mse: 686.7247\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 364.4186 - mae: 14.4208 - mse: 364.4186 - val_loss: 685.3002 - val_mae: 19.8839 - val_mse: 685.3002\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 360.1908 - mae: 14.3794 - mse: 360.1908 - val_loss: 688.5981 - val_mae: 19.5888 - val_mse: 688.5981\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2676.8977 - mae: 43.0481 - mse: 2676.8809 - val_loss: 809.7593 - val_mae: 23.3009 - val_mse: 809.7402\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1037.4396 - mae: 26.4039 - mse: 1037.4200 - val_loss: 717.9097 - val_mae: 21.7335 - val_mse: 717.8907\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 955.9871 - mae: 25.1754 - mse: 955.9683 - val_loss: 689.4614 - val_mae: 21.4177 - val_mse: 689.4425\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 892.9511 - mae: 24.4760 - mse: 892.9323 - val_loss: 647.2901 - val_mae: 20.2471 - val_mse: 647.2713\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 850.5328 - mae: 23.6285 - mse: 850.5142 - val_loss: 626.5714 - val_mae: 19.6521 - val_mse: 626.5527\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 823.8949 - mae: 23.1279 - mse: 823.8767 - val_loss: 615.0259 - val_mae: 19.6655 - val_mse: 615.0073\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 807.5759 - mae: 22.9831 - mse: 807.5577 - val_loss: 603.2205 - val_mae: 19.5155 - val_mse: 603.2021\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 798.2797 - mae: 22.7742 - mse: 798.2614 - val_loss: 604.1860 - val_mae: 19.7213 - val_mse: 604.1678\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 791.0900 - mae: 22.7874 - mse: 791.0715 - val_loss: 588.3911 - val_mae: 18.9052 - val_mse: 588.3729\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 769.0256 - mae: 22.4233 - mse: 769.0081 - val_loss: 585.9377 - val_mae: 19.0856 - val_mse: 585.9199\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 763.8194 - mae: 22.3661 - mse: 763.8014 - val_loss: 586.0493 - val_mae: 19.3354 - val_mse: 586.0314\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 740.7639 - mae: 21.9366 - mse: 740.7462 - val_loss: 573.9280 - val_mae: 18.5295 - val_mse: 573.9103\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 733.9842 - mae: 21.7775 - mse: 733.9666 - val_loss: 573.5745 - val_mae: 18.9596 - val_mse: 573.5569\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 725.0007 - mae: 21.5707 - mse: 724.9828 - val_loss: 593.2007 - val_mae: 19.7038 - val_mse: 593.1832\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 720.1530 - mae: 21.5913 - mse: 720.1353 - val_loss: 566.0383 - val_mae: 18.6068 - val_mse: 566.0210\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 737.7308 - mae: 21.8331 - mse: 737.7137 - val_loss: 568.1804 - val_mae: 18.8773 - val_mse: 568.1631\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 711.4163 - mae: 21.2090 - mse: 711.3990 - val_loss: 598.3353 - val_mae: 20.0011 - val_mse: 598.3182\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 694.6279 - mae: 21.2328 - mse: 694.6106 - val_loss: 571.9534 - val_mae: 19.0970 - val_mse: 571.9363\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 702.0309 - mae: 21.3141 - mse: 702.0138 - val_loss: 572.2300 - val_mae: 19.1713 - val_mse: 572.2128\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 699.5621 - mae: 21.1323 - mse: 699.5451 - val_loss: 554.8082 - val_mae: 18.0441 - val_mse: 554.7912\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 698.9686 - mae: 21.1288 - mse: 698.9515 - val_loss: 557.6402 - val_mae: 18.5328 - val_mse: 557.6232\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 685.7426 - mae: 20.8802 - mse: 685.7258 - val_loss: 550.5804 - val_mae: 18.4083 - val_mse: 550.5637\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 690.0720 - mae: 21.0051 - mse: 690.0549 - val_loss: 546.7871 - val_mae: 17.9587 - val_mse: 546.7703\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 687.3180 - mae: 20.8390 - mse: 687.3015 - val_loss: 561.1879 - val_mae: 18.8677 - val_mse: 561.1714\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 679.9622 - mae: 20.6458 - mse: 679.9458 - val_loss: 545.9202 - val_mae: 17.9888 - val_mse: 545.9037\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 677.7957 - mae: 20.6164 - mse: 677.7793 - val_loss: 562.5504 - val_mae: 18.8872 - val_mse: 562.5339\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 663.3634 - mae: 20.4398 - mse: 663.3468 - val_loss: 549.6991 - val_mae: 18.2852 - val_mse: 549.6827\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 672.0226 - mae: 20.5935 - mse: 672.0062 - val_loss: 541.9049 - val_mae: 17.8461 - val_mse: 541.8884\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 672.7706 - mae: 20.4435 - mse: 672.7543 - val_loss: 560.9559 - val_mae: 18.8290 - val_mse: 560.9399\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 673.3059 - mae: 20.5055 - mse: 673.2898 - val_loss: 548.4586 - val_mae: 18.2120 - val_mse: 548.4426\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 655.3162 - mae: 20.3852 - mse: 655.3002 - val_loss: 548.2217 - val_mae: 18.1603 - val_mse: 548.2054\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 659.2814 - mae: 20.3418 - mse: 659.2656 - val_loss: 545.2599 - val_mae: 18.0826 - val_mse: 545.2441\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 664.2379 - mae: 20.4372 - mse: 664.2221 - val_loss: 558.0225 - val_mae: 18.6851 - val_mse: 558.0069\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 664.0422 - mae: 20.3824 - mse: 664.0266 - val_loss: 561.1258 - val_mae: 18.8389 - val_mse: 561.1105\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 662.1210 - mae: 20.3815 - mse: 662.1054 - val_loss: 545.9568 - val_mae: 18.1233 - val_mse: 545.9415\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 663.3076 - mae: 20.3048 - mse: 663.2922 - val_loss: 540.3801 - val_mae: 17.6147 - val_mse: 540.3648\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 643.5695 - mae: 20.0383 - mse: 643.5544 - val_loss: 554.0220 - val_mae: 18.6340 - val_mse: 554.0070\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 637.7711 - mae: 19.8687 - mse: 637.7560 - val_loss: 547.7317 - val_mae: 18.2606 - val_mse: 547.7166\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 651.5720 - mae: 20.2105 - mse: 651.5570 - val_loss: 539.4531 - val_mae: 17.6935 - val_mse: 539.4381\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 644.4806 - mae: 19.9102 - mse: 644.4659 - val_loss: 543.0081 - val_mae: 17.9399 - val_mse: 542.9933\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 646.4172 - mae: 20.1003 - mse: 646.4032 - val_loss: 543.0501 - val_mae: 17.8761 - val_mse: 543.0353\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 667.9028 - mae: 20.2851 - mse: 667.8882 - val_loss: 552.5295 - val_mae: 18.4904 - val_mse: 552.5150\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 641.1360 - mae: 19.7979 - mse: 641.1213 - val_loss: 547.4653 - val_mae: 18.1974 - val_mse: 547.4509\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.9327 - mae: 20.0491 - mse: 645.9186 - val_loss: 544.9496 - val_mae: 18.0291 - val_mse: 544.9354\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 640.0778 - mae: 19.8420 - mse: 640.0637 - val_loss: 544.2986 - val_mae: 17.9561 - val_mse: 544.2842\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 642.7385 - mae: 19.9921 - mse: 642.7244 - val_loss: 543.2977 - val_mae: 17.8113 - val_mse: 543.2837\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 648.5511 - mae: 20.0897 - mse: 648.5370 - val_loss: 539.7828 - val_mae: 17.6665 - val_mse: 539.7687\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 640.7572 - mae: 19.8732 - mse: 640.7432 - val_loss: 543.2541 - val_mae: 17.9131 - val_mse: 543.2401\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 636.3786 - mae: 19.8787 - mse: 636.3644 - val_loss: 545.9396 - val_mae: 18.0856 - val_mse: 545.9258\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 636.4786 - mae: 19.8336 - mse: 636.4648 - val_loss: 543.8665 - val_mae: 17.8602 - val_mse: 543.8528\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 641.9749 - mae: 20.0089 - mse: 641.9609 - val_loss: 542.1461 - val_mae: 17.6309 - val_mse: 542.1324\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.6534 - mae: 19.9416 - mse: 645.6397 - val_loss: 542.8644 - val_mae: 17.8597 - val_mse: 542.8509\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 618.3207 - mae: 19.4810 - mse: 618.3071 - val_loss: 540.8827 - val_mae: 17.7223 - val_mse: 540.8693\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.9709 - mae: 19.8173 - mse: 634.9575 - val_loss: 547.5934 - val_mae: 18.1811 - val_mse: 547.5800\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 635.9490 - mae: 19.7281 - mse: 635.9354 - val_loss: 542.0623 - val_mae: 17.8777 - val_mse: 542.0489\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 629.3209 - mae: 19.6935 - mse: 629.3076 - val_loss: 550.3361 - val_mae: 18.3139 - val_mse: 550.3229\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 643.3924 - mae: 19.9919 - mse: 643.3788 - val_loss: 540.9615 - val_mae: 17.7375 - val_mse: 540.9482\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 651.3154 - mae: 20.1328 - mse: 651.3022 - val_loss: 546.4647 - val_mae: 18.1758 - val_mse: 546.4517\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 629.5250 - mae: 19.7921 - mse: 629.5118 - val_loss: 541.1937 - val_mae: 17.6464 - val_mse: 541.1805\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 635.3524 - mae: 19.7901 - mse: 635.3393 - val_loss: 547.5507 - val_mae: 18.2373 - val_mse: 547.5375\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.1883 - mae: 19.7740 - mse: 633.1750 - val_loss: 548.6321 - val_mae: 18.3583 - val_mse: 548.6190\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 620.7321 - mae: 19.6761 - mse: 620.7189 - val_loss: 547.4225 - val_mae: 18.2322 - val_mse: 547.4096\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 629.2374 - mae: 19.6557 - mse: 629.2242 - val_loss: 546.9761 - val_mae: 18.1702 - val_mse: 546.9632\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 618.2511 - mae: 19.4534 - mse: 618.2379 - val_loss: 556.1063 - val_mae: 18.6299 - val_mse: 556.0933\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 628.7936 - mae: 19.5909 - mse: 628.7806 - val_loss: 542.4283 - val_mae: 17.7934 - val_mse: 542.4153\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 609.1139 - mae: 19.3053 - mse: 609.1010 - val_loss: 549.7124 - val_mae: 18.3385 - val_mse: 549.6995\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 606.8297 - mae: 19.2880 - mse: 606.8170 - val_loss: 541.5662 - val_mae: 17.9029 - val_mse: 541.5534\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 608.6187 - mae: 19.4208 - mse: 608.6059 - val_loss: 539.8948 - val_mae: 17.6112 - val_mse: 539.8820\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 613.8563 - mae: 19.4740 - mse: 613.8434 - val_loss: 540.8568 - val_mae: 17.6647 - val_mse: 540.8438\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 625.1084 - mae: 19.6965 - mse: 625.0954 - val_loss: 541.0138 - val_mae: 17.7073 - val_mse: 541.0007\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 605.0461 - mae: 19.2987 - mse: 605.0331 - val_loss: 542.4220 - val_mae: 17.9954 - val_mse: 542.4092\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 603.9011 - mae: 19.3906 - mse: 603.8883 - val_loss: 543.8715 - val_mae: 17.2741 - val_mse: 543.8586\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 603.7092 - mae: 19.2679 - mse: 603.6966 - val_loss: 540.5472 - val_mae: 17.5924 - val_mse: 540.5345\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 601.8149 - mae: 19.2986 - mse: 601.8019 - val_loss: 543.2720 - val_mae: 17.9220 - val_mse: 543.2592\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 620.5963 - mae: 19.6342 - mse: 620.5837 - val_loss: 542.0184 - val_mae: 17.5573 - val_mse: 542.0059\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 597.9293 - mae: 19.0391 - mse: 597.9163 - val_loss: 541.8279 - val_mae: 17.9579 - val_mse: 541.8153\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.6896 - mae: 19.2838 - mse: 608.6772 - val_loss: 543.1237 - val_mae: 17.8854 - val_mse: 543.1109\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 601.2306 - mae: 19.1756 - mse: 601.2182 - val_loss: 551.6455 - val_mae: 18.3849 - val_mse: 551.6330\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 612.7356 - mae: 19.4531 - mse: 612.7227 - val_loss: 544.1330 - val_mae: 17.9764 - val_mse: 544.1205\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 595.0076 - mae: 19.1744 - mse: 594.9952 - val_loss: 544.2991 - val_mae: 17.9201 - val_mse: 544.2866\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 599.1092 - mae: 19.1206 - mse: 599.0963 - val_loss: 554.6691 - val_mae: 18.5473 - val_mse: 554.6567\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 605.7370 - mae: 19.3132 - mse: 605.7247 - val_loss: 554.0082 - val_mae: 18.4890 - val_mse: 553.9957\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 611.3726 - mae: 19.3399 - mse: 611.3600 - val_loss: 545.7281 - val_mae: 18.1249 - val_mse: 545.7155\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 597.3903 - mae: 19.1219 - mse: 597.3782 - val_loss: 543.5498 - val_mae: 18.0247 - val_mse: 543.5374\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 597.5805 - mae: 19.0394 - mse: 597.5681 - val_loss: 541.5761 - val_mae: 17.6734 - val_mse: 541.5637\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.7935 - mae: 19.0336 - mse: 594.7811 - val_loss: 542.6520 - val_mae: 17.8116 - val_mse: 542.6396\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.4808 - mae: 19.0985 - mse: 594.4680 - val_loss: 539.2023 - val_mae: 17.5956 - val_mse: 539.1900\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 587.4879 - mae: 18.9584 - mse: 587.4754 - val_loss: 540.7288 - val_mae: 17.6841 - val_mse: 540.7161\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 601.6844 - mae: 19.1041 - mse: 601.6718 - val_loss: 549.6177 - val_mae: 18.3473 - val_mse: 549.6050\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.9870 - mae: 19.1388 - mse: 594.9743 - val_loss: 546.7312 - val_mae: 18.1520 - val_mse: 546.7187\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 580.8340 - mae: 18.8550 - mse: 580.8211 - val_loss: 541.1433 - val_mae: 17.8493 - val_mse: 541.1309\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 593.5645 - mae: 19.1467 - mse: 593.5521 - val_loss: 541.1735 - val_mae: 17.8656 - val_mse: 541.1609\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 583.6542 - mae: 18.8011 - mse: 583.6416 - val_loss: 545.2295 - val_mae: 18.0425 - val_mse: 545.2167\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 603.3374 - mae: 19.1864 - mse: 603.3245 - val_loss: 546.0450 - val_mae: 17.9986 - val_mse: 546.0323\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.4222 - mae: 18.8697 - mse: 586.4091 - val_loss: 542.7144 - val_mae: 17.5800 - val_mse: 542.7014\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 595.3027 - mae: 19.1913 - mse: 595.2902 - val_loss: 543.5861 - val_mae: 17.8612 - val_mse: 543.5732\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.8926 - mae: 18.9758 - mse: 586.8799 - val_loss: 541.8395 - val_mae: 17.7297 - val_mse: 541.8266\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 600.8801 - mae: 19.1000 - mse: 600.8674 - val_loss: 544.6008 - val_mae: 18.0650 - val_mse: 544.5880\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 593.2249 - mae: 19.0976 - mse: 593.2124 - val_loss: 543.5766 - val_mae: 17.9196 - val_mse: 543.5638\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 590.3029 - mae: 19.0573 - mse: 590.2899 - val_loss: 542.1009 - val_mae: 17.6678 - val_mse: 542.0878\n"
     ]
    }
   ],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 14.9905, Train MSE: 386.6925\n",
      "Val   MAE: 18.9784, Val   MSE: 620.7819\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 12.5579, Train MSE: 289.7504\n",
      "Val   MAE: 19.5888, Val   MSE: 688.5981\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 16.9910, Train MSE: 482.8449\n",
      "Val   MAE: 17.6678, Val   MSE: 542.0878\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 2458.5361 - mae: 39.6388 - mse: 2458.5361 - val_loss: 635.7730 - val_mae: 19.9263 - val_mse: 635.7730\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 592.0991 - mae: 19.0866 - mse: 592.0991 - val_loss: 593.1281 - val_mae: 18.9652 - val_mse: 593.1281\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 565.4344 - mae: 18.6234 - mse: 565.4344 - val_loss: 586.8325 - val_mae: 18.6837 - val_mse: 586.8325\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 550.4894 - mae: 18.3252 - mse: 550.4894 - val_loss: 580.1015 - val_mae: 18.6102 - val_mse: 580.1015\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 545.6429 - mae: 18.2447 - mse: 545.6429 - val_loss: 585.0308 - val_mae: 18.8251 - val_mse: 585.0308\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 534.9498 - mae: 18.0316 - mse: 534.9498 - val_loss: 581.0776 - val_mae: 18.9278 - val_mse: 581.0776\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 533.0734 - mae: 17.9901 - mse: 533.0734 - val_loss: 583.7133 - val_mae: 19.0290 - val_mse: 583.7133\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 526.8136 - mae: 17.7948 - mse: 526.8136 - val_loss: 583.2328 - val_mae: 19.0137 - val_mse: 583.2328\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 522.2639 - mae: 17.7346 - mse: 522.2639 - val_loss: 577.1132 - val_mae: 18.9090 - val_mse: 577.1132\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 517.3381 - mae: 17.6840 - mse: 517.3381 - val_loss: 579.1020 - val_mae: 18.9134 - val_mse: 579.1020\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 512.6995 - mae: 17.5475 - mse: 512.6995 - val_loss: 580.6564 - val_mae: 18.3926 - val_mse: 580.6564\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 507.6537 - mae: 17.4460 - mse: 507.6537 - val_loss: 580.1611 - val_mae: 18.1186 - val_mse: 580.1611\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 500.4032 - mae: 17.2922 - mse: 500.4032 - val_loss: 582.0159 - val_mae: 18.6225 - val_mse: 582.0159\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 496.1217 - mae: 17.0959 - mse: 496.1217 - val_loss: 583.5967 - val_mae: 18.7956 - val_mse: 583.5967\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 490.7175 - mae: 17.0686 - mse: 490.7175 - val_loss: 584.9886 - val_mae: 18.7077 - val_mse: 584.9886\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 488.4857 - mae: 16.9612 - mse: 488.4857 - val_loss: 577.7966 - val_mae: 18.1729 - val_mse: 577.7966\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 485.6463 - mae: 16.9686 - mse: 485.6463 - val_loss: 586.1998 - val_mae: 18.7691 - val_mse: 586.1998\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 481.5434 - mae: 16.8523 - mse: 481.5434 - val_loss: 594.0181 - val_mae: 18.2521 - val_mse: 594.0181\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 478.4642 - mae: 16.7503 - mse: 478.4642 - val_loss: 581.0709 - val_mae: 18.3109 - val_mse: 581.0709\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 474.9665 - mae: 16.7136 - mse: 474.9665 - val_loss: 580.4901 - val_mae: 18.3355 - val_mse: 580.4901\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 471.6110 - mae: 16.6531 - mse: 471.6110 - val_loss: 586.5109 - val_mae: 18.0528 - val_mse: 586.5109\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 472.9215 - mae: 16.6582 - mse: 472.9215 - val_loss: 587.3855 - val_mae: 18.5470 - val_mse: 587.3855\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 467.4870 - mae: 16.5100 - mse: 467.4870 - val_loss: 586.8978 - val_mae: 18.2299 - val_mse: 586.8978\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 463.5933 - mae: 16.5124 - mse: 463.5933 - val_loss: 602.5533 - val_mae: 18.2402 - val_mse: 602.5533\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 462.5472 - mae: 16.4264 - mse: 462.5472 - val_loss: 586.8224 - val_mae: 18.5362 - val_mse: 586.8224\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 460.8041 - mae: 16.3959 - mse: 460.8041 - val_loss: 592.4475 - val_mae: 18.7390 - val_mse: 592.4475\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 459.4734 - mae: 16.3810 - mse: 459.4734 - val_loss: 591.1221 - val_mae: 18.6981 - val_mse: 591.1221\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 455.0350 - mae: 16.3290 - mse: 455.0350 - val_loss: 597.2117 - val_mae: 18.2312 - val_mse: 597.2117\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 453.5121 - mae: 16.2422 - mse: 453.5121 - val_loss: 592.6020 - val_mae: 18.0869 - val_mse: 592.6020\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 450.4963 - mae: 16.2284 - mse: 450.4963 - val_loss: 588.1058 - val_mae: 18.2270 - val_mse: 588.1058\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 452.0044 - mae: 16.2131 - mse: 452.0044 - val_loss: 588.2376 - val_mae: 18.2325 - val_mse: 588.2376\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 443.8969 - mae: 16.0820 - mse: 443.8969 - val_loss: 596.3685 - val_mae: 18.1460 - val_mse: 596.3685\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 443.8719 - mae: 16.0585 - mse: 443.8719 - val_loss: 596.7574 - val_mae: 18.2996 - val_mse: 596.7574\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 443.4117 - mae: 16.0009 - mse: 443.4117 - val_loss: 600.7138 - val_mae: 18.4429 - val_mse: 600.7138\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 438.5679 - mae: 15.9623 - mse: 438.5679 - val_loss: 591.9633 - val_mae: 18.4471 - val_mse: 591.9633\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 435.4845 - mae: 15.8742 - mse: 435.4845 - val_loss: 613.2568 - val_mae: 18.2166 - val_mse: 613.2568\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 439.3389 - mae: 15.9479 - mse: 439.3389 - val_loss: 604.6010 - val_mae: 18.4227 - val_mse: 604.6010\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 433.4923 - mae: 15.8350 - mse: 433.4923 - val_loss: 614.4742 - val_mae: 18.2219 - val_mse: 614.4742\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 431.2766 - mae: 15.8060 - mse: 431.2766 - val_loss: 610.8809 - val_mae: 18.6826 - val_mse: 610.8809\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 429.4709 - mae: 15.7852 - mse: 429.4709 - val_loss: 614.7798 - val_mae: 18.2652 - val_mse: 614.7798\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 427.7517 - mae: 15.7232 - mse: 427.7517 - val_loss: 608.4463 - val_mae: 18.2770 - val_mse: 608.4463\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 423.5946 - mae: 15.5928 - mse: 423.5946 - val_loss: 609.0212 - val_mae: 18.8004 - val_mse: 609.0212\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 422.5923 - mae: 15.6370 - mse: 422.5923 - val_loss: 616.7444 - val_mae: 18.6161 - val_mse: 616.7444\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 421.1428 - mae: 15.6158 - mse: 421.1428 - val_loss: 620.6327 - val_mae: 18.3958 - val_mse: 620.6327\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 418.4596 - mae: 15.4759 - mse: 418.4596 - val_loss: 615.4996 - val_mae: 18.5535 - val_mse: 615.4996\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 418.7693 - mae: 15.5035 - mse: 418.7693 - val_loss: 616.6686 - val_mae: 18.6447 - val_mse: 616.6686\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 412.5388 - mae: 15.3314 - mse: 412.5388 - val_loss: 624.6095 - val_mae: 19.0387 - val_mse: 624.6095\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 417.3247 - mae: 15.4867 - mse: 417.3247 - val_loss: 628.4005 - val_mae: 18.5219 - val_mse: 628.4005\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 410.4067 - mae: 15.3241 - mse: 410.4067 - val_loss: 624.3425 - val_mae: 18.8259 - val_mse: 624.3425\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 410.1833 - mae: 15.3219 - mse: 410.1833 - val_loss: 619.0526 - val_mae: 18.9189 - val_mse: 619.0526\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 408.1032 - mae: 15.2604 - mse: 408.1032 - val_loss: 630.8411 - val_mae: 18.5221 - val_mse: 630.8411\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 404.2273 - mae: 15.1997 - mse: 404.2273 - val_loss: 622.5911 - val_mae: 18.8668 - val_mse: 622.5911\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 401.4041 - mae: 15.0940 - mse: 401.4041 - val_loss: 622.4760 - val_mae: 18.7216 - val_mse: 622.4760\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 401.0381 - mae: 15.0883 - mse: 401.0381 - val_loss: 626.6517 - val_mae: 19.0732 - val_mse: 626.6517\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 401.5802 - mae: 15.0811 - mse: 401.5802 - val_loss: 625.0194 - val_mae: 18.5374 - val_mse: 625.0194\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 398.4048 - mae: 15.0403 - mse: 398.4048 - val_loss: 630.8238 - val_mae: 19.2665 - val_mse: 630.8238\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 398.8117 - mae: 15.0146 - mse: 398.8117 - val_loss: 636.8971 - val_mae: 18.7775 - val_mse: 636.8971\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 392.5822 - mae: 14.8842 - mse: 392.5822 - val_loss: 628.0814 - val_mae: 18.9146 - val_mse: 628.0814\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 392.1755 - mae: 14.8530 - mse: 392.1755 - val_loss: 639.8966 - val_mae: 19.2943 - val_mse: 639.8966\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 392.7980 - mae: 14.8985 - mse: 392.7980 - val_loss: 643.4178 - val_mae: 19.2852 - val_mse: 643.4178\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 389.2272 - mae: 14.8572 - mse: 389.2272 - val_loss: 652.4472 - val_mae: 18.8911 - val_mse: 652.4472\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 388.5624 - mae: 14.7885 - mse: 388.5624 - val_loss: 624.7892 - val_mae: 18.6965 - val_mse: 624.7892\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 386.9550 - mae: 14.7417 - mse: 386.9550 - val_loss: 639.9275 - val_mae: 19.1204 - val_mse: 639.9275\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 383.8503 - mae: 14.6908 - mse: 383.8503 - val_loss: 644.1257 - val_mae: 18.8276 - val_mse: 644.1257\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 384.5464 - mae: 14.6684 - mse: 384.5464 - val_loss: 640.7191 - val_mae: 18.9761 - val_mse: 640.7191\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 384.1512 - mae: 14.7042 - mse: 384.1512 - val_loss: 648.3246 - val_mae: 19.1362 - val_mse: 648.3246\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 378.5367 - mae: 14.5934 - mse: 378.5367 - val_loss: 647.6848 - val_mae: 18.8246 - val_mse: 647.6848\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 379.3177 - mae: 14.5583 - mse: 379.3177 - val_loss: 628.9677 - val_mae: 19.0280 - val_mse: 628.9677\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 375.1926 - mae: 14.4385 - mse: 375.1926 - val_loss: 643.8671 - val_mae: 19.1254 - val_mse: 643.8671\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 372.6542 - mae: 14.4440 - mse: 372.6542 - val_loss: 630.5907 - val_mae: 19.1356 - val_mse: 630.5907\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 377.3541 - mae: 14.5057 - mse: 377.3541 - val_loss: 658.1465 - val_mae: 19.3056 - val_mse: 658.1465\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 370.6107 - mae: 14.3288 - mse: 370.6107 - val_loss: 643.3191 - val_mae: 19.4603 - val_mse: 643.3191\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 370.5186 - mae: 14.3385 - mse: 370.5186 - val_loss: 636.6006 - val_mae: 19.3562 - val_mse: 636.6006\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 369.7437 - mae: 14.3646 - mse: 369.7437 - val_loss: 651.9379 - val_mae: 19.0101 - val_mse: 651.9379\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 370.7081 - mae: 14.3170 - mse: 370.7081 - val_loss: 650.5788 - val_mae: 19.2348 - val_mse: 650.5788\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 365.0837 - mae: 14.2548 - mse: 365.0837 - val_loss: 641.0682 - val_mae: 19.2387 - val_mse: 641.0682\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 364.9244 - mae: 14.2021 - mse: 364.9244 - val_loss: 644.8311 - val_mae: 19.2296 - val_mse: 644.8311\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 359.4732 - mae: 14.0846 - mse: 359.4732 - val_loss: 662.7535 - val_mae: 19.1325 - val_mse: 662.7535\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 362.9542 - mae: 14.1737 - mse: 362.9542 - val_loss: 656.6185 - val_mae: 19.8179 - val_mse: 656.6185\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 361.7248 - mae: 14.1302 - mse: 361.7248 - val_loss: 657.3473 - val_mae: 19.7444 - val_mse: 657.3473\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 360.6564 - mae: 14.1032 - mse: 360.6564 - val_loss: 644.5602 - val_mae: 19.3802 - val_mse: 644.5602\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 357.8780 - mae: 14.0630 - mse: 357.8780 - val_loss: 650.2802 - val_mae: 19.3917 - val_mse: 650.2802\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 358.5233 - mae: 13.9867 - mse: 358.5233 - val_loss: 665.4820 - val_mae: 19.7442 - val_mse: 665.4820\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 357.1659 - mae: 14.0412 - mse: 357.1659 - val_loss: 662.6143 - val_mae: 19.7266 - val_mse: 662.6143\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 351.4620 - mae: 13.8509 - mse: 351.4620 - val_loss: 662.6477 - val_mae: 19.4177 - val_mse: 662.6477\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 350.8144 - mae: 13.8373 - mse: 350.8144 - val_loss: 675.6611 - val_mae: 19.4127 - val_mse: 675.6611\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 349.5067 - mae: 13.7661 - mse: 349.5067 - val_loss: 675.0624 - val_mae: 19.6578 - val_mse: 675.0624\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 346.2614 - mae: 13.7545 - mse: 346.2614 - val_loss: 661.7955 - val_mae: 19.5085 - val_mse: 661.7955\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 351.5706 - mae: 13.8968 - mse: 351.5706 - val_loss: 691.0737 - val_mae: 19.5115 - val_mse: 691.0737\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 350.0004 - mae: 13.8300 - mse: 350.0004 - val_loss: 672.0854 - val_mae: 20.0968 - val_mse: 672.0854\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 348.2680 - mae: 13.7986 - mse: 348.2680 - val_loss: 675.5613 - val_mae: 19.7636 - val_mse: 675.5613\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 344.3310 - mae: 13.7398 - mse: 344.3310 - val_loss: 681.9594 - val_mae: 19.3468 - val_mse: 681.9594\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 344.4957 - mae: 13.7497 - mse: 344.4957 - val_loss: 676.8378 - val_mae: 19.7202 - val_mse: 676.8378\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 339.1604 - mae: 13.5472 - mse: 339.1604 - val_loss: 667.7018 - val_mae: 20.1740 - val_mse: 667.7018\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 340.6933 - mae: 13.6166 - mse: 340.6933 - val_loss: 682.9610 - val_mae: 19.5539 - val_mse: 682.9610\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 339.8478 - mae: 13.6273 - mse: 339.8478 - val_loss: 681.5209 - val_mae: 20.0937 - val_mse: 681.5209\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 335.7550 - mae: 13.5370 - mse: 335.7550 - val_loss: 666.2888 - val_mae: 19.7361 - val_mse: 666.2888\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 338.2979 - mae: 13.5877 - mse: 338.2979 - val_loss: 681.1068 - val_mae: 20.1129 - val_mse: 681.1068\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 331.8390 - mae: 13.4738 - mse: 331.8390 - val_loss: 684.5989 - val_mae: 19.6965 - val_mse: 684.5989\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 333.3459 - mae: 13.5127 - mse: 333.3459 - val_loss: 703.4744 - val_mae: 19.7659 - val_mse: 703.4744\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 5796.4482 - mae: 72.1152 - mse: 5796.4482 - val_loss: 5428.0703 - val_mae: 69.5475 - val_mse: 5428.0703\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 4559.3721 - mae: 63.4411 - mse: 4559.3721 - val_loss: 3782.3362 - val_mae: 57.5807 - val_mse: 3782.3362\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 3025.4043 - mae: 50.9843 - mse: 3025.4043 - val_loss: 2270.0771 - val_mae: 43.7525 - val_mse: 2270.0771\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1642.9723 - mae: 36.6133 - mse: 1642.9723 - val_loss: 1156.4967 - val_mae: 29.8938 - val_mse: 1156.4967\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 852.7097 - mae: 25.4241 - mse: 852.7097 - val_loss: 753.6252 - val_mae: 22.7123 - val_mse: 753.6252\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 579.1253 - mae: 19.8948 - mse: 579.1253 - val_loss: 609.4966 - val_mae: 19.6182 - val_mse: 609.4966\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 517.1766 - mae: 17.9093 - mse: 517.1766 - val_loss: 588.4144 - val_mae: 18.8439 - val_mse: 588.4144\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 503.1847 - mae: 17.3530 - mse: 503.1847 - val_loss: 587.1151 - val_mae: 18.5407 - val_mse: 587.1151\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 492.0299 - mae: 17.0221 - mse: 492.0299 - val_loss: 585.3670 - val_mae: 18.2877 - val_mse: 585.3670\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 489.9018 - mae: 17.0670 - mse: 489.9018 - val_loss: 583.6254 - val_mae: 18.3726 - val_mse: 583.6254\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 486.2424 - mae: 16.9046 - mse: 486.2424 - val_loss: 590.6036 - val_mae: 18.2895 - val_mse: 590.6036\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 478.7317 - mae: 16.7645 - mse: 478.7317 - val_loss: 589.7401 - val_mae: 18.2422 - val_mse: 589.7401\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 469.3407 - mae: 16.5266 - mse: 469.3407 - val_loss: 588.5279 - val_mae: 18.2919 - val_mse: 588.5279\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 464.8748 - mae: 16.5436 - mse: 464.8748 - val_loss: 589.2715 - val_mae: 18.3917 - val_mse: 589.2715\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 467.7982 - mae: 16.5559 - mse: 467.7982 - val_loss: 594.5060 - val_mae: 18.2173 - val_mse: 594.5060\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 467.4999 - mae: 16.5303 - mse: 467.4999 - val_loss: 597.0453 - val_mae: 18.1378 - val_mse: 597.0453\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 455.0528 - mae: 16.2744 - mse: 455.0528 - val_loss: 598.2703 - val_mae: 18.6976 - val_mse: 598.2703\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 455.3662 - mae: 16.2110 - mse: 455.3662 - val_loss: 595.5269 - val_mae: 18.2054 - val_mse: 595.5269\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 453.8940 - mae: 16.1264 - mse: 453.8940 - val_loss: 604.3814 - val_mae: 18.6648 - val_mse: 604.3814\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 445.7794 - mae: 16.1374 - mse: 445.7794 - val_loss: 606.2678 - val_mae: 18.8000 - val_mse: 606.2678\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 451.3862 - mae: 16.1883 - mse: 451.3862 - val_loss: 596.3268 - val_mae: 18.4729 - val_mse: 596.3268\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 443.4238 - mae: 15.9192 - mse: 443.4238 - val_loss: 608.6549 - val_mae: 18.5380 - val_mse: 608.6549\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 436.2742 - mae: 15.9752 - mse: 436.2742 - val_loss: 607.4398 - val_mae: 18.7278 - val_mse: 607.4398\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 431.9779 - mae: 15.7376 - mse: 431.9779 - val_loss: 618.3260 - val_mae: 18.8613 - val_mse: 618.3260\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 432.4831 - mae: 15.7707 - mse: 432.4831 - val_loss: 605.6167 - val_mae: 18.4488 - val_mse: 605.6167\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 432.8773 - mae: 15.8130 - mse: 432.8773 - val_loss: 612.6912 - val_mae: 18.5901 - val_mse: 612.6912\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 430.3658 - mae: 15.7080 - mse: 430.3658 - val_loss: 622.6835 - val_mae: 18.4880 - val_mse: 622.6835\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 432.8759 - mae: 15.7502 - mse: 432.8759 - val_loss: 617.0026 - val_mae: 18.8315 - val_mse: 617.0026\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 420.7254 - mae: 15.5836 - mse: 420.7254 - val_loss: 626.6431 - val_mae: 19.1262 - val_mse: 626.6431\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 420.0498 - mae: 15.4884 - mse: 420.0498 - val_loss: 619.5325 - val_mae: 18.7394 - val_mse: 619.5325\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 414.5485 - mae: 15.3583 - mse: 414.5485 - val_loss: 626.0486 - val_mae: 19.0283 - val_mse: 626.0486\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 427.3454 - mae: 15.6665 - mse: 427.3454 - val_loss: 626.0175 - val_mae: 18.6416 - val_mse: 626.0175\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 410.7364 - mae: 15.3334 - mse: 410.7364 - val_loss: 630.4557 - val_mae: 18.6771 - val_mse: 630.4557\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 411.5216 - mae: 15.3539 - mse: 411.5216 - val_loss: 631.3912 - val_mae: 18.6681 - val_mse: 631.3912\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 406.2679 - mae: 15.2435 - mse: 406.2679 - val_loss: 616.1282 - val_mae: 18.4065 - val_mse: 616.1282\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 407.7523 - mae: 15.2776 - mse: 407.7523 - val_loss: 639.5122 - val_mae: 18.5656 - val_mse: 639.5122\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 401.1582 - mae: 15.0508 - mse: 401.1582 - val_loss: 643.2403 - val_mae: 18.7925 - val_mse: 643.2403\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 408.1985 - mae: 15.2271 - mse: 408.1985 - val_loss: 646.1599 - val_mae: 18.9423 - val_mse: 646.1599\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 403.3208 - mae: 15.1800 - mse: 403.3208 - val_loss: 643.7217 - val_mae: 18.8323 - val_mse: 643.7217\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 398.4675 - mae: 14.9870 - mse: 398.4675 - val_loss: 630.5864 - val_mae: 19.1027 - val_mse: 630.5864\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 398.4413 - mae: 15.0422 - mse: 398.4413 - val_loss: 628.2627 - val_mae: 19.0423 - val_mse: 628.2627\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 389.9315 - mae: 14.7941 - mse: 389.9315 - val_loss: 630.7091 - val_mae: 18.8530 - val_mse: 630.7091\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 394.4932 - mae: 14.9931 - mse: 394.4932 - val_loss: 621.3525 - val_mae: 18.8060 - val_mse: 621.3525\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 387.4674 - mae: 14.8234 - mse: 387.4674 - val_loss: 639.2552 - val_mae: 19.0180 - val_mse: 639.2552\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 391.7945 - mae: 14.9021 - mse: 391.7945 - val_loss: 632.1355 - val_mae: 18.9644 - val_mse: 632.1355\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 389.2780 - mae: 14.9194 - mse: 389.2780 - val_loss: 640.6691 - val_mae: 18.7117 - val_mse: 640.6691\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 379.6132 - mae: 14.7095 - mse: 379.6132 - val_loss: 642.8758 - val_mae: 18.9274 - val_mse: 642.8758\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 379.3251 - mae: 14.5826 - mse: 379.3251 - val_loss: 637.8971 - val_mae: 18.8875 - val_mse: 637.8971\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 378.7244 - mae: 14.7089 - mse: 378.7244 - val_loss: 652.1053 - val_mae: 18.8706 - val_mse: 652.1053\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 383.3096 - mae: 14.6557 - mse: 383.3096 - val_loss: 630.1693 - val_mae: 18.9309 - val_mse: 630.1693\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 376.2023 - mae: 14.5791 - mse: 376.2023 - val_loss: 647.0646 - val_mae: 18.9421 - val_mse: 647.0646\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 378.7879 - mae: 14.6036 - mse: 378.7879 - val_loss: 639.6603 - val_mae: 18.9353 - val_mse: 639.6603\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 373.6377 - mae: 14.5203 - mse: 373.6377 - val_loss: 644.1799 - val_mae: 19.1382 - val_mse: 644.1799\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 373.2657 - mae: 14.4666 - mse: 373.2657 - val_loss: 642.1434 - val_mae: 19.1713 - val_mse: 642.1434\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 372.0321 - mae: 14.4806 - mse: 372.0321 - val_loss: 647.3722 - val_mae: 18.9453 - val_mse: 647.3722\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 372.8530 - mae: 14.5039 - mse: 372.8530 - val_loss: 637.2548 - val_mae: 19.0075 - val_mse: 637.2548\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 372.3662 - mae: 14.5011 - mse: 372.3662 - val_loss: 640.9135 - val_mae: 18.8555 - val_mse: 640.9135\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 366.0653 - mae: 14.3028 - mse: 366.0653 - val_loss: 652.7143 - val_mae: 19.2028 - val_mse: 652.7143\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 365.6978 - mae: 14.3595 - mse: 365.6978 - val_loss: 655.2885 - val_mae: 19.4721 - val_mse: 655.2885\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 363.7900 - mae: 14.3724 - mse: 363.7900 - val_loss: 647.7147 - val_mae: 19.1950 - val_mse: 647.7147\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 362.5185 - mae: 14.2346 - mse: 362.5185 - val_loss: 667.0544 - val_mae: 19.1262 - val_mse: 667.0544\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 365.6109 - mae: 14.3333 - mse: 365.6109 - val_loss: 653.2950 - val_mae: 19.3518 - val_mse: 653.2950\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 367.0483 - mae: 14.3330 - mse: 367.0483 - val_loss: 648.5920 - val_mae: 18.9390 - val_mse: 648.5920\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 360.6687 - mae: 14.2301 - mse: 360.6687 - val_loss: 662.2191 - val_mae: 18.9927 - val_mse: 662.2191\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 360.1481 - mae: 14.2159 - mse: 360.1481 - val_loss: 660.6798 - val_mae: 19.0321 - val_mse: 660.6798\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 353.2858 - mae: 14.0756 - mse: 353.2858 - val_loss: 658.0389 - val_mae: 19.2258 - val_mse: 658.0389\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 357.4315 - mae: 14.0595 - mse: 357.4315 - val_loss: 657.5878 - val_mae: 19.2631 - val_mse: 657.5878\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 355.4567 - mae: 14.1289 - mse: 355.4567 - val_loss: 661.7214 - val_mae: 19.4010 - val_mse: 661.7214\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 351.9076 - mae: 14.1250 - mse: 351.9076 - val_loss: 675.1317 - val_mae: 19.1770 - val_mse: 675.1317\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 353.5512 - mae: 13.9967 - mse: 353.5512 - val_loss: 650.3568 - val_mae: 19.3640 - val_mse: 650.3568\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 350.0104 - mae: 13.9490 - mse: 350.0104 - val_loss: 657.8741 - val_mae: 19.2512 - val_mse: 657.8741\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 351.4073 - mae: 13.9455 - mse: 351.4073 - val_loss: 664.4634 - val_mae: 19.2066 - val_mse: 664.4634\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 356.4348 - mae: 14.1863 - mse: 356.4348 - val_loss: 663.6841 - val_mae: 19.1696 - val_mse: 663.6841\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 344.4818 - mae: 13.8594 - mse: 344.4818 - val_loss: 659.7780 - val_mae: 19.0316 - val_mse: 659.7780\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 348.8786 - mae: 13.9961 - mse: 348.8786 - val_loss: 656.0975 - val_mae: 19.2598 - val_mse: 656.0975\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 345.4017 - mae: 13.8990 - mse: 345.4017 - val_loss: 670.3854 - val_mae: 19.1479 - val_mse: 670.3854\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 339.6882 - mae: 13.6619 - mse: 339.6882 - val_loss: 665.3740 - val_mae: 19.3151 - val_mse: 665.3740\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 336.9396 - mae: 13.7569 - mse: 336.9396 - val_loss: 658.0590 - val_mae: 19.2922 - val_mse: 658.0590\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 337.9334 - mae: 13.6880 - mse: 337.9334 - val_loss: 662.6927 - val_mae: 19.5295 - val_mse: 662.6927\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 338.4600 - mae: 13.7481 - mse: 338.4600 - val_loss: 662.2188 - val_mae: 19.2461 - val_mse: 662.2188\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 337.1360 - mae: 13.7797 - mse: 337.1360 - val_loss: 678.6419 - val_mae: 19.2514 - val_mse: 678.6419\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 337.9948 - mae: 13.7071 - mse: 337.9948 - val_loss: 661.0528 - val_mae: 19.1881 - val_mse: 661.0528\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 333.4145 - mae: 13.6044 - mse: 333.4145 - val_loss: 676.3660 - val_mae: 19.6834 - val_mse: 676.3660\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 339.0039 - mae: 13.7712 - mse: 339.0039 - val_loss: 663.7173 - val_mae: 19.2770 - val_mse: 663.7173\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 329.9764 - mae: 13.4860 - mse: 329.9764 - val_loss: 670.0532 - val_mae: 19.2544 - val_mse: 670.0532\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 328.1650 - mae: 13.5336 - mse: 328.1650 - val_loss: 657.0569 - val_mae: 19.3386 - val_mse: 657.0569\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 333.5830 - mae: 13.6112 - mse: 333.5830 - val_loss: 664.4654 - val_mae: 19.3806 - val_mse: 664.4654\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 331.5257 - mae: 13.4927 - mse: 331.5257 - val_loss: 669.7144 - val_mae: 19.3874 - val_mse: 669.7144\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 328.2456 - mae: 13.4326 - mse: 328.2456 - val_loss: 676.8617 - val_mae: 19.3295 - val_mse: 676.8617\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 330.2781 - mae: 13.5156 - mse: 330.2781 - val_loss: 661.5812 - val_mae: 19.3262 - val_mse: 661.5812\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 326.1586 - mae: 13.4713 - mse: 326.1586 - val_loss: 679.0533 - val_mae: 19.5325 - val_mse: 679.0533\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 326.7358 - mae: 13.5145 - mse: 326.7358 - val_loss: 675.8921 - val_mae: 19.7243 - val_mse: 675.8921\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 321.4016 - mae: 13.4191 - mse: 321.4016 - val_loss: 680.3428 - val_mae: 19.6279 - val_mse: 680.3428\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 322.5152 - mae: 13.3668 - mse: 322.5152 - val_loss: 682.9684 - val_mae: 19.4659 - val_mse: 682.9684\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 325.2420 - mae: 13.4489 - mse: 325.2420 - val_loss: 675.0811 - val_mae: 19.4467 - val_mse: 675.0811\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 319.3077 - mae: 13.3317 - mse: 319.3077 - val_loss: 677.6663 - val_mae: 19.5365 - val_mse: 677.6663\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 320.7596 - mae: 13.3737 - mse: 320.7596 - val_loss: 690.0417 - val_mae: 19.8474 - val_mse: 690.0417\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 324.6841 - mae: 13.4273 - mse: 324.6841 - val_loss: 689.0594 - val_mae: 19.5898 - val_mse: 689.0594\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 314.0706 - mae: 13.2022 - mse: 314.0706 - val_loss: 683.5732 - val_mae: 19.8229 - val_mse: 683.5732\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 318.3765 - mae: 13.2390 - mse: 318.3765 - val_loss: 688.9412 - val_mae: 19.4856 - val_mse: 688.9412\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2940.8687 - mae: 45.2293 - mse: 2940.8452 - val_loss: 711.0649 - val_mae: 22.2677 - val_mse: 711.0389\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 980.9933 - mae: 25.4854 - mse: 980.9666 - val_loss: 637.2064 - val_mae: 20.7109 - val_mse: 637.1798\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 919.4822 - mae: 24.7518 - mse: 919.4554 - val_loss: 618.4116 - val_mae: 20.1527 - val_mse: 618.3845\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 881.3577 - mae: 24.0350 - mse: 881.3304 - val_loss: 611.8503 - val_mae: 20.1986 - val_mse: 611.8230\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 861.0094 - mae: 23.7413 - mse: 860.9818 - val_loss: 602.5735 - val_mae: 19.8850 - val_mse: 602.5458\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 815.0007 - mae: 22.9749 - mse: 814.9730 - val_loss: 578.5081 - val_mae: 19.0123 - val_mse: 578.4799\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 814.6927 - mae: 23.0268 - mse: 814.6647 - val_loss: 598.8294 - val_mae: 19.8553 - val_mse: 598.8015\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 812.9886 - mae: 22.9693 - mse: 812.9601 - val_loss: 587.2026 - val_mae: 19.5028 - val_mse: 587.1743\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 798.9915 - mae: 22.6987 - mse: 798.9630 - val_loss: 576.0940 - val_mae: 18.9124 - val_mse: 576.0655\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 780.6609 - mae: 22.2685 - mse: 780.6326 - val_loss: 584.0781 - val_mae: 19.2176 - val_mse: 584.0496\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 785.8481 - mae: 22.5838 - mse: 785.8199 - val_loss: 596.3722 - val_mae: 19.7358 - val_mse: 596.3438\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 769.8351 - mae: 22.3034 - mse: 769.8063 - val_loss: 587.7161 - val_mae: 19.3864 - val_mse: 587.6876\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 764.2627 - mae: 22.1565 - mse: 764.2343 - val_loss: 598.9772 - val_mae: 19.9378 - val_mse: 598.9488\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 761.8734 - mae: 22.1980 - mse: 761.8450 - val_loss: 593.3304 - val_mae: 19.6754 - val_mse: 593.3021\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 739.2862 - mae: 21.8298 - mse: 739.2574 - val_loss: 581.7664 - val_mae: 19.1255 - val_mse: 581.7379\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 740.9733 - mae: 22.0129 - mse: 740.9452 - val_loss: 574.4871 - val_mae: 18.9036 - val_mse: 574.4589\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 729.2029 - mae: 21.6755 - mse: 729.1747 - val_loss: 595.1714 - val_mae: 19.8711 - val_mse: 595.1432\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 725.8553 - mae: 21.5583 - mse: 725.8274 - val_loss: 574.3094 - val_mae: 19.1141 - val_mse: 574.2814\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 724.1827 - mae: 21.6392 - mse: 724.1548 - val_loss: 582.5822 - val_mae: 19.4109 - val_mse: 582.5546\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 739.8652 - mae: 21.8414 - mse: 739.8375 - val_loss: 582.9860 - val_mae: 19.3103 - val_mse: 582.9584\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 718.0485 - mae: 21.5262 - mse: 718.0213 - val_loss: 576.0551 - val_mae: 19.1601 - val_mse: 576.0275\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 696.4619 - mae: 21.2177 - mse: 696.4345 - val_loss: 589.7264 - val_mae: 19.6154 - val_mse: 589.6991\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 721.1556 - mae: 21.4919 - mse: 721.1287 - val_loss: 592.3089 - val_mae: 19.8217 - val_mse: 592.2819\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 715.8833 - mae: 21.3546 - mse: 715.8563 - val_loss: 574.1503 - val_mae: 19.0245 - val_mse: 574.1233\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 696.6408 - mae: 21.0839 - mse: 696.6143 - val_loss: 578.4980 - val_mae: 19.2453 - val_mse: 578.4713\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 708.8600 - mae: 21.3003 - mse: 708.8333 - val_loss: 608.6534 - val_mae: 20.2817 - val_mse: 608.6269\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 687.2502 - mae: 20.9006 - mse: 687.2239 - val_loss: 573.8176 - val_mae: 18.9911 - val_mse: 573.7912\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 696.8911 - mae: 21.0747 - mse: 696.8651 - val_loss: 566.3044 - val_mae: 18.3342 - val_mse: 566.2784\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 696.2445 - mae: 20.9556 - mse: 696.2189 - val_loss: 562.7994 - val_mae: 18.3848 - val_mse: 562.7736\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 692.4276 - mae: 20.9519 - mse: 692.4016 - val_loss: 571.7637 - val_mae: 18.9748 - val_mse: 571.7382\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 682.2756 - mae: 20.7270 - mse: 682.2501 - val_loss: 568.1732 - val_mae: 17.9582 - val_mse: 568.1476\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 687.8922 - mae: 20.9286 - mse: 687.8670 - val_loss: 562.7264 - val_mae: 18.3265 - val_mse: 562.7012\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 673.0989 - mae: 20.7330 - mse: 673.0737 - val_loss: 575.4258 - val_mae: 19.1713 - val_mse: 575.4010\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 673.6087 - mae: 20.6464 - mse: 673.5842 - val_loss: 559.4542 - val_mae: 18.2032 - val_mse: 559.4293\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 671.6842 - mae: 20.7149 - mse: 671.6597 - val_loss: 572.8850 - val_mae: 19.0829 - val_mse: 572.8604\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 682.4752 - mae: 20.9307 - mse: 682.4509 - val_loss: 583.1494 - val_mae: 19.5082 - val_mse: 583.1253\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 666.1450 - mae: 20.3567 - mse: 666.1210 - val_loss: 575.3293 - val_mae: 19.2132 - val_mse: 575.3054\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 670.3649 - mae: 20.4619 - mse: 670.3411 - val_loss: 583.3354 - val_mae: 19.5769 - val_mse: 583.3118\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 663.8362 - mae: 20.5103 - mse: 663.8128 - val_loss: 568.7953 - val_mae: 18.9952 - val_mse: 568.7719\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 655.8590 - mae: 20.3925 - mse: 655.8357 - val_loss: 559.3073 - val_mae: 18.5159 - val_mse: 559.2840\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 666.7177 - mae: 20.4647 - mse: 666.6946 - val_loss: 561.2380 - val_mae: 18.2452 - val_mse: 561.2150\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 660.5533 - mae: 20.5167 - mse: 660.5307 - val_loss: 568.5621 - val_mae: 18.9372 - val_mse: 568.5395\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 665.4626 - mae: 20.5186 - mse: 665.4399 - val_loss: 565.6866 - val_mae: 18.7113 - val_mse: 565.6642\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 666.0560 - mae: 20.4209 - mse: 666.0339 - val_loss: 561.0346 - val_mae: 18.6098 - val_mse: 561.0125\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 668.6405 - mae: 20.4889 - mse: 668.6187 - val_loss: 568.0175 - val_mae: 18.8845 - val_mse: 567.9956\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 649.0064 - mae: 20.2141 - mse: 648.9845 - val_loss: 581.5820 - val_mae: 19.4583 - val_mse: 581.5604\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 677.8128 - mae: 20.7450 - mse: 677.7911 - val_loss: 565.6207 - val_mae: 18.6497 - val_mse: 565.5992\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 646.6980 - mae: 20.2806 - mse: 646.6769 - val_loss: 574.0312 - val_mae: 19.1675 - val_mse: 574.0099\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 657.4098 - mae: 20.4891 - mse: 657.3885 - val_loss: 579.8433 - val_mae: 19.4352 - val_mse: 579.8224\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 645.8271 - mae: 20.2324 - mse: 645.8063 - val_loss: 579.9963 - val_mae: 19.4019 - val_mse: 579.9756\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 652.1553 - mae: 20.2462 - mse: 652.1342 - val_loss: 581.6483 - val_mae: 19.3515 - val_mse: 581.6277\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 661.1077 - mae: 20.4331 - mse: 661.0869 - val_loss: 565.4622 - val_mae: 18.7176 - val_mse: 565.4417\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 648.1132 - mae: 20.1578 - mse: 648.0922 - val_loss: 567.8088 - val_mae: 18.8885 - val_mse: 567.7885\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 641.3483 - mae: 19.9858 - mse: 641.3276 - val_loss: 566.3743 - val_mae: 18.7485 - val_mse: 566.3540\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 641.6631 - mae: 20.0968 - mse: 641.6431 - val_loss: 578.1505 - val_mae: 19.3068 - val_mse: 578.1307\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 649.6243 - mae: 20.2359 - mse: 649.6044 - val_loss: 560.9314 - val_mae: 18.5448 - val_mse: 560.9115\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 638.1724 - mae: 19.8606 - mse: 638.1528 - val_loss: 562.4960 - val_mae: 18.2004 - val_mse: 562.4763\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 636.4410 - mae: 20.0169 - mse: 636.4214 - val_loss: 563.0875 - val_mae: 18.6714 - val_mse: 563.0679\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.0164 - mae: 20.0565 - mse: 644.9968 - val_loss: 561.9479 - val_mae: 18.5211 - val_mse: 561.9284\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 646.6204 - mae: 20.2568 - mse: 646.6011 - val_loss: 564.5883 - val_mae: 18.1876 - val_mse: 564.5690\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.4972 - mae: 19.8589 - mse: 633.4783 - val_loss: 561.8823 - val_mae: 18.4050 - val_mse: 561.8633\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 628.4188 - mae: 19.8703 - mse: 628.3997 - val_loss: 580.9592 - val_mae: 19.4000 - val_mse: 580.9402\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 640.1861 - mae: 20.0684 - mse: 640.1674 - val_loss: 562.2661 - val_mae: 18.3962 - val_mse: 562.2471\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.9003 - mae: 19.8777 - mse: 634.8813 - val_loss: 568.4600 - val_mae: 18.6510 - val_mse: 568.4411\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 642.0659 - mae: 20.1445 - mse: 642.0468 - val_loss: 562.2929 - val_mae: 18.4712 - val_mse: 562.2742\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 625.9789 - mae: 19.7634 - mse: 625.9601 - val_loss: 571.1982 - val_mae: 18.9306 - val_mse: 571.1796\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.7831 - mae: 19.7449 - mse: 614.7642 - val_loss: 577.4372 - val_mae: 19.2076 - val_mse: 577.4188\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 619.4669 - mae: 19.6017 - mse: 619.4489 - val_loss: 569.5706 - val_mae: 18.7590 - val_mse: 569.5521\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 631.2927 - mae: 19.9202 - mse: 631.2744 - val_loss: 563.0321 - val_mae: 18.6437 - val_mse: 563.0137\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.2391 - mae: 19.6051 - mse: 616.2210 - val_loss: 565.3250 - val_mae: 18.6522 - val_mse: 565.3068\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 628.8947 - mae: 19.9498 - mse: 628.8768 - val_loss: 588.9000 - val_mae: 19.7288 - val_mse: 588.8818\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 624.3224 - mae: 19.8509 - mse: 624.3047 - val_loss: 566.5029 - val_mae: 17.9895 - val_mse: 566.4846\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 615.4599 - mae: 19.7072 - mse: 615.4420 - val_loss: 561.2645 - val_mae: 18.5351 - val_mse: 561.2465\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 621.8184 - mae: 19.6286 - mse: 621.8004 - val_loss: 559.6697 - val_mae: 18.3052 - val_mse: 559.6519\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 635.2451 - mae: 19.9163 - mse: 635.2272 - val_loss: 562.1300 - val_mae: 18.3038 - val_mse: 562.1123\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 623.1230 - mae: 19.7028 - mse: 623.1055 - val_loss: 563.5493 - val_mae: 18.4565 - val_mse: 563.5317\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 609.2678 - mae: 19.5430 - mse: 609.2505 - val_loss: 576.7678 - val_mae: 19.2605 - val_mse: 576.7502\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 608.4985 - mae: 19.5061 - mse: 608.4810 - val_loss: 564.4812 - val_mae: 18.4685 - val_mse: 564.4637\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 622.0375 - mae: 19.8153 - mse: 622.0200 - val_loss: 593.5737 - val_mae: 19.8407 - val_mse: 593.5562\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 624.0924 - mae: 19.7832 - mse: 624.0752 - val_loss: 568.2226 - val_mae: 18.8776 - val_mse: 568.2053\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.3681 - mae: 19.7183 - mse: 616.3506 - val_loss: 562.5756 - val_mae: 18.2804 - val_mse: 562.5582\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 610.7686 - mae: 19.5343 - mse: 610.7515 - val_loss: 564.1598 - val_mae: 18.5267 - val_mse: 564.1426\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 612.3559 - mae: 19.6088 - mse: 612.3387 - val_loss: 568.5248 - val_mae: 18.7543 - val_mse: 568.5077\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 623.9680 - mae: 19.6113 - mse: 623.9512 - val_loss: 565.1414 - val_mae: 18.5893 - val_mse: 565.1243\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 604.5049 - mae: 19.3587 - mse: 604.4879 - val_loss: 570.0824 - val_mae: 18.8350 - val_mse: 570.0654\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 600.5074 - mae: 19.4795 - mse: 600.4904 - val_loss: 560.2213 - val_mae: 18.4836 - val_mse: 560.2042\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 605.8632 - mae: 19.4800 - mse: 605.8462 - val_loss: 563.8469 - val_mae: 18.1141 - val_mse: 563.8297\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 623.3857 - mae: 19.7320 - mse: 623.3687 - val_loss: 570.6172 - val_mae: 18.9947 - val_mse: 570.6002\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 614.4044 - mae: 19.6004 - mse: 614.3875 - val_loss: 566.8971 - val_mae: 18.7394 - val_mse: 566.8802\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 604.7905 - mae: 19.3608 - mse: 604.7737 - val_loss: 565.3253 - val_mae: 18.3771 - val_mse: 565.3083\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 610.7694 - mae: 19.5281 - mse: 610.7525 - val_loss: 565.4156 - val_mae: 18.4982 - val_mse: 565.3986\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 606.9001 - mae: 19.4494 - mse: 606.8832 - val_loss: 563.1505 - val_mae: 18.5113 - val_mse: 563.1338\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 604.1901 - mae: 19.3254 - mse: 604.1732 - val_loss: 565.9614 - val_mae: 18.6278 - val_mse: 565.9444\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 596.5780 - mae: 19.2887 - mse: 596.5613 - val_loss: 561.8404 - val_mae: 18.1990 - val_mse: 561.8234\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 604.4316 - mae: 19.4194 - mse: 604.4150 - val_loss: 561.5395 - val_mae: 18.4253 - val_mse: 561.5226\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 602.7519 - mae: 19.4464 - mse: 602.7352 - val_loss: 564.2027 - val_mae: 18.2512 - val_mse: 564.1857\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 608.6509 - mae: 19.4371 - mse: 608.6341 - val_loss: 560.5173 - val_mae: 18.4028 - val_mse: 560.5005\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 603.0427 - mae: 19.3482 - mse: 603.0259 - val_loss: 564.3420 - val_mae: 18.5793 - val_mse: 564.3252\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 597.2889 - mae: 19.2319 - mse: 597.2725 - val_loss: 574.2911 - val_mae: 19.1161 - val_mse: 574.2743\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 603.6262 - mae: 19.3525 - mse: 603.6092 - val_loss: 563.2688 - val_mae: 18.3192 - val_mse: 563.2518\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 13.0254, Train MSE: 332.5452\n",
      "Val   MAE: 19.7659, Val   MSE: 703.4744\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 11.3073, Train MSE: 247.4965\n",
      "Val   MAE: 19.4856, Val   MSE: 688.9412\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 17.0177, Train MSE: 473.6156\n",
      "Val   MAE: 18.3192, Val   MSE: 563.2518\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 166.8389 - mae: 9.3844 - mse: 166.8389 - val_loss: 166.1515 - val_mae: 9.4419 - val_mse: 166.1515\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 135.9458 - mae: 8.5229 - mse: 135.9458 - val_loss: 163.0283 - val_mae: 9.2846 - val_mse: 163.0283\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.0581 - mae: 8.4276 - mse: 134.0581 - val_loss: 161.7386 - val_mae: 9.2282 - val_mse: 161.7386\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.7576 - mae: 8.4219 - mse: 132.7576 - val_loss: 160.9894 - val_mae: 9.0971 - val_mse: 160.9894\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.5367 - mae: 8.3871 - mse: 132.5367 - val_loss: 161.0836 - val_mae: 9.3114 - val_mse: 161.0836\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.0862 - mae: 8.3657 - mse: 132.0862 - val_loss: 160.6924 - val_mae: 9.2404 - val_mse: 160.6924\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.9480 - mae: 8.3315 - mse: 130.9480 - val_loss: 159.5093 - val_mae: 9.1753 - val_mse: 159.5093\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.4969 - mae: 8.3328 - mse: 130.4969 - val_loss: 159.4434 - val_mae: 9.1460 - val_mse: 159.4434\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.1871 - mae: 8.3083 - mse: 130.1871 - val_loss: 159.7067 - val_mae: 9.3024 - val_mse: 159.7067\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.2186 - mae: 8.2918 - mse: 129.2186 - val_loss: 158.1958 - val_mae: 9.1265 - val_mse: 158.1958\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.5139 - mae: 8.2735 - mse: 128.5139 - val_loss: 159.2822 - val_mae: 8.9846 - val_mse: 159.2822\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.8709 - mae: 8.2339 - mse: 127.8709 - val_loss: 160.3525 - val_mae: 9.2076 - val_mse: 160.3525\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.8584 - mae: 8.2151 - mse: 126.8584 - val_loss: 158.3531 - val_mae: 9.1442 - val_mse: 158.3531\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.4841 - mae: 8.2124 - mse: 126.4841 - val_loss: 158.6361 - val_mae: 9.2278 - val_mse: 158.6361\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.3045 - mae: 8.1913 - mse: 125.3045 - val_loss: 158.3530 - val_mae: 9.2022 - val_mse: 158.3530\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.4592 - mae: 8.1558 - mse: 124.4592 - val_loss: 158.5604 - val_mae: 9.1227 - val_mse: 158.5604\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 123.2815 - mae: 8.1414 - mse: 123.2815 - val_loss: 159.7749 - val_mae: 9.2507 - val_mse: 159.7749\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 123.0912 - mae: 8.1366 - mse: 123.0912 - val_loss: 159.2828 - val_mae: 9.1257 - val_mse: 159.2828\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.0957 - mae: 8.0974 - mse: 122.0957 - val_loss: 158.7296 - val_mae: 9.0359 - val_mse: 158.7296\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 120.6532 - mae: 8.0753 - mse: 120.6532 - val_loss: 161.3533 - val_mae: 9.2725 - val_mse: 161.3533\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 120.4870 - mae: 8.0416 - mse: 120.4870 - val_loss: 160.2501 - val_mae: 9.0343 - val_mse: 160.2501\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.9994 - mae: 8.0131 - mse: 118.9994 - val_loss: 159.6836 - val_mae: 9.1687 - val_mse: 159.6836\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.3086 - mae: 8.0059 - mse: 118.3086 - val_loss: 161.3232 - val_mae: 9.2202 - val_mse: 161.3232\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 116.9652 - mae: 7.9743 - mse: 116.9652 - val_loss: 162.0010 - val_mae: 9.1973 - val_mse: 162.0010\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.7592 - mae: 7.9696 - mse: 116.7592 - val_loss: 162.7767 - val_mae: 9.2102 - val_mse: 162.7767\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.8485 - mae: 7.9321 - mse: 115.8485 - val_loss: 164.0368 - val_mae: 9.1866 - val_mse: 164.0368\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.9859 - mae: 7.8900 - mse: 114.9859 - val_loss: 163.9616 - val_mae: 9.4207 - val_mse: 163.9616\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.9315 - mae: 7.8817 - mse: 113.9315 - val_loss: 164.8606 - val_mae: 9.1393 - val_mse: 164.8606\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 112.3854 - mae: 7.8318 - mse: 112.3854 - val_loss: 165.8883 - val_mae: 9.1957 - val_mse: 165.8883\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 112.2717 - mae: 7.8314 - mse: 112.2717 - val_loss: 167.2308 - val_mae: 9.6244 - val_mse: 167.2308\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.3483 - mae: 7.7779 - mse: 110.3483 - val_loss: 167.0930 - val_mae: 9.4705 - val_mse: 167.0930\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.6226 - mae: 7.7845 - mse: 110.6226 - val_loss: 169.2419 - val_mae: 9.4050 - val_mse: 169.2419\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.0492 - mae: 7.7624 - mse: 110.0492 - val_loss: 167.5684 - val_mae: 9.3448 - val_mse: 167.5684\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 108.2831 - mae: 7.6880 - mse: 108.2831 - val_loss: 169.5584 - val_mae: 9.4653 - val_mse: 169.5584\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.9465 - mae: 7.6972 - mse: 107.9465 - val_loss: 171.1241 - val_mae: 9.5054 - val_mse: 171.1241\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.1880 - mae: 7.6868 - mse: 107.1880 - val_loss: 170.3097 - val_mae: 9.3008 - val_mse: 170.3097\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 105.9898 - mae: 7.6131 - mse: 105.9898 - val_loss: 168.9179 - val_mae: 9.5817 - val_mse: 168.9179\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 105.2317 - mae: 7.6012 - mse: 105.2317 - val_loss: 170.0460 - val_mae: 9.5625 - val_mse: 170.0460\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 104.8849 - mae: 7.5994 - mse: 104.8849 - val_loss: 174.1537 - val_mae: 9.7748 - val_mse: 174.1537\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 103.8235 - mae: 7.5794 - mse: 103.8235 - val_loss: 174.1976 - val_mae: 9.6799 - val_mse: 174.1976\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 103.2856 - mae: 7.5502 - mse: 103.2856 - val_loss: 173.4772 - val_mae: 9.5240 - val_mse: 173.4772\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 102.2844 - mae: 7.5202 - mse: 102.2844 - val_loss: 173.2163 - val_mae: 9.6871 - val_mse: 173.2163\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 100.7835 - mae: 7.4460 - mse: 100.7835 - val_loss: 174.9066 - val_mae: 9.6288 - val_mse: 174.9066\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 100.9107 - mae: 7.4703 - mse: 100.9107 - val_loss: 175.6835 - val_mae: 9.7574 - val_mse: 175.6835\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 99.8291 - mae: 7.4315 - mse: 99.8291 - val_loss: 176.0972 - val_mae: 9.5807 - val_mse: 176.0972\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 99.3659 - mae: 7.4324 - mse: 99.3659 - val_loss: 176.8741 - val_mae: 9.4924 - val_mse: 176.8741\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 98.3143 - mae: 7.3875 - mse: 98.3143 - val_loss: 176.7740 - val_mae: 9.5726 - val_mse: 176.7740\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 97.0955 - mae: 7.3316 - mse: 97.0955 - val_loss: 178.9008 - val_mae: 9.7870 - val_mse: 178.9008\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 96.5825 - mae: 7.3192 - mse: 96.5825 - val_loss: 181.4977 - val_mae: 9.6056 - val_mse: 181.4977\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 95.3581 - mae: 7.2770 - mse: 95.3581 - val_loss: 180.9895 - val_mae: 9.6930 - val_mse: 180.9895\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 95.4550 - mae: 7.3171 - mse: 95.4550 - val_loss: 182.1070 - val_mae: 9.9053 - val_mse: 182.1070\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 94.8356 - mae: 7.2798 - mse: 94.8356 - val_loss: 179.7923 - val_mae: 9.6787 - val_mse: 179.7923\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.1935 - mae: 7.2450 - mse: 94.1935 - val_loss: 180.4543 - val_mae: 9.7312 - val_mse: 180.4543\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 92.6047 - mae: 7.2011 - mse: 92.6047 - val_loss: 187.0144 - val_mae: 10.1005 - val_mse: 187.0144\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 91.3525 - mae: 7.1366 - mse: 91.3525 - val_loss: 184.7939 - val_mae: 9.9921 - val_mse: 184.7939\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 92.6225 - mae: 7.2074 - mse: 92.6225 - val_loss: 183.7392 - val_mae: 9.6595 - val_mse: 183.7392\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 91.6745 - mae: 7.1551 - mse: 91.6745 - val_loss: 184.2739 - val_mae: 9.7187 - val_mse: 184.2739\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 89.6479 - mae: 7.0750 - mse: 89.6479 - val_loss: 182.6762 - val_mae: 9.8933 - val_mse: 182.6762\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 87.8728 - mae: 7.0161 - mse: 87.8728 - val_loss: 191.8329 - val_mae: 10.1612 - val_mse: 191.8329\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 87.4199 - mae: 6.9951 - mse: 87.4199 - val_loss: 191.1639 - val_mae: 9.7833 - val_mse: 191.1639\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 87.1928 - mae: 6.9857 - mse: 87.1928 - val_loss: 190.6629 - val_mae: 9.9352 - val_mse: 190.6629\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 86.7735 - mae: 6.9975 - mse: 86.7735 - val_loss: 189.2123 - val_mae: 9.8216 - val_mse: 189.2123\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 85.6018 - mae: 6.9270 - mse: 85.6018 - val_loss: 188.6808 - val_mae: 9.9328 - val_mse: 188.6808\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 84.7835 - mae: 6.9063 - mse: 84.7835 - val_loss: 191.4211 - val_mae: 10.2812 - val_mse: 191.4211\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 83.9117 - mae: 6.8806 - mse: 83.9117 - val_loss: 190.3025 - val_mae: 9.9388 - val_mse: 190.3025\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 82.2960 - mae: 6.7873 - mse: 82.2960 - val_loss: 192.0640 - val_mae: 9.9846 - val_mse: 192.0640\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 83.0272 - mae: 6.8514 - mse: 83.0272 - val_loss: 192.5510 - val_mae: 10.0922 - val_mse: 192.5510\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 81.6333 - mae: 6.7958 - mse: 81.6333 - val_loss: 195.5185 - val_mae: 10.1211 - val_mse: 195.5185\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 81.6503 - mae: 6.8062 - mse: 81.6503 - val_loss: 193.4310 - val_mae: 10.1266 - val_mse: 193.4310\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 80.3144 - mae: 6.7318 - mse: 80.3144 - val_loss: 195.8212 - val_mae: 10.1962 - val_mse: 195.8212\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 79.2762 - mae: 6.6720 - mse: 79.2762 - val_loss: 198.7369 - val_mae: 10.4416 - val_mse: 198.7369\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 79.6742 - mae: 6.7374 - mse: 79.6742 - val_loss: 193.6024 - val_mae: 10.0326 - val_mse: 193.6024\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 79.3646 - mae: 6.7009 - mse: 79.3646 - val_loss: 198.4933 - val_mae: 10.3463 - val_mse: 198.4933\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 77.5456 - mae: 6.6459 - mse: 77.5456 - val_loss: 200.5806 - val_mae: 10.2508 - val_mse: 200.5806\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 76.3819 - mae: 6.5737 - mse: 76.3819 - val_loss: 199.7491 - val_mae: 10.2920 - val_mse: 199.7491\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 75.1195 - mae: 6.5376 - mse: 75.1195 - val_loss: 201.7570 - val_mae: 10.3177 - val_mse: 201.7570\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 76.5331 - mae: 6.5713 - mse: 76.5331 - val_loss: 198.1046 - val_mae: 10.1097 - val_mse: 198.1046\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 73.9991 - mae: 6.4812 - mse: 73.9991 - val_loss: 198.7490 - val_mae: 10.2930 - val_mse: 198.7490\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 74.1648 - mae: 6.4990 - mse: 74.1648 - val_loss: 200.3866 - val_mae: 10.3887 - val_mse: 200.3866\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 72.7650 - mae: 6.4451 - mse: 72.7650 - val_loss: 203.1336 - val_mae: 10.4491 - val_mse: 203.1336\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 72.4872 - mae: 6.4592 - mse: 72.4872 - val_loss: 209.9131 - val_mae: 10.5953 - val_mse: 209.9131\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 71.5692 - mae: 6.4102 - mse: 71.5692 - val_loss: 203.6712 - val_mae: 10.0967 - val_mse: 203.6712\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 71.0736 - mae: 6.3256 - mse: 71.0736 - val_loss: 206.3253 - val_mae: 10.5949 - val_mse: 206.3253\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 70.5339 - mae: 6.3974 - mse: 70.5339 - val_loss: 210.8477 - val_mae: 10.7109 - val_mse: 210.8477\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 71.2520 - mae: 6.3882 - mse: 71.2520 - val_loss: 202.8220 - val_mae: 10.2860 - val_mse: 202.8220\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.9347 - mae: 6.2729 - mse: 68.9347 - val_loss: 205.7198 - val_mae: 10.3776 - val_mse: 205.7198\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.6925 - mae: 6.2573 - mse: 68.6925 - val_loss: 210.1939 - val_mae: 10.5948 - val_mse: 210.1939\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.3067 - mae: 6.2819 - mse: 68.3067 - val_loss: 210.0113 - val_mae: 10.6627 - val_mse: 210.0113\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.1812 - mae: 6.2626 - mse: 68.1812 - val_loss: 205.2603 - val_mae: 10.1669 - val_mse: 205.2603\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.9714 - mae: 6.2879 - mse: 68.9714 - val_loss: 205.9097 - val_mae: 10.2254 - val_mse: 205.9097\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 65.6873 - mae: 6.1436 - mse: 65.6873 - val_loss: 209.7129 - val_mae: 10.6166 - val_mse: 209.7129\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 65.0360 - mae: 6.0946 - mse: 65.0360 - val_loss: 213.4272 - val_mae: 10.4319 - val_mse: 213.4272\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 65.4608 - mae: 6.1511 - mse: 65.4608 - val_loss: 206.5180 - val_mae: 10.2716 - val_mse: 206.5180\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 63.4730 - mae: 6.0718 - mse: 63.4730 - val_loss: 215.4352 - val_mae: 10.7349 - val_mse: 215.4352\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 64.4909 - mae: 6.1226 - mse: 64.4909 - val_loss: 211.7318 - val_mae: 10.5075 - val_mse: 211.7318\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 64.0662 - mae: 6.0650 - mse: 64.0662 - val_loss: 206.3816 - val_mae: 10.2765 - val_mse: 206.3816\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 63.0059 - mae: 6.0670 - mse: 63.0059 - val_loss: 216.8810 - val_mae: 10.6878 - val_mse: 216.8810\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 61.7215 - mae: 5.9776 - mse: 61.7215 - val_loss: 219.2032 - val_mae: 10.7743 - val_mse: 219.2032\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 60.8183 - mae: 5.9240 - mse: 60.8183 - val_loss: 219.4277 - val_mae: 10.8124 - val_mse: 219.4277\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 61.1346 - mae: 5.9416 - mse: 61.1346 - val_loss: 217.1682 - val_mae: 10.6579 - val_mse: 217.1682\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 242.0373 - mae: 11.3548 - mse: 242.0373 - val_loss: 246.4373 - val_mae: 11.0393 - val_mse: 246.4373\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 160.1482 - mae: 8.9578 - mse: 160.1482 - val_loss: 178.7572 - val_mae: 9.2221 - val_mse: 178.7572\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.8757 - mae: 8.2646 - mse: 132.8757 - val_loss: 159.8809 - val_mae: 9.0483 - val_mse: 159.8809\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.3666 - mae: 8.2279 - mse: 128.3666 - val_loss: 158.2437 - val_mae: 9.0656 - val_mse: 158.2437\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.8588 - mae: 8.2113 - mse: 126.8588 - val_loss: 158.5294 - val_mae: 9.2304 - val_mse: 158.5294\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.0256 - mae: 8.2195 - mse: 126.0256 - val_loss: 158.6018 - val_mae: 9.1543 - val_mse: 158.6018\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.7529 - mae: 8.1600 - mse: 124.7529 - val_loss: 160.4242 - val_mae: 9.2666 - val_mse: 160.4242\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.9003 - mae: 8.1616 - mse: 123.9003 - val_loss: 158.4507 - val_mae: 9.0437 - val_mse: 158.4507\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.6395 - mae: 8.1296 - mse: 122.6395 - val_loss: 158.9262 - val_mae: 9.1586 - val_mse: 158.9262\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.5247 - mae: 8.1093 - mse: 121.5247 - val_loss: 159.8071 - val_mae: 9.1713 - val_mse: 159.8071\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.4981 - mae: 8.1014 - mse: 121.4981 - val_loss: 161.9303 - val_mae: 9.2527 - val_mse: 161.9303\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.7435 - mae: 8.0922 - mse: 120.7435 - val_loss: 158.3829 - val_mae: 9.0662 - val_mse: 158.3829\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.6893 - mae: 8.0410 - mse: 119.6893 - val_loss: 160.2796 - val_mae: 9.1443 - val_mse: 160.2796\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.4391 - mae: 8.0105 - mse: 119.4391 - val_loss: 160.1167 - val_mae: 9.1332 - val_mse: 160.1167\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.8408 - mae: 8.0029 - mse: 118.8408 - val_loss: 161.0244 - val_mae: 9.2206 - val_mse: 161.0244\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.9087 - mae: 7.9553 - mse: 117.9087 - val_loss: 162.3781 - val_mae: 9.3261 - val_mse: 162.3781\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.1296 - mae: 7.9822 - mse: 117.1296 - val_loss: 161.7259 - val_mae: 9.2602 - val_mse: 161.7259\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.1834 - mae: 7.8989 - mse: 116.1834 - val_loss: 160.5134 - val_mae: 9.1533 - val_mse: 160.5134\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.0991 - mae: 7.9083 - mse: 115.0991 - val_loss: 168.0449 - val_mae: 9.5833 - val_mse: 168.0449\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.5360 - mae: 7.9281 - mse: 115.5360 - val_loss: 161.8250 - val_mae: 9.2308 - val_mse: 161.8250\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.2643 - mae: 7.8709 - mse: 114.2643 - val_loss: 169.4922 - val_mae: 9.7356 - val_mse: 169.4922\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.4364 - mae: 7.8809 - mse: 114.4364 - val_loss: 166.0367 - val_mae: 9.4388 - val_mse: 166.0367\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 112.6591 - mae: 7.8290 - mse: 112.6591 - val_loss: 165.6909 - val_mae: 9.3343 - val_mse: 165.6909\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 112.6029 - mae: 7.8573 - mse: 112.6029 - val_loss: 163.6500 - val_mae: 9.3173 - val_mse: 163.6500\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.7761 - mae: 7.7760 - mse: 110.7761 - val_loss: 165.0412 - val_mae: 9.3343 - val_mse: 165.0412\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 112.1027 - mae: 7.8597 - mse: 112.1027 - val_loss: 164.3010 - val_mae: 9.4890 - val_mse: 164.3010\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.9106 - mae: 7.7366 - mse: 110.9106 - val_loss: 170.2387 - val_mae: 9.6099 - val_mse: 170.2387\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 109.9384 - mae: 7.7464 - mse: 109.9384 - val_loss: 170.8186 - val_mae: 9.7264 - val_mse: 170.8186\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 109.2164 - mae: 7.7160 - mse: 109.2164 - val_loss: 171.0473 - val_mae: 9.5367 - val_mse: 171.0473\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 109.0899 - mae: 7.7422 - mse: 109.0899 - val_loss: 170.7810 - val_mae: 9.4105 - val_mse: 170.7810\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.1435 - mae: 7.6349 - mse: 107.1435 - val_loss: 171.6095 - val_mae: 9.5726 - val_mse: 171.6095\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.8797 - mae: 7.6797 - mse: 107.8797 - val_loss: 172.0300 - val_mae: 9.5612 - val_mse: 172.0300\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.6870 - mae: 7.6637 - mse: 107.6870 - val_loss: 169.1435 - val_mae: 9.3635 - val_mse: 169.1435\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 104.9277 - mae: 7.5453 - mse: 104.9277 - val_loss: 177.1237 - val_mae: 9.8709 - val_mse: 177.1237\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 106.7385 - mae: 7.6951 - mse: 106.7385 - val_loss: 168.6396 - val_mae: 9.4047 - val_mse: 168.6396\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 105.1182 - mae: 7.5856 - mse: 105.1182 - val_loss: 172.5012 - val_mae: 9.5221 - val_mse: 172.5012\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 104.9470 - mae: 7.5600 - mse: 104.9470 - val_loss: 174.2125 - val_mae: 9.5524 - val_mse: 174.2125\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 105.1623 - mae: 7.5951 - mse: 105.1623 - val_loss: 171.1335 - val_mae: 9.4574 - val_mse: 171.1335\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 103.2318 - mae: 7.5283 - mse: 103.2318 - val_loss: 176.3948 - val_mae: 9.7292 - val_mse: 176.3948\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 103.3508 - mae: 7.5471 - mse: 103.3508 - val_loss: 176.4002 - val_mae: 9.7742 - val_mse: 176.4002\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 102.0697 - mae: 7.4925 - mse: 102.0697 - val_loss: 184.7806 - val_mae: 10.0082 - val_mse: 184.7806\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 101.9238 - mae: 7.4547 - mse: 101.9238 - val_loss: 177.1449 - val_mae: 9.7011 - val_mse: 177.1449\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 101.9117 - mae: 7.4797 - mse: 101.9117 - val_loss: 173.4280 - val_mae: 9.5398 - val_mse: 173.4280\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 100.6510 - mae: 7.4541 - mse: 100.6510 - val_loss: 176.9454 - val_mae: 9.7645 - val_mse: 176.9454\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 100.0992 - mae: 7.4688 - mse: 100.0992 - val_loss: 176.0111 - val_mae: 9.5397 - val_mse: 176.0111\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 100.8808 - mae: 7.4575 - mse: 100.8808 - val_loss: 174.4331 - val_mae: 9.7022 - val_mse: 174.4331\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 99.9922 - mae: 7.4267 - mse: 99.9922 - val_loss: 174.9632 - val_mae: 9.5614 - val_mse: 174.9632\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 98.8126 - mae: 7.3742 - mse: 98.8126 - val_loss: 177.1598 - val_mae: 9.7821 - val_mse: 177.1598\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 99.7848 - mae: 7.4090 - mse: 99.7848 - val_loss: 177.9537 - val_mae: 9.7579 - val_mse: 177.9537\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 100.5119 - mae: 7.4218 - mse: 100.5119 - val_loss: 182.5883 - val_mae: 9.9741 - val_mse: 182.5883\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 97.6670 - mae: 7.3153 - mse: 97.6670 - val_loss: 174.8232 - val_mae: 9.5389 - val_mse: 174.8232\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 97.4540 - mae: 7.3494 - mse: 97.4540 - val_loss: 177.2058 - val_mae: 9.6996 - val_mse: 177.2058\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 98.3403 - mae: 7.3094 - mse: 98.3403 - val_loss: 175.6400 - val_mae: 9.8203 - val_mse: 175.6400\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 96.0354 - mae: 7.2583 - mse: 96.0354 - val_loss: 176.0718 - val_mae: 9.6724 - val_mse: 176.0718\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 98.3544 - mae: 7.3607 - mse: 98.3544 - val_loss: 176.2180 - val_mae: 9.7503 - val_mse: 176.2180\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.5574 - mae: 7.2446 - mse: 94.5574 - val_loss: 182.5827 - val_mae: 9.9397 - val_mse: 182.5827\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 96.9257 - mae: 7.3172 - mse: 96.9257 - val_loss: 182.2078 - val_mae: 9.9582 - val_mse: 182.2078\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 97.5467 - mae: 7.3209 - mse: 97.5467 - val_loss: 176.9784 - val_mae: 9.6347 - val_mse: 176.9784\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.6346 - mae: 7.2553 - mse: 94.6346 - val_loss: 177.3991 - val_mae: 9.7715 - val_mse: 177.3991\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 95.1780 - mae: 7.2815 - mse: 95.1780 - val_loss: 184.2953 - val_mae: 10.0145 - val_mse: 184.2953\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 95.6854 - mae: 7.2622 - mse: 95.6854 - val_loss: 177.8717 - val_mae: 9.7353 - val_mse: 177.8717\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.4222 - mae: 7.2747 - mse: 94.4222 - val_loss: 180.0231 - val_mae: 9.8721 - val_mse: 180.0231\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.9239 - mae: 7.1620 - mse: 92.9239 - val_loss: 183.3459 - val_mae: 9.9181 - val_mse: 183.3459\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.5195 - mae: 7.1845 - mse: 92.5195 - val_loss: 180.0546 - val_mae: 9.7850 - val_mse: 180.0546\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.6816 - mae: 7.1764 - mse: 92.6816 - val_loss: 182.9445 - val_mae: 9.9394 - val_mse: 182.9445\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 93.1814 - mae: 7.1552 - mse: 93.1814 - val_loss: 181.0499 - val_mae: 9.8552 - val_mse: 181.0499\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 93.0031 - mae: 7.2320 - mse: 93.0031 - val_loss: 186.1455 - val_mae: 9.9913 - val_mse: 186.1455\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.0719 - mae: 7.1111 - mse: 91.0719 - val_loss: 182.7243 - val_mae: 9.7968 - val_mse: 182.7243\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.2112 - mae: 7.1278 - mse: 92.2112 - val_loss: 179.5204 - val_mae: 9.8567 - val_mse: 179.5204\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.2644 - mae: 7.0704 - mse: 91.2644 - val_loss: 187.2346 - val_mae: 10.0385 - val_mse: 187.2346\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.1414 - mae: 7.1859 - mse: 92.1414 - val_loss: 181.2107 - val_mae: 9.7120 - val_mse: 181.2107\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 90.9806 - mae: 7.1107 - mse: 90.9806 - val_loss: 182.9898 - val_mae: 9.8105 - val_mse: 182.9898\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.3351 - mae: 7.1151 - mse: 91.3351 - val_loss: 180.5033 - val_mae: 9.7627 - val_mse: 180.5033\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.4980 - mae: 7.1309 - mse: 91.4980 - val_loss: 189.1539 - val_mae: 10.0402 - val_mse: 189.1539\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 90.9869 - mae: 7.1269 - mse: 90.9869 - val_loss: 187.4343 - val_mae: 10.0809 - val_mse: 187.4343\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 89.4586 - mae: 7.0627 - mse: 89.4586 - val_loss: 186.2622 - val_mae: 9.8539 - val_mse: 186.2622\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.9407 - mae: 7.0167 - mse: 87.9407 - val_loss: 184.1866 - val_mae: 9.8850 - val_mse: 184.1866\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.4971 - mae: 7.1293 - mse: 91.4971 - val_loss: 189.4949 - val_mae: 10.1703 - val_mse: 189.4949\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 89.9572 - mae: 7.0508 - mse: 89.9572 - val_loss: 191.3023 - val_mae: 10.1316 - val_mse: 191.3023\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.6369 - mae: 6.9801 - mse: 88.6369 - val_loss: 187.5120 - val_mae: 9.9100 - val_mse: 187.5120\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.1512 - mae: 7.0638 - mse: 88.1512 - val_loss: 187.1875 - val_mae: 9.8168 - val_mse: 187.1875\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 89.2141 - mae: 7.0752 - mse: 89.2141 - val_loss: 188.4303 - val_mae: 10.0156 - val_mse: 188.4303\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.6316 - mae: 7.0075 - mse: 87.6316 - val_loss: 189.6194 - val_mae: 9.9803 - val_mse: 189.6194\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.9427 - mae: 6.9833 - mse: 87.9427 - val_loss: 188.1437 - val_mae: 9.9523 - val_mse: 188.1437\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 89.7088 - mae: 7.0916 - mse: 89.7088 - val_loss: 188.5234 - val_mae: 10.0271 - val_mse: 188.5234\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.1086 - mae: 7.0210 - mse: 88.1086 - val_loss: 187.0307 - val_mae: 9.8966 - val_mse: 187.0307\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.7747 - mae: 6.9517 - mse: 86.7747 - val_loss: 185.5566 - val_mae: 9.8382 - val_mse: 185.5566\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.6186 - mae: 6.9951 - mse: 87.6186 - val_loss: 188.9018 - val_mae: 9.9172 - val_mse: 188.9018\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 85.9942 - mae: 6.9486 - mse: 85.9942 - val_loss: 188.9116 - val_mae: 10.0755 - val_mse: 188.9116\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.6132 - mae: 6.9626 - mse: 86.6132 - val_loss: 189.3700 - val_mae: 9.9386 - val_mse: 189.3700\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 85.4168 - mae: 6.9477 - mse: 85.4168 - val_loss: 191.0650 - val_mae: 9.9764 - val_mse: 191.0650\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.3296 - mae: 7.0237 - mse: 87.3296 - val_loss: 190.2183 - val_mae: 10.0404 - val_mse: 190.2183\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.5418 - mae: 6.9403 - mse: 86.5418 - val_loss: 192.5490 - val_mae: 10.2035 - val_mse: 192.5490\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 82.8850 - mae: 6.8328 - mse: 82.8850 - val_loss: 189.2639 - val_mae: 9.9955 - val_mse: 189.2639\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 85.9607 - mae: 6.9481 - mse: 85.9607 - val_loss: 190.1934 - val_mae: 10.0852 - val_mse: 190.1934\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.7599 - mae: 6.8357 - mse: 83.7599 - val_loss: 186.7269 - val_mae: 9.9118 - val_mse: 186.7269\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 82.8641 - mae: 6.8114 - mse: 82.8641 - val_loss: 192.7150 - val_mae: 10.1215 - val_mse: 192.7150\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 84.7158 - mae: 6.8963 - mse: 84.7158 - val_loss: 190.1366 - val_mae: 10.0183 - val_mse: 190.1366\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 84.7684 - mae: 6.9069 - mse: 84.7684 - val_loss: 193.4455 - val_mae: 10.2209 - val_mse: 193.4455\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.7811 - mae: 7.0023 - mse: 86.7811 - val_loss: 189.1355 - val_mae: 10.1574 - val_mse: 189.1355\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 174.7156 - mae: 9.6634 - mse: 174.6998 - val_loss: 168.5984 - val_mae: 9.0498 - val_mse: 168.5824\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 147.2442 - mae: 8.8319 - mse: 147.2282 - val_loss: 165.5106 - val_mae: 8.9875 - val_mse: 165.4946\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 143.2845 - mae: 8.7454 - mse: 143.2686 - val_loss: 163.0781 - val_mae: 8.9583 - val_mse: 163.0622\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 142.2874 - mae: 8.6946 - mse: 142.2715 - val_loss: 160.5559 - val_mae: 9.0276 - val_mse: 160.5401\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 142.0632 - mae: 8.7178 - mse: 142.0474 - val_loss: 163.8413 - val_mae: 8.9314 - val_mse: 163.8255\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 139.4315 - mae: 8.5952 - mse: 139.4157 - val_loss: 160.6369 - val_mae: 8.9770 - val_mse: 160.6211\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 138.5757 - mae: 8.5600 - mse: 138.5600 - val_loss: 159.5378 - val_mae: 9.0087 - val_mse: 159.5220\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 139.0403 - mae: 8.6191 - mse: 139.0247 - val_loss: 161.1316 - val_mae: 8.9350 - val_mse: 161.1161\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.8118 - mae: 8.5608 - mse: 137.7962 - val_loss: 158.6681 - val_mae: 8.9882 - val_mse: 158.6525\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 136.9715 - mae: 8.5364 - mse: 136.9560 - val_loss: 158.8181 - val_mae: 8.9745 - val_mse: 158.8025\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 137.0998 - mae: 8.5453 - mse: 137.0842 - val_loss: 158.5696 - val_mae: 9.0140 - val_mse: 158.5540\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.3829 - mae: 8.5218 - mse: 137.3674 - val_loss: 160.0363 - val_mae: 8.9104 - val_mse: 160.0209\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 136.0529 - mae: 8.5174 - mse: 136.0375 - val_loss: 158.4910 - val_mae: 8.9364 - val_mse: 158.4755\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 136.0668 - mae: 8.4913 - mse: 136.0513 - val_loss: 159.8270 - val_mae: 8.8976 - val_mse: 159.8116\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 136.1658 - mae: 8.4827 - mse: 136.1503 - val_loss: 159.2114 - val_mae: 8.8751 - val_mse: 159.1960\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 135.9449 - mae: 8.5106 - mse: 135.9295 - val_loss: 159.1120 - val_mae: 8.9194 - val_mse: 159.0966\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 135.1823 - mae: 8.4859 - mse: 135.1669 - val_loss: 157.7920 - val_mae: 8.9766 - val_mse: 157.7766\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.8239 - mae: 8.4002 - mse: 133.8085 - val_loss: 158.5611 - val_mae: 8.9147 - val_mse: 158.5457\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 135.7856 - mae: 8.4469 - mse: 135.7702 - val_loss: 157.6140 - val_mae: 8.9234 - val_mse: 157.5986\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.8899 - mae: 8.4102 - mse: 133.8746 - val_loss: 158.4642 - val_mae: 8.8414 - val_mse: 158.4488\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.9885 - mae: 8.4405 - mse: 134.9731 - val_loss: 157.2645 - val_mae: 8.9120 - val_mse: 157.2491\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.3973 - mae: 8.4438 - mse: 134.3819 - val_loss: 157.8028 - val_mae: 8.8895 - val_mse: 157.7875\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.3457 - mae: 8.4165 - mse: 134.3304 - val_loss: 156.9019 - val_mae: 8.9121 - val_mse: 156.8866\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.0216 - mae: 8.4399 - mse: 134.0063 - val_loss: 159.7006 - val_mae: 8.8548 - val_mse: 159.6853\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.8051 - mae: 8.4486 - mse: 134.7898 - val_loss: 157.7982 - val_mae: 8.9053 - val_mse: 157.7828\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.6203 - mae: 8.3853 - mse: 133.6048 - val_loss: 157.6686 - val_mae: 8.9048 - val_mse: 157.6532\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.0816 - mae: 8.4229 - mse: 134.0661 - val_loss: 159.3957 - val_mae: 8.8205 - val_mse: 159.3802\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.6210 - mae: 8.3733 - mse: 132.6056 - val_loss: 158.2026 - val_mae: 8.8696 - val_mse: 158.1873\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.5046 - mae: 8.3815 - mse: 131.4891 - val_loss: 158.7150 - val_mae: 8.8746 - val_mse: 158.6995\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.9350 - mae: 8.3481 - mse: 131.9194 - val_loss: 158.4245 - val_mae: 8.8730 - val_mse: 158.4090\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.7795 - mae: 8.4322 - mse: 133.7639 - val_loss: 157.4674 - val_mae: 8.8845 - val_mse: 157.4518\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.8190 - mae: 8.3546 - mse: 132.8034 - val_loss: 157.6581 - val_mae: 8.8886 - val_mse: 157.6425\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.2816 - mae: 8.3496 - mse: 131.2660 - val_loss: 158.8893 - val_mae: 8.8510 - val_mse: 158.8736\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.2948 - mae: 8.3486 - mse: 132.2790 - val_loss: 157.4124 - val_mae: 8.8983 - val_mse: 157.3967\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.7429 - mae: 8.3478 - mse: 131.7271 - val_loss: 157.5016 - val_mae: 8.8918 - val_mse: 157.4857\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.8026 - mae: 8.3821 - mse: 131.7867 - val_loss: 160.1493 - val_mae: 8.8549 - val_mse: 160.1333\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.9514 - mae: 8.3559 - mse: 131.9354 - val_loss: 156.8583 - val_mae: 8.8754 - val_mse: 156.8423\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.8931 - mae: 8.3504 - mse: 131.8770 - val_loss: 156.6515 - val_mae: 8.9510 - val_mse: 156.6355\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.1029 - mae: 8.3498 - mse: 132.0868 - val_loss: 158.0612 - val_mae: 8.8849 - val_mse: 158.0451\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.0607 - mae: 8.3710 - mse: 132.0445 - val_loss: 157.5531 - val_mae: 8.8927 - val_mse: 157.5370\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.4671 - mae: 8.3158 - mse: 130.4509 - val_loss: 157.1105 - val_mae: 8.9166 - val_mse: 157.0942\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.3562 - mae: 8.3558 - mse: 131.3399 - val_loss: 157.8192 - val_mae: 8.9190 - val_mse: 157.8027\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.9556 - mae: 8.4014 - mse: 131.9391 - val_loss: 158.5682 - val_mae: 8.8784 - val_mse: 158.5517\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.1012 - mae: 8.3583 - mse: 131.0847 - val_loss: 158.1293 - val_mae: 8.8872 - val_mse: 158.1127\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.5074 - mae: 8.3350 - mse: 131.4907 - val_loss: 157.0419 - val_mae: 8.8854 - val_mse: 157.0251\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.0380 - mae: 8.2955 - mse: 130.0211 - val_loss: 158.8387 - val_mae: 8.8589 - val_mse: 158.8219\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.1868 - mae: 8.3419 - mse: 131.1699 - val_loss: 156.8753 - val_mae: 8.9416 - val_mse: 156.8582\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.7823 - mae: 8.3666 - mse: 130.7652 - val_loss: 158.2542 - val_mae: 8.8828 - val_mse: 158.2370\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.6416 - mae: 8.3602 - mse: 131.6244 - val_loss: 159.2248 - val_mae: 8.8510 - val_mse: 159.2076\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.8732 - mae: 8.3267 - mse: 129.8558 - val_loss: 158.1367 - val_mae: 8.8725 - val_mse: 158.1193\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.5594 - mae: 8.2939 - mse: 129.5419 - val_loss: 159.9581 - val_mae: 8.8630 - val_mse: 159.9405\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.1180 - mae: 8.2995 - mse: 130.1004 - val_loss: 158.9720 - val_mae: 8.8673 - val_mse: 158.9543\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.4477 - mae: 8.3252 - mse: 129.4299 - val_loss: 158.8516 - val_mae: 8.8685 - val_mse: 158.8337\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.7266 - mae: 8.3210 - mse: 129.7086 - val_loss: 158.2054 - val_mae: 8.8665 - val_mse: 158.1873\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.8969 - mae: 8.3368 - mse: 130.8787 - val_loss: 158.1694 - val_mae: 8.8591 - val_mse: 158.1511\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.8905 - mae: 8.3314 - mse: 129.8722 - val_loss: 158.5454 - val_mae: 8.8667 - val_mse: 158.5270\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.4301 - mae: 8.2849 - mse: 129.4115 - val_loss: 159.5139 - val_mae: 8.8504 - val_mse: 159.4953\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.9547 - mae: 8.2848 - mse: 128.9359 - val_loss: 158.0250 - val_mae: 8.8958 - val_mse: 158.0062\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.0432 - mae: 8.3465 - mse: 130.0243 - val_loss: 159.0756 - val_mae: 8.9077 - val_mse: 159.0567\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.9490 - mae: 8.3309 - mse: 129.9299 - val_loss: 158.7878 - val_mae: 8.8932 - val_mse: 158.7687\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.2122 - mae: 8.3414 - mse: 129.1930 - val_loss: 158.4675 - val_mae: 8.8792 - val_mse: 158.4483\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.5147 - mae: 8.3161 - mse: 130.4955 - val_loss: 158.7070 - val_mae: 8.8885 - val_mse: 158.6877\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.8631 - mae: 8.3385 - mse: 128.8438 - val_loss: 158.9036 - val_mae: 8.8897 - val_mse: 158.8841\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.5172 - mae: 8.3020 - mse: 129.4977 - val_loss: 159.2972 - val_mae: 8.8775 - val_mse: 159.2776\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.4766 - mae: 8.2937 - mse: 128.4569 - val_loss: 159.0228 - val_mae: 8.8851 - val_mse: 159.0029\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.5824 - mae: 8.2827 - mse: 128.5624 - val_loss: 159.2309 - val_mae: 8.8892 - val_mse: 159.2108\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.1849 - mae: 8.3466 - mse: 129.1648 - val_loss: 158.6570 - val_mae: 8.8938 - val_mse: 158.6367\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.3017 - mae: 8.2818 - mse: 128.2812 - val_loss: 159.3478 - val_mae: 8.8789 - val_mse: 159.3273\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.4493 - mae: 8.2570 - mse: 127.4287 - val_loss: 159.2193 - val_mae: 8.8883 - val_mse: 159.1985\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.7718 - mae: 8.2986 - mse: 127.7509 - val_loss: 158.3557 - val_mae: 8.8963 - val_mse: 158.3347\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.5030 - mae: 8.2745 - mse: 128.4819 - val_loss: 158.0641 - val_mae: 8.9370 - val_mse: 158.0430\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.1699 - mae: 8.3041 - mse: 128.1486 - val_loss: 159.8725 - val_mae: 8.8805 - val_mse: 159.8512\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.2952 - mae: 8.2824 - mse: 128.2738 - val_loss: 158.4978 - val_mae: 8.8957 - val_mse: 158.4761\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.7025 - mae: 8.2846 - mse: 127.6806 - val_loss: 158.6376 - val_mae: 8.8738 - val_mse: 158.6157\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.7103 - mae: 8.2664 - mse: 126.6883 - val_loss: 158.6240 - val_mae: 8.8796 - val_mse: 158.6018\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.9235 - mae: 8.2804 - mse: 126.9013 - val_loss: 158.8382 - val_mae: 8.9269 - val_mse: 158.8159\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.2494 - mae: 8.2662 - mse: 127.2269 - val_loss: 158.2885 - val_mae: 8.9227 - val_mse: 158.2658\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.5941 - mae: 8.2965 - mse: 127.5713 - val_loss: 159.9410 - val_mae: 8.9209 - val_mse: 159.9182\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.5692 - mae: 8.2470 - mse: 127.5462 - val_loss: 157.6232 - val_mae: 8.9108 - val_mse: 157.6001\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.2261 - mae: 8.2602 - mse: 126.2029 - val_loss: 159.0437 - val_mae: 8.8837 - val_mse: 159.0204\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.7402 - mae: 8.2830 - mse: 128.7168 - val_loss: 158.2966 - val_mae: 8.8984 - val_mse: 158.2732\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.9147 - mae: 8.2520 - mse: 126.8912 - val_loss: 158.9614 - val_mae: 8.8890 - val_mse: 158.9377\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.5496 - mae: 8.2319 - mse: 126.5258 - val_loss: 161.0369 - val_mae: 8.8982 - val_mse: 161.0130\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.6174 - mae: 8.2525 - mse: 127.5932 - val_loss: 159.4514 - val_mae: 8.8953 - val_mse: 159.4272\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.9539 - mae: 8.2944 - mse: 127.9296 - val_loss: 157.7726 - val_mae: 8.8934 - val_mse: 157.7482\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.2267 - mae: 8.2696 - mse: 126.2021 - val_loss: 159.8555 - val_mae: 8.8764 - val_mse: 159.8308\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.2650 - mae: 8.2405 - mse: 126.2402 - val_loss: 157.8467 - val_mae: 8.8755 - val_mse: 157.8216\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.0047 - mae: 8.2305 - mse: 124.9794 - val_loss: 158.6983 - val_mae: 8.8812 - val_mse: 158.6729\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.5217 - mae: 8.2727 - mse: 126.4963 - val_loss: 159.3621 - val_mae: 8.8827 - val_mse: 159.3365\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.3719 - mae: 8.2394 - mse: 126.3462 - val_loss: 159.0424 - val_mae: 8.8772 - val_mse: 159.0166\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.2954 - mae: 8.1867 - mse: 125.2694 - val_loss: 158.6100 - val_mae: 8.8855 - val_mse: 158.5838\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.6900 - mae: 8.2169 - mse: 125.6637 - val_loss: 158.9000 - val_mae: 8.8745 - val_mse: 158.8736\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.7722 - mae: 8.2259 - mse: 127.7458 - val_loss: 159.3380 - val_mae: 8.8467 - val_mse: 159.3116\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.1473 - mae: 8.2568 - mse: 127.1207 - val_loss: 158.8084 - val_mae: 8.8819 - val_mse: 158.7817\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.1387 - mae: 8.2254 - mse: 126.1120 - val_loss: 158.8883 - val_mae: 8.8826 - val_mse: 158.8613\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.8869 - mae: 8.2478 - mse: 126.8598 - val_loss: 158.9878 - val_mae: 8.9021 - val_mse: 158.9605\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.8123 - mae: 8.2243 - mse: 125.7849 - val_loss: 158.4956 - val_mae: 8.9078 - val_mse: 158.4682\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.9238 - mae: 8.1939 - mse: 124.8962 - val_loss: 160.3551 - val_mae: 8.8642 - val_mse: 160.3273\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.4349 - mae: 8.2026 - mse: 125.4070 - val_loss: 159.3370 - val_mae: 8.8937 - val_mse: 159.3090\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.2437 - mae: 8.2023 - mse: 124.2156 - val_loss: 158.5168 - val_mae: 8.9059 - val_mse: 158.4884\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 5.8009, Train MSE: 58.3097\n",
      "Val   MAE: 10.6579, Val   MSE: 217.1682\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8803, Train MSE: 60.1047\n",
      "Val   MAE: 10.1574, Val   MSE: 189.1355\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.4787, Train MSE: 103.1420\n",
      "Val   MAE: 8.9109, Val   MSE: 159.6969\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 167.2787 - mae: 9.3436 - mse: 167.2787 - val_loss: 159.3064 - val_mae: 9.0306 - val_mse: 159.3064\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.1004 - mae: 8.3719 - mse: 133.1004 - val_loss: 160.1475 - val_mae: 9.0318 - val_mse: 160.1475\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.0404 - mae: 8.3340 - mse: 131.0404 - val_loss: 160.2778 - val_mae: 8.9844 - val_mse: 160.2778\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.7288 - mae: 8.2674 - mse: 129.7288 - val_loss: 159.9987 - val_mae: 9.2060 - val_mse: 159.9987\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.5721 - mae: 8.2498 - mse: 128.5721 - val_loss: 159.5683 - val_mae: 9.0973 - val_mse: 159.5683\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.3426 - mae: 8.2004 - mse: 127.3426 - val_loss: 162.7677 - val_mae: 9.4794 - val_mse: 162.7677\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.9455 - mae: 8.1970 - mse: 126.9455 - val_loss: 160.7518 - val_mae: 9.1385 - val_mse: 160.7518\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.1874 - mae: 8.1896 - mse: 126.1874 - val_loss: 161.9267 - val_mae: 9.2370 - val_mse: 161.9267\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.9824 - mae: 8.1292 - mse: 124.9824 - val_loss: 162.7209 - val_mae: 9.3769 - val_mse: 162.7209\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 123.3685 - mae: 8.0921 - mse: 123.3685 - val_loss: 163.6905 - val_mae: 9.1192 - val_mse: 163.6905\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.5351 - mae: 8.0388 - mse: 122.5351 - val_loss: 161.7636 - val_mae: 9.2579 - val_mse: 161.7636\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 121.9053 - mae: 8.0388 - mse: 121.9053 - val_loss: 163.4239 - val_mae: 9.3734 - val_mse: 163.4239\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 120.5482 - mae: 7.9899 - mse: 120.5482 - val_loss: 164.5313 - val_mae: 9.4357 - val_mse: 164.5313\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.7122 - mae: 7.9734 - mse: 119.7122 - val_loss: 166.0693 - val_mae: 9.3095 - val_mse: 166.0693\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.9676 - mae: 7.9033 - mse: 117.9676 - val_loss: 165.9491 - val_mae: 9.1823 - val_mse: 165.9491\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 116.6217 - mae: 7.8479 - mse: 116.6217 - val_loss: 164.9948 - val_mae: 9.5912 - val_mse: 164.9948\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 114.9197 - mae: 7.8009 - mse: 114.9197 - val_loss: 167.1044 - val_mae: 9.3999 - val_mse: 167.1044\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 114.1465 - mae: 7.7675 - mse: 114.1465 - val_loss: 166.2478 - val_mae: 9.4446 - val_mse: 166.2478\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.4337 - mae: 7.7184 - mse: 113.4337 - val_loss: 166.6628 - val_mae: 9.3576 - val_mse: 166.6628\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 111.1424 - mae: 7.6556 - mse: 111.1424 - val_loss: 167.8576 - val_mae: 9.5846 - val_mse: 167.8576\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 109.6076 - mae: 7.6111 - mse: 109.6076 - val_loss: 170.0452 - val_mae: 9.2578 - val_mse: 170.0452\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 109.1170 - mae: 7.6026 - mse: 109.1170 - val_loss: 170.1055 - val_mae: 9.3676 - val_mse: 170.1055\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 107.2179 - mae: 7.5228 - mse: 107.2179 - val_loss: 172.7596 - val_mae: 9.7534 - val_mse: 172.7596\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 106.0833 - mae: 7.5068 - mse: 106.0833 - val_loss: 173.3359 - val_mae: 9.3571 - val_mse: 173.3359\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 104.6834 - mae: 7.4362 - mse: 104.6834 - val_loss: 173.7698 - val_mae: 9.4845 - val_mse: 173.7698\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 103.9780 - mae: 7.4279 - mse: 103.9780 - val_loss: 173.4175 - val_mae: 9.7031 - val_mse: 173.4175\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 101.9624 - mae: 7.3377 - mse: 101.9624 - val_loss: 174.6817 - val_mae: 9.6694 - val_mse: 174.6817\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 100.1980 - mae: 7.2646 - mse: 100.1980 - val_loss: 178.3414 - val_mae: 9.7363 - val_mse: 178.3414\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 98.8057 - mae: 7.2446 - mse: 98.8057 - val_loss: 176.8455 - val_mae: 9.5985 - val_mse: 176.8455\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 98.2035 - mae: 7.1768 - mse: 98.2035 - val_loss: 179.7961 - val_mae: 9.9617 - val_mse: 179.7961\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 96.3465 - mae: 7.1361 - mse: 96.3465 - val_loss: 177.9331 - val_mae: 9.6237 - val_mse: 177.9331\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 94.8974 - mae: 7.0767 - mse: 94.8974 - val_loss: 178.2032 - val_mae: 9.5556 - val_mse: 178.2032\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 93.5998 - mae: 7.0151 - mse: 93.5998 - val_loss: 184.6247 - val_mae: 9.8633 - val_mse: 184.6247\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 92.3097 - mae: 6.9685 - mse: 92.3097 - val_loss: 186.0424 - val_mae: 10.0652 - val_mse: 186.0424\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 92.0030 - mae: 6.9773 - mse: 92.0030 - val_loss: 183.3791 - val_mae: 9.8872 - val_mse: 183.3791\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 91.4307 - mae: 6.9299 - mse: 91.4307 - val_loss: 185.7831 - val_mae: 9.9460 - val_mse: 185.7831\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 89.8320 - mae: 6.8616 - mse: 89.8320 - val_loss: 188.7256 - val_mae: 10.0417 - val_mse: 188.7256\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 88.2555 - mae: 6.8025 - mse: 88.2555 - val_loss: 191.8211 - val_mae: 9.9682 - val_mse: 191.8211\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 87.9977 - mae: 6.8025 - mse: 87.9977 - val_loss: 187.9778 - val_mae: 9.8521 - val_mse: 187.9778\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 87.0255 - mae: 6.7621 - mse: 87.0255 - val_loss: 194.3708 - val_mae: 9.9625 - val_mse: 194.3708\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 85.8539 - mae: 6.7125 - mse: 85.8539 - val_loss: 194.9867 - val_mae: 10.1499 - val_mse: 194.9867\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 84.6570 - mae: 6.6440 - mse: 84.6570 - val_loss: 193.8444 - val_mae: 10.0528 - val_mse: 193.8444\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 83.4664 - mae: 6.6065 - mse: 83.4664 - val_loss: 194.8392 - val_mae: 10.1117 - val_mse: 194.8392\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 83.1993 - mae: 6.6082 - mse: 83.1993 - val_loss: 197.0727 - val_mae: 9.9428 - val_mse: 197.0727\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 81.4268 - mae: 6.5171 - mse: 81.4268 - val_loss: 197.2276 - val_mae: 10.2156 - val_mse: 197.2276\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 82.5304 - mae: 6.5723 - mse: 82.5304 - val_loss: 198.5606 - val_mae: 10.2698 - val_mse: 198.5606\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 80.7286 - mae: 6.5272 - mse: 80.7286 - val_loss: 196.1881 - val_mae: 9.9720 - val_mse: 196.1881\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 78.7522 - mae: 6.4353 - mse: 78.7522 - val_loss: 196.2262 - val_mae: 9.9403 - val_mse: 196.2262\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 77.3290 - mae: 6.3456 - mse: 77.3290 - val_loss: 197.4536 - val_mae: 10.1056 - val_mse: 197.4536\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 77.2995 - mae: 6.3744 - mse: 77.2995 - val_loss: 201.9763 - val_mae: 10.2037 - val_mse: 201.9763\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 77.9852 - mae: 6.4152 - mse: 77.9852 - val_loss: 199.7942 - val_mae: 10.1705 - val_mse: 199.7942\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 76.0505 - mae: 6.3184 - mse: 76.0505 - val_loss: 208.7911 - val_mae: 10.5014 - val_mse: 208.7911\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 75.7613 - mae: 6.2967 - mse: 75.7613 - val_loss: 205.4158 - val_mae: 10.4543 - val_mse: 205.4158\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 74.7439 - mae: 6.2794 - mse: 74.7439 - val_loss: 215.1801 - val_mae: 10.8895 - val_mse: 215.1801\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 74.3590 - mae: 6.2610 - mse: 74.3590 - val_loss: 206.5057 - val_mae: 10.4099 - val_mse: 206.5057\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 73.5126 - mae: 6.2381 - mse: 73.5126 - val_loss: 208.9133 - val_mae: 10.3121 - val_mse: 208.9133\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 72.4746 - mae: 6.1382 - mse: 72.4746 - val_loss: 216.8517 - val_mae: 10.8228 - val_mse: 216.8517\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 71.8579 - mae: 6.1248 - mse: 71.8579 - val_loss: 211.3927 - val_mae: 10.4410 - val_mse: 211.3927\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 70.5148 - mae: 6.0752 - mse: 70.5148 - val_loss: 211.0354 - val_mae: 10.5838 - val_mse: 211.0354\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 69.4611 - mae: 6.0480 - mse: 69.4611 - val_loss: 210.7150 - val_mae: 10.4576 - val_mse: 210.7150\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 68.5007 - mae: 5.9586 - mse: 68.5007 - val_loss: 213.6153 - val_mae: 10.5271 - val_mse: 213.6153\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 67.8838 - mae: 5.9764 - mse: 67.8838 - val_loss: 216.0592 - val_mae: 10.5160 - val_mse: 216.0592\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 67.7448 - mae: 6.0038 - mse: 67.7448 - val_loss: 214.5337 - val_mae: 10.6501 - val_mse: 214.5337\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 67.1352 - mae: 5.9392 - mse: 67.1352 - val_loss: 217.6281 - val_mae: 10.7584 - val_mse: 217.6281\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 67.3989 - mae: 5.9584 - mse: 67.3989 - val_loss: 224.6804 - val_mae: 10.9449 - val_mse: 224.6804\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 66.4386 - mae: 5.9201 - mse: 66.4386 - val_loss: 209.3524 - val_mae: 10.4188 - val_mse: 209.3524\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 64.2747 - mae: 5.7945 - mse: 64.2747 - val_loss: 221.2670 - val_mae: 10.5705 - val_mse: 221.2670\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 64.8349 - mae: 5.8651 - mse: 64.8349 - val_loss: 216.3232 - val_mae: 10.6742 - val_mse: 216.3232\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 63.6163 - mae: 5.7825 - mse: 63.6163 - val_loss: 216.1951 - val_mae: 10.5261 - val_mse: 216.1951\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 63.6695 - mae: 5.8201 - mse: 63.6695 - val_loss: 225.3611 - val_mae: 10.8843 - val_mse: 225.3611\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 62.6617 - mae: 5.7999 - mse: 62.6617 - val_loss: 229.0216 - val_mae: 10.9063 - val_mse: 229.0216\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 62.4033 - mae: 5.7705 - mse: 62.4033 - val_loss: 221.1657 - val_mae: 10.6698 - val_mse: 221.1657\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 63.0327 - mae: 5.7696 - mse: 63.0327 - val_loss: 215.6049 - val_mae: 10.5198 - val_mse: 215.6049\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 60.7871 - mae: 5.6985 - mse: 60.7871 - val_loss: 218.3108 - val_mae: 10.6840 - val_mse: 218.3108\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 60.5505 - mae: 5.6586 - mse: 60.5505 - val_loss: 224.2840 - val_mae: 10.7511 - val_mse: 224.2840\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 59.2847 - mae: 5.6126 - mse: 59.2847 - val_loss: 221.2395 - val_mae: 10.8172 - val_mse: 221.2395\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 58.2881 - mae: 5.5842 - mse: 58.2881 - val_loss: 222.0803 - val_mae: 10.5716 - val_mse: 222.0803\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 57.0011 - mae: 5.4799 - mse: 57.0011 - val_loss: 218.6350 - val_mae: 10.5778 - val_mse: 218.6350\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 57.6528 - mae: 5.5222 - mse: 57.6528 - val_loss: 228.4870 - val_mae: 11.0407 - val_mse: 228.4870\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 57.1214 - mae: 5.5037 - mse: 57.1214 - val_loss: 230.0878 - val_mae: 10.9296 - val_mse: 230.0878\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 57.6911 - mae: 5.5880 - mse: 57.6911 - val_loss: 226.7625 - val_mae: 10.7915 - val_mse: 226.7625\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 57.9235 - mae: 5.5672 - mse: 57.9235 - val_loss: 222.4672 - val_mae: 10.6652 - val_mse: 222.4672\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 56.6668 - mae: 5.5076 - mse: 56.6668 - val_loss: 229.9711 - val_mae: 10.8761 - val_mse: 229.9711\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 55.3511 - mae: 5.4191 - mse: 55.3511 - val_loss: 226.1934 - val_mae: 10.8439 - val_mse: 226.1934\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 54.5488 - mae: 5.4041 - mse: 54.5488 - val_loss: 223.5127 - val_mae: 10.7042 - val_mse: 223.5127\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 53.8030 - mae: 5.3593 - mse: 53.8030 - val_loss: 233.7807 - val_mae: 10.8216 - val_mse: 233.7807\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 53.1069 - mae: 5.2771 - mse: 53.1069 - val_loss: 232.2941 - val_mae: 10.8086 - val_mse: 232.2941\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 52.3709 - mae: 5.2962 - mse: 52.3709 - val_loss: 229.9199 - val_mae: 10.8046 - val_mse: 229.9199\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 53.5637 - mae: 5.3199 - mse: 53.5637 - val_loss: 228.1534 - val_mae: 10.8381 - val_mse: 228.1534\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 51.1823 - mae: 5.2330 - mse: 51.1823 - val_loss: 224.9227 - val_mae: 10.6471 - val_mse: 224.9227\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 49.2899 - mae: 5.1489 - mse: 49.2899 - val_loss: 230.0075 - val_mae: 10.9216 - val_mse: 230.0075\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 50.3932 - mae: 5.1944 - mse: 50.3932 - val_loss: 234.8492 - val_mae: 11.0237 - val_mse: 234.8492\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 51.3982 - mae: 5.2641 - mse: 51.3982 - val_loss: 237.1799 - val_mae: 11.0138 - val_mse: 237.1799\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 51.5376 - mae: 5.2431 - mse: 51.5376 - val_loss: 228.7542 - val_mae: 10.7626 - val_mse: 228.7542\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 50.0701 - mae: 5.1845 - mse: 50.0701 - val_loss: 244.6116 - val_mae: 11.2437 - val_mse: 244.6116\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 49.9089 - mae: 5.2233 - mse: 49.9089 - val_loss: 229.2349 - val_mae: 10.7226 - val_mse: 229.2349\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 48.1963 - mae: 5.0806 - mse: 48.1963 - val_loss: 244.6172 - val_mae: 11.1448 - val_mse: 244.6172\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 47.9693 - mae: 5.0821 - mse: 47.9693 - val_loss: 234.5308 - val_mae: 10.8614 - val_mse: 234.5308\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 47.4100 - mae: 5.0924 - mse: 47.4100 - val_loss: 237.4022 - val_mae: 10.9175 - val_mse: 237.4022\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 46.0267 - mae: 4.9595 - mse: 46.0267 - val_loss: 233.4242 - val_mae: 10.7836 - val_mse: 233.4242\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 240.3707 - mae: 11.3942 - mse: 240.3707 - val_loss: 237.2528 - val_mae: 10.7722 - val_mse: 237.2528\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 158.3267 - mae: 8.8901 - mse: 158.3267 - val_loss: 165.9028 - val_mae: 8.9582 - val_mse: 165.9028\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.7308 - mae: 8.2504 - mse: 132.7308 - val_loss: 158.1130 - val_mae: 8.9502 - val_mse: 158.1130\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.4978 - mae: 8.2007 - mse: 127.4978 - val_loss: 158.7803 - val_mae: 9.1574 - val_mse: 158.7803\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.3929 - mae: 8.2298 - mse: 127.3929 - val_loss: 160.7028 - val_mae: 9.2518 - val_mse: 160.7028\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.5267 - mae: 8.2148 - mse: 126.5267 - val_loss: 159.7784 - val_mae: 9.2539 - val_mse: 159.7784\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.1074 - mae: 8.1259 - mse: 124.1074 - val_loss: 160.5017 - val_mae: 9.2504 - val_mse: 160.5017\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.5147 - mae: 8.0914 - mse: 123.5147 - val_loss: 161.9692 - val_mae: 9.2746 - val_mse: 161.9692\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.6382 - mae: 8.0844 - mse: 122.6382 - val_loss: 159.7572 - val_mae: 9.1800 - val_mse: 159.7572\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.2731 - mae: 8.0443 - mse: 122.2731 - val_loss: 161.5504 - val_mae: 9.2936 - val_mse: 161.5504\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.5436 - mae: 7.9642 - mse: 119.5436 - val_loss: 161.3004 - val_mae: 9.2196 - val_mse: 161.3004\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.4941 - mae: 7.9938 - mse: 119.4941 - val_loss: 161.0167 - val_mae: 9.1853 - val_mse: 161.0167\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.4674 - mae: 7.9568 - mse: 118.4674 - val_loss: 164.6061 - val_mae: 9.3902 - val_mse: 164.6061\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.6365 - mae: 7.9364 - mse: 117.6365 - val_loss: 162.5852 - val_mae: 9.1816 - val_mse: 162.5852\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.9049 - mae: 7.8868 - mse: 115.9049 - val_loss: 165.5547 - val_mae: 9.5256 - val_mse: 165.5547\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.6395 - mae: 7.9095 - mse: 116.6395 - val_loss: 163.5874 - val_mae: 9.3100 - val_mse: 163.5874\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.7574 - mae: 7.8409 - mse: 114.7574 - val_loss: 165.8707 - val_mae: 9.3913 - val_mse: 165.8707\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.6709 - mae: 7.8144 - mse: 113.6709 - val_loss: 164.9719 - val_mae: 9.4163 - val_mse: 164.9719\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.2165 - mae: 7.7857 - mse: 113.2165 - val_loss: 163.6483 - val_mae: 9.1846 - val_mse: 163.6483\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 111.5923 - mae: 7.7152 - mse: 111.5923 - val_loss: 164.6759 - val_mae: 9.4692 - val_mse: 164.6759\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 111.2763 - mae: 7.7392 - mse: 111.2763 - val_loss: 167.4110 - val_mae: 9.4288 - val_mse: 167.4110\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 110.9482 - mae: 7.7320 - mse: 110.9482 - val_loss: 166.8318 - val_mae: 9.5303 - val_mse: 166.8318\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 107.8579 - mae: 7.6169 - mse: 107.8579 - val_loss: 168.5430 - val_mae: 9.3376 - val_mse: 168.5430\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 108.8979 - mae: 7.6445 - mse: 108.8979 - val_loss: 167.0799 - val_mae: 9.3523 - val_mse: 167.0799\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 106.2801 - mae: 7.5344 - mse: 106.2801 - val_loss: 169.9385 - val_mae: 9.3950 - val_mse: 169.9385\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 106.0878 - mae: 7.5486 - mse: 106.0878 - val_loss: 169.0350 - val_mae: 9.6141 - val_mse: 169.0350\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 106.5176 - mae: 7.5541 - mse: 106.5176 - val_loss: 166.3313 - val_mae: 9.3603 - val_mse: 166.3313\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 104.9612 - mae: 7.4913 - mse: 104.9612 - val_loss: 170.2966 - val_mae: 9.4309 - val_mse: 170.2966\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 104.2408 - mae: 7.4530 - mse: 104.2408 - val_loss: 169.5866 - val_mae: 9.4203 - val_mse: 169.5866\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 103.7287 - mae: 7.4645 - mse: 103.7287 - val_loss: 176.2120 - val_mae: 9.5043 - val_mse: 176.2120\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 103.3515 - mae: 7.4191 - mse: 103.3515 - val_loss: 170.9568 - val_mae: 9.6109 - val_mse: 170.9568\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 101.8122 - mae: 7.3803 - mse: 101.8122 - val_loss: 175.2404 - val_mae: 9.7962 - val_mse: 175.2404\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 100.3424 - mae: 7.3638 - mse: 100.3424 - val_loss: 171.0300 - val_mae: 9.4894 - val_mse: 171.0300\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 101.4719 - mae: 7.3618 - mse: 101.4719 - val_loss: 171.2502 - val_mae: 9.4636 - val_mse: 171.2502\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 99.7015 - mae: 7.3184 - mse: 99.7015 - val_loss: 168.8370 - val_mae: 9.3347 - val_mse: 168.8370\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 98.2136 - mae: 7.2451 - mse: 98.2136 - val_loss: 175.1012 - val_mae: 9.7330 - val_mse: 175.1012\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 99.4143 - mae: 7.2998 - mse: 99.4143 - val_loss: 172.2097 - val_mae: 9.5652 - val_mse: 172.2097\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 97.2650 - mae: 7.2263 - mse: 97.2650 - val_loss: 170.7248 - val_mae: 9.3119 - val_mse: 170.7248\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 97.7312 - mae: 7.2168 - mse: 97.7312 - val_loss: 172.3727 - val_mae: 9.5195 - val_mse: 172.3727\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 96.7402 - mae: 7.1111 - mse: 96.7402 - val_loss: 174.4565 - val_mae: 9.6631 - val_mse: 174.4565\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 95.9141 - mae: 7.1521 - mse: 95.9141 - val_loss: 175.0674 - val_mae: 9.6046 - val_mse: 175.0674\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.1846 - mae: 7.0850 - mse: 94.1846 - val_loss: 177.1850 - val_mae: 9.6514 - val_mse: 177.1850\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 94.2012 - mae: 7.1452 - mse: 94.2012 - val_loss: 179.7231 - val_mae: 9.7278 - val_mse: 179.7231\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.3638 - mae: 6.9659 - mse: 92.3638 - val_loss: 178.2695 - val_mae: 9.6395 - val_mse: 178.2695\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 92.3809 - mae: 7.0242 - mse: 92.3809 - val_loss: 179.1212 - val_mae: 9.6824 - val_mse: 179.1212\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 93.2612 - mae: 7.0555 - mse: 93.2612 - val_loss: 178.2316 - val_mae: 9.8214 - val_mse: 178.2316\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.9611 - mae: 7.0655 - mse: 91.9611 - val_loss: 182.5519 - val_mae: 9.8452 - val_mse: 182.5519\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.1588 - mae: 7.0303 - mse: 91.1588 - val_loss: 183.4556 - val_mae: 9.9131 - val_mse: 183.4556\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 91.6353 - mae: 6.9540 - mse: 91.6353 - val_loss: 181.9101 - val_mae: 9.8925 - val_mse: 181.9101\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 89.6113 - mae: 6.9350 - mse: 89.6113 - val_loss: 179.7870 - val_mae: 9.7482 - val_mse: 179.7870\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 90.8331 - mae: 6.9736 - mse: 90.8331 - val_loss: 180.2603 - val_mae: 9.8530 - val_mse: 180.2603\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 90.7380 - mae: 6.9908 - mse: 90.7380 - val_loss: 181.1455 - val_mae: 9.6794 - val_mse: 181.1455\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.7271 - mae: 6.8743 - mse: 87.7271 - val_loss: 178.4091 - val_mae: 9.6949 - val_mse: 178.4091\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.6513 - mae: 6.8851 - mse: 88.6513 - val_loss: 179.9738 - val_mae: 9.7328 - val_mse: 179.9738\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 87.3975 - mae: 6.8452 - mse: 87.3975 - val_loss: 183.8171 - val_mae: 9.8470 - val_mse: 183.8171\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.3279 - mae: 6.8555 - mse: 88.3279 - val_loss: 181.5441 - val_mae: 9.9178 - val_mse: 181.5441\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.7988 - mae: 6.8117 - mse: 86.7988 - val_loss: 186.8129 - val_mae: 9.9664 - val_mse: 186.8129\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 88.3571 - mae: 6.8564 - mse: 88.3571 - val_loss: 180.6061 - val_mae: 9.6961 - val_mse: 180.6061\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.7953 - mae: 6.7980 - mse: 86.7953 - val_loss: 184.8558 - val_mae: 9.8581 - val_mse: 184.8558\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.7655 - mae: 6.7639 - mse: 86.7655 - val_loss: 182.9875 - val_mae: 9.9323 - val_mse: 182.9875\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 84.7097 - mae: 6.7503 - mse: 84.7097 - val_loss: 186.5336 - val_mae: 9.9943 - val_mse: 186.5336\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.3980 - mae: 6.7895 - mse: 86.3980 - val_loss: 182.2756 - val_mae: 9.6985 - val_mse: 182.2756\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 84.0126 - mae: 6.7178 - mse: 84.0126 - val_loss: 188.0591 - val_mae: 9.9946 - val_mse: 188.0591\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 86.2330 - mae: 6.7798 - mse: 86.2330 - val_loss: 182.6499 - val_mae: 9.8289 - val_mse: 182.6499\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.8342 - mae: 6.7205 - mse: 83.8342 - val_loss: 184.4612 - val_mae: 9.8319 - val_mse: 184.4612\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.4918 - mae: 6.6631 - mse: 83.4918 - val_loss: 183.9844 - val_mae: 9.8000 - val_mse: 183.9844\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 85.1970 - mae: 6.7542 - mse: 85.1970 - val_loss: 181.0672 - val_mae: 9.7460 - val_mse: 181.0672\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.8029 - mae: 6.6633 - mse: 83.8029 - val_loss: 184.6990 - val_mae: 9.8322 - val_mse: 184.6990\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.7383 - mae: 6.6672 - mse: 83.7383 - val_loss: 186.9408 - val_mae: 9.8294 - val_mse: 186.9408\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 81.5940 - mae: 6.6892 - mse: 81.5940 - val_loss: 183.0953 - val_mae: 9.8210 - val_mse: 183.0953\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.1596 - mae: 6.7302 - mse: 83.1596 - val_loss: 185.8266 - val_mae: 9.9906 - val_mse: 185.8266\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.0034 - mae: 6.6274 - mse: 83.0034 - val_loss: 189.5429 - val_mae: 10.0652 - val_mse: 189.5429\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 83.6223 - mae: 6.6911 - mse: 83.6223 - val_loss: 187.1311 - val_mae: 9.8958 - val_mse: 187.1311\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 82.0935 - mae: 6.5833 - mse: 82.0935 - val_loss: 184.9980 - val_mae: 9.9245 - val_mse: 184.9980\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 81.0156 - mae: 6.5817 - mse: 81.0156 - val_loss: 191.8149 - val_mae: 10.0754 - val_mse: 191.8149\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 80.8444 - mae: 6.6003 - mse: 80.8444 - val_loss: 186.6160 - val_mae: 9.9118 - val_mse: 186.6160\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 81.3597 - mae: 6.6220 - mse: 81.3597 - val_loss: 185.5059 - val_mae: 9.8541 - val_mse: 185.5059\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 81.1583 - mae: 6.6025 - mse: 81.1583 - val_loss: 188.6654 - val_mae: 10.0602 - val_mse: 188.6654\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 79.8865 - mae: 6.4938 - mse: 79.8865 - val_loss: 184.1520 - val_mae: 9.8622 - val_mse: 184.1520\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 79.5424 - mae: 6.4827 - mse: 79.5424 - val_loss: 188.8815 - val_mae: 10.0318 - val_mse: 188.8815\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 79.1388 - mae: 6.5058 - mse: 79.1388 - val_loss: 192.4761 - val_mae: 10.2263 - val_mse: 192.4761\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 76.5187 - mae: 6.4229 - mse: 76.5187 - val_loss: 192.6115 - val_mae: 10.1009 - val_mse: 192.6115\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 79.5426 - mae: 6.5020 - mse: 79.5426 - val_loss: 193.1741 - val_mae: 10.1008 - val_mse: 193.1741\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 80.3977 - mae: 6.5795 - mse: 80.3977 - val_loss: 193.9599 - val_mae: 10.1621 - val_mse: 193.9599\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 78.2890 - mae: 6.4717 - mse: 78.2890 - val_loss: 189.7295 - val_mae: 9.9346 - val_mse: 189.7295\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 78.8190 - mae: 6.4823 - mse: 78.8190 - val_loss: 194.4093 - val_mae: 10.2797 - val_mse: 194.4093\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 77.7786 - mae: 6.4250 - mse: 77.7786 - val_loss: 192.5507 - val_mae: 10.1544 - val_mse: 192.5507\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 77.1576 - mae: 6.4334 - mse: 77.1576 - val_loss: 194.1816 - val_mae: 10.1675 - val_mse: 194.1816\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 78.4465 - mae: 6.4821 - mse: 78.4465 - val_loss: 191.5997 - val_mae: 10.0328 - val_mse: 191.5997\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 77.1573 - mae: 6.4180 - mse: 77.1573 - val_loss: 192.5696 - val_mae: 10.1349 - val_mse: 192.5696\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 77.1518 - mae: 6.4445 - mse: 77.1518 - val_loss: 190.7044 - val_mae: 9.9822 - val_mse: 190.7044\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 75.4635 - mae: 6.3733 - mse: 75.4635 - val_loss: 194.7923 - val_mae: 10.1915 - val_mse: 194.7923\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 76.2181 - mae: 6.4095 - mse: 76.2181 - val_loss: 188.3090 - val_mae: 9.9164 - val_mse: 188.3090\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 75.8561 - mae: 6.3187 - mse: 75.8561 - val_loss: 187.9435 - val_mae: 10.0015 - val_mse: 187.9435\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 75.2413 - mae: 6.3511 - mse: 75.2413 - val_loss: 188.4892 - val_mae: 9.9311 - val_mse: 188.4892\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 75.8491 - mae: 6.3523 - mse: 75.8491 - val_loss: 191.5980 - val_mae: 10.0887 - val_mse: 191.5980\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 76.4375 - mae: 6.3571 - mse: 76.4375 - val_loss: 193.0790 - val_mae: 10.2458 - val_mse: 193.0790\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 76.2174 - mae: 6.3591 - mse: 76.2174 - val_loss: 189.8318 - val_mae: 10.0752 - val_mse: 189.8318\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 73.8549 - mae: 6.2891 - mse: 73.8549 - val_loss: 192.7819 - val_mae: 10.0873 - val_mse: 192.7819\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 73.1981 - mae: 6.2581 - mse: 73.1981 - val_loss: 190.5544 - val_mae: 9.8531 - val_mse: 190.5544\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 177.5023 - mae: 9.6723 - mse: 177.4815 - val_loss: 162.8854 - val_mae: 9.0157 - val_mse: 162.8637\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 145.8315 - mae: 8.8067 - mse: 145.8097 - val_loss: 162.1194 - val_mae: 8.9744 - val_mse: 162.0976\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 145.1571 - mae: 8.7987 - mse: 145.1352 - val_loss: 159.8962 - val_mae: 8.9726 - val_mse: 159.8740\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 141.6491 - mae: 8.6667 - mse: 141.6270 - val_loss: 163.8217 - val_mae: 8.9538 - val_mse: 163.7997\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 140.1801 - mae: 8.6377 - mse: 140.1580 - val_loss: 160.3189 - val_mae: 8.9807 - val_mse: 160.2966\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 139.7638 - mae: 8.5965 - mse: 139.7417 - val_loss: 164.5835 - val_mae: 8.9550 - val_mse: 164.5614\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 138.9861 - mae: 8.5425 - mse: 138.9639 - val_loss: 162.3277 - val_mae: 8.9476 - val_mse: 162.3055\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 136.5784 - mae: 8.5053 - mse: 136.5562 - val_loss: 163.3218 - val_mae: 8.9643 - val_mse: 163.2996\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 136.7809 - mae: 8.5382 - mse: 136.7587 - val_loss: 159.5863 - val_mae: 9.0109 - val_mse: 159.5640\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 136.9179 - mae: 8.5056 - mse: 136.8956 - val_loss: 159.9762 - val_mae: 9.0125 - val_mse: 159.9539\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.8882 - mae: 8.4187 - mse: 134.8660 - val_loss: 159.6706 - val_mae: 9.0173 - val_mse: 159.6484\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.7946 - mae: 8.4803 - mse: 134.7724 - val_loss: 162.8796 - val_mae: 8.9555 - val_mse: 162.8575\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.0482 - mae: 8.4221 - mse: 134.0258 - val_loss: 161.4471 - val_mae: 8.9893 - val_mse: 161.4249\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.3497 - mae: 8.4785 - mse: 134.3274 - val_loss: 163.2262 - val_mae: 8.9758 - val_mse: 163.2040\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.7834 - mae: 8.4083 - mse: 133.7612 - val_loss: 161.2985 - val_mae: 8.9692 - val_mse: 161.2764\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.4283 - mae: 8.3915 - mse: 132.4063 - val_loss: 162.6732 - val_mae: 8.9966 - val_mse: 162.6510\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.8837 - mae: 8.4580 - mse: 134.8616 - val_loss: 160.3477 - val_mae: 9.0470 - val_mse: 160.3255\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.3015 - mae: 8.4183 - mse: 133.2794 - val_loss: 160.8252 - val_mae: 9.0296 - val_mse: 160.8032\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.0653 - mae: 8.3792 - mse: 133.0432 - val_loss: 162.9745 - val_mae: 8.9712 - val_mse: 162.9525\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.5129 - mae: 8.3520 - mse: 131.4909 - val_loss: 160.6033 - val_mae: 9.0320 - val_mse: 160.5813\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.1477 - mae: 8.3707 - mse: 132.1256 - val_loss: 160.8255 - val_mae: 9.0458 - val_mse: 160.8034\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.7948 - mae: 8.3784 - mse: 131.7727 - val_loss: 160.7415 - val_mae: 9.1244 - val_mse: 160.7193\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 133.4613 - mae: 8.4109 - mse: 133.4392 - val_loss: 161.1526 - val_mae: 9.0615 - val_mse: 161.1305\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.8289 - mae: 8.3426 - mse: 130.8067 - val_loss: 161.4987 - val_mae: 9.0380 - val_mse: 161.4766\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.2081 - mae: 8.3934 - mse: 132.1860 - val_loss: 164.5926 - val_mae: 8.9881 - val_mse: 164.5706\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.3979 - mae: 8.3458 - mse: 131.3757 - val_loss: 161.4972 - val_mae: 9.0387 - val_mse: 161.4750\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.6276 - mae: 8.3760 - mse: 131.6054 - val_loss: 163.6147 - val_mae: 8.9731 - val_mse: 163.5926\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.3683 - mae: 8.2898 - mse: 130.3461 - val_loss: 162.3647 - val_mae: 9.0074 - val_mse: 162.3425\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.7739 - mae: 8.3000 - mse: 129.7518 - val_loss: 161.8290 - val_mae: 9.0236 - val_mse: 161.8068\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.2275 - mae: 8.2977 - mse: 129.2052 - val_loss: 161.3593 - val_mae: 9.0508 - val_mse: 161.3369\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 130.0962 - mae: 8.2831 - mse: 130.0737 - val_loss: 161.2092 - val_mae: 9.0463 - val_mse: 161.1867\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.2330 - mae: 8.2892 - mse: 129.2104 - val_loss: 160.4828 - val_mae: 9.1045 - val_mse: 160.4602\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.6427 - mae: 8.2993 - mse: 129.6201 - val_loss: 161.6409 - val_mae: 9.0449 - val_mse: 161.6182\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.3638 - mae: 8.2648 - mse: 129.3411 - val_loss: 161.7004 - val_mae: 9.0170 - val_mse: 161.6776\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.5519 - mae: 8.2119 - mse: 127.5291 - val_loss: 161.1228 - val_mae: 9.0218 - val_mse: 161.0999\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.7635 - mae: 8.2816 - mse: 128.7405 - val_loss: 161.8285 - val_mae: 9.0108 - val_mse: 161.8055\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.7590 - mae: 8.2713 - mse: 128.7360 - val_loss: 163.0269 - val_mae: 9.0031 - val_mse: 163.0038\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.2088 - mae: 8.2663 - mse: 128.1857 - val_loss: 161.1349 - val_mae: 9.0293 - val_mse: 161.1118\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.5732 - mae: 8.2732 - mse: 127.5500 - val_loss: 164.1821 - val_mae: 8.9970 - val_mse: 164.1589\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.1449 - mae: 8.2079 - mse: 126.1216 - val_loss: 161.7877 - val_mae: 9.0447 - val_mse: 161.7643\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.5360 - mae: 8.2506 - mse: 128.5125 - val_loss: 162.3852 - val_mae: 9.0037 - val_mse: 162.3617\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 128.2396 - mae: 8.2655 - mse: 128.2160 - val_loss: 163.1996 - val_mae: 9.0078 - val_mse: 163.1759\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.9365 - mae: 8.1920 - mse: 126.9129 - val_loss: 161.4725 - val_mae: 9.0357 - val_mse: 161.4487\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.4085 - mae: 8.2009 - mse: 127.3847 - val_loss: 161.2914 - val_mae: 9.0238 - val_mse: 161.2676\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.7541 - mae: 8.1956 - mse: 125.7301 - val_loss: 162.5310 - val_mae: 9.0359 - val_mse: 162.5071\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.3995 - mae: 8.1613 - mse: 124.3755 - val_loss: 161.9886 - val_mae: 9.0187 - val_mse: 161.9644\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.8758 - mae: 8.2222 - mse: 126.8516 - val_loss: 162.3931 - val_mae: 9.0337 - val_mse: 162.3689\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.0076 - mae: 8.1795 - mse: 124.9832 - val_loss: 162.1979 - val_mae: 9.0369 - val_mse: 162.1734\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 127.3089 - mae: 8.2413 - mse: 127.2844 - val_loss: 162.3054 - val_mae: 9.0327 - val_mse: 162.2809\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 126.8491 - mae: 8.2311 - mse: 126.8245 - val_loss: 161.9580 - val_mae: 9.0259 - val_mse: 161.9333\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.6088 - mae: 8.1787 - mse: 125.5840 - val_loss: 160.5176 - val_mae: 9.0704 - val_mse: 160.4926\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.9004 - mae: 8.2145 - mse: 125.8754 - val_loss: 162.4480 - val_mae: 9.0188 - val_mse: 162.4230\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.7997 - mae: 8.1920 - mse: 125.7746 - val_loss: 161.7041 - val_mae: 9.0164 - val_mse: 161.6789\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.4018 - mae: 8.1498 - mse: 124.3765 - val_loss: 162.1337 - val_mae: 9.0273 - val_mse: 162.1083\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.5814 - mae: 8.1970 - mse: 125.5560 - val_loss: 162.2321 - val_mae: 9.0189 - val_mse: 162.2066\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.0055 - mae: 8.1426 - mse: 123.9799 - val_loss: 162.9046 - val_mae: 9.0255 - val_mse: 162.8789\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.6043 - mae: 8.1667 - mse: 124.5785 - val_loss: 162.8977 - val_mae: 9.0159 - val_mse: 162.8719\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.1969 - mae: 8.1648 - mse: 124.1709 - val_loss: 164.4194 - val_mae: 9.0106 - val_mse: 164.3933\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.1185 - mae: 8.1275 - mse: 122.0923 - val_loss: 161.1210 - val_mae: 9.0480 - val_mse: 161.0946\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 124.8387 - mae: 8.1791 - mse: 124.8123 - val_loss: 161.3076 - val_mae: 9.0453 - val_mse: 161.2810\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 125.5915 - mae: 8.2428 - mse: 125.5648 - val_loss: 164.2631 - val_mae: 9.0310 - val_mse: 164.2364\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.8052 - mae: 8.1117 - mse: 122.7783 - val_loss: 164.6652 - val_mae: 9.0273 - val_mse: 164.6382\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.6464 - mae: 8.1549 - mse: 123.6192 - val_loss: 162.6575 - val_mae: 9.0030 - val_mse: 162.6302\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.0700 - mae: 8.1665 - mse: 125.0427 - val_loss: 163.0821 - val_mae: 9.0443 - val_mse: 163.0547\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.6503 - mae: 8.1478 - mse: 123.6228 - val_loss: 164.0625 - val_mae: 9.0369 - val_mse: 164.0349\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.3837 - mae: 8.1139 - mse: 122.3560 - val_loss: 162.8722 - val_mae: 9.0650 - val_mse: 162.8444\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.4786 - mae: 8.1090 - mse: 122.4506 - val_loss: 164.7941 - val_mae: 9.0342 - val_mse: 164.7661\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.2988 - mae: 8.0989 - mse: 122.2705 - val_loss: 163.5030 - val_mae: 9.0254 - val_mse: 163.4748\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.9821 - mae: 8.1184 - mse: 122.9538 - val_loss: 164.3177 - val_mae: 9.0199 - val_mse: 164.2893\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 123.1520 - mae: 8.1694 - mse: 123.1233 - val_loss: 164.9044 - val_mae: 9.0445 - val_mse: 164.8757\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.5117 - mae: 8.0864 - mse: 122.4829 - val_loss: 163.9776 - val_mae: 9.0419 - val_mse: 163.9487\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.5225 - mae: 8.1040 - mse: 122.4935 - val_loss: 162.6273 - val_mae: 9.0371 - val_mse: 162.5982\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 122.0857 - mae: 8.1091 - mse: 122.0565 - val_loss: 164.1727 - val_mae: 9.0449 - val_mse: 164.1434\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.7354 - mae: 8.1003 - mse: 122.7060 - val_loss: 162.9910 - val_mae: 9.0348 - val_mse: 162.9616\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.4588 - mae: 8.0979 - mse: 121.4293 - val_loss: 166.5259 - val_mae: 9.0439 - val_mse: 166.4964\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.6759 - mae: 8.0811 - mse: 121.6462 - val_loss: 163.1425 - val_mae: 9.0559 - val_mse: 163.1128\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.9799 - mae: 8.0480 - mse: 120.9500 - val_loss: 163.5848 - val_mae: 9.0751 - val_mse: 163.5548\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.7095 - mae: 8.1384 - mse: 121.6794 - val_loss: 164.5948 - val_mae: 9.0418 - val_mse: 164.5646\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.8459 - mae: 8.0653 - mse: 120.8155 - val_loss: 163.8058 - val_mae: 9.0351 - val_mse: 163.7754\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.1084 - mae: 8.1258 - mse: 122.0778 - val_loss: 164.3783 - val_mae: 9.0621 - val_mse: 164.3477\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 121.1080 - mae: 8.0605 - mse: 121.0772 - val_loss: 164.1109 - val_mae: 9.0957 - val_mse: 164.0800\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.3633 - mae: 8.0733 - mse: 121.3323 - val_loss: 163.9016 - val_mae: 9.1043 - val_mse: 163.8704\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.6309 - mae: 8.0664 - mse: 119.5996 - val_loss: 164.1166 - val_mae: 9.1023 - val_mse: 164.0853\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.7796 - mae: 8.0956 - mse: 120.7482 - val_loss: 163.9911 - val_mae: 9.0963 - val_mse: 163.9595\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.1050 - mae: 8.0266 - mse: 118.0732 - val_loss: 164.9765 - val_mae: 9.0785 - val_mse: 164.9446\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.6143 - mae: 8.0200 - mse: 118.5823 - val_loss: 163.8499 - val_mae: 9.0867 - val_mse: 163.8177\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.4023 - mae: 7.9861 - mse: 118.3699 - val_loss: 164.1348 - val_mae: 9.0927 - val_mse: 164.1022\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.9956 - mae: 8.0489 - mse: 119.9629 - val_loss: 165.0108 - val_mae: 9.0755 - val_mse: 164.9779\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.8181 - mae: 8.0050 - mse: 118.7850 - val_loss: 164.0859 - val_mae: 9.0467 - val_mse: 164.0528\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.8305 - mae: 8.0650 - mse: 119.7972 - val_loss: 163.1566 - val_mae: 9.0518 - val_mse: 163.1231\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 117.8014 - mae: 8.0089 - mse: 117.7678 - val_loss: 163.2163 - val_mae: 9.0701 - val_mse: 163.1824\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.7461 - mae: 7.9946 - mse: 118.7120 - val_loss: 164.7962 - val_mae: 9.0453 - val_mse: 164.7622\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 119.9860 - mae: 8.0338 - mse: 119.9519 - val_loss: 162.9660 - val_mae: 9.0309 - val_mse: 162.9317\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.8320 - mae: 7.9532 - mse: 117.7976 - val_loss: 163.2319 - val_mae: 9.0657 - val_mse: 163.1974\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 117.1138 - mae: 7.9648 - mse: 117.0790 - val_loss: 164.0101 - val_mae: 9.0791 - val_mse: 163.9751\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.8886 - mae: 8.0177 - mse: 118.8536 - val_loss: 165.9251 - val_mae: 9.0341 - val_mse: 165.8900\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 116.3373 - mae: 7.9264 - mse: 116.3019 - val_loss: 164.2860 - val_mae: 9.0609 - val_mse: 164.2504\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.5000 - mae: 8.0729 - mse: 119.4644 - val_loss: 165.4259 - val_mae: 9.0393 - val_mse: 165.3902\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 118.1275 - mae: 7.9769 - mse: 118.0918 - val_loss: 163.6262 - val_mae: 9.0491 - val_mse: 163.5902\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.4913 - mae: 7.9985 - mse: 117.4551 - val_loss: 164.4289 - val_mae: 9.0384 - val_mse: 164.3925\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 4.9412, Train MSE: 46.5678\n",
      "Val   MAE: 10.7836, Val   MSE: 233.4242\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8803, Train MSE: 60.1047\n",
      "Val   MAE: 10.1574, Val   MSE: 189.1355\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.7478, Train MSE: 115.6297\n",
      "Val   MAE: 9.0384, Val   MSE: 164.3925\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2605.2383 - mae: 42.3895 - mse: 2605.2156 - val_loss: 818.3028 - val_mae: 23.9746 - val_mse: 818.2789\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1094.7161 - mae: 27.1154 - mse: 1094.6918 - val_loss: 753.7225 - val_mae: 23.0136 - val_mse: 753.6988\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1025.5061 - mae: 26.3106 - mse: 1025.4824 - val_loss: 645.9583 - val_mae: 20.5030 - val_mse: 645.9342\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 951.6646 - mae: 25.1346 - mse: 951.6406 - val_loss: 694.1804 - val_mae: 22.2308 - val_mse: 694.1564\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 859.6870 - mae: 23.9276 - mse: 859.6631 - val_loss: 645.2232 - val_mae: 21.1662 - val_mse: 645.1993\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 861.2122 - mae: 23.8621 - mse: 861.1886 - val_loss: 617.9825 - val_mae: 20.5509 - val_mse: 617.9588\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 819.6909 - mae: 23.1326 - mse: 819.6674 - val_loss: 594.8398 - val_mae: 19.8378 - val_mse: 594.8163\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 793.8657 - mae: 22.8965 - mse: 793.8424 - val_loss: 614.5390 - val_mae: 20.5412 - val_mse: 614.5156\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 781.5039 - mae: 22.4987 - mse: 781.4807 - val_loss: 644.3341 - val_mae: 21.4457 - val_mse: 644.3111\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 789.5452 - mae: 22.6354 - mse: 789.5222 - val_loss: 566.3270 - val_mae: 18.9798 - val_mse: 566.3039\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 788.1170 - mae: 22.6464 - mse: 788.0941 - val_loss: 584.2050 - val_mae: 19.7057 - val_mse: 584.1821\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 759.3566 - mae: 22.1408 - mse: 759.3337 - val_loss: 569.5504 - val_mae: 19.2787 - val_mse: 569.5278\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 739.6255 - mae: 21.8807 - mse: 739.6030 - val_loss: 615.6581 - val_mae: 20.6934 - val_mse: 615.6356\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 730.3621 - mae: 21.5834 - mse: 730.3394 - val_loss: 579.9269 - val_mae: 19.6615 - val_mse: 579.9044\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 731.5066 - mae: 21.6052 - mse: 731.4844 - val_loss: 557.1885 - val_mae: 18.8443 - val_mse: 557.1660\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 706.8682 - mae: 21.3066 - mse: 706.8461 - val_loss: 569.2407 - val_mae: 19.3245 - val_mse: 569.2183\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 720.6785 - mae: 21.4891 - mse: 720.6567 - val_loss: 553.0104 - val_mae: 18.7456 - val_mse: 552.9879\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 690.7202 - mae: 20.9079 - mse: 690.6977 - val_loss: 552.4496 - val_mae: 18.6512 - val_mse: 552.4273\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 703.5380 - mae: 21.1000 - mse: 703.5161 - val_loss: 607.4388 - val_mae: 20.3725 - val_mse: 607.4167\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 711.6912 - mae: 21.2211 - mse: 711.6691 - val_loss: 582.5454 - val_mae: 19.5898 - val_mse: 582.5233\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 689.3130 - mae: 20.9576 - mse: 689.2907 - val_loss: 560.1470 - val_mae: 18.8131 - val_mse: 560.1250\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 685.5899 - mae: 20.9111 - mse: 685.5677 - val_loss: 581.1132 - val_mae: 19.5275 - val_mse: 581.0912\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 695.6056 - mae: 20.9851 - mse: 695.5834 - val_loss: 559.1487 - val_mae: 18.8689 - val_mse: 559.1267\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 673.8807 - mae: 20.5024 - mse: 673.8585 - val_loss: 567.9337 - val_mae: 19.1674 - val_mse: 567.9117\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 685.4793 - mae: 20.7119 - mse: 685.4576 - val_loss: 576.2431 - val_mae: 19.3721 - val_mse: 576.2211\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 674.9119 - mae: 20.5651 - mse: 674.8900 - val_loss: 563.8171 - val_mae: 18.8825 - val_mse: 563.7952\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 692.1112 - mae: 20.8068 - mse: 692.0898 - val_loss: 549.1870 - val_mae: 18.3845 - val_mse: 549.1651\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 670.8525 - mae: 20.4226 - mse: 670.8306 - val_loss: 559.7971 - val_mae: 18.9541 - val_mse: 559.7753\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 677.6492 - mae: 20.5624 - mse: 677.6271 - val_loss: 550.7354 - val_mae: 18.4837 - val_mse: 550.7136\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 679.2303 - mae: 20.6647 - mse: 679.2086 - val_loss: 551.0255 - val_mae: 18.4207 - val_mse: 551.0038\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 667.2678 - mae: 20.3927 - mse: 667.2460 - val_loss: 569.8167 - val_mae: 19.2198 - val_mse: 569.7950\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 673.7700 - mae: 20.6789 - mse: 673.7484 - val_loss: 538.5732 - val_mae: 17.7716 - val_mse: 538.5516\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 661.8200 - mae: 20.3786 - mse: 661.7983 - val_loss: 548.2579 - val_mae: 18.3926 - val_mse: 548.2363\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 659.3880 - mae: 20.3538 - mse: 659.3666 - val_loss: 555.9757 - val_mae: 18.7193 - val_mse: 555.9542\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 653.5429 - mae: 20.1953 - mse: 653.5213 - val_loss: 557.6751 - val_mae: 18.8238 - val_mse: 557.6537\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 647.4466 - mae: 20.0800 - mse: 647.4251 - val_loss: 544.7961 - val_mae: 18.2057 - val_mse: 544.7747\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 658.4673 - mae: 20.3425 - mse: 658.4457 - val_loss: 549.4678 - val_mae: 18.4584 - val_mse: 549.4463\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 648.6425 - mae: 20.1449 - mse: 648.6210 - val_loss: 572.3040 - val_mae: 19.3123 - val_mse: 572.2828\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 657.5917 - mae: 20.2902 - mse: 657.5705 - val_loss: 553.1326 - val_mae: 18.6333 - val_mse: 553.1113\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 651.4206 - mae: 20.1348 - mse: 651.3994 - val_loss: 551.2733 - val_mae: 18.5091 - val_mse: 551.2521\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 654.9765 - mae: 20.1607 - mse: 654.9551 - val_loss: 552.9546 - val_mae: 18.5266 - val_mse: 552.9333\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 654.7759 - mae: 20.2101 - mse: 654.7546 - val_loss: 548.5739 - val_mae: 18.2942 - val_mse: 548.5526\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 639.0939 - mae: 19.8701 - mse: 639.0727 - val_loss: 563.1662 - val_mae: 18.9204 - val_mse: 563.1450\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 657.9721 - mae: 20.3048 - mse: 657.9512 - val_loss: 564.5093 - val_mae: 18.8906 - val_mse: 564.4882\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.9243 - mae: 19.8452 - mse: 633.9031 - val_loss: 549.3305 - val_mae: 18.2982 - val_mse: 549.3095\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 652.3359 - mae: 20.0693 - mse: 652.3148 - val_loss: 562.4613 - val_mae: 18.9045 - val_mse: 562.4402\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.3543 - mae: 20.0420 - mse: 645.3333 - val_loss: 545.4786 - val_mae: 18.1504 - val_mse: 545.4576\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 638.4312 - mae: 19.8452 - mse: 638.4102 - val_loss: 559.6406 - val_mae: 18.7467 - val_mse: 559.6196\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 630.7782 - mae: 19.7456 - mse: 630.7572 - val_loss: 551.8966 - val_mae: 18.3965 - val_mse: 551.8754\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 653.1307 - mae: 20.1617 - mse: 653.1097 - val_loss: 554.9595 - val_mae: 18.5208 - val_mse: 554.9384\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.5375 - mae: 20.0353 - mse: 644.5162 - val_loss: 552.1800 - val_mae: 18.4333 - val_mse: 552.1588\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 638.8739 - mae: 19.7860 - mse: 638.8525 - val_loss: 543.4630 - val_mae: 18.0280 - val_mse: 543.4416\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 612.4203 - mae: 19.3436 - mse: 612.3990 - val_loss: 551.1407 - val_mae: 18.4609 - val_mse: 551.1196\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 629.3013 - mae: 19.6655 - mse: 629.2796 - val_loss: 569.1196 - val_mae: 19.0564 - val_mse: 569.0983\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 635.6176 - mae: 19.7615 - mse: 635.5960 - val_loss: 545.9364 - val_mae: 18.0910 - val_mse: 545.9150\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.9182 - mae: 19.5717 - mse: 620.8966 - val_loss: 543.1201 - val_mae: 17.9467 - val_mse: 543.0986\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 629.0964 - mae: 19.8174 - mse: 629.0746 - val_loss: 556.7905 - val_mae: 18.6320 - val_mse: 556.7689\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 623.6448 - mae: 19.5499 - mse: 623.6231 - val_loss: 562.7587 - val_mae: 18.7877 - val_mse: 562.7371\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 623.6385 - mae: 19.5693 - mse: 623.6172 - val_loss: 557.8221 - val_mae: 18.5758 - val_mse: 557.8007\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 627.0561 - mae: 19.7571 - mse: 627.0345 - val_loss: 558.6357 - val_mae: 18.5651 - val_mse: 558.6141\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 612.3417 - mae: 19.4775 - mse: 612.3201 - val_loss: 558.2602 - val_mae: 18.6279 - val_mse: 558.2387\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 627.6016 - mae: 19.5243 - mse: 627.5798 - val_loss: 556.7571 - val_mae: 18.5155 - val_mse: 556.7354\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 625.9859 - mae: 19.8321 - mse: 625.9644 - val_loss: 555.1703 - val_mae: 18.5730 - val_mse: 555.1483\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.5751 - mae: 19.3943 - mse: 616.5534 - val_loss: 558.4666 - val_mae: 18.6035 - val_mse: 558.4448\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.1024 - mae: 19.4074 - mse: 614.0809 - val_loss: 568.6660 - val_mae: 18.9640 - val_mse: 568.6441\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.8156 - mae: 19.4808 - mse: 620.7942 - val_loss: 553.0880 - val_mae: 18.3626 - val_mse: 553.0662\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.7597 - mae: 19.6355 - mse: 620.7377 - val_loss: 548.5986 - val_mae: 18.2216 - val_mse: 548.5768\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.2750 - mae: 19.4666 - mse: 614.2529 - val_loss: 559.4615 - val_mae: 18.7462 - val_mse: 559.4397\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 603.9449 - mae: 19.2758 - mse: 603.9230 - val_loss: 551.6954 - val_mae: 18.4641 - val_mse: 551.6735\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 617.1562 - mae: 19.5773 - mse: 617.1341 - val_loss: 572.0426 - val_mae: 19.1336 - val_mse: 572.0205\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.7045 - mae: 19.5790 - mse: 616.6823 - val_loss: 545.9951 - val_mae: 18.1099 - val_mse: 545.9729\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 603.9688 - mae: 19.2821 - mse: 603.9470 - val_loss: 549.1022 - val_mae: 18.2594 - val_mse: 549.0800\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.8331 - mae: 19.6506 - mse: 620.8105 - val_loss: 549.2994 - val_mae: 18.1108 - val_mse: 549.2771\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 620.6740 - mae: 19.5220 - mse: 620.6519 - val_loss: 562.2747 - val_mae: 18.7748 - val_mse: 562.2524\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 606.5022 - mae: 19.4091 - mse: 606.4802 - val_loss: 557.7348 - val_mae: 18.5438 - val_mse: 557.7125\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 609.0309 - mae: 19.3349 - mse: 609.0085 - val_loss: 568.6888 - val_mae: 18.9665 - val_mse: 568.6662\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 606.1035 - mae: 19.3114 - mse: 606.0811 - val_loss: 555.7001 - val_mae: 18.4910 - val_mse: 555.6776\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 590.1472 - mae: 19.0428 - mse: 590.1247 - val_loss: 561.9113 - val_mae: 18.8045 - val_mse: 561.8885\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 602.0610 - mae: 19.1945 - mse: 602.0385 - val_loss: 552.1327 - val_mae: 18.2133 - val_mse: 552.1099\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 607.5114 - mae: 19.3941 - mse: 607.4889 - val_loss: 553.5601 - val_mae: 18.3215 - val_mse: 553.5375\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 592.5140 - mae: 18.9933 - mse: 592.4913 - val_loss: 556.2832 - val_mae: 18.4839 - val_mse: 556.2604\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 603.8221 - mae: 19.1375 - mse: 603.7993 - val_loss: 562.8272 - val_mae: 18.6991 - val_mse: 562.8044\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 596.7999 - mae: 19.1928 - mse: 596.7770 - val_loss: 559.9759 - val_mae: 18.5676 - val_mse: 559.9529\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 598.7526 - mae: 19.1765 - mse: 598.7295 - val_loss: 564.1555 - val_mae: 18.7727 - val_mse: 564.1323\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 598.5871 - mae: 19.0619 - mse: 598.5640 - val_loss: 553.7782 - val_mae: 18.4059 - val_mse: 553.7551\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 604.2059 - mae: 19.2909 - mse: 604.1829 - val_loss: 562.2991 - val_mae: 18.6838 - val_mse: 562.2758\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 584.7825 - mae: 18.9472 - mse: 584.7593 - val_loss: 551.5732 - val_mae: 18.2455 - val_mse: 551.5499\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 603.0412 - mae: 19.2641 - mse: 603.0178 - val_loss: 560.0716 - val_mae: 18.6880 - val_mse: 560.0483\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 593.5673 - mae: 19.1257 - mse: 593.5438 - val_loss: 562.3530 - val_mae: 18.7056 - val_mse: 562.3295\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.8262 - mae: 18.9753 - mse: 586.8024 - val_loss: 573.5425 - val_mae: 19.1311 - val_mse: 573.5189\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.7724 - mae: 19.1414 - mse: 594.7485 - val_loss: 552.5517 - val_mae: 18.2870 - val_mse: 552.5278\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 600.9100 - mae: 19.3161 - mse: 600.8860 - val_loss: 555.4037 - val_mae: 18.3074 - val_mse: 555.3797\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 598.4580 - mae: 19.0476 - mse: 598.4339 - val_loss: 559.3728 - val_mae: 18.4764 - val_mse: 559.3486\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 596.9909 - mae: 19.1704 - mse: 596.9667 - val_loss: 566.2030 - val_mae: 18.7645 - val_mse: 566.1786\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 584.9539 - mae: 18.9082 - mse: 584.9294 - val_loss: 571.5578 - val_mae: 19.0879 - val_mse: 571.5333\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 583.2264 - mae: 18.9654 - mse: 583.2018 - val_loss: 555.4081 - val_mae: 18.3929 - val_mse: 555.3835\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 577.4641 - mae: 18.9308 - mse: 577.4396 - val_loss: 557.9344 - val_mae: 18.4745 - val_mse: 557.9097\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.5590 - mae: 18.9819 - mse: 586.5342 - val_loss: 556.1086 - val_mae: 18.3480 - val_mse: 556.0839\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.3010 - mae: 18.9169 - mse: 586.2759 - val_loss: 558.7501 - val_mae: 18.5540 - val_mse: 558.7252\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 592.9972 - mae: 19.1330 - mse: 592.9721 - val_loss: 562.7236 - val_mae: 18.7958 - val_mse: 562.6985\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding an additional deep layer\n",
      "Train MAE: 17.7530, Train MSE: 482.9104\n",
      "Val   MAE: 18.7958, Val   MSE: 562.6985\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2043.7771 - mae: 36.2918 - mse: 2043.7443 - val_loss: 740.1660 - val_mae: 21.5077 - val_mse: 740.1331\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 862.6675 - mae: 23.7707 - mse: 862.6343 - val_loss: 665.7065 - val_mae: 20.3517 - val_mse: 665.6734\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 783.3321 - mae: 22.5739 - mse: 783.2989 - val_loss: 656.5284 - val_mae: 20.9448 - val_mse: 656.4955\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 758.4573 - mae: 22.2203 - mse: 758.4246 - val_loss: 643.0283 - val_mae: 20.7852 - val_mse: 642.9957\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 741.6042 - mae: 21.9622 - mse: 741.5714 - val_loss: 596.6270 - val_mae: 18.9125 - val_mse: 596.5941\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 717.4796 - mae: 21.4951 - mse: 717.4468 - val_loss: 590.2758 - val_mae: 18.9425 - val_mse: 590.2429\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 717.9295 - mae: 21.3851 - mse: 717.8972 - val_loss: 611.4385 - val_mae: 20.2399 - val_mse: 611.4056\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 699.6489 - mae: 21.0721 - mse: 699.6159 - val_loss: 582.5150 - val_mae: 19.1269 - val_mse: 582.4820\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 678.3455 - mae: 20.7611 - mse: 678.3122 - val_loss: 575.3346 - val_mae: 18.4318 - val_mse: 575.3016\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 655.6895 - mae: 20.3733 - mse: 655.6568 - val_loss: 571.1461 - val_mae: 18.9355 - val_mse: 571.1132\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 668.6019 - mae: 20.6839 - mse: 668.5691 - val_loss: 568.0941 - val_mae: 18.7612 - val_mse: 568.0613\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 664.6365 - mae: 20.4710 - mse: 664.6036 - val_loss: 644.0660 - val_mae: 21.1828 - val_mse: 644.0336\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 657.5295 - mae: 20.3849 - mse: 657.4968 - val_loss: 563.2056 - val_mae: 18.7997 - val_mse: 563.1729\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 652.4091 - mae: 20.1643 - mse: 652.3764 - val_loss: 552.0575 - val_mae: 18.1151 - val_mse: 552.0248\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 638.6812 - mae: 20.0043 - mse: 638.6481 - val_loss: 573.6199 - val_mae: 19.1493 - val_mse: 573.5872\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 643.6855 - mae: 20.0193 - mse: 643.6525 - val_loss: 565.7786 - val_mae: 18.8100 - val_mse: 565.7457\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 625.6731 - mae: 19.8104 - mse: 625.6401 - val_loss: 550.2233 - val_mae: 17.9820 - val_mse: 550.1904\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.7429 - mae: 20.0611 - mse: 644.7096 - val_loss: 547.2403 - val_mae: 17.8577 - val_mse: 547.2072\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.7939 - mae: 19.9098 - mse: 634.7609 - val_loss: 550.8446 - val_mae: 18.1802 - val_mse: 550.8115\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.3148 - mae: 19.5665 - mse: 614.2817 - val_loss: 550.7093 - val_mae: 18.1041 - val_mse: 550.6764\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 624.4859 - mae: 19.6582 - mse: 624.4530 - val_loss: 548.7579 - val_mae: 17.7308 - val_mse: 548.7247\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.0298 - mae: 19.8041 - mse: 632.9968 - val_loss: 552.8589 - val_mae: 17.3731 - val_mse: 552.8257\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 624.4180 - mae: 19.6648 - mse: 624.3850 - val_loss: 545.7501 - val_mae: 17.5587 - val_mse: 545.7169\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.5860 - mae: 19.5638 - mse: 620.5530 - val_loss: 543.9689 - val_mae: 17.9461 - val_mse: 543.9359\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 621.8102 - mae: 19.5032 - mse: 621.7772 - val_loss: 544.6450 - val_mae: 17.5754 - val_mse: 544.6118\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.7205 - mae: 19.5018 - mse: 616.6874 - val_loss: 556.4490 - val_mae: 17.3510 - val_mse: 556.4156\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 615.5460 - mae: 19.4066 - mse: 615.5128 - val_loss: 549.7120 - val_mae: 18.2395 - val_mse: 549.6788\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 613.3201 - mae: 19.2946 - mse: 613.2866 - val_loss: 562.1656 - val_mae: 18.6908 - val_mse: 562.1324\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 621.9646 - mae: 19.5959 - mse: 621.9313 - val_loss: 541.5008 - val_mae: 17.5957 - val_mse: 541.4675\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 620.0612 - mae: 19.4679 - mse: 620.0280 - val_loss: 558.0912 - val_mae: 18.4957 - val_mse: 558.0580\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 598.8799 - mae: 19.1497 - mse: 598.8467 - val_loss: 550.3433 - val_mae: 18.1177 - val_mse: 550.3101\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 597.9457 - mae: 19.1002 - mse: 597.9127 - val_loss: 552.6206 - val_mae: 18.3699 - val_mse: 552.5875\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 599.8632 - mae: 19.1929 - mse: 599.8298 - val_loss: 545.7356 - val_mae: 17.8210 - val_mse: 545.7025\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 612.4774 - mae: 19.3997 - mse: 612.4443 - val_loss: 542.9993 - val_mae: 17.4515 - val_mse: 542.9663\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 602.2899 - mae: 19.1622 - mse: 602.2568 - val_loss: 544.3023 - val_mae: 17.9149 - val_mse: 544.2691\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 598.6213 - mae: 19.0459 - mse: 598.5883 - val_loss: 543.7771 - val_mae: 17.9839 - val_mse: 543.7441\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 589.2122 - mae: 18.9038 - mse: 589.1792 - val_loss: 582.4577 - val_mae: 19.5035 - val_mse: 582.4250\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 589.9241 - mae: 19.0361 - mse: 589.8910 - val_loss: 542.0881 - val_mae: 17.8865 - val_mse: 542.0551\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 595.5162 - mae: 18.9903 - mse: 595.4830 - val_loss: 549.5169 - val_mae: 18.3403 - val_mse: 549.4839\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 595.6143 - mae: 18.9595 - mse: 595.5812 - val_loss: 546.8780 - val_mae: 17.7385 - val_mse: 546.8448\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 595.3159 - mae: 19.0427 - mse: 595.2831 - val_loss: 541.7490 - val_mae: 17.5777 - val_mse: 541.7158\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 595.9395 - mae: 19.1269 - mse: 595.9064 - val_loss: 572.5049 - val_mae: 19.1843 - val_mse: 572.4720\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 579.7395 - mae: 18.7608 - mse: 579.7065 - val_loss: 547.0312 - val_mae: 17.4725 - val_mse: 546.9982\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 584.8555 - mae: 18.8918 - mse: 584.8228 - val_loss: 553.9003 - val_mae: 18.3972 - val_mse: 553.8673\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 596.7277 - mae: 19.0109 - mse: 596.6948 - val_loss: 549.9131 - val_mae: 17.4059 - val_mse: 549.8800\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 590.8678 - mae: 18.8710 - mse: 590.8348 - val_loss: 550.6181 - val_mae: 18.4392 - val_mse: 550.5854\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 587.1785 - mae: 18.9188 - mse: 587.1456 - val_loss: 539.6887 - val_mae: 17.5766 - val_mse: 539.6561\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 585.7333 - mae: 18.7942 - mse: 585.7004 - val_loss: 545.4960 - val_mae: 17.9722 - val_mse: 545.4633\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 577.0334 - mae: 18.6878 - mse: 577.0006 - val_loss: 553.3102 - val_mae: 17.2176 - val_mse: 553.2770\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.3954 - mae: 18.7940 - mse: 586.3621 - val_loss: 546.3525 - val_mae: 17.8605 - val_mse: 546.3193\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 580.1324 - mae: 18.8240 - mse: 580.0994 - val_loss: 542.8612 - val_mae: 17.4246 - val_mse: 542.8282\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 580.9020 - mae: 18.7921 - mse: 580.8691 - val_loss: 543.9005 - val_mae: 17.6971 - val_mse: 543.8676\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 577.2548 - mae: 18.7255 - mse: 577.2217 - val_loss: 541.7699 - val_mae: 17.7475 - val_mse: 541.7369\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 584.6945 - mae: 18.8062 - mse: 584.6612 - val_loss: 545.2716 - val_mae: 17.5218 - val_mse: 545.2385\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 574.0925 - mae: 18.7475 - mse: 574.0596 - val_loss: 541.8609 - val_mae: 17.6679 - val_mse: 541.8277\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 574.4870 - mae: 18.6097 - mse: 574.4537 - val_loss: 544.5364 - val_mae: 17.8967 - val_mse: 544.5034\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 579.1893 - mae: 18.6550 - mse: 579.1564 - val_loss: 551.1580 - val_mae: 18.3834 - val_mse: 551.1250\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 567.9573 - mae: 18.5621 - mse: 567.9241 - val_loss: 553.7325 - val_mae: 18.3388 - val_mse: 553.6996\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 561.6531 - mae: 18.3401 - mse: 561.6199 - val_loss: 545.4360 - val_mae: 17.5448 - val_mse: 545.4028\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 567.5628 - mae: 18.6219 - mse: 567.5297 - val_loss: 544.9035 - val_mae: 17.6294 - val_mse: 544.8704\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 572.4319 - mae: 18.6112 - mse: 572.3990 - val_loss: 551.9703 - val_mae: 18.1183 - val_mse: 551.9373\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 572.2119 - mae: 18.5656 - mse: 572.1785 - val_loss: 552.3889 - val_mae: 18.2204 - val_mse: 552.3558\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 559.3011 - mae: 18.3299 - mse: 559.2681 - val_loss: 545.1397 - val_mae: 17.7062 - val_mse: 545.1067\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 566.6419 - mae: 18.4541 - mse: 566.6090 - val_loss: 547.0190 - val_mae: 18.0207 - val_mse: 546.9859\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 580.9609 - mae: 18.8307 - mse: 580.9277 - val_loss: 546.8658 - val_mae: 17.9372 - val_mse: 546.8327\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 574.5672 - mae: 18.6654 - mse: 574.5342 - val_loss: 557.0510 - val_mae: 18.4425 - val_mse: 557.0180\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 558.0351 - mae: 18.2469 - mse: 558.0019 - val_loss: 551.3663 - val_mae: 18.1054 - val_mse: 551.3330\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 569.1646 - mae: 18.6872 - mse: 569.1310 - val_loss: 555.8144 - val_mae: 18.3204 - val_mse: 555.7811\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 564.7996 - mae: 18.4299 - mse: 564.7662 - val_loss: 554.9351 - val_mae: 17.5070 - val_mse: 554.9016\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 556.8304 - mae: 18.2784 - mse: 556.7969 - val_loss: 555.3376 - val_mae: 18.2018 - val_mse: 555.3040\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 563.9619 - mae: 18.4931 - mse: 563.9283 - val_loss: 559.9870 - val_mae: 18.5374 - val_mse: 559.9534\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 555.7631 - mae: 18.3746 - mse: 555.7294 - val_loss: 557.6129 - val_mae: 17.4855 - val_mse: 557.5787\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 563.5741 - mae: 18.4765 - mse: 563.5399 - val_loss: 577.3556 - val_mae: 18.9981 - val_mse: 577.3218\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 582.6591 - mae: 18.8178 - mse: 582.6249 - val_loss: 548.9329 - val_mae: 17.9541 - val_mse: 548.8989\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 560.9086 - mae: 18.4352 - mse: 560.8745 - val_loss: 560.3221 - val_mae: 18.2707 - val_mse: 560.2880\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 563.3844 - mae: 18.3012 - mse: 563.3503 - val_loss: 557.4128 - val_mae: 18.3966 - val_mse: 557.3788\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 555.5446 - mae: 18.2599 - mse: 555.5107 - val_loss: 554.9720 - val_mae: 18.2737 - val_mse: 554.9379\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 558.3634 - mae: 18.4426 - mse: 558.3292 - val_loss: 554.1472 - val_mae: 18.2185 - val_mse: 554.1130\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 553.0858 - mae: 18.2684 - mse: 553.0516 - val_loss: 552.5381 - val_mae: 17.8806 - val_mse: 552.5034\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 549.0195 - mae: 18.2796 - mse: 548.9847 - val_loss: 552.2289 - val_mae: 17.8605 - val_mse: 552.1942\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 547.1268 - mae: 18.1165 - mse: 547.0922 - val_loss: 554.3940 - val_mae: 17.9524 - val_mse: 554.3592\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 561.1758 - mae: 18.4794 - mse: 561.1410 - val_loss: 553.9042 - val_mae: 18.0512 - val_mse: 553.8693\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 568.6783 - mae: 18.5668 - mse: 568.6437 - val_loss: 551.5811 - val_mae: 18.0381 - val_mse: 551.5460\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 556.6866 - mae: 18.3083 - mse: 556.6516 - val_loss: 551.8940 - val_mae: 17.7033 - val_mse: 551.8588\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 547.1979 - mae: 18.1330 - mse: 547.1624 - val_loss: 551.0945 - val_mae: 17.7140 - val_mse: 551.0591\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 554.1168 - mae: 18.2797 - mse: 554.0812 - val_loss: 560.2301 - val_mae: 18.4917 - val_mse: 560.1946\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 556.2230 - mae: 18.4193 - mse: 556.1879 - val_loss: 551.6246 - val_mae: 18.0962 - val_mse: 551.5892\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 557.3960 - mae: 18.2992 - mse: 557.3606 - val_loss: 556.0728 - val_mae: 18.3157 - val_mse: 556.0373\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 551.8658 - mae: 18.3057 - mse: 551.8302 - val_loss: 551.6768 - val_mae: 17.8631 - val_mse: 551.6408\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 554.3088 - mae: 18.3067 - mse: 554.2726 - val_loss: 555.9327 - val_mae: 18.0582 - val_mse: 555.8968\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 545.6276 - mae: 18.1286 - mse: 545.5917 - val_loss: 557.4504 - val_mae: 18.1472 - val_mse: 557.4145\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 547.7809 - mae: 18.1770 - mse: 547.7444 - val_loss: 550.9520 - val_mae: 17.7784 - val_mse: 550.9160\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 545.4306 - mae: 18.1120 - mse: 545.3943 - val_loss: 563.1194 - val_mae: 18.5476 - val_mse: 563.0831\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 543.0710 - mae: 18.0718 - mse: 543.0344 - val_loss: 554.0743 - val_mae: 17.7084 - val_mse: 554.0375\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 545.0247 - mae: 18.0846 - mse: 544.9877 - val_loss: 553.2261 - val_mae: 17.5582 - val_mse: 553.1890\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 542.7266 - mae: 18.1279 - mse: 542.6893 - val_loss: 563.4059 - val_mae: 18.5891 - val_mse: 563.3690\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 539.6201 - mae: 18.0271 - mse: 539.5826 - val_loss: 552.1541 - val_mae: 17.8421 - val_mse: 552.1166\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 543.1499 - mae: 18.1557 - mse: 543.1124 - val_loss: 564.6122 - val_mae: 17.4002 - val_mse: 564.5743\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 550.9843 - mae: 18.2123 - mse: 550.9465 - val_loss: 554.2556 - val_mae: 18.0392 - val_mse: 554.2175\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 545.9799 - mae: 18.1619 - mse: 545.9415 - val_loss: 555.4326 - val_mae: 18.1013 - val_mse: 555.3945\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Reverting to 2 deep layers, increasing to 128 kernels per layer\n",
      "Train MAE: 16.6529, Train MSE: 456.7876\n",
      "Val   MAE: 18.1013, Val   MSE: 555.3945\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"Reverting to 2 deep layers, increasing to 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model but increased overfitting a little bit. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 2384.5278 - mae: 40.1743 - mse: 2384.4958 - val_loss: 773.5165 - val_mae: 22.5112 - val_mse: 773.4837\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1098.0477 - mae: 27.3556 - mse: 1098.0146 - val_loss: 718.1125 - val_mae: 22.0730 - val_mse: 718.0798\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1014.6326 - mae: 26.1078 - mse: 1014.5998 - val_loss: 676.3630 - val_mae: 21.4453 - val_mse: 676.3304\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 903.6113 - mae: 24.4630 - mse: 903.5787 - val_loss: 632.5463 - val_mae: 19.9884 - val_mse: 632.5135\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 908.5967 - mae: 24.4921 - mse: 908.5642 - val_loss: 620.4692 - val_mae: 20.0110 - val_mse: 620.4368\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 840.6660 - mae: 23.5479 - mse: 840.6335 - val_loss: 611.2001 - val_mae: 19.9724 - val_mse: 611.1677\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 882.4814 - mae: 24.1692 - mse: 882.4492 - val_loss: 603.2355 - val_mae: 19.8305 - val_mse: 603.2030\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 825.8441 - mae: 23.3558 - mse: 825.8121 - val_loss: 595.3162 - val_mae: 19.6389 - val_mse: 595.2841\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 812.3975 - mae: 22.9702 - mse: 812.3651 - val_loss: 586.4069 - val_mae: 19.2523 - val_mse: 586.3748\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 808.3128 - mae: 23.0453 - mse: 808.2806 - val_loss: 585.2964 - val_mae: 19.3332 - val_mse: 585.2644\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 789.5206 - mae: 22.7526 - mse: 789.4883 - val_loss: 606.7220 - val_mae: 20.1936 - val_mse: 606.6901\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 792.5143 - mae: 22.7788 - mse: 792.4824 - val_loss: 575.0380 - val_mae: 19.1738 - val_mse: 575.0060\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 790.7426 - mae: 22.6761 - mse: 790.7108 - val_loss: 588.3107 - val_mae: 19.6766 - val_mse: 588.2791\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 771.1302 - mae: 22.4608 - mse: 771.0983 - val_loss: 571.4871 - val_mae: 19.1560 - val_mse: 571.4555\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 764.7205 - mae: 22.3951 - mse: 764.6890 - val_loss: 572.5111 - val_mae: 19.0475 - val_mse: 572.4797\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 771.9071 - mae: 22.4744 - mse: 771.8757 - val_loss: 566.0132 - val_mae: 18.7102 - val_mse: 565.9818\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 757.4980 - mae: 22.2360 - mse: 757.4668 - val_loss: 576.9291 - val_mae: 19.3049 - val_mse: 576.8979\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 746.9558 - mae: 21.9500 - mse: 746.9249 - val_loss: 597.5574 - val_mae: 20.0789 - val_mse: 597.5267\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 743.9935 - mae: 22.1389 - mse: 743.9627 - val_loss: 560.4619 - val_mae: 18.3943 - val_mse: 560.4312\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 745.6349 - mae: 22.0324 - mse: 745.6041 - val_loss: 573.5305 - val_mae: 19.3052 - val_mse: 573.4999\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 742.9529 - mae: 21.8753 - mse: 742.9224 - val_loss: 556.8035 - val_mae: 18.4160 - val_mse: 556.7728\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 724.0789 - mae: 21.6286 - mse: 724.0482 - val_loss: 581.0381 - val_mae: 19.5911 - val_mse: 581.0076\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 732.3721 - mae: 21.7471 - mse: 732.3414 - val_loss: 569.2170 - val_mae: 19.1441 - val_mse: 569.1865\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 736.6516 - mae: 21.8673 - mse: 736.6215 - val_loss: 562.3152 - val_mae: 18.7509 - val_mse: 562.2849\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 721.5941 - mae: 21.6478 - mse: 721.5639 - val_loss: 555.3343 - val_mae: 18.5480 - val_mse: 555.3040\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 714.6141 - mae: 21.2231 - mse: 714.5840 - val_loss: 573.7977 - val_mae: 19.3846 - val_mse: 573.7676\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 720.4768 - mae: 21.6399 - mse: 720.4465 - val_loss: 565.7130 - val_mae: 19.1017 - val_mse: 565.6832\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 722.4480 - mae: 21.4027 - mse: 722.4180 - val_loss: 557.6379 - val_mae: 18.8109 - val_mse: 557.6083\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 700.6514 - mae: 21.2250 - mse: 700.6221 - val_loss: 564.5480 - val_mae: 19.1008 - val_mse: 564.5182\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 700.8925 - mae: 21.0958 - mse: 700.8625 - val_loss: 550.6530 - val_mae: 18.3879 - val_mse: 550.6235\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 722.5830 - mae: 21.4644 - mse: 722.5535 - val_loss: 545.8555 - val_mae: 17.9271 - val_mse: 545.8259\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 704.1799 - mae: 21.1223 - mse: 704.1505 - val_loss: 550.1854 - val_mae: 18.4239 - val_mse: 550.1561\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 716.6382 - mae: 21.3529 - mse: 716.6090 - val_loss: 547.9932 - val_mae: 18.2734 - val_mse: 547.9639\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 708.2766 - mae: 21.2157 - mse: 708.2479 - val_loss: 548.2879 - val_mae: 17.6273 - val_mse: 548.2585\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 691.8818 - mae: 21.0182 - mse: 691.8525 - val_loss: 541.8765 - val_mae: 17.8636 - val_mse: 541.8474\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 682.4304 - mae: 20.8444 - mse: 682.4014 - val_loss: 541.1645 - val_mae: 17.9415 - val_mse: 541.1356\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 683.4927 - mae: 20.8347 - mse: 683.4641 - val_loss: 555.8754 - val_mae: 18.6413 - val_mse: 555.8467\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 692.8737 - mae: 20.9612 - mse: 692.8451 - val_loss: 545.4686 - val_mae: 18.0875 - val_mse: 545.4398\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 698.9418 - mae: 20.7860 - mse: 698.9135 - val_loss: 544.0283 - val_mae: 18.0409 - val_mse: 543.9998\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 683.3071 - mae: 20.7775 - mse: 683.2784 - val_loss: 558.8420 - val_mae: 18.8710 - val_mse: 558.8135\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 667.6131 - mae: 20.5507 - mse: 667.5844 - val_loss: 542.6089 - val_mae: 18.1210 - val_mse: 542.5804\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 678.7745 - mae: 20.6294 - mse: 678.7460 - val_loss: 541.7949 - val_mae: 18.0331 - val_mse: 541.7665\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 662.5562 - mae: 20.3241 - mse: 662.5278 - val_loss: 539.1780 - val_mae: 17.5103 - val_mse: 539.1497\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 672.9883 - mae: 20.5685 - mse: 672.9600 - val_loss: 539.1757 - val_mae: 17.7598 - val_mse: 539.1472\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 673.8732 - mae: 20.6086 - mse: 673.8449 - val_loss: 540.0017 - val_mae: 17.7228 - val_mse: 539.9733\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 672.8088 - mae: 20.5331 - mse: 672.7808 - val_loss: 540.5165 - val_mae: 17.6789 - val_mse: 540.4883\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 666.6042 - mae: 20.3960 - mse: 666.5759 - val_loss: 559.3179 - val_mae: 18.8342 - val_mse: 559.2898\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 655.9717 - mae: 20.2488 - mse: 655.9435 - val_loss: 539.7540 - val_mae: 17.5361 - val_mse: 539.7259\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 652.2511 - mae: 20.0995 - mse: 652.2231 - val_loss: 546.8931 - val_mae: 18.2030 - val_mse: 546.8651\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 666.7274 - mae: 20.4942 - mse: 666.6996 - val_loss: 541.5400 - val_mae: 17.9066 - val_mse: 541.5118\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 664.8472 - mae: 20.3914 - mse: 664.8195 - val_loss: 543.0802 - val_mae: 17.9136 - val_mse: 543.0523\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 659.6844 - mae: 20.4080 - mse: 659.6563 - val_loss: 559.5291 - val_mae: 18.7297 - val_mse: 559.5013\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 653.6256 - mae: 20.2185 - mse: 653.5980 - val_loss: 550.0396 - val_mae: 18.4036 - val_mse: 550.0119\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 648.4743 - mae: 20.0591 - mse: 648.4465 - val_loss: 551.7518 - val_mae: 18.3526 - val_mse: 551.7242\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.7156 - mae: 19.9969 - mse: 645.6879 - val_loss: 551.1511 - val_mae: 18.4176 - val_mse: 551.1235\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 654.4597 - mae: 20.2927 - mse: 654.4322 - val_loss: 540.9567 - val_mae: 17.6469 - val_mse: 540.9290\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 636.4089 - mae: 19.8638 - mse: 636.3812 - val_loss: 540.5592 - val_mae: 17.7196 - val_mse: 540.5314\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 648.3242 - mae: 20.0487 - mse: 648.2962 - val_loss: 541.8320 - val_mae: 17.7996 - val_mse: 541.8043\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 647.0912 - mae: 20.1039 - mse: 647.0632 - val_loss: 539.8278 - val_mae: 17.6827 - val_mse: 539.8002\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.3083 - mae: 19.7766 - mse: 644.2808 - val_loss: 562.5289 - val_mae: 18.8694 - val_mse: 562.5014\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 642.5883 - mae: 19.8925 - mse: 642.5607 - val_loss: 547.9855 - val_mae: 18.2151 - val_mse: 547.9580\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 637.1273 - mae: 19.9517 - mse: 637.0993 - val_loss: 540.0040 - val_mae: 17.5414 - val_mse: 539.9762\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.5567 - mae: 19.5612 - mse: 616.5289 - val_loss: 545.0151 - val_mae: 18.0442 - val_mse: 544.9874\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 640.8802 - mae: 19.9164 - mse: 640.8524 - val_loss: 559.1949 - val_mae: 18.6548 - val_mse: 559.1673\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.0765 - mae: 19.8217 - mse: 633.0489 - val_loss: 547.4395 - val_mae: 18.1288 - val_mse: 547.4117\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 632.4915 - mae: 19.7179 - mse: 632.4638 - val_loss: 544.9565 - val_mae: 17.9502 - val_mse: 544.9286\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.4476 - mae: 19.8275 - mse: 633.4196 - val_loss: 539.0037 - val_mae: 17.4804 - val_mse: 538.9758\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 635.3105 - mae: 19.8164 - mse: 635.2830 - val_loss: 545.8103 - val_mae: 18.1247 - val_mse: 545.7826\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.1784 - mae: 19.9527 - mse: 634.1505 - val_loss: 540.2756 - val_mae: 17.5124 - val_mse: 540.2476\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.2766 - mae: 19.2808 - mse: 608.2484 - val_loss: 554.1274 - val_mae: 18.4275 - val_mse: 554.0995\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 631.1587 - mae: 19.8105 - mse: 631.1309 - val_loss: 546.6854 - val_mae: 18.1181 - val_mse: 546.6572\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.1165 - mae: 19.5942 - mse: 616.0885 - val_loss: 543.7925 - val_mae: 17.9547 - val_mse: 543.7644\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.9219 - mae: 19.4392 - mse: 614.8937 - val_loss: 543.4061 - val_mae: 17.8743 - val_mse: 543.3777\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 624.8259 - mae: 19.7079 - mse: 624.7977 - val_loss: 542.1503 - val_mae: 17.8001 - val_mse: 542.1221\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 623.3741 - mae: 19.7530 - mse: 623.3460 - val_loss: 543.9756 - val_mae: 17.9362 - val_mse: 543.9471\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 619.4193 - mae: 19.5812 - mse: 619.3909 - val_loss: 548.0529 - val_mae: 18.2323 - val_mse: 548.0245\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 628.0886 - mae: 19.6633 - mse: 628.0601 - val_loss: 544.2897 - val_mae: 17.8833 - val_mse: 544.2612\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 617.4849 - mae: 19.4325 - mse: 617.4562 - val_loss: 540.4095 - val_mae: 17.7121 - val_mse: 540.3811\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 600.2163 - mae: 19.1653 - mse: 600.1875 - val_loss: 550.1517 - val_mae: 18.2667 - val_mse: 550.1230\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 614.2412 - mae: 19.4090 - mse: 614.2125 - val_loss: 545.7585 - val_mae: 18.0463 - val_mse: 545.7297\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.9067 - mae: 19.0847 - mse: 594.8777 - val_loss: 547.3958 - val_mae: 18.0750 - val_mse: 547.3669\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 602.9936 - mae: 19.2389 - mse: 602.9647 - val_loss: 559.5959 - val_mae: 18.6504 - val_mse: 559.5668\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.6670 - mae: 19.4323 - mse: 608.6376 - val_loss: 542.7387 - val_mae: 17.7052 - val_mse: 542.7092\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 609.2659 - mae: 19.3135 - mse: 609.2365 - val_loss: 554.4276 - val_mae: 18.5222 - val_mse: 554.3979\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 605.0615 - mae: 19.3733 - mse: 605.0320 - val_loss: 545.2810 - val_mae: 17.7764 - val_mse: 545.2511\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 604.1853 - mae: 19.3369 - mse: 604.1553 - val_loss: 543.0957 - val_mae: 17.8016 - val_mse: 543.0658\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.1974 - mae: 19.3118 - mse: 608.1677 - val_loss: 548.8929 - val_mae: 18.1540 - val_mse: 548.8629\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 607.4863 - mae: 19.3402 - mse: 607.4561 - val_loss: 546.2822 - val_mae: 18.1818 - val_mse: 546.2520\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.3102 - mae: 19.5722 - mse: 616.2800 - val_loss: 551.6048 - val_mae: 18.4146 - val_mse: 551.5746\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.6226 - mae: 19.3771 - mse: 608.5922 - val_loss: 543.0347 - val_mae: 17.8167 - val_mse: 543.0042\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.4842 - mae: 19.2898 - mse: 608.4534 - val_loss: 552.4902 - val_mae: 18.4010 - val_mse: 552.4594\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.9902 - mae: 19.1648 - mse: 594.9593 - val_loss: 551.2208 - val_mae: 18.2427 - val_mse: 551.1898\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 599.3663 - mae: 19.2574 - mse: 599.3351 - val_loss: 548.0786 - val_mae: 17.9622 - val_mse: 548.0474\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 590.8159 - mae: 18.9966 - mse: 590.7847 - val_loss: 552.2858 - val_mae: 18.3863 - val_mse: 552.2546\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 606.6030 - mae: 19.3630 - mse: 606.5716 - val_loss: 555.0034 - val_mae: 18.5426 - val_mse: 554.9719\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 611.3058 - mae: 19.5257 - mse: 611.2742 - val_loss: 544.1727 - val_mae: 17.8234 - val_mse: 544.1408\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 600.6371 - mae: 19.2623 - mse: 600.6053 - val_loss: 548.0467 - val_mae: 18.1162 - val_mse: 548.0149\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 586.5583 - mae: 18.9890 - mse: 586.5264 - val_loss: 556.1894 - val_mae: 18.5177 - val_mse: 556.1572\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 594.5796 - mae: 19.1508 - mse: 594.5473 - val_loss: 557.9647 - val_mae: 18.6578 - val_mse: 557.9324\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 587.1703 - mae: 18.9523 - mse: 587.1377 - val_loss: 547.9534 - val_mae: 18.1056 - val_mse: 547.9205\n"
     ]
    }
   ],
   "source": [
    "# increasing dropout rate\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "2 deep layers,  kernels per layer, increased dropout from 0.4 to 0.6\n",
      "Train MAE: 17.3375, Train MSE: 484.7930\n",
      "Val   MAE: 18.1056, Val   MSE: 547.9205\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"2 deep layers,  kernels per layer, increased dropout from 0.4 to 0.6\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 2280.0955 - mae: 39.3034 - mse: 2280.0520 - val_loss: 945.4214 - val_mae: 26.6470 - val_mse: 945.3775\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1227.9375 - mae: 28.7019 - mse: 1227.8926 - val_loss: 871.6758 - val_mae: 25.5128 - val_mse: 871.6315\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1076.1665 - mae: 26.9413 - mse: 1076.1224 - val_loss: 849.0858 - val_mae: 25.2917 - val_mse: 849.0414\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1003.8463 - mae: 26.0472 - mse: 1003.8018 - val_loss: 776.9370 - val_mae: 23.9686 - val_mse: 776.8925\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 973.1307 - mae: 25.4490 - mse: 973.0862 - val_loss: 705.2618 - val_mae: 22.6258 - val_mse: 705.2171\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 906.1047 - mae: 24.4818 - mse: 906.0599 - val_loss: 760.2264 - val_mae: 23.8342 - val_mse: 760.1818\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 875.0835 - mae: 23.9024 - mse: 875.0388 - val_loss: 826.6641 - val_mae: 25.1458 - val_mse: 826.6198\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 842.9484 - mae: 23.6983 - mse: 842.9041 - val_loss: 771.8544 - val_mae: 24.1450 - val_mse: 771.8101\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 841.3907 - mae: 23.5559 - mse: 841.3462 - val_loss: 681.3618 - val_mae: 22.1579 - val_mse: 681.3173\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 811.3052 - mae: 23.1019 - mse: 811.2614 - val_loss: 699.6108 - val_mae: 22.5514 - val_mse: 699.5667\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 794.5706 - mae: 22.8285 - mse: 794.5266 - val_loss: 617.9315 - val_mae: 20.5674 - val_mse: 617.8873\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 769.0079 - mae: 22.3605 - mse: 768.9637 - val_loss: 626.6009 - val_mae: 20.8779 - val_mse: 626.5565\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 752.6587 - mae: 22.1231 - mse: 752.6144 - val_loss: 653.8594 - val_mae: 21.5543 - val_mse: 653.8152\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 761.0298 - mae: 22.2202 - mse: 760.9859 - val_loss: 676.9686 - val_mae: 22.0387 - val_mse: 676.9246\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 755.0775 - mae: 22.1966 - mse: 755.0334 - val_loss: 667.4284 - val_mae: 21.8945 - val_mse: 667.3844\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 739.5100 - mae: 21.7795 - mse: 739.4659 - val_loss: 635.8348 - val_mae: 21.1218 - val_mse: 635.7908\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 733.5984 - mae: 21.6112 - mse: 733.5545 - val_loss: 647.7943 - val_mae: 21.4491 - val_mse: 647.7505\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 722.1444 - mae: 21.5207 - mse: 722.1007 - val_loss: 610.7500 - val_mae: 20.5016 - val_mse: 610.7061\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 709.3132 - mae: 21.1638 - mse: 709.2693 - val_loss: 612.1750 - val_mae: 20.5498 - val_mse: 612.1312\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 716.9882 - mae: 21.4721 - mse: 716.9443 - val_loss: 662.1952 - val_mae: 21.8128 - val_mse: 662.1516\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 702.5325 - mae: 21.1552 - mse: 702.4886 - val_loss: 608.7064 - val_mae: 20.4550 - val_mse: 608.6625\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 709.1915 - mae: 21.2559 - mse: 709.1480 - val_loss: 622.7844 - val_mae: 20.8278 - val_mse: 622.7408\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 701.5662 - mae: 21.3739 - mse: 701.5225 - val_loss: 591.2401 - val_mae: 20.0373 - val_mse: 591.1965\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 696.6965 - mae: 20.9676 - mse: 696.6530 - val_loss: 636.0965 - val_mae: 21.1803 - val_mse: 636.0532\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 694.9131 - mae: 21.0503 - mse: 694.8698 - val_loss: 583.2714 - val_mae: 19.7058 - val_mse: 583.2277\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 698.0625 - mae: 20.8810 - mse: 698.0189 - val_loss: 571.2522 - val_mae: 19.3141 - val_mse: 571.2086\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 685.2663 - mae: 20.6608 - mse: 685.2225 - val_loss: 605.0547 - val_mae: 20.3025 - val_mse: 605.0112\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 678.3698 - mae: 20.8201 - mse: 678.3263 - val_loss: 570.2331 - val_mae: 19.2292 - val_mse: 570.1894\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 690.7823 - mae: 20.8032 - mse: 690.7389 - val_loss: 618.4395 - val_mae: 20.5996 - val_mse: 618.3961\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 681.8284 - mae: 20.7472 - mse: 681.7846 - val_loss: 559.0647 - val_mae: 18.8526 - val_mse: 559.0211\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 664.1736 - mae: 20.4579 - mse: 664.1296 - val_loss: 559.6023 - val_mae: 18.8615 - val_mse: 559.5587\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 680.8796 - mae: 20.7375 - mse: 680.8361 - val_loss: 593.4993 - val_mae: 19.9107 - val_mse: 593.4557\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 682.4479 - mae: 20.6439 - mse: 682.4039 - val_loss: 570.3149 - val_mae: 19.2567 - val_mse: 570.2714\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 670.5926 - mae: 20.5200 - mse: 670.5489 - val_loss: 560.3967 - val_mae: 18.8799 - val_mse: 560.3530\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 669.3111 - mae: 20.5512 - mse: 669.2673 - val_loss: 558.1246 - val_mae: 18.7698 - val_mse: 558.0809\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 667.7879 - mae: 20.4045 - mse: 667.7443 - val_loss: 576.4866 - val_mae: 19.3948 - val_mse: 576.4429\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 665.1494 - mae: 20.4007 - mse: 665.1056 - val_loss: 555.3135 - val_mae: 18.7186 - val_mse: 555.2695\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 677.0105 - mae: 20.5811 - mse: 676.9667 - val_loss: 582.5209 - val_mae: 19.6904 - val_mse: 582.4772\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 662.8741 - mae: 20.4470 - mse: 662.8304 - val_loss: 605.3058 - val_mae: 20.2875 - val_mse: 605.2619\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 671.5497 - mae: 20.6029 - mse: 671.5062 - val_loss: 569.5616 - val_mae: 19.1912 - val_mse: 569.5176\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 668.7571 - mae: 20.3542 - mse: 668.7126 - val_loss: 557.2179 - val_mae: 18.8292 - val_mse: 557.1736\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 659.6813 - mae: 20.3195 - mse: 659.6370 - val_loss: 579.0129 - val_mae: 19.4966 - val_mse: 578.9686\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 662.9232 - mae: 20.4345 - mse: 662.8792 - val_loss: 591.2072 - val_mae: 19.8764 - val_mse: 591.1628\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 661.4449 - mae: 20.3556 - mse: 661.4008 - val_loss: 603.4232 - val_mae: 20.1433 - val_mse: 603.3788\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 671.8464 - mae: 20.5921 - mse: 671.8018 - val_loss: 560.3878 - val_mae: 18.8206 - val_mse: 560.3433\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 645.4816 - mae: 20.0309 - mse: 645.4369 - val_loss: 570.7437 - val_mae: 19.2053 - val_mse: 570.6988\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 657.2783 - mae: 20.2482 - mse: 657.2336 - val_loss: 582.8788 - val_mae: 19.4987 - val_mse: 582.8339\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.3257 - mae: 20.0401 - mse: 644.2807 - val_loss: 594.6269 - val_mae: 19.8775 - val_mse: 594.5820\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 661.2963 - mae: 20.2519 - mse: 661.2512 - val_loss: 570.4639 - val_mae: 19.1292 - val_mse: 570.4188\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 659.4193 - mae: 20.2620 - mse: 659.3740 - val_loss: 586.1541 - val_mae: 19.6251 - val_mse: 586.1088\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.4934 - mae: 19.9065 - mse: 633.4482 - val_loss: 581.2422 - val_mae: 19.4684 - val_mse: 581.1967\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 655.9106 - mae: 20.3095 - mse: 655.8650 - val_loss: 561.0867 - val_mae: 18.7129 - val_mse: 561.0409\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 639.7666 - mae: 19.9692 - mse: 639.7211 - val_loss: 578.4876 - val_mae: 19.3593 - val_mse: 578.4417\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 650.0211 - mae: 20.1586 - mse: 649.9755 - val_loss: 569.1722 - val_mae: 19.0487 - val_mse: 569.1264\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.1517 - mae: 19.9591 - mse: 644.1057 - val_loss: 558.5118 - val_mae: 18.7114 - val_mse: 558.4655\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 635.1691 - mae: 19.8502 - mse: 635.1227 - val_loss: 585.8926 - val_mae: 19.6075 - val_mse: 585.8461\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 646.8427 - mae: 20.0777 - mse: 646.7961 - val_loss: 563.5313 - val_mae: 18.8458 - val_mse: 563.4846\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 641.9163 - mae: 20.0307 - mse: 641.8696 - val_loss: 573.9868 - val_mae: 19.2573 - val_mse: 573.9402\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 641.6808 - mae: 19.9117 - mse: 641.6340 - val_loss: 549.8378 - val_mae: 18.3171 - val_mse: 549.7909\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.9126 - mae: 19.8768 - mse: 634.8657 - val_loss: 559.5350 - val_mae: 18.6809 - val_mse: 559.4878\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 644.5341 - mae: 19.8852 - mse: 644.4869 - val_loss: 557.7640 - val_mae: 18.6876 - val_mse: 557.7167\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 636.9308 - mae: 19.8563 - mse: 636.8838 - val_loss: 557.3229 - val_mae: 18.7312 - val_mse: 557.2756\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 637.2646 - mae: 19.8981 - mse: 637.2172 - val_loss: 564.9145 - val_mae: 18.9628 - val_mse: 564.8671\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 626.4260 - mae: 19.7513 - mse: 626.3781 - val_loss: 565.8060 - val_mae: 18.9583 - val_mse: 565.7582\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 619.7546 - mae: 19.6360 - mse: 619.7067 - val_loss: 562.2437 - val_mae: 18.8541 - val_mse: 562.1957\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 633.0368 - mae: 19.8029 - mse: 632.9891 - val_loss: 564.6820 - val_mae: 18.9849 - val_mse: 564.6339\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 634.1524 - mae: 19.9079 - mse: 634.1042 - val_loss: 573.6393 - val_mae: 19.2076 - val_mse: 573.5912\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 636.9601 - mae: 19.8592 - mse: 636.9116 - val_loss: 549.3620 - val_mae: 18.3907 - val_mse: 549.3132\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 641.5472 - mae: 19.9342 - mse: 641.4983 - val_loss: 568.0235 - val_mae: 19.0951 - val_mse: 567.9747\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 646.8222 - mae: 20.0053 - mse: 646.7734 - val_loss: 572.2105 - val_mae: 19.1999 - val_mse: 572.1616\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 624.0665 - mae: 19.8130 - mse: 624.0175 - val_loss: 561.0267 - val_mae: 18.7625 - val_mse: 560.9775\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 621.6818 - mae: 19.6352 - mse: 621.6326 - val_loss: 588.5629 - val_mae: 19.6963 - val_mse: 588.5136\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 616.4935 - mae: 19.5020 - mse: 616.4440 - val_loss: 583.0148 - val_mae: 19.5507 - val_mse: 582.9651\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 617.8438 - mae: 19.5410 - mse: 617.7938 - val_loss: 572.3585 - val_mae: 19.1772 - val_mse: 572.3085\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 619.3597 - mae: 19.5686 - mse: 619.3094 - val_loss: 570.3079 - val_mae: 19.1511 - val_mse: 570.2577\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 628.4019 - mae: 19.6735 - mse: 628.3516 - val_loss: 548.9626 - val_mae: 18.3015 - val_mse: 548.9120\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 611.1880 - mae: 19.3678 - mse: 611.1375 - val_loss: 560.6142 - val_mae: 18.7834 - val_mse: 560.5635\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 630.5770 - mae: 19.7780 - mse: 630.5264 - val_loss: 555.2428 - val_mae: 18.5553 - val_mse: 555.1919\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 623.0268 - mae: 19.5836 - mse: 622.9758 - val_loss: 543.8478 - val_mae: 18.0355 - val_mse: 543.7967\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 628.8748 - mae: 19.7226 - mse: 628.8235 - val_loss: 554.0688 - val_mae: 18.5450 - val_mse: 554.0178\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 632.0417 - mae: 19.6662 - mse: 631.9907 - val_loss: 561.6874 - val_mae: 18.8258 - val_mse: 561.6359\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.6808 - mae: 19.4435 - mse: 608.6293 - val_loss: 551.7795 - val_mae: 18.3966 - val_mse: 551.7278\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 611.5306 - mae: 19.3955 - mse: 611.4788 - val_loss: 574.3465 - val_mae: 19.2639 - val_mse: 574.2948\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 624.7759 - mae: 19.6200 - mse: 624.7239 - val_loss: 551.8392 - val_mae: 18.3048 - val_mse: 551.7870\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 619.3885 - mae: 19.5571 - mse: 619.3365 - val_loss: 561.2357 - val_mae: 18.7413 - val_mse: 561.1831\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 614.8167 - mae: 19.6303 - mse: 614.7639 - val_loss: 561.8692 - val_mae: 18.7844 - val_mse: 561.8164\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 605.1108 - mae: 19.3076 - mse: 605.0580 - val_loss: 564.5546 - val_mae: 18.8342 - val_mse: 564.5014\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 610.7374 - mae: 19.3604 - mse: 610.6841 - val_loss: 568.4031 - val_mae: 19.0173 - val_mse: 568.3497\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 610.9001 - mae: 19.5812 - mse: 610.8466 - val_loss: 557.1195 - val_mae: 18.4195 - val_mse: 557.0659\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 609.3649 - mae: 19.4234 - mse: 609.3112 - val_loss: 569.9353 - val_mae: 19.0314 - val_mse: 569.8815\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 611.3351 - mae: 19.4064 - mse: 611.2811 - val_loss: 559.9988 - val_mae: 18.4938 - val_mse: 559.9446\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 601.3417 - mae: 19.1179 - mse: 601.2875 - val_loss: 560.4440 - val_mae: 18.7213 - val_mse: 560.3900\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 607.8086 - mae: 19.4362 - mse: 607.7542 - val_loss: 566.9929 - val_mae: 18.8552 - val_mse: 566.9382\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 612.8098 - mae: 19.4594 - mse: 612.7550 - val_loss: 566.1584 - val_mae: 18.8725 - val_mse: 566.1036\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 617.2814 - mae: 19.4877 - mse: 617.2264 - val_loss: 572.5005 - val_mae: 19.1221 - val_mse: 572.4454\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 610.3157 - mae: 19.3526 - mse: 610.2606 - val_loss: 561.4332 - val_mae: 18.6713 - val_mse: 561.3779\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 599.0176 - mae: 19.2108 - mse: 598.9622 - val_loss: 563.0166 - val_mae: 18.7780 - val_mse: 562.9608\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 608.1638 - mae: 19.4490 - mse: 608.1079 - val_loss: 564.6084 - val_mae: 18.8250 - val_mse: 564.5525\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 607.5837 - mae: 19.2924 - mse: 607.5275 - val_loss: 565.8785 - val_mae: 18.9420 - val_mse: 565.8223\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 605.1689 - mae: 19.3573 - mse: 605.1125 - val_loss: 565.3717 - val_mae: 18.8862 - val_mse: 565.3151\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "3 deep layers, 128 kernels per layer, 0.6 dropout rate\n",
      "Train MAE: 18.1621, Train MSE: 503.0637\n",
      "Val   MAE: 18.8862, Val   MSE: 565.3151\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"3 deep layers, 128 kernels per layer, 0.6 dropout rate\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model continues to have the best MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 2ms/step - loss: 185.9329 - mae: 9.9309 - mse: 185.9113 - val_loss: 168.5940 - val_mae: 9.0832 - val_mse: 168.5720\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 150.0187 - mae: 8.9774 - mse: 149.9966 - val_loss: 170.0541 - val_mae: 9.0456 - val_mse: 170.0321\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 147.8211 - mae: 8.8369 - mse: 147.7990 - val_loss: 168.0266 - val_mae: 8.9826 - val_mse: 168.0045\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 144.7279 - mae: 8.7605 - mse: 144.7059 - val_loss: 163.3463 - val_mae: 8.9470 - val_mse: 163.3241\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 142.6960 - mae: 8.6970 - mse: 142.6739 - val_loss: 162.7365 - val_mae: 8.9233 - val_mse: 162.7144\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 140.6227 - mae: 8.6763 - mse: 140.6006 - val_loss: 166.1049 - val_mae: 8.9388 - val_mse: 166.0828\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 139.9390 - mae: 8.6580 - mse: 139.9169 - val_loss: 162.5222 - val_mae: 8.8953 - val_mse: 162.5001\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 139.2179 - mae: 8.5739 - mse: 139.1959 - val_loss: 162.4277 - val_mae: 8.8965 - val_mse: 162.4056\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.0206 - mae: 8.5642 - mse: 136.9985 - val_loss: 158.8185 - val_mae: 8.9495 - val_mse: 158.7964\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.9381 - mae: 8.5436 - mse: 137.9160 - val_loss: 162.6627 - val_mae: 8.8914 - val_mse: 162.6406\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.7083 - mae: 8.5239 - mse: 137.6861 - val_loss: 163.3191 - val_mae: 8.8980 - val_mse: 163.2970\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.4008 - mae: 8.4860 - mse: 135.3787 - val_loss: 158.4660 - val_mae: 8.9449 - val_mse: 158.4439\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.7022 - mae: 8.4962 - mse: 135.6800 - val_loss: 161.2435 - val_mae: 8.8845 - val_mse: 161.2214\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 136.0474 - mae: 8.4725 - mse: 136.0252 - val_loss: 157.1718 - val_mae: 9.0113 - val_mse: 157.1495\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.5027 - mae: 8.4819 - mse: 135.4805 - val_loss: 158.3609 - val_mae: 8.9053 - val_mse: 158.3388\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.4027 - mae: 8.4874 - mse: 135.3804 - val_loss: 159.0916 - val_mae: 8.8919 - val_mse: 159.0694\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 136.9825 - mae: 8.4959 - mse: 136.9603 - val_loss: 160.4089 - val_mae: 8.8841 - val_mse: 160.3867\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 134.5680 - mae: 8.4317 - mse: 134.5457 - val_loss: 163.0725 - val_mae: 8.9057 - val_mse: 163.0504\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.0787 - mae: 8.4376 - mse: 134.0565 - val_loss: 158.5993 - val_mae: 8.9052 - val_mse: 158.5771\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.0793 - mae: 8.4545 - mse: 135.0571 - val_loss: 159.9048 - val_mae: 8.8880 - val_mse: 159.8826\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.7478 - mae: 8.4949 - mse: 135.7257 - val_loss: 158.5078 - val_mae: 8.8987 - val_mse: 158.4856\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.2552 - mae: 8.4413 - mse: 135.2329 - val_loss: 156.7964 - val_mae: 8.9962 - val_mse: 156.7741\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.4222 - mae: 8.4179 - mse: 133.3998 - val_loss: 158.6007 - val_mae: 8.9260 - val_mse: 158.5783\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.8522 - mae: 8.4517 - mse: 133.8298 - val_loss: 159.4805 - val_mae: 8.9274 - val_mse: 159.4581\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.3393 - mae: 8.4035 - mse: 133.3168 - val_loss: 161.2412 - val_mae: 8.9028 - val_mse: 161.2188\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.6974 - mae: 8.4303 - mse: 133.6748 - val_loss: 158.5691 - val_mae: 8.9267 - val_mse: 158.5465\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.7154 - mae: 8.4060 - mse: 133.6928 - val_loss: 158.5703 - val_mae: 8.9530 - val_mse: 158.5476\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.0964 - mae: 8.4088 - mse: 133.0738 - val_loss: 157.1447 - val_mae: 9.0220 - val_mse: 157.1218\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.5575 - mae: 8.3937 - mse: 132.5346 - val_loss: 157.3627 - val_mae: 8.9927 - val_mse: 157.3398\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.1762 - mae: 8.3943 - mse: 134.1533 - val_loss: 157.7100 - val_mae: 8.9565 - val_mse: 157.6871\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.7304 - mae: 8.4317 - mse: 133.7074 - val_loss: 157.7498 - val_mae: 8.9796 - val_mse: 157.7268\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.9754 - mae: 8.4005 - mse: 132.9523 - val_loss: 157.7427 - val_mae: 8.9838 - val_mse: 157.7196\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.2524 - mae: 8.4367 - mse: 133.2293 - val_loss: 158.2276 - val_mae: 8.9247 - val_mse: 158.2044\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.9431 - mae: 8.4191 - mse: 133.9198 - val_loss: 158.1878 - val_mae: 8.9227 - val_mse: 158.1645\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.5559 - mae: 8.4032 - mse: 133.5325 - val_loss: 157.1349 - val_mae: 8.9869 - val_mse: 157.1114\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.1472 - mae: 8.3836 - mse: 133.1237 - val_loss: 157.5854 - val_mae: 8.9335 - val_mse: 157.5620\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 131.4223 - mae: 8.3376 - mse: 131.3987 - val_loss: 157.3316 - val_mae: 9.0508 - val_mse: 157.3080\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.1857 - mae: 8.3999 - mse: 131.1620 - val_loss: 158.4739 - val_mae: 8.9515 - val_mse: 158.4502\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.4268 - mae: 8.3958 - mse: 132.4028 - val_loss: 157.1064 - val_mae: 9.0198 - val_mse: 157.0824\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 132.3690 - mae: 8.4188 - mse: 132.3450 - val_loss: 158.1602 - val_mae: 8.9133 - val_mse: 158.1362\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.2770 - mae: 8.3893 - mse: 132.2529 - val_loss: 158.3078 - val_mae: 8.9466 - val_mse: 158.2836\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.8176 - mae: 8.3580 - mse: 130.7933 - val_loss: 158.6444 - val_mae: 8.9156 - val_mse: 158.6200\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.4492 - mae: 8.3923 - mse: 132.4246 - val_loss: 159.2530 - val_mae: 8.8912 - val_mse: 159.2285\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.6559 - mae: 8.3669 - mse: 130.6312 - val_loss: 157.2578 - val_mae: 8.9223 - val_mse: 157.2330\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.4439 - mae: 8.3999 - mse: 131.4190 - val_loss: 157.5294 - val_mae: 8.9461 - val_mse: 157.5045\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.9933 - mae: 8.3427 - mse: 129.9682 - val_loss: 158.1962 - val_mae: 8.8989 - val_mse: 158.1712\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.8299 - mae: 8.3125 - mse: 129.8047 - val_loss: 159.2200 - val_mae: 8.9159 - val_mse: 159.1948\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.0636 - mae: 8.3768 - mse: 132.0383 - val_loss: 158.0105 - val_mae: 8.9926 - val_mse: 157.9851\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.3730 - mae: 8.3461 - mse: 131.3475 - val_loss: 157.6844 - val_mae: 8.9728 - val_mse: 157.6589\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.6499 - mae: 8.3529 - mse: 129.6243 - val_loss: 158.9574 - val_mae: 8.9056 - val_mse: 158.9316\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.2778 - mae: 8.3670 - mse: 130.2519 - val_loss: 157.2575 - val_mae: 8.9759 - val_mse: 157.2315\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.3107 - mae: 8.3224 - mse: 129.2847 - val_loss: 158.0753 - val_mae: 8.8829 - val_mse: 158.0492\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.2594 - mae: 8.3206 - mse: 130.2330 - val_loss: 157.9728 - val_mae: 8.9081 - val_mse: 157.9464\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.0968 - mae: 8.3407 - mse: 130.0704 - val_loss: 159.2814 - val_mae: 8.8787 - val_mse: 159.2550\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 129.3591 - mae: 8.2688 - mse: 129.3324 - val_loss: 157.9389 - val_mae: 8.9307 - val_mse: 157.9122\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.9067 - mae: 8.3226 - mse: 128.8798 - val_loss: 158.9740 - val_mae: 8.8898 - val_mse: 158.9471\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.8706 - mae: 8.3511 - mse: 130.8436 - val_loss: 157.2263 - val_mae: 8.9277 - val_mse: 157.1992\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.4710 - mae: 8.3432 - mse: 130.4438 - val_loss: 156.9301 - val_mae: 8.9355 - val_mse: 156.9028\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.4616 - mae: 8.3134 - mse: 129.4343 - val_loss: 158.1703 - val_mae: 8.8907 - val_mse: 158.1429\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.1560 - mae: 8.2956 - mse: 129.1285 - val_loss: 157.7897 - val_mae: 8.9276 - val_mse: 157.7621\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.1426 - mae: 8.3000 - mse: 128.1148 - val_loss: 157.4791 - val_mae: 9.0033 - val_mse: 157.4511\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.6705 - mae: 8.3236 - mse: 128.6425 - val_loss: 158.1551 - val_mae: 8.9385 - val_mse: 158.1269\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.7000 - mae: 8.3410 - mse: 129.6717 - val_loss: 158.6231 - val_mae: 8.9096 - val_mse: 158.5947\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.8939 - mae: 8.3046 - mse: 128.8655 - val_loss: 159.0999 - val_mae: 8.9321 - val_mse: 159.0713\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.8801 - mae: 8.3104 - mse: 127.8513 - val_loss: 159.0789 - val_mae: 8.8957 - val_mse: 159.0500\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.3160 - mae: 8.3032 - mse: 127.2870 - val_loss: 158.0780 - val_mae: 8.9279 - val_mse: 158.0488\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.3149 - mae: 8.3540 - mse: 129.2857 - val_loss: 158.8266 - val_mae: 8.9181 - val_mse: 158.7973\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.3604 - mae: 8.2923 - mse: 129.3310 - val_loss: 157.9165 - val_mae: 8.9354 - val_mse: 157.8870\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.8857 - mae: 8.3165 - mse: 128.8561 - val_loss: 159.6189 - val_mae: 8.8635 - val_mse: 159.5893\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.0559 - mae: 8.3085 - mse: 128.0260 - val_loss: 157.4719 - val_mae: 8.9429 - val_mse: 157.4419\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.8093 - mae: 8.2896 - mse: 127.7792 - val_loss: 158.9981 - val_mae: 8.8935 - val_mse: 158.9678\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.4194 - mae: 8.2973 - mse: 127.3890 - val_loss: 158.7674 - val_mae: 8.8819 - val_mse: 158.7369\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.7534 - mae: 8.3312 - mse: 129.7229 - val_loss: 158.6714 - val_mae: 8.8962 - val_mse: 158.6407\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.8724 - mae: 8.3253 - mse: 128.8415 - val_loss: 157.3571 - val_mae: 8.9387 - val_mse: 157.3260\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.1864 - mae: 8.3096 - mse: 128.1551 - val_loss: 158.3554 - val_mae: 8.8873 - val_mse: 158.3240\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 126.8384 - mae: 8.2658 - mse: 126.8069 - val_loss: 159.2061 - val_mae: 8.8581 - val_mse: 159.1743\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.5743 - mae: 8.2999 - mse: 127.5425 - val_loss: 157.6018 - val_mae: 8.8904 - val_mse: 157.5698\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.5852 - mae: 8.2764 - mse: 126.5530 - val_loss: 157.1176 - val_mae: 8.9526 - val_mse: 157.0851\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.6887 - mae: 8.3052 - mse: 126.6561 - val_loss: 157.3606 - val_mae: 8.9209 - val_mse: 157.3279\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.9348 - mae: 8.2923 - mse: 126.9019 - val_loss: 157.2398 - val_mae: 8.9974 - val_mse: 157.2067\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.8928 - mae: 8.2431 - mse: 125.8597 - val_loss: 157.8205 - val_mae: 8.9126 - val_mse: 157.7871\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.7876 - mae: 8.2939 - mse: 127.7542 - val_loss: 158.7070 - val_mae: 8.8592 - val_mse: 158.6735\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.7010 - mae: 8.2376 - mse: 125.6673 - val_loss: 157.8119 - val_mae: 8.9333 - val_mse: 157.7781\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.7267 - mae: 8.2585 - mse: 125.6928 - val_loss: 158.4928 - val_mae: 8.8974 - val_mse: 158.4588\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.3609 - mae: 8.2359 - mse: 125.3267 - val_loss: 158.5184 - val_mae: 8.9130 - val_mse: 158.4840\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.9845 - mae: 8.2600 - mse: 125.9501 - val_loss: 158.4464 - val_mae: 8.9080 - val_mse: 158.4117\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.7870 - mae: 8.2689 - mse: 126.7521 - val_loss: 159.1105 - val_mae: 8.9579 - val_mse: 159.0754\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.1154 - mae: 8.2490 - mse: 126.0803 - val_loss: 158.8660 - val_mae: 8.8903 - val_mse: 158.8308\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.9608 - mae: 8.2571 - mse: 124.9252 - val_loss: 158.7849 - val_mae: 8.8983 - val_mse: 158.7492\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.3400 - mae: 8.2465 - mse: 125.3042 - val_loss: 157.5267 - val_mae: 9.0074 - val_mse: 157.4905\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.9064 - mae: 8.3001 - mse: 126.8701 - val_loss: 159.0004 - val_mae: 8.8755 - val_mse: 158.9640\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.5783 - mae: 8.2459 - mse: 126.5418 - val_loss: 160.1617 - val_mae: 8.8924 - val_mse: 160.1253\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.3584 - mae: 8.1905 - mse: 124.3217 - val_loss: 158.5652 - val_mae: 8.9791 - val_mse: 158.5281\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.9766 - mae: 8.2441 - mse: 124.9394 - val_loss: 159.0528 - val_mae: 8.9256 - val_mse: 159.0154\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.6425 - mae: 8.2671 - mse: 125.6049 - val_loss: 160.2301 - val_mae: 8.8620 - val_mse: 160.1925\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.9444 - mae: 8.2188 - mse: 123.9067 - val_loss: 157.9192 - val_mae: 8.9556 - val_mse: 157.8812\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.5326 - mae: 8.2328 - mse: 124.4946 - val_loss: 158.6541 - val_mae: 8.8619 - val_mse: 158.6159\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.7671 - mae: 8.2025 - mse: 122.7286 - val_loss: 159.3563 - val_mae: 8.8872 - val_mse: 159.3176\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.6260 - mae: 8.2650 - mse: 126.5873 - val_loss: 158.5636 - val_mae: 8.9181 - val_mse: 158.5248\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.3865 - mae: 8.2241 - mse: 125.3475 - val_loss: 159.3932 - val_mae: 8.8971 - val_mse: 159.3540\n"
     ]
    }
   ],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding a layer\n",
      "Train MAE: 7.9032, Train MSE: 119.7173\n",
      "Val   MAE: 8.8971, Val   MSE: 159.3540\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very nearly identical to the initial configurations. Doubling the kernels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 1s 3ms/step - loss: 160.4022 - mae: 9.2465 - mse: 160.3602 - val_loss: 168.4850 - val_mae: 9.0377 - val_mse: 168.4428\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 144.7906 - mae: 8.7934 - mse: 144.7483 - val_loss: 164.0674 - val_mae: 8.9951 - val_mse: 164.0248\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 143.3723 - mae: 8.7512 - mse: 143.3296 - val_loss: 164.8219 - val_mae: 8.9419 - val_mse: 164.7791\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 139.4269 - mae: 8.6430 - mse: 139.3838 - val_loss: 160.2244 - val_mae: 9.0327 - val_mse: 160.1812\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 138.0066 - mae: 8.5284 - mse: 137.9633 - val_loss: 166.1969 - val_mae: 8.9518 - val_mse: 166.1535\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.4465 - mae: 8.5308 - mse: 137.4031 - val_loss: 161.0059 - val_mae: 8.9341 - val_mse: 160.9623\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 137.0757 - mae: 8.5353 - mse: 137.0321 - val_loss: 168.8796 - val_mae: 8.9359 - val_mse: 168.8358\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.3308 - mae: 8.4814 - mse: 135.2868 - val_loss: 161.2309 - val_mae: 8.8675 - val_mse: 161.1868\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.8335 - mae: 8.4952 - mse: 134.7893 - val_loss: 159.5040 - val_mae: 8.8730 - val_mse: 159.4597\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 135.2992 - mae: 8.4642 - mse: 135.2547 - val_loss: 158.3961 - val_mae: 8.8820 - val_mse: 158.3514\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.7946 - mae: 8.4569 - mse: 133.7499 - val_loss: 160.6725 - val_mae: 8.8657 - val_mse: 160.6277\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.0659 - mae: 8.4212 - mse: 133.0209 - val_loss: 158.6974 - val_mae: 8.8875 - val_mse: 158.6522\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.2604 - mae: 8.4231 - mse: 134.2152 - val_loss: 162.2876 - val_mae: 8.8657 - val_mse: 162.2423\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 134.9142 - mae: 8.4444 - mse: 134.8687 - val_loss: 159.5402 - val_mae: 8.8653 - val_mse: 159.4947\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.2004 - mae: 8.3721 - mse: 133.1547 - val_loss: 159.5397 - val_mae: 8.8477 - val_mse: 159.4940\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 134.3419 - mae: 8.4387 - mse: 134.2959 - val_loss: 159.4659 - val_mae: 8.8796 - val_mse: 159.4198\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.1683 - mae: 8.3666 - mse: 131.1219 - val_loss: 159.3882 - val_mae: 8.8570 - val_mse: 159.3417\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.4258 - mae: 8.3959 - mse: 132.3790 - val_loss: 158.2128 - val_mae: 8.8732 - val_mse: 158.1659\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 132.1174 - mae: 8.3782 - mse: 132.0702 - val_loss: 160.0573 - val_mae: 8.8627 - val_mse: 160.0100\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 133.3281 - mae: 8.3916 - mse: 133.2806 - val_loss: 159.3640 - val_mae: 8.8643 - val_mse: 159.3165\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.4920 - mae: 8.3807 - mse: 131.4442 - val_loss: 160.2081 - val_mae: 8.9069 - val_mse: 160.1601\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.7363 - mae: 8.3720 - mse: 131.6881 - val_loss: 158.1501 - val_mae: 8.9273 - val_mse: 158.1018\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 131.9941 - mae: 8.3906 - mse: 131.9456 - val_loss: 161.1308 - val_mae: 8.8544 - val_mse: 161.0821\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.1868 - mae: 8.2995 - mse: 130.1380 - val_loss: 156.5800 - val_mae: 9.0915 - val_mse: 156.5308\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.2456 - mae: 8.3841 - mse: 131.1964 - val_loss: 157.6430 - val_mae: 8.9108 - val_mse: 157.5936\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 131.2387 - mae: 8.3598 - mse: 131.1891 - val_loss: 158.7104 - val_mae: 8.8692 - val_mse: 158.6607\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.3579 - mae: 8.3198 - mse: 130.3079 - val_loss: 158.4182 - val_mae: 8.8874 - val_mse: 158.3680\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 130.7705 - mae: 8.3634 - mse: 130.7202 - val_loss: 158.9632 - val_mae: 8.8504 - val_mse: 158.9127\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 129.8090 - mae: 8.2941 - mse: 129.7582 - val_loss: 156.8248 - val_mae: 8.9560 - val_mse: 156.7737\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 129.9548 - mae: 8.3158 - mse: 129.9035 - val_loss: 157.2097 - val_mae: 8.9139 - val_mse: 157.1582\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 129.5581 - mae: 8.3228 - mse: 129.5064 - val_loss: 162.3614 - val_mae: 8.8807 - val_mse: 162.3096\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 128.6247 - mae: 8.2953 - mse: 128.5725 - val_loss: 157.5195 - val_mae: 8.9609 - val_mse: 157.4670\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 129.3908 - mae: 8.3127 - mse: 129.3382 - val_loss: 157.4324 - val_mae: 8.9241 - val_mse: 157.3795\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.6805 - mae: 8.2761 - mse: 128.6274 - val_loss: 158.2361 - val_mae: 8.8594 - val_mse: 158.1828\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.8866 - mae: 8.2741 - mse: 127.8329 - val_loss: 155.1230 - val_mae: 8.9752 - val_mse: 155.0690\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 127.7676 - mae: 8.2816 - mse: 127.7134 - val_loss: 155.7251 - val_mae: 9.0105 - val_mse: 155.6706\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.1136 - mae: 8.2919 - mse: 128.0587 - val_loss: 157.4816 - val_mae: 8.8301 - val_mse: 157.4267\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 128.2208 - mae: 8.3200 - mse: 128.1655 - val_loss: 157.9173 - val_mae: 8.8462 - val_mse: 157.8617\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.7768 - mae: 8.3172 - mse: 128.7209 - val_loss: 156.7353 - val_mae: 8.8650 - val_mse: 156.6792\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 127.9869 - mae: 8.2985 - mse: 127.9304 - val_loss: 158.6942 - val_mae: 8.8604 - val_mse: 158.6375\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 128.4570 - mae: 8.3032 - mse: 128.3999 - val_loss: 157.4420 - val_mae: 8.8791 - val_mse: 157.3848\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.3114 - mae: 8.2478 - mse: 125.2536 - val_loss: 159.2012 - val_mae: 8.8558 - val_mse: 159.1431\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.6056 - mae: 8.2616 - mse: 126.5472 - val_loss: 155.8473 - val_mae: 8.9264 - val_mse: 155.7884\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.1462 - mae: 8.2395 - mse: 126.0870 - val_loss: 157.1151 - val_mae: 8.8763 - val_mse: 157.0556\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.3064 - mae: 8.2350 - mse: 125.2463 - val_loss: 158.2891 - val_mae: 8.8543 - val_mse: 158.2288\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.4718 - mae: 8.2267 - mse: 125.4110 - val_loss: 156.2341 - val_mae: 8.9683 - val_mse: 156.1729\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.1777 - mae: 8.2179 - mse: 125.1161 - val_loss: 157.8094 - val_mae: 8.8806 - val_mse: 157.7476\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.0818 - mae: 8.2256 - mse: 126.0198 - val_loss: 158.5518 - val_mae: 8.8138 - val_mse: 158.4896\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 126.1077 - mae: 8.2170 - mse: 126.0453 - val_loss: 157.6543 - val_mae: 8.8027 - val_mse: 157.5916\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.9215 - mae: 8.2414 - mse: 125.8584 - val_loss: 155.8211 - val_mae: 8.8553 - val_mse: 155.7577\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.9930 - mae: 8.1959 - mse: 124.9291 - val_loss: 156.3784 - val_mae: 8.8721 - val_mse: 156.3143\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.9935 - mae: 8.2008 - mse: 124.9290 - val_loss: 155.5544 - val_mae: 8.9016 - val_mse: 155.4894\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.4695 - mae: 8.1836 - mse: 123.4041 - val_loss: 156.5139 - val_mae: 8.8065 - val_mse: 156.4483\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.4707 - mae: 8.1460 - mse: 122.4046 - val_loss: 157.6186 - val_mae: 8.7801 - val_mse: 157.5522\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 125.1357 - mae: 8.1951 - mse: 125.0691 - val_loss: 158.2306 - val_mae: 8.8205 - val_mse: 158.1639\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.8871 - mae: 8.1888 - mse: 122.8199 - val_loss: 156.6777 - val_mae: 8.9759 - val_mse: 156.6099\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.0004 - mae: 8.1536 - mse: 122.9323 - val_loss: 157.5266 - val_mae: 8.8923 - val_mse: 157.4580\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.9682 - mae: 8.1339 - mse: 122.8993 - val_loss: 157.9959 - val_mae: 8.8644 - val_mse: 157.9267\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 124.1910 - mae: 8.1901 - mse: 124.1216 - val_loss: 156.4641 - val_mae: 8.9011 - val_mse: 156.3946\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.7796 - mae: 8.1979 - mse: 122.7095 - val_loss: 158.7733 - val_mae: 8.8740 - val_mse: 158.7027\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.9274 - mae: 8.1862 - mse: 122.8567 - val_loss: 158.1559 - val_mae: 8.8747 - val_mse: 158.0846\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 123.1153 - mae: 8.1719 - mse: 123.0435 - val_loss: 156.9986 - val_mae: 8.8680 - val_mse: 156.9265\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 122.2807 - mae: 8.1028 - mse: 122.2082 - val_loss: 156.3746 - val_mae: 8.8187 - val_mse: 156.3017\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.6443 - mae: 8.0745 - mse: 120.5709 - val_loss: 159.1667 - val_mae: 8.8038 - val_mse: 159.0932\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.3114 - mae: 8.1405 - mse: 121.2373 - val_loss: 158.3578 - val_mae: 8.8524 - val_mse: 158.2833\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.5240 - mae: 8.1516 - mse: 121.4492 - val_loss: 158.3407 - val_mae: 8.8756 - val_mse: 158.2659\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.0126 - mae: 8.1286 - mse: 120.9372 - val_loss: 157.9523 - val_mae: 8.9494 - val_mse: 157.8766\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 121.4219 - mae: 8.1617 - mse: 121.3458 - val_loss: 158.4231 - val_mae: 8.7855 - val_mse: 158.3468\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.5391 - mae: 8.0751 - mse: 119.4623 - val_loss: 158.2117 - val_mae: 8.9582 - val_mse: 158.1344\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.5783 - mae: 8.0975 - mse: 119.5006 - val_loss: 157.7747 - val_mae: 8.9098 - val_mse: 157.6966\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.7631 - mae: 8.0687 - mse: 119.6845 - val_loss: 159.1688 - val_mae: 8.8363 - val_mse: 159.0898\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.7225 - mae: 8.0981 - mse: 119.6433 - val_loss: 157.6640 - val_mae: 8.8747 - val_mse: 157.5842\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.4752 - mae: 8.0390 - mse: 119.3950 - val_loss: 157.2592 - val_mae: 8.9675 - val_mse: 157.1785\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.6726 - mae: 8.0627 - mse: 118.5914 - val_loss: 157.8702 - val_mae: 8.8469 - val_mse: 157.7886\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 119.7501 - mae: 8.0660 - mse: 119.6680 - val_loss: 157.5418 - val_mae: 8.8517 - val_mse: 157.4595\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.3603 - mae: 8.0617 - mse: 117.2775 - val_loss: 159.1422 - val_mae: 8.9917 - val_mse: 159.0587\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.9484 - mae: 8.0070 - mse: 117.8646 - val_loss: 159.6393 - val_mae: 8.8699 - val_mse: 159.5553\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.9605 - mae: 8.0374 - mse: 118.8760 - val_loss: 158.4114 - val_mae: 8.8523 - val_mse: 158.3266\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 118.2521 - mae: 8.0534 - mse: 118.1669 - val_loss: 158.9862 - val_mae: 8.8978 - val_mse: 158.9007\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.4150 - mae: 7.9829 - mse: 117.3294 - val_loss: 158.1318 - val_mae: 8.8993 - val_mse: 158.0457\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.3771 - mae: 7.9454 - mse: 115.2906 - val_loss: 158.4371 - val_mae: 8.9167 - val_mse: 158.3500\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 120.4999 - mae: 8.0951 - mse: 120.4126 - val_loss: 160.3067 - val_mae: 8.9205 - val_mse: 160.2191\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.1062 - mae: 7.9853 - mse: 114.0178 - val_loss: 160.7775 - val_mae: 8.8683 - val_mse: 160.6886\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.0767 - mae: 7.9965 - mse: 116.9874 - val_loss: 157.9775 - val_mae: 8.9207 - val_mse: 157.8877\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.2406 - mae: 8.0366 - mse: 117.1502 - val_loss: 159.6920 - val_mae: 8.8401 - val_mse: 159.6015\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 117.5616 - mae: 8.0526 - mse: 117.4704 - val_loss: 159.3009 - val_mae: 8.9589 - val_mse: 159.2092\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.8728 - mae: 7.9309 - mse: 113.7804 - val_loss: 163.9952 - val_mae: 8.9669 - val_mse: 163.9026\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.2940 - mae: 7.9907 - mse: 116.2010 - val_loss: 162.2930 - val_mae: 8.9631 - val_mse: 162.1994\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.2411 - mae: 8.0479 - mse: 116.1469 - val_loss: 161.3263 - val_mae: 8.9072 - val_mse: 161.2318\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.5149 - mae: 7.9197 - mse: 114.4198 - val_loss: 161.6512 - val_mae: 8.9268 - val_mse: 161.5557\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.4923 - mae: 7.9537 - mse: 114.3964 - val_loss: 160.2056 - val_mae: 8.9813 - val_mse: 160.1092\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.8213 - mae: 7.9999 - mse: 116.7247 - val_loss: 160.4769 - val_mae: 8.8895 - val_mse: 160.3799\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 115.4251 - mae: 7.9498 - mse: 115.3277 - val_loss: 160.3415 - val_mae: 8.8612 - val_mse: 160.2438\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 114.4556 - mae: 7.9296 - mse: 114.3573 - val_loss: 160.4327 - val_mae: 8.9671 - val_mse: 160.3340\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.0761 - mae: 7.9950 - mse: 113.9767 - val_loss: 162.9843 - val_mae: 8.8713 - val_mse: 162.8844\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 116.6363 - mae: 8.0062 - mse: 116.5360 - val_loss: 159.7512 - val_mae: 8.8985 - val_mse: 159.6506\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 113.0876 - mae: 7.9078 - mse: 112.9863 - val_loss: 162.4366 - val_mae: 8.8885 - val_mse: 162.3351\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.6770 - mae: 7.9844 - mse: 114.5750 - val_loss: 159.7254 - val_mae: 8.9319 - val_mse: 159.6227\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 114.8407 - mae: 7.9467 - mse: 114.7377 - val_loss: 161.1671 - val_mae: 8.9123 - val_mse: 161.0639\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 111.5859 - mae: 7.8783 - mse: 111.4822 - val_loss: 159.8013 - val_mae: 8.9109 - val_mse: 159.6969\n"
     ]
    }
   ],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding a layer\n",
      "Train MAE: 7.4787, Train MSE: 103.1420\n",
      "Val   MAE: 8.9109, Val   MSE: 159.6969\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model has the best MAE scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance\n",
    "\n",
    "**XGBoost**\n",
    "The XGBoost model did not return meaningful results for this dataset. The best metrics for XGBoost were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- RMSE: 22.35\n",
    "- r<sup>2</sup>: 0.138\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- RMSE: 11.73\n",
    "- - r<sup>2</sup>: 0.002\n",
    "\n",
    "**k-Nearest Neighbors**\n",
    "This model also did not perform well on this data set. Its best metrics were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- Accuracy: 0.052\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- Accuracy: 0.217\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "The deep learning model returned the most promising results and are described in the next section.\n",
    "\n",
    "### Final Models\n",
    "\n",
    "The final models are both based on a deep learning architecture. \n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "The best model for the Maximum Peak Position is named reg_model3_1. It is a 3-layer, regularized model trained on the song dataset that does not contain genre information.Its final metrics were:\n",
    "\n",
    "- Training mean absolute error: 16.99\n",
    "- Validation mean absolute error: 17.67\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "The best model for the Maximum Rank Increase is named reg_model4_2. It is also a 3-layer, regularized model trained on the song dataset without genre.\n",
    "Its final metrics:\n",
    "\n",
    "- Training mean absolute error: 7.75\n",
    "- Validation mean absolute error: 8.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genre of a song, at least how it is captured on Spotify, has very little influence on its movement on the Billboard Hot 100 list. This gives FutureProduct Advisors consultants and their customers significant leeway when they are selecting music for their social and advertising campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristics of a given song appear to be the most important factors in its performance on the Hot 100 list. Features such as tempo, danceability, etc. have the most predictive power for the Hot 100 list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital music has a large number of characteristics that can be pulled into machine learning models. The factors that have the largest influence on popularity are often unexpected, and modeling and analysis of songs requires methodical and careful selection and vetting of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "reg_model3_1 (predicting highest ranking on the Billboard Hot 100) and reg_model4_2 (predicting largest week over week ranking increase) are ready for initial deployment to FutureProduct Advisors. \n",
    "\n",
    "Detailed training will be provided, but at a high level here is how users will engage:\n",
    "\n",
    "1. Consultants will identify a list of songs they'd like to explore.\n",
    "    - We recommend using the songs in places 90-100 of the Billboard Hot 100 at a minimum\n",
    "2. Consultants download those songs' metadata from Spotify as a CSV file (detailed instructions to come)\n",
    "3. Consultants load the CSV file into these models, and the models will return predicted activity for each song.\n",
    "\n",
    "There is also an additional opportunity to built on and refine these models by indluding additional data and experimenting with other modeling approaches. If the FutureProduct Advisors consultants have positive feedback on these prototype tools, we will be happy to partner on future enhancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
