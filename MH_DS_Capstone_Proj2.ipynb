{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Max_Peak_Position'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_pos_dist = df_cleaned['Max_Peak_Position'].value_counts()\n",
    "peak_pos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(peak_pos_dist)\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "def diff(a, b):\n",
    "    return a - b\n",
    "\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_preview_url', 'spotify_track_explicit', \n",
    "                                        'spotify_track_popularity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df with the max weekly rank change for each song in df_hotlist_2000s\n",
    "df_max_rank_change = df_hotlist_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song in df_hotlist_2000s\n",
    "df_max_peak_pos = df_hotlist_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting full list of songs in the time period being studied\n",
    "songid_list = df_hotlist_2000s['SongID'].unique()\n",
    "\n",
    "# creating a features df with only songs in df_hotlist_2000s\n",
    "df_features_2000s = df_features_all[df_features_all['SongID'].isin(songid_list)]\n",
    "\n",
    "# checking for duplicates\n",
    "print(len(df_features_2000s))\n",
    "print(len(pd.unique(df_features_2000s['SongID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates and rechecking\n",
    "df_features_2000s = df_features_2000s.drop_duplicates(subset='SongID')\n",
    "\n",
    "print(len(df_features_2000s))\n",
    "print(len(pd.unique(df_features_2000s['SongID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding max peak position to features df\n",
    "df_2000s_data = df_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to features df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "# removing entries with missing values and defining as a new df\n",
    "df_cleaned = df_2000s_data[df_2000s_data.notna().all(axis=1)]\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Genre\n",
    "\n",
    "The dataset has genre in a single column; the entry for each song has a variety of genres listed in that single column. In order to explore genre, I'll need to break this field out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_cleaned['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_cleaned['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I created a new csv that contains genres which appear in 50 or more song entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading list of genres with 50 or more instances in df_cleaned\n",
    "df_genres_50_up = pd.read_csv('genre_counts_50+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_50_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating each new genre column and initializing to 0\n",
    "for genre in final_genres_list:\n",
    "    df_cleaned[genre] = 0\n",
    "\n",
    "# iterating through rows to set values to 1 when genre column appears in original spotify_genre column\n",
    "for idx, genre_string in enumerate(df_cleaned['spotify_genre']):\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        for genre in genre_list:\n",
    "            df_cleaned.at[idx, genre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviewing full df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_cleaned.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing fields used for prep/cleaning but not needed for analysis\n",
    "df_cleaned = df_cleaned.drop(['SongID', 'spotify_genre', 'spotify_track_id'], axis=1)\n",
    "\n",
    "# my code added columns for all genres in spotify_genre, removing unwanted columns and creating a clean df with genre\n",
    "last_col_to_keep_genre = 'emo rap'\n",
    "df_cleaned_genre = df_cleaned.loc[:, :last_col_to_keep_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "\n",
    "# creating a clean df for analysis without genre\n",
    "last_col_to_keep_no_genre = 'Max_Rank_Change'\n",
    "df_cleaned_no_genre = df_cleaned.loc[:, :last_col_to_keep_no_genre]\n",
    "# removing NaN rows\n",
    "df_cleaned_genre = df_cleaned_genre.dropna()\n",
    "df_cleaned_no_genre = df_cleaned_no_genre.dropna()\n",
    "\n",
    "df_cleaned_genre.info(), df_cleaned_no_genre.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (1__1 variables)\n",
    "2. Max Peak Position, with genre (1_2 variables)\n",
    "3. Max Rank Change, no genre (2_1 variables)\n",
    "4. Max Rank Change, with genre (2_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X1_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y1_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_1_train, X1_1_test, y1_1_train, y1_1_test = train_test_split(X1_1, y1_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_1_train_scaled = scaler.fit_transform(X1_1_train)\n",
    "X1_1_test_scaled = scaler.fit_transform(X1_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X1_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y1_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X1_2_train, X1_2_test, y1_2_train, y1_2_test = train_test_split(X1_2, y1_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X1_2_train_scaled = scaler.fit_transform(X1_2_train)\n",
    "X1_2_test_scaled = scaler.fit_transform(X1_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X2_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_1_train, X2_1_test, y2_1_train, y2_1_test = train_test_split(X2_1, y2_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_1_train_scaled = scaler.fit_transform(X2_1_train)\n",
    "X2_1_test_scaled = scaler.fit_transform(X2_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X2_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y2_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X2_2_train, X2_2_test, y2_2_train, y2_2_test = train_test_split(X2_2, y2_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X2_2_train_scaled = scaler.fit_transform(X2_2_train)\n",
    "X2_2_test_scaled = scaler.fit_transform(X2_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (3__1 variables)\n",
    "2. Max Peak Position, with genre (3_2 variables)\n",
    "3. Max Rank Change, no genre (4_1 variables)\n",
    "4. Max Rank Change, with genre (4_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X3_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y3_1 = df_cleaned_no_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_1_train, X3_1_test, y3_1_train, y3_1_test = train_test_split(X3_1, y3_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_1_train_final, X3_1_val, y3_1_train_final, y3_1_val = train_test_split(X3_1_train, y3_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X3_1_train_final)\n",
    "X3_1_train_scaled = scaler.transform(X3_1_train_final)\n",
    "X3_1_val_scaled = scaler.transform(X3_1_val)\n",
    "X3_1_test_scaled = scaler.transform(X3_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X3_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y3_2 = df_cleaned_genre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X3_2_train, X3_2_test, y3_2_train, y3_2_test = train_test_split(X3_2, y3_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X3_2_train_final, X3_2_val, y3_2_train_final, y3_2_val = train_test_split(X3_2_train, y3_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X3_2_train_final)\n",
    "X3_2_train_scaled = scaler.transform(X3_2_train_final)\n",
    "X3_2_val_scaled = scaler.transform(X3_2_val)\n",
    "X3_2_test_scaled = scaler.transform(X3_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X4_1 = df_cleaned_no_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_1 = df_cleaned_no_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_1_train, X4_1_test, y4_1_train, y4_1_test = train_test_split(X4_1, y4_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_1_train_final, X4_1_val, y4_1_train_final, y4_1_val = train_test_split(X4_1_train, y4_1_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_1_train_final)\n",
    "X4_1_train_scaled = scaler.transform(X4_1_train_final)\n",
    "X4_1_val_scaled = scaler.transform(X4_1_val)\n",
    "X4_1_test_scaled = scaler.transform(X4_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X4_2 = df_cleaned_genre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y4_2 = df_cleaned_genre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X4_2_train, X4_2_test, y4_2_train, y4_2_test = train_test_split(X4_2, y4_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X4_2_train_final, X4_2_val, y4_2_train_final, y4_2_val = train_test_split(X4_2_train, y4_2_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X4_2_train_final)\n",
    "X4_2_train_scaled = scaler.transform(X4_2_train_final)\n",
    "X4_2_val_scaled = scaler.transform(X4_2_val)\n",
    "X4_2_test_scaled = scaler.transform(X4_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_model1_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_1.fit(X1_1_train, y1_1_train)\n",
    "y1_1_pred = xgb_model1_1.predict(X1_1_test)\n",
    "y1_1_pred = np.clip(np.round(y1_1_pred), 1, 100)\n",
    "\n",
    "rmse1_1 = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred))\n",
    "r2_1_1 = r2_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_1:.3f}')\n",
    "print(f'R²: {r2_1_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.005, 0.01, 0.015,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [1.0, 1.1, 1.2],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.015],\n",
    "    'subsample': [0.73, 0.74, 0.75],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_1 = GridSearchCV(estimator=xgb_model1_1,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_1.fit(X1_1_train, y1_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb1_1.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_1_pred_best = best_xgb1_1.predict(X1_1_test)\n",
    "y1_1_pred_best = np.clip(np.round(y1_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_1_best = np.sqrt(mean_squared_error(y1_1_test, y1_1_pred_best))\n",
    "r2_1_1_best = r2_score(y1_1_test, y1_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_1_best:.3f}')\n",
    "print(f'R²: {r2_1_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_model1_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model1_2.fit(X1_2_train, y1_2_train)\n",
    "y1_2_pred = xgb_model1_2.predict(X1_2_test)\n",
    "y1_2_pred = np.clip(np.round(y1_2_pred), 1, 100)\n",
    "\n",
    "rmse1_2 = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred))\n",
    "r2_1_2 = r2_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse1_2:.3f}')\n",
    "print(f'R²: {r2_1_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [8, 9, 10],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [7, 8],\n",
    "    'learning_rate': [0.013, 0.015, 0.017],\n",
    "    'subsample': [0.5, 0.6],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb1_2 = GridSearchCV(estimator=xgb_model1_2,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb1_2.fit(X1_2_train, y1_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb1_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model \n",
    "best_xgb1_2 = grid_search_xgb1_2.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y1_2_pred_best = best_xgb1_2.predict(X1_2_test)\n",
    "y1_2_pred_best = np.clip(np.round(y1_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse1_2_best = np.sqrt(mean_squared_error(y1_2_test, y1_2_pred_best))\n",
    "r2_1_2_best = r2_score(y1_2_test, y1_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse1_2_best:.3f}')\n",
    "print(f'R²: {r2_1_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_1 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_1.fit(X2_1_train, y2_1_train)\n",
    "y2_1_pred = xgb_model2_1.predict(X2_1_test)\n",
    "y2_1_pred = np.clip(np.round(y2_1_pred), 1, 100)\n",
    "\n",
    "rmse2_1 = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred))\n",
    "r2_2_1 = r2_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_1:.3f}')\n",
    "print(f'R²: {r2_2_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3 for max_rank_change\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.9],\n",
    "        }\n",
    "\n",
    "grid_search_xgb2_1 = GridSearchCV(estimator=xgb_model2_1,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_1.fit(X2_1_train, y2_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_1 = grid_search_xgb2_1.best_estimator_\n",
    "\n",
    "y2_1_pred_best = best_xgb2_1.predict(X2_1_test)\n",
    "y2_1_pred_best = np.clip(np.round(y2_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_1_best = np.sqrt(mean_squared_error(y2_1_test, y2_1_pred_best))\n",
    "r2_2_1_best = r2_score(y2_1_test, y2_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_1_best:.3f}')\n",
    "print(f'R²: {r2_2_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_model2_2 = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_model2_2.fit(X2_2_train, y2_2_train)\n",
    "y2_2_pred = xgb_model2_2.predict(X2_2_test)\n",
    "y2_2_pred = np.clip(np.round(y2_2_pred), 1, 100)\n",
    "\n",
    "rmse2_2 = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred))\n",
    "r2_2_2 = r2_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse2_2:.3f}')\n",
    "print(f'R²: {r2_2_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1, 1.2],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [2],\n",
    "    'learning_rate': [0.003, 0.005, 0.007],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb2_2 = GridSearchCV(estimator=xgb_model2_2,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb2_2.fit(X2_2_train, y2_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb2_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb2_2 = grid_search_xgb2_2.best_estimator_\n",
    "\n",
    "y2_2_pred_best = best_xgb2_2.predict(X2_2_test)\n",
    "y2_2_pred_best = np.clip(np.round(y2_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse2_2_best = np.sqrt(mean_squared_error(y2_2_test, y2_2_pred_best))\n",
    "r2_2_2_best = r2_score(y2_2_test, y2_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse2_2_best:.3f}')\n",
    "print(f'R²: {r2_2_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, in all cases the r^2 values for the model were less than 0.02, indicating a near-zero fit of the model to the test data. XGBoost will not be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_1.fit(X1_1_train_scaled, y1_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_1 = grid_search1_1.best_params_\n",
    "standard_best_score1_1 = grid_search1_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model1_1 = grid_search1_1.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y1_1_pred = final_model1_1.predict(X1_1_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_1 = accuracy_score(y1_1_test, y1_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search1_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search1_2.fit(X1_2_train_scaled, y1_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params1_2 = grid_search1_2.best_params_\n",
    "standard_best_score1_2 = grid_search1_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params1_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score1_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model1_2 = grid_search1_2.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y1_2_pred = final_model1_2.predict(X1_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy1_2 = accuracy_score(y1_2_test, y1_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy1_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_1 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_1.fit(X2_1_train_scaled, y2_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_1 = grid_search2_1.best_params_\n",
    "standard_best_score2_1 = grid_search2_1.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_1}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model2_1 = grid_search2_1.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y1_1_pred = final_model1_1.predict(X2_1_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_1 = accuracy_score(y2_1_test, y2_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search2_2 = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search2_2.fit(X2_2_train_scaled, y2_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params2_2 = grid_search2_2.best_params_\n",
    "standard_best_score2_2 = grid_search2_2.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params2_2}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score2_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model with best parameters\n",
    "final_model2_2 = grid_search2_2.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y2_2_pred = final_model2_2.predict(X2_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy2_2 = accuracy_score(y2_2_test, y2_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy2_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed better when the genre features were excluded. However, none of the models achieved an accuracy above 7% for either peak position or maximum rank change so k-NN will not be explored further for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model3_2.fit(\n",
    "    X3_2_train_scaled, y3_2_train_final,\n",
    "    validation_data=(X3_2_val_scaled, y3_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model3_2.evaluate(X3_2_train_scaled, y3_2_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model3_2.evaluate(X3_2_val_scaled, y3_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model4_1.fit(\n",
    "    X4_1_train_scaled, y4_1_train_final,\n",
    "    validation_data=(X4_1_val_scaled, y4_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model4_2 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X4_2_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model4_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model4_2.fit(\n",
    "    X4_2_train_scaled, y4_2_train_final,\n",
    "    validation_data=(X4_2_val_scaled, y4_2_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model4_2.evaluate(X4_2_train_scaled, y4_2_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model4_2.evaluate(X4_2_val_scaled, y4_2_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"Reverting to 2 deep layers, increasing to 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"2 deep layers,  kernels per layer, increased dropout from 0.4 to 0.6\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"3 deep layers, 128 kernels per layer, 0.6 dropout rate\")\n",
    "train_scores_reg3_1 = reg_model3_1.evaluate(X3_1_train_scaled, y3_1_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model3_1.evaluate(X3_1_val_scaled, y3_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an additional layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model3_1 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X3_1_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model3_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model3_1.fit(\n",
    "    X3_1_train_scaled, y3_1_train_final,\n",
    "    validation_data=(X3_1_val_scaled, y3_1_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg4_1 = reg_model4_1.evaluate(X4_1_train_scaled, y4_1_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model4_1.evaluate(X4_1_val_scaled, y4_1_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
