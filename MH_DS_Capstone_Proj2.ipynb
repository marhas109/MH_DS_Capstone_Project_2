{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project seeks to build machine learning models to achieve two things:\n",
    "\n",
    "1. Predict the highest ranking a song will achieve on the Billboard Hot 100 list.\n",
    "2. Predict the largest week over week increase in a given song's ranking on the Hot 100 list.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "- A song's genre has a minimal impact on its ranking on the Billboard Hot 100 list.\n",
    "- Song characteristics such as tempo, danceability, etc. appear to be the strongest drivers of their performance on the Hot 100 list.\n",
    "- Analyzing music can quickly lead to an overwhelming number of features, so thoughtful and careful data selection is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mean_squared_error, r2_score, pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Highest Ranking: 76\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peak_pos_dist = df_clean_withgenre['Max_Peak_Position'].value_counts()\n",
    "df_peak_pos_dist = pd.DataFrame(peak_pos_dist)\n",
    "df_peak_pos_dist = df_peak_pos_dist.reset_index()\n",
    "print(f\"Mean Highest Ranking: {df_clean_withgenre['Max_Peak_Position'].mean():.0f}\")\n",
    "\n",
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='orange')\n",
    "plt.xlabel('Highest Position')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Highest Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Largest Week over Week Rank Change: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Largest Week over Week Rank Change')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_rank_change = df_clean_withgenre['Max_Rank_Change'].value_counts()\n",
    "df_max_rank_change = pd.DataFrame(max_rank_change)\n",
    "df_max_rank_change = df_max_rank_change.reset_index()\n",
    "df_max_rank_change = df_max_rank_change[df_max_rank_change['Max_Rank_Change'] > 0]\n",
    "print(f\"Mean Largest Week over Week Rank Change: {df_clean_withgenre['Max_Rank_Change'].mean():.0f}\")\n",
    "\n",
    "plt.bar(df_max_rank_change['Max_Rank_Change'], df_max_rank_change['count'], color='orange')\n",
    "plt.xlabel('Largest Week over Week Rank Change')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Largest Week over Week Rank Change')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveness_dist = df_clean_withgenre['liveness'].value_counts()\n",
    "df_liveness_dist = pd.DataFrame(liveness_dist)\n",
    "df_liveness_dist = df_liveness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_liveness_dist['liveness'], df_liveness_dist['count'], color='orange')\n",
    "plt.xlabel('Liveness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Liveness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "danceability_dist = df_clean_withgenre['danceability'].value_counts()\n",
    "df_danceability_dist = pd.DataFrame(danceability_dist)\n",
    "df_danceability_dist = df_danceability_dist.reset_index()\n",
    "\n",
    "plt.bar(df_danceability_dist['danceability'], df_danceability_dist['count'], color='skyblue')\n",
    "plt.xlabel('Danceability Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Danceability Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acousticness_dist = df_clean_withgenre['acousticness'].value_counts()\n",
    "df_acousticness_dist = pd.DataFrame(acousticness_dist)\n",
    "df_acousticness_dist = df_acousticness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_acousticness_dist['acousticness'], df_acousticness_dist['count'], color='orange')\n",
    "plt.xlabel('Acousticness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Acousticness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frequency of Peak Rankings')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='skyblue')\n",
    "plt.xlabel('Highest Ranking')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Peak Rankings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_18008\\3177740419.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_18008\\3177740419.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer', 'Instance'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "def diff(a, b):\n",
    "    return a - b\n",
    "\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_id', 'spotify_track_preview_url',  \n",
    "                                        'spotify_track_explicit', 'spotify_track_popularity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112098 entries, 0 to 112097\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   WeekID                     112098 non-null  datetime64[ns]\n",
      " 1   Week Position              112098 non-null  int64         \n",
      " 2   SongID                     112098 non-null  object        \n",
      " 3   Previous Week Position     101571 non-null  float64       \n",
      " 4   Peak Position              112098 non-null  int64         \n",
      " 5   Weeks on Chart             112098 non-null  int64         \n",
      " 6   Rank_Change                112098 non-null  float64       \n",
      " 7   spotify_genre              108091 non-null  object        \n",
      " 8   spotify_track_duration_ms  104590 non-null  float64       \n",
      " 9   danceability               104287 non-null  float64       \n",
      " 10  energy                     104287 non-null  float64       \n",
      " 11  key                        104287 non-null  float64       \n",
      " 12  loudness                   104287 non-null  float64       \n",
      " 13  mode                       104287 non-null  float64       \n",
      " 14  speechiness                104287 non-null  float64       \n",
      " 15  acousticness               104287 non-null  float64       \n",
      " 16  instrumentalness           104287 non-null  float64       \n",
      " 17  liveness                   104287 non-null  float64       \n",
      " 18  valence                    104287 non-null  float64       \n",
      " 19  tempo                      104287 non-null  float64       \n",
      " 20  time_signature             104287 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(3), object(2)\n",
      "memory usage: 18.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# combining the hotlist and features into one dataframe\n",
    "\n",
    "df_hotlist_and_features_2000s = pd.merge(df_hotlist_2000s, df_features_all, on='SongID', how='left')\n",
    "df_hotlist_and_features_2000s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset has genre in a single column and the entry for each song has a variety of genres listed in that single column. \n",
    "This does not allow me to explore genre in a systematic way.\n",
    "I'll need to break genre out so that each genre has its own column with a 1 or 0 to indicate whether each song is tagged with that genre \n",
    "(ending with a one-hot encoded structure).\n",
    "\"\"\"\n",
    "\n",
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_hotlist_and_features_2000s['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I'm only including genres that appear in 100 or more songs (i.e. at least 0.1% of songs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading list of genres with 100 or more instances in df_cleaned\n",
    "df_genres_100_up = pd.read_csv('genre_counts_100+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_100_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating a list of genres and counts\n",
    "genre_data = []\n",
    "\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre'] :\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        row_dict = {genre: (1 if genre in genre_list else 0) for genre in final_genres_list} # dict with 1 if genre exists in list, 0 if not\n",
    "    else:\n",
    "         row_dict = {genre: 0 for genre in final_genres_list} # 0 of genre does not exist in list\n",
    "    genre_data.append(row_dict)\n",
    "\n",
    "# creating a df with the list of dicts\n",
    "genre_df = pd.DataFrame(genre_data)\n",
    "\n",
    "# concatenating genre data into df_clean\n",
    "df_hotlist_and_features_2000s = pd.concat([df_hotlist_and_features_2000s, genre_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  \n",
       "0           0                 0           0  \n",
       "1           0                 0           0  \n",
       "2           0                 0           0  \n",
       "3           0                 0           0  \n",
       "4           0                 0           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing spotify_genre and spot-checking resulting df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_hotlist_and_features_2000s = df_hotlist_and_features_2000s.drop(['spotify_genre'], axis=1)\n",
    "df_hotlist_and_features_2000s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new df with most 5 most popular genres by week\n",
    "\n",
    "# Get all genre column names\n",
    "genre_start_idx = df_hotlist_and_features_2000s.columns.get_loc('pop')\n",
    "genre_cols = df_hotlist_and_features_2000s.columns[genre_start_idx:].tolist()\n",
    "\n",
    "# Group by WeekID and sum the genre columns to get counts\n",
    "genre_counts = df_hotlist_and_features_2000s.groupby('WeekID')[genre_cols].sum()\n",
    "\n",
    "# For each week, find the top 5 genres\n",
    "top_genres = []\n",
    "for week_id in genre_counts.index:\n",
    "    # Get the genre counts for this week and sort them\n",
    "    week_genres = genre_counts.loc[week_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Get the top 3 genre names\n",
    "    top_5 = week_genres.head(5).index.tolist()\n",
    "    \n",
    "    # Pad with None if there are fewer than 5 genres\n",
    "    while len(top_5) < 5:\n",
    "        top_5.append(None)\n",
    "    \n",
    "    top_genres.append({\n",
    "        'WeekID': week_id,\n",
    "        'Most_Popular_Genre': top_5[0],\n",
    "        '2nd_Most_Popular_Genre': top_5[1],\n",
    "        '3rd_Most_Popular_Genre': top_5[2],\n",
    "        '4th_Most_Popular_Genre': top_5[3],\n",
    "        '5th_Most_Popular_Genre': top_5[4]\n",
    "    })\n",
    "\n",
    "# Create the new dataframe\n",
    "df_top_genres = pd.DataFrame(top_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the main df indicating whether each song is in a genre that's in the top 5 genres for a given week\n",
    "\n",
    "def is_in_top5_genres(row, df_top_genres):\n",
    "    week = row['WeekID']\n",
    "\n",
    "    top_genres = df_top_genres[df_top_genres['WeekID'] == week]\n",
    "\n",
    "    if len(top_genres) == 0:\n",
    "        return 0\n",
    "    \n",
    "    top_5 = [\n",
    "        top_genres.iloc[0]['Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['2nd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['3rd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['4th_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['5th_Most_Popular_Genre'],\n",
    "        ]\n",
    "\n",
    "    for genre in top_5:\n",
    "        if genre in row.index and row[genre] == 1:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "df_hotlist_and_features_2000s['in_top5_genres'] = df_hotlist_and_features_2000s.apply(\n",
    "    lambda row: is_in_top5_genres(row, df_top_genres), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new df with mean score of appearance in top 5 weekly genres\n",
    "df_mean_genre_match = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['in_top5_genres'].mean()\n",
    "df_mean_genre_match.rename(columns={'in_top5_genres': 'In_Top5genres_Mean'}, inplace=True)\n",
    "df_mean_genre_match.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max weekly rank change for each song \n",
    "df_max_rank_change = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song \n",
    "df_max_peak_pos = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum(), df_mean_genre_match['In_Top5genres_Mean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>in_top5_genres</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  in_top5_genres  \\\n",
       "0           0                 0           0               0   \n",
       "1           0                 0           0               0   \n",
       "2           0                 0           0               1   \n",
       "3           0                 0           0               1   \n",
       "4           0                 0           0               0   \n",
       "\n",
       "   Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "0                 69             -8.0                 0.0  \n",
       "1                 69             10.0                 0.0  \n",
       "2                  1              9.0                 1.0  \n",
       "3                 26             31.0                 1.0  \n",
       "4                 19             19.0                 0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding max peak position to main df\n",
    "df_2000s_data = df_hotlist_and_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to main df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "# adding in top5genres mean to main df\n",
    "df_2000s_data = df_2000s_data.join(df_mean_genre_match, on='SongID')\n",
    "\n",
    "df_2000s_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103420, 112098)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a clean df with genre information\n",
    "df_clean_withgenre = df_2000s_data.drop(['WeekID', 'Week Position', 'Previous Week Position', 'Peak Position',\n",
    "                                         'Weeks on Chart', 'Rank_Change', 'in_top5_genres'], axis=1)\n",
    "\n",
    "# counting duplicates and all rows\n",
    "df_clean_withgenre.duplicated().sum(), len(df_clean_withgenre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 8678\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 820\n"
     ]
    }
   ],
   "source": [
    "# removing duplicate rows\n",
    "df_clean_withgenre = df_clean_withgenre.drop_duplicates()\n",
    "\n",
    "# checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7858\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# dropping rows with missing values (unfortunately there is no reliable way to infer or estimate song characteristics)\n",
    "df_clean_withgenre = df_clean_withgenre.dropna()\n",
    "\n",
    "# re-checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a clean df with no genre\n",
    "nogenre_cols = ['spotify_track_duration_ms', 'danceability', 'energy', 'key', 'loudness',\n",
    "                'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                'time_signature', 'Max_Peak_Position', 'Max_Rank_Change', 'In_Top5genres_Mean']\n",
    "df_clean_nogenre = df_clean_withgenre[nogenre_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.00000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>0.313566</td>\n",
       "      <td>0.278315</td>\n",
       "      <td>0.282260</td>\n",
       "      <td>0.252354</td>\n",
       "      <td>0.162001</td>\n",
       "      <td>0.153092</td>\n",
       "      <td>0.145839</td>\n",
       "      <td>0.181598</td>\n",
       "      <td>0.187580</td>\n",
       "      <td>0.113006</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.147875</td>\n",
       "      <td>0.116187</td>\n",
       "      <td>0.072410</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.049631</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.037414</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.032706</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.021125</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.017943</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.018962</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.00789</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.010562</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>0.463972</td>\n",
       "      <td>0.448198</td>\n",
       "      <td>0.450128</td>\n",
       "      <td>0.434391</td>\n",
       "      <td>0.368475</td>\n",
       "      <td>0.360099</td>\n",
       "      <td>0.352967</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.390401</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0.304414</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.320470</td>\n",
       "      <td>0.259183</td>\n",
       "      <td>0.268604</td>\n",
       "      <td>0.252559</td>\n",
       "      <td>0.219815</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.233815</td>\n",
       "      <td>0.234296</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>0.189786</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.176870</td>\n",
       "      <td>0.168216</td>\n",
       "      <td>0.160538</td>\n",
       "      <td>0.199717</td>\n",
       "      <td>0.177876</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>0.134133</td>\n",
       "      <td>0.144655</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.133675</td>\n",
       "      <td>0.127562</td>\n",
       "      <td>0.148392</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.130422</td>\n",
       "      <td>0.116964</td>\n",
       "      <td>0.124136</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>0.136398</td>\n",
       "      <td>0.119578</td>\n",
       "      <td>0.115365</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.138181</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>0.107574</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.092627</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>0.115901</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.104644</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.08848</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.095941</td>\n",
       "      <td>0.077111</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.089185</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.084865</td>\n",
       "      <td>0.076291</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.094630</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.078725</td>\n",
       "      <td>0.064672</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.073776</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.062689</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.159028</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature          pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003     0.313566   \n",
       "std       0.140660     0.223494    29.558487        0.273062     0.463972   \n",
       "min       0.020000     0.034900    48.718000        0.000000     0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000     0.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000     0.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000     1.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000     1.000000   \n",
       "\n",
       "         dance pop      pop rap          rap  contemporary country  \\\n",
       "count  7858.000000  7858.000000  7858.000000           7858.000000   \n",
       "mean      0.278315     0.282260     0.252354              0.162001   \n",
       "std       0.448198     0.450128     0.434391              0.368475   \n",
       "min       0.000000     0.000000     0.000000              0.000000   \n",
       "25%       0.000000     0.000000     0.000000              0.000000   \n",
       "50%       0.000000     0.000000     0.000000              0.000000   \n",
       "75%       1.000000     1.000000     1.000000              0.000000   \n",
       "max       1.000000     1.000000     1.000000              1.000000   \n",
       "\n",
       "           country  country road  post-teen pop      hip hop          r&b  \\\n",
       "count  7858.000000   7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean      0.153092      0.145839       0.181598     0.187580     0.113006   \n",
       "std       0.360099      0.352967       0.385538     0.390401     0.316620   \n",
       "min       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       urban contemporary         trap  southern hip hop     pop rock  \\\n",
       "count         7858.000000  7858.000000       7858.000000  7858.000000   \n",
       "mean             0.103334     0.147875          0.116187     0.072410   \n",
       "std              0.304414     0.354998          0.320470     0.259183   \n",
       "min              0.000000     0.000000          0.000000     0.000000   \n",
       "25%              0.000000     0.000000          0.000000     0.000000   \n",
       "50%              0.000000     0.000000          0.000000     0.000000   \n",
       "75%              0.000000     0.000000          0.000000     0.000000   \n",
       "max              1.000000     1.000000          1.000000     1.000000   \n",
       "\n",
       "           hip pop  modern country rock   neo mellow  post-grunge  \\\n",
       "count  7858.000000          7858.000000  7858.000000  7858.000000   \n",
       "mean      0.078264             0.068465     0.050904     0.049631   \n",
       "std       0.268604             0.252559     0.219815     0.217195   \n",
       "min       0.000000             0.000000     0.000000     0.000000   \n",
       "25%       0.000000             0.000000     0.000000     0.000000   \n",
       "50%       0.000000             0.000000     0.000000     0.000000   \n",
       "75%       0.000000             0.000000     0.000000     0.000000   \n",
       "max       1.000000             1.000000     1.000000     1.000000   \n",
       "\n",
       "       gangster rap  atl hip hop  alternative metal     neo soul  \\\n",
       "count   7858.000000  7858.000000        7858.000000  7858.000000   \n",
       "mean       0.058030     0.058285           0.034869     0.037414   \n",
       "std        0.233815     0.234296           0.183459     0.189786   \n",
       "min        0.000000     0.000000           0.000000     0.000000   \n",
       "25%        0.000000     0.000000           0.000000     0.000000   \n",
       "50%        0.000000     0.000000           0.000000     0.000000   \n",
       "75%        0.000000     0.000000           0.000000     0.000000   \n",
       "max        1.000000     1.000000           1.000000     1.000000   \n",
       "\n",
       "       dirty south rap  country dawn  modern rock     nu metal  canadian pop  \\\n",
       "count      7858.000000   7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean          0.037541      0.032324     0.029142     0.026470      0.041614   \n",
       "std           0.190096      0.176870     0.168216     0.160538      0.199717   \n",
       "min           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "75%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "max           1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "              rock  melodic rap  deep pop r&b          edm  new jack swing  \\\n",
       "count  7858.000000  7858.000000   7858.000000  7858.000000     7858.000000   \n",
       "mean      0.032706     0.034742      0.019598     0.019725        0.018325   \n",
       "std       0.177876     0.183136      0.138623     0.139063        0.134133   \n",
       "min       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "25%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "50%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "75%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "max       1.000000     1.000000      1.000000     1.000000        1.000000   \n",
       "\n",
       "       permanent wave  miami hip hop  country pop  oklahoma country  \\\n",
       "count     7858.000000    7858.000000  7858.000000       7858.000000   \n",
       "mean         0.021379       0.021125     0.018198          0.016544   \n",
       "std          0.144655       0.143810     0.133675          0.127562   \n",
       "min          0.000000       0.000000     0.000000          0.000000   \n",
       "25%          0.000000       0.000000     0.000000          0.000000   \n",
       "50%          0.000000       0.000000     0.000000          0.000000   \n",
       "75%          0.000000       0.000000     0.000000          0.000000   \n",
       "max          1.000000       1.000000     1.000000          1.000000   \n",
       "\n",
       "             latin  tropical house   electropop       uk pop  \\\n",
       "count  7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.022525        0.016671     0.017307     0.013871   \n",
       "std       0.148392        0.128043     0.130422     0.116964   \n",
       "min       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       east coast hip hop  alternative rock    viral pop  quiet storm  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.015653          0.017943     0.016671     0.015271   \n",
       "std              0.124136          0.132755     0.128043     0.122637   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       chicago rap      redneck     pop punk        crunk  country rock  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean      0.018962     0.014508     0.013489     0.012344      0.012471   \n",
       "std       0.136398     0.119578     0.115365     0.110423      0.110984   \n",
       "min       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       toronto rap  canadian hip hop     boy band  hardcore hip hop  \\\n",
       "count  7858.000000       7858.000000  7858.000000       7858.000000   \n",
       "mean      0.019598          0.019471     0.014762          0.012090   \n",
       "std       0.138623          0.138181     0.120607          0.109293   \n",
       "min       0.000000          0.000000     0.000000          0.000000   \n",
       "25%       0.000000          0.000000     0.000000          0.000000   \n",
       "50%       0.000000          0.000000     0.000000          0.000000   \n",
       "75%       0.000000          0.000000     0.000000          0.000000   \n",
       "max       1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       queens hip hop  talent show          emo      europop  acoustic pop  \\\n",
       "count     7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean         0.011708     0.010435     0.010435     0.009799      0.008654   \n",
       "std          0.107574     0.101625     0.101625     0.098510      0.092627   \n",
       "min          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max          1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       alternative r&b  conscious hip hop  australian pop  detroit hip hop  \\\n",
       "count      7858.000000        7858.000000     7858.000000      7858.000000   \n",
       "mean          0.010817           0.015526        0.008145         0.013617   \n",
       "std           0.103447           0.123639        0.089885         0.115901   \n",
       "min           0.000000           0.000000        0.000000         0.000000   \n",
       "25%           0.000000           0.000000        0.000000         0.000000   \n",
       "50%           0.000000           0.000000        0.000000         0.000000   \n",
       "75%           0.000000           0.000000        0.000000         0.000000   \n",
       "max           1.000000           1.000000        1.000000         1.000000   \n",
       "\n",
       "          rap rock    latin pop  barbadian pop       g funk  girl group  \\\n",
       "count  7858.000000  7858.000000    7858.000000  7858.000000  7858.00000   \n",
       "mean      0.007126     0.011072       0.005218     0.010817     0.00789   \n",
       "std       0.084123     0.104644       0.072049     0.103447     0.08848   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "50%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "75%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "max       1.000000     1.000000       1.000000     1.000000     1.00000   \n",
       "\n",
       "       canadian rock     tropical   piano rock    indie pop  electro house  \\\n",
       "count    7858.000000  7858.000000  7858.000000  7858.000000    7858.000000   \n",
       "mean        0.005218     0.009290     0.005981     0.007126       0.007126   \n",
       "std         0.072049     0.095941     0.077111     0.084123       0.084123   \n",
       "min         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "25%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "50%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "75%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "max         1.000000     1.000000     1.000000     1.000000       1.000000   \n",
       "\n",
       "       indie poptimism    rap metal  west coast rap  new orleans rap  \\\n",
       "count      7858.000000  7858.000000     7858.000000      7858.000000   \n",
       "mean          0.005727     0.007126        0.008017         0.010562   \n",
       "std           0.075462     0.084123        0.089185         0.102236   \n",
       "min           0.000000     0.000000        0.000000         0.000000   \n",
       "25%           0.000000     0.000000        0.000000         0.000000   \n",
       "50%           0.000000     0.000000        0.000000         0.000000   \n",
       "75%           0.000000     0.000000        0.000000         0.000000   \n",
       "max           1.000000     1.000000        1.000000         1.000000   \n",
       "\n",
       "       metropopolis    candy pop       lilith  australian country  \\\n",
       "count   7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean       0.004581     0.007254     0.005854            0.005090   \n",
       "std        0.067535     0.084865     0.076291            0.071169   \n",
       "min        0.000000     0.000000     0.000000            0.000000   \n",
       "25%        0.000000     0.000000     0.000000            0.000000   \n",
       "50%        0.000000     0.000000     0.000000            0.000000   \n",
       "75%        0.000000     0.000000     0.000000            0.000000   \n",
       "max        1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "        philly rap   funk metal    reggaeton      dfw rap  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.012471     0.005218     0.009035     0.005090   \n",
       "std       0.110984     0.072049     0.094630     0.071169   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       canadian contemporary r&b         soul  mexican pop  adult standards  \\\n",
       "count                7858.000000  7858.000000  7858.000000      7858.000000   \n",
       "mean                    0.007636     0.006108     0.006872         0.006108   \n",
       "std                     0.087053     0.077922     0.082617         0.077922   \n",
       "min                     0.000000     0.000000     0.000000         0.000000   \n",
       "25%                     0.000000     0.000000     0.000000         0.000000   \n",
       "50%                     0.000000     0.000000     0.000000         0.000000   \n",
       "75%                     0.000000     0.000000     0.000000         0.000000   \n",
       "max                     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        nc hip hop  british soul   trap queen    hollywood  arkansas country  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean      0.007126      0.004327     0.004709     0.029906          0.004072   \n",
       "std       0.084123      0.065640     0.068462     0.170338          0.063688   \n",
       "min       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "          atl trap  underground hip hop  texas country     uk dance  \\\n",
       "count  7858.000000          7858.000000    7858.000000  7858.000000   \n",
       "mean      0.007126             0.006236       0.004200     0.003181   \n",
       "std       0.084123             0.078725       0.064672     0.056318   \n",
       "min       0.000000             0.000000       0.000000     0.000000   \n",
       "25%       0.000000             0.000000       0.000000     0.000000   \n",
       "50%       0.000000             0.000000       0.000000     0.000000   \n",
       "75%       0.000000             0.000000       0.000000     0.000000   \n",
       "max       1.000000             1.000000       1.000000     1.000000   \n",
       "\n",
       "             house  new wave pop      brostep    dancehall  progressive house  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean      0.003309      0.004327     0.002800     0.003181           0.003818   \n",
       "std       0.057430      0.065640     0.052841     0.056318           0.061674   \n",
       "min       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "              funk  singer-songwriter  latin hip hop         idol  \\\n",
       "count  7858.000000        7858.000000    7858.000000  7858.000000   \n",
       "mean      0.004581           0.005090       0.004327     0.005472   \n",
       "std       0.067535           0.071169       0.065640     0.073776   \n",
       "min       0.000000           0.000000       0.000000     0.000000   \n",
       "25%       0.000000           0.000000       0.000000     0.000000   \n",
       "50%       0.000000           0.000000       0.000000     0.000000   \n",
       "75%       0.000000           0.000000       0.000000     0.000000   \n",
       "max       1.000000           1.000000       1.000000     1.000000   \n",
       "\n",
       "       garage rock  mellow gold  baroque pop     big room      art pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.004709     0.005218     0.002927     0.003945     0.004581   \n",
       "std       0.068462     0.072049     0.054026     0.062689     0.067535   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       reggae fusion     cali rap  bronx hip hop     folk-pop  country rap  \\\n",
       "count    7858.000000  7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean        0.003309     0.003054       0.002800     0.002418     0.002418   \n",
       "std         0.057430     0.055184       0.052841     0.049116     0.049116   \n",
       "min         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "75%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "max         1.000000     1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       stomp and holler  neon pop punk      emo rap         punk   indie rock  \\\n",
       "count       7858.000000    7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.003563       0.002418     0.006363     0.003054     0.004072   \n",
       "std            0.059590       0.049116     0.079519     0.055184     0.063688   \n",
       "min            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         funk rock  memphis hip hop  modern alternative rock  lgbtq+ hip hop  \\\n",
       "count  7858.000000      7858.000000              7858.000000     7858.000000   \n",
       "mean      0.002927         0.003436                 0.001782        0.003436   \n",
       "std       0.054026         0.058520                 0.042174        0.058520   \n",
       "min       0.000000         0.000000                 0.000000        0.000000   \n",
       "25%       0.000000         0.000000                 0.000000        0.000000   \n",
       "50%       0.000000         0.000000                 0.000000        0.000000   \n",
       "75%       0.000000         0.000000                 0.000000        0.000000   \n",
       "max       1.000000         1.000000                 1.000000        1.000000   \n",
       "\n",
       "       progressive electro house  alternative hip hop   blues rock  \\\n",
       "count                7858.000000          7858.000000  7858.000000   \n",
       "mean                    0.001654             0.003563     0.002545   \n",
       "std                     0.040643             0.059590     0.050389   \n",
       "min                     0.000000             0.000000     0.000000   \n",
       "25%                     0.000000             0.000000     0.000000   \n",
       "50%                     0.000000             0.000000     0.000000   \n",
       "75%                     0.000000             0.000000     0.000000   \n",
       "max                     1.000000             1.000000     1.000000   \n",
       "\n",
       "       colombian pop    eurodance  classic rock  baton rouge rap  \\\n",
       "count    7858.000000  7858.000000   7858.000000      7858.000000   \n",
       "mean        0.002800     0.002163      0.003309         0.005727   \n",
       "std         0.052841     0.046465      0.057430         0.075462   \n",
       "min         0.000000     0.000000      0.000000         0.000000   \n",
       "25%         0.000000     0.000000      0.000000         0.000000   \n",
       "50%         0.000000     0.000000      0.000000         0.000000   \n",
       "75%         0.000000     0.000000      0.000000         0.000000   \n",
       "max         1.000000     1.000000      1.000000         1.000000   \n",
       "\n",
       "       australian dance         folk      pop emo    soft rock       motown  \\\n",
       "count       7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.001909     0.002927     0.002927     0.003818     0.002800   \n",
       "std            0.043652     0.054026     0.054026     0.061674     0.052841   \n",
       "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             pixie  canadian country    wrestling    glee club   complextro  \\\n",
       "count  7858.000000       7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.002545          0.002418     0.001909     0.025961     0.002036   \n",
       "std       0.050389          0.049116     0.043652     0.159028     0.045081   \n",
       "min       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        vapor trap     etherpop  pittsburgh rap  escape room  indietronica  \\\n",
       "count  7858.000000  7858.000000     7858.000000  7858.000000   7858.000000   \n",
       "mean      0.002036     0.001273        0.004836     0.002036      0.002800   \n",
       "std       0.045081     0.035653        0.069376     0.045081      0.052841   \n",
       "min       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000        1.000000     1.000000      1.000000   \n",
       "\n",
       "             comic  german techno  new jersey rap  trap latino  houston rap  \\\n",
       "count  7858.000000    7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.003054       0.001782        0.001527     0.003818     0.002291   \n",
       "std       0.055184       0.042174        0.039051     0.061674     0.047809   \n",
       "min       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       social media pop  puerto rican pop  deep southern trap  heartland rock  \\\n",
       "count       7858.000000       7858.000000         7858.000000     7858.000000   \n",
       "mean           0.006108          0.002418            0.001527        0.002036   \n",
       "std            0.077922          0.049116            0.039051        0.045081   \n",
       "min            0.000000          0.000000            0.000000        0.000000   \n",
       "25%            0.000000          0.000000            0.000000        0.000000   \n",
       "50%            0.000000          0.000000            0.000000        0.000000   \n",
       "75%            0.000000          0.000000            0.000000        0.000000   \n",
       "max            1.000000          1.000000            1.000000        1.000000   \n",
       "\n",
       "       alternative dance  bubblegum dance  alberta country  outlaw country  \\\n",
       "count        7858.000000      7858.000000      7858.000000     7858.000000   \n",
       "mean            0.002163         0.001273         0.001527        0.002036   \n",
       "std             0.046465         0.035653         0.039051        0.045081   \n",
       "min             0.000000         0.000000         0.000000        0.000000   \n",
       "25%             0.000000         0.000000         0.000000        0.000000   \n",
       "50%             0.000000         0.000000         0.000000        0.000000   \n",
       "75%             0.000000         0.000000         0.000000        0.000000   \n",
       "max             1.000000         1.000000         1.000000        1.000000   \n",
       "\n",
       "       country gospel  florida rap    hard rock  canadian metal  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean         0.001400     0.003691     0.002291        0.001273   \n",
       "std          0.037391     0.060641     0.047809        0.035653   \n",
       "min          0.000000     0.000000     0.000000        0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "       christian rock         soca  indiecoustica  harlem hip hop  \\\n",
       "count     7858.000000  7858.000000    7858.000000     7858.000000   \n",
       "mean         0.001909     0.001400       0.002418        0.000891   \n",
       "std          0.043652     0.037391       0.049116        0.029835   \n",
       "min          0.000000     0.000000       0.000000        0.000000   \n",
       "25%          0.000000     0.000000       0.000000        0.000000   \n",
       "50%          0.000000     0.000000       0.000000        0.000000   \n",
       "75%          0.000000     0.000000       0.000000        0.000000   \n",
       "max          1.000000     1.000000       1.000000        1.000000   \n",
       "\n",
       "          new rave  electronic trap  christian music       grunge  \\\n",
       "count  7858.000000      7858.000000      7858.000000  7858.000000   \n",
       "mean      0.002036         0.001145         0.002036     0.002163   \n",
       "std       0.045081         0.033825         0.045081     0.046465   \n",
       "min       0.000000         0.000000         0.000000     0.000000   \n",
       "25%       0.000000         0.000000         0.000000     0.000000   \n",
       "50%       0.000000         0.000000         0.000000     0.000000   \n",
       "75%       0.000000         0.000000         0.000000     0.000000   \n",
       "max       1.000000         1.000000         1.000000     1.000000   \n",
       "\n",
       "        show tunes   viral trap     la indie  swedish pop  swedish electropop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean      0.001909     0.001145     0.000891     0.001145            0.001145   \n",
       "std       0.043652     0.033825     0.029835     0.033825            0.033825   \n",
       "min       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "       reggaeton flow   dance-punk  celtic rock  socal pop punk       lounge  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean         0.001654     0.002291     0.000891        0.001782     0.002036   \n",
       "std          0.040643     0.047809     0.029835        0.042174     0.045081   \n",
       "min          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000     1.000000   \n",
       "\n",
       "       chicano rap    stomp pop          ccm   vocal jazz   glam metal  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.001400     0.000382     0.001654     0.001654     0.001400   \n",
       "std       0.037391     0.019537     0.040643     0.040643     0.037391   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "           worship   irish rock  electropowerpop      electro  indie pop rap  \\\n",
       "count  7858.000000  7858.000000      7858.000000  7858.000000    7858.000000   \n",
       "mean      0.001527     0.001654         0.001654     0.001018       0.002036   \n",
       "std       0.039051     0.040643         0.040643     0.031893       0.045081   \n",
       "min       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "25%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "50%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "75%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "max       1.000000     1.000000         1.000000     1.000000       1.000000   \n",
       "\n",
       "       canadian contemporary country       bounce  christian alternative rock  \\\n",
       "count                    7858.000000  7858.000000                 7858.000000   \n",
       "mean                        0.001018     0.001018                    0.001909   \n",
       "std                         0.031893     0.031893                    0.043652   \n",
       "min                         0.000000     0.000000                    0.000000   \n",
       "25%                         0.000000     0.000000                    0.000000   \n",
       "50%                         0.000000     0.000000                    0.000000   \n",
       "75%                         0.000000     0.000000                    0.000000   \n",
       "max                         1.000000     1.000000                    1.000000   \n",
       "\n",
       "       south african rock  deep talent show        disco        hyphy  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.001018          0.004836     0.001145     0.001145   \n",
       "std              0.031893          0.069376     0.033825     0.033825   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       disco house  canadian latin  australian hip hop      nyc rap  \\\n",
       "count  7858.000000     7858.000000         7858.000000  7858.000000   \n",
       "mean      0.000891        0.000891            0.001273     0.001273   \n",
       "std       0.029835        0.029835            0.035653     0.035653   \n",
       "min       0.000000        0.000000            0.000000     0.000000   \n",
       "25%       0.000000        0.000000            0.000000     0.000000   \n",
       "50%       0.000000        0.000000            0.000000     0.000000   \n",
       "75%       0.000000        0.000000            0.000000     0.000000   \n",
       "max       1.000000        1.000000            1.000000     1.000000   \n",
       "\n",
       "       brill building pop        k-pop       nz pop  minnesota hip hop  \\\n",
       "count         7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean             0.001018     0.003436     0.001018           0.000382   \n",
       "std              0.031893     0.058520     0.031893           0.019537   \n",
       "min              0.000000     0.000000     0.000000           0.000000   \n",
       "25%              0.000000     0.000000     0.000000           0.000000   \n",
       "50%              0.000000     0.000000     0.000000           0.000000   \n",
       "75%              0.000000     0.000000     0.000000           0.000000   \n",
       "max              1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "       modern blues rock   album rock  modern folk rock  uk americana  \\\n",
       "count        7858.000000  7858.000000       7858.000000   7858.000000   \n",
       "mean            0.001273     0.002418          0.001400      0.001400   \n",
       "std             0.035653     0.049116          0.037391      0.037391   \n",
       "min             0.000000     0.000000          0.000000      0.000000   \n",
       "25%             0.000000     0.000000          0.000000      0.000000   \n",
       "50%             0.000000     0.000000          0.000000      0.000000   \n",
       "75%             0.000000     0.000000          0.000000      0.000000   \n",
       "max             1.000000     1.000000          1.000000      1.000000   \n",
       "\n",
       "       old school hip hop   punk blues      dmv rap  industrial metal  \\\n",
       "count         7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean             0.001145     0.001273     0.002418          0.001273   \n",
       "std              0.033825     0.035653     0.049116          0.035653   \n",
       "min              0.000000     0.000000     0.000000          0.000000   \n",
       "25%              0.000000     0.000000     0.000000          0.000000   \n",
       "50%              0.000000     0.000000     0.000000          0.000000   \n",
       "75%              0.000000     0.000000     0.000000          0.000000   \n",
       "max              1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "        skate punk  swedish synthpop   moombahton  Max_Peak_Position  \\\n",
       "count  7858.000000       7858.000000  7858.000000        7858.000000   \n",
       "mean      0.001145          0.000509     0.001018          75.775261   \n",
       "std       0.033825          0.022558     0.031893          24.561479   \n",
       "min       0.000000          0.000000     0.000000           1.000000   \n",
       "25%       0.000000          0.000000     0.000000          66.000000   \n",
       "50%       0.000000          0.000000     0.000000          84.000000   \n",
       "75%       0.000000          0.000000     0.000000          95.000000   \n",
       "max       1.000000          1.000000     1.000000         100.000000   \n",
       "\n",
       "       Max_Rank_Change  In_Top5genres_Mean  \n",
       "count      7858.000000         7858.000000  \n",
       "mean         12.531815            0.383706  \n",
       "std          11.760207            0.466419  \n",
       "min          -8.000000            0.000000  \n",
       "25%           3.000000            0.000000  \n",
       "50%          11.000000            0.000000  \n",
       "75%          17.000000            1.000000  \n",
       "max          79.000000            1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_withgenre = df_clean_withgenre.drop(['SongID'], axis=1)\n",
    "df_clean_withgenre.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003   \n",
       "std       0.140660     0.223494    29.558487        0.273062   \n",
       "min       0.020000     0.034900    48.718000        0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000   \n",
       "\n",
       "       Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "count        7858.000000      7858.000000         7858.000000  \n",
       "mean           75.775261        12.531815            0.383706  \n",
       "std            24.561479        11.760207            0.466419  \n",
       "min             1.000000        -8.000000            0.000000  \n",
       "25%            66.000000         3.000000            0.000000  \n",
       "50%            84.000000        11.000000            0.000000  \n",
       "75%            95.000000        17.000000            1.000000  \n",
       "max           100.000000        79.000000            1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_nogenre.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre__1 variables)\n",
    "2. Max Peak Position, with genre (withgenre_1 variables)\n",
    "3. Max Rank Change, no genre (nogenre_2 variables)\n",
    "4. Max Rank Change, with genre (withgenre_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X_nogenre_1 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_1 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_1_train, X_nogenre_1_test, y_nogenre_1_train, y_nogenre_1_test = train_test_split(X_nogenre_1, y_nogenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_1_train_scaled = scaler.fit_transform(X_nogenre_1_train)\n",
    "X_nogenre_1_test_scaled = scaler.fit_transform(X_nogenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X_withgenre_1 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_1 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_1_train, X_withgenre_1_test, y_withgenre_1_train, y_withgenre_1_test = train_test_split(X_withgenre_1, y_withgenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_1_train_scaled = scaler.fit_transform(X_withgenre_1_train)\n",
    "X_withgenre_1_test_scaled = scaler.fit_transform(X_withgenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X_nogenre_2 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_2 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_2_train, X_nogenre_2_test, y_nogenre_2_train, y_nogenre_2_test = train_test_split(X_nogenre_2, y_nogenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_2_train_scaled = scaler.fit_transform(X_nogenre_2_train)\n",
    "X_nogenre_2_test_scaled = scaler.fit_transform(X_nogenre_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X_withgenre_2 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_2 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_2_train, X_withgenre_2_test, y_withgenre_2_train, y_withgenre_2_test = train_test_split(X_withgenre_2, y_withgenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_2_train_scaled = scaler.fit_transform(X_withgenre_2_train)\n",
    "X_withgenre_2_test_scaled = scaler.fit_transform(X_withgenre_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data for the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre_3 variables)\n",
    "2. Max Peak Position, with genre (withgenre_3 variables)\n",
    "3. Max Rank Change, no genre (nogenre_4 variables)\n",
    "4. Max Rank Change, with genre (withgenre_4 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X_nogenre_3 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_3 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_3_train, X_nogenre_3_test, y_nogenre_3_train, y_nogenre_3_test = train_test_split(X_nogenre_3, y_nogenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_3_train_final, X_nogenre_3_val, y_nogenre_3_train_final, y_nogenre_3_val = train_test_split(X_nogenre_3_train, y_nogenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X_nogenre_3_train_final)\n",
    "X_nogenre_3_train_scaled = scaler.transform(X_nogenre_3_train_final)\n",
    "X_nogenre_3_val_scaled = scaler.transform(X_nogenre_3_val)\n",
    "X_nogenre_3_test_scaled = scaler.transform(X_nogenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X_withgenre_3 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_3 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_3_train, X_withgenre_3_test, y_withgenre_3_train, y_withgenre_3_test = train_test_split(X_withgenre_3, y_withgenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_3_train_final, X_withgenre_3_val, y_withgenre_3_train_final, y_withgenre_3_val = train_test_split(X_withgenre_3_train, y_withgenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_3_train_final)\n",
    "X_withgenre_3_train_scaled = scaler.transform(X_withgenre_3_train_final)\n",
    "X_withgenre_3_val_scaled = scaler.transform(X_withgenre_3_val)\n",
    "X_withgenre_3_test_scaled = scaler.transform(X_withgenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X_nogenre_4 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_4 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_4_train, X_nogenre_4_test, y_nogenre_4_train, y_nogenre_4_test = train_test_split(X_nogenre_4, y_nogenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_4_train_final, X_nogenre_4_val, y_nogenre_4_train_final, y_nogenre_4_val = train_test_split(X_nogenre_4_train, y_nogenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_nogenre_4_train_final)\n",
    "X_nogenre_4_train_scaled = scaler.transform(X_nogenre_4_train_final)\n",
    "X_nogenre_4_val_scaled = scaler.transform(X_nogenre_4_val)\n",
    "X_nogenre_4_test_scaled = scaler.transform(X_nogenre_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X_withgenre_4 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_4 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_4_train, X_withgenre_4_test, y_withgenre_4_train, y_withgenre_4_test = train_test_split(X_withgenre_4, y_withgenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_4_train_final, X_withgenre_4_val, y_withgenre_4_train_final, y_withgenre_4_val = train_test_split(X_withgenre_4_train, y_withgenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_4_train_final)\n",
    "X_withgenre_4_train_scaled = scaler.transform(X_withgenre_4_train_final)\n",
    "X_withgenre_4_val_scaled = scaler.transform(X_withgenre_4_val)\n",
    "X_withgenre_4_test_scaled = scaler.transform(X_withgenre_4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 25.189\n",
      "R²: 0.009\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_maxpeak_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "y_nogenre_1_pred = xgb_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "y_nogenre_1_pred = np.clip(np.round(y_nogenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_nogenre_1 = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred))\n",
    "r2_nogenre_1 = r2_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1:.3f}')\n",
    "print(f'R²: {r2_nogenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_1 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_1.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.14928596 0.1497933  0.14933232 0.1519076  0.1538763  0.15317112\n",
      " 0.14956039 0.14933785 0.15139946 0.15320263 0.15275146 0.1530459\n",
      " 0.14947191 0.14825419 0.14977659 0.13353322 0.13985333 0.13783634\n",
      " 0.15055884 0.14790804 0.15182688 0.13723545 0.1396863  0.13820686\n",
      " 0.12488542 0.12580568 0.13303351 0.14951099 0.14986331 0.15052954\n",
      " 0.15149311 0.15313528 0.15195441 0.15239897 0.15245192 0.15317686\n",
      " 0.15170226 0.15120819 0.15521759 0.1480341  0.14980488 0.15103229\n",
      " 0.1380379  0.14412221 0.14086841 0.14997244 0.15279588 0.15103062\n",
      " 0.13215508 0.13735918 0.14146951 0.11673868 0.12705541 0.12914339\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.9, 1.0, 1.1]\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_2 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_2.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [2],\n",
    "    'learning_rate': [0.07, 0.1, 0.13],\n",
    "    'subsample': [0.85],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_3 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_3.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.851\n",
      "R²: 0.185\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb_nogenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_nogenre_1_pred_best = best_xgb1_1.predict(X_nogenre_1_test)\n",
    "y_nogenre_1_pred_best = np.clip(np.round(y_nogenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_nogenre_1_best = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred_best))\n",
    "r2_nogenre_1_best = r2_score(y_nogenre_1_test, y_nogenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1_best:.3f}')\n",
    "print(f'R²: {r2_nogenre_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.751\n",
      "R²: 0.192\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_maxpeak_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_withgenre.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "y_withgenre_1_pred = xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred = np.clip(np.round(y_withgenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred))\n",
    "r2_withgenre_1 = r2_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_1 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_1.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.22290783 0.22476627        nan 0.22961248 0.23208994        nan\n",
      " 0.22492174 0.22395395        nan 0.22358264 0.22480186        nan\n",
      " 0.22946347 0.22976409        nan 0.22246672 0.22280164        nan\n",
      " 0.22506559 0.22599194        nan 0.22510593 0.22972054        nan\n",
      " 0.21419413 0.21712435        nan 0.22480632 0.22179151        nan\n",
      " 0.22939501 0.23055565        nan 0.22415408 0.22779516        nan\n",
      " 0.22766875 0.22672504        nan 0.2268309  0.22682267        nan\n",
      " 0.22377956 0.22375016        nan 0.22684716 0.22516409        nan\n",
      " 0.2178275  0.21981466        nan 0.20304011 0.21110715        nan\n",
      " 0.22454622 0.22334156        nan 0.22969245 0.2287244         nan\n",
      " 0.2288182  0.22858987        nan 0.22661289 0.22694678        nan\n",
      " 0.22954861 0.22734727        nan 0.21356106 0.22451347        nan\n",
      " 0.22868726 0.22631047        nan 0.21676042 0.22319322        nan\n",
      " 0.21143588 0.20957587        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.15, 0.2, 0.25],\n",
    "    'subsample': [0.9, 1.0, 1.1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_2 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_2.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.12, 0.15, 0.17],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.5, 0.6],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_3 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_3.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21.607\n",
      "R²: 0.271\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model \n",
    "best_xgb_maxpeak_withgenre = grid_search_xgb_withgenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_withgenre_1_pred_best = best_xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred_best = np.clip(np.round(y_withgenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred_best))\n",
    "r2_withgenre_2_best = r2_score(y_withgenre_1_test, y_withgenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.605\n",
      "R²: -0.097\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_nogenre.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "y_nogenre_2_pred = xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred = np.clip(np.round(y_nogenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxrank_nogenre_2 = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred))\n",
    "r2_maxrank_nogenre_2 = r2_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_1 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_1.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.00972437        nan 0.01147871        nan 0.0099106         nan\n",
      " 0.01288679        nan 0.01492339        nan 0.0120229         nan\n",
      " 0.01355836        nan 0.01520104        nan 0.011404          nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_2 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_2.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.918\n",
      "R²: 0.020\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_nogenre = grid_search_xgb_maxrank_nogenre_2.best_estimator_\n",
    "\n",
    "y_nogenre_2_pred_best = best_xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred_best = np.clip(np.round(y_nogenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxrank_nogenre_2_best = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred_best))\n",
    "r2_maxrank_nogenre_2_best = r2_score(y_nogenre_2_test, y_nogenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.342\n",
      "R²: -0.051\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_withgenre.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "y_withgenre_2_pred = xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred = np.clip(np.round(y_withgenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxpeak_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred))\n",
    "r2_maxpeak_withgenre_1 = r2_score(y_withgenre_2_test, y_withgenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_1 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_1.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.0224043  0.0229799  0.02310325 0.023507   0.02370381 0.02407839\n",
      " 0.03057899 0.03026649 0.0313546  0.03141223 0.0324422  0.03149776\n",
      " 0.03260914 0.03307942 0.03341982 0.0323358  0.03272635 0.03061318\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_2 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_2.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.013, 0.015, 0.017],\n",
    "    'subsample': [0.5, 0.6, 0.7],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_3 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_3.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.764\n",
      "R²: 0.045\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_withgenre = grid_search_xgb_maxrank_withgenre_3.best_estimator_\n",
    "\n",
    "y_withgenre_2_pred_best = best_xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred_best = np.clip(np.round(y_withgenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxpeak_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred_best))\n",
    "r2_maxpeak_withgenre_2_best = r2_score(y_withgenre_2_test, y_withgenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, the Max Rank Change models both had an r<sup>2</sup> value less than 0.001, essentially indicating no fit of the model to the test data. Max Peak Position performed better, but the highest r<sup>2</sup> value was 0.271 so their predictive value is low.\n",
    "\n",
    "Given the lack of predictive power in these outcomes, I'm shifting focus to the other two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 0.0439\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_params_\n",
    "standard_best_score_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0326\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_nogenre_1_pred = final_model_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_nogenre_1 = accuracy_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_nogenre_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.0409\n"
     ]
    }
   ],
   "source": [
    "# parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_withgenre.fit(X_withgenre_1_train_scaled, y_withgenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_params_\n",
    "standard_best_score_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0351\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_withgenre_1_pred = final_model_maxpeak_withgenre.predict(X_withgenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_withgenre = accuracy_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2350\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_nogenre.fit(X_nogenre_2_train_scaled, y_nogenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_nogenre = grid_search_maxrank_nogenre.best_params_\n",
    "standard_best_score_maxrank_nogenre = grid_search_maxrank_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2163\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_nogenre = grid_search_maxrank_nogenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_nogenre_2_pred = final_model_maxrank_nogenre.predict(X_nogenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_nogenre = accuracy_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_nogenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2379\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_withgenre.fit(X_withgenre_2_train_scaled, y_withgenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_withgenre = grid_search_maxrank_withgenre.best_params_\n",
    "standard_best_score_maxrank_withgenre = grid_search_maxrank_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2173\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_withgenre = grid_search_maxrank_withgenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_maxrank_withgenre_2_pred = final_model_maxrank_withgenre.predict(X_withgenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_withgenre = accuracy_score(y_withgenre_2_test, y_maxrank_withgenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed poorly on the Max Peak Position data, with a maximum accuracy of 0.035. The performance on the Max Rank Change was better, but the maximum accuracy was still just 0.217.\n",
    "\n",
    "I will not explore k-NN further and instead focus on the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2419.2927 - mae: 39.6480 - mse: 2419.2927 - val_loss: 689.4341 - val_mae: 21.1786 - val_mse: 689.4341\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 665.7994 - mae: 20.4834 - mse: 665.7994 - val_loss: 615.3640 - val_mae: 19.6692 - val_mse: 615.3640\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 608.4424 - mae: 19.4169 - mse: 608.4424 - val_loss: 568.2308 - val_mae: 19.1297 - val_mse: 568.2308\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.5310 - mae: 18.8835 - mse: 580.5310 - val_loss: 552.3713 - val_mae: 18.7825 - val_mse: 552.3713\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 563.8795 - mae: 18.5870 - mse: 563.8795 - val_loss: 532.4030 - val_mae: 18.4636 - val_mse: 532.4030\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 552.4023 - mae: 18.3202 - mse: 552.4023 - val_loss: 531.0229 - val_mae: 18.1369 - val_mse: 531.0229\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 543.6500 - mae: 18.0884 - mse: 543.6500 - val_loss: 524.1937 - val_mae: 18.6026 - val_mse: 524.1937\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 538.1226 - mae: 18.0059 - mse: 538.1226 - val_loss: 513.3173 - val_mae: 18.1826 - val_mse: 513.3173\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4422 - mae: 17.8144 - mse: 531.4422 - val_loss: 509.8279 - val_mae: 17.9694 - val_mse: 509.8279\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 527.5864 - mae: 17.7839 - mse: 527.5864 - val_loss: 503.0971 - val_mae: 18.0760 - val_mse: 503.0971\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 521.5410 - mae: 17.6273 - mse: 521.5410 - val_loss: 510.4745 - val_mae: 18.4716 - val_mse: 510.4745\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 515.6498 - mae: 17.4958 - mse: 515.6498 - val_loss: 504.4003 - val_mae: 18.2854 - val_mse: 504.4003\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 509.2001 - mae: 17.3392 - mse: 509.2001 - val_loss: 492.7363 - val_mae: 17.6454 - val_mse: 492.7363\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 506.5855 - mae: 17.3311 - mse: 506.5855 - val_loss: 494.3658 - val_mae: 17.1993 - val_mse: 494.3658\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 503.0619 - mae: 17.1726 - mse: 503.0619 - val_loss: 489.0531 - val_mae: 17.2873 - val_mse: 489.0531\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 502.0815 - mae: 17.2147 - mse: 502.0815 - val_loss: 490.0863 - val_mae: 17.6323 - val_mse: 490.0863\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 498.6240 - mae: 17.1130 - mse: 498.6240 - val_loss: 488.2263 - val_mae: 17.5757 - val_mse: 488.2263\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 494.3361 - mae: 17.0147 - mse: 494.3361 - val_loss: 493.1820 - val_mae: 17.7805 - val_mse: 493.1820\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 494.9956 - mae: 17.0423 - mse: 494.9956 - val_loss: 491.9977 - val_mae: 17.4102 - val_mse: 491.9977\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 491.8429 - mae: 17.0051 - mse: 491.8429 - val_loss: 501.6566 - val_mae: 16.9908 - val_mse: 501.6566\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 487.2634 - mae: 16.9084 - mse: 487.2634 - val_loss: 490.1104 - val_mae: 17.3877 - val_mse: 490.1104\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 486.1548 - mae: 16.9138 - mse: 486.1548 - val_loss: 490.3252 - val_mae: 17.5433 - val_mse: 490.3252\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 486.3818 - mae: 16.9306 - mse: 486.3818 - val_loss: 496.2074 - val_mae: 17.0037 - val_mse: 496.2074\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 482.7021 - mae: 16.7403 - mse: 482.7021 - val_loss: 489.6503 - val_mae: 17.2741 - val_mse: 489.6503\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.2343 - mae: 16.7226 - mse: 478.2343 - val_loss: 499.0989 - val_mae: 18.0239 - val_mse: 499.0989\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 480.3737 - mae: 16.7965 - mse: 480.3737 - val_loss: 515.3365 - val_mae: 18.6585 - val_mse: 515.3365\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.9332 - mae: 16.7417 - mse: 478.9332 - val_loss: 486.3153 - val_mae: 17.4821 - val_mse: 486.3153\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 474.6300 - mae: 16.6473 - mse: 474.6300 - val_loss: 486.3873 - val_mae: 17.5012 - val_mse: 486.3873\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 473.7096 - mae: 16.6590 - mse: 473.7096 - val_loss: 485.6329 - val_mae: 17.4391 - val_mse: 485.6329\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 473.0536 - mae: 16.6453 - mse: 473.0536 - val_loss: 488.7411 - val_mae: 17.4251 - val_mse: 488.7411\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 470.8673 - mae: 16.6039 - mse: 470.8673 - val_loss: 517.8712 - val_mae: 16.9943 - val_mse: 517.8712\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 468.8740 - mae: 16.5972 - mse: 468.8740 - val_loss: 490.3929 - val_mae: 17.4073 - val_mse: 490.3929\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 466.5359 - mae: 16.4863 - mse: 466.5359 - val_loss: 490.4259 - val_mae: 17.5247 - val_mse: 490.4259\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.0333 - mae: 16.5200 - mse: 465.0333 - val_loss: 496.1299 - val_mae: 17.7819 - val_mse: 496.1299\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.6708 - mae: 16.4928 - mse: 465.6708 - val_loss: 495.2829 - val_mae: 17.6568 - val_mse: 495.2829\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 463.1415 - mae: 16.4833 - mse: 463.1415 - val_loss: 501.4291 - val_mae: 18.0463 - val_mse: 501.4291\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 460.4989 - mae: 16.4622 - mse: 460.4989 - val_loss: 502.9875 - val_mae: 18.1109 - val_mse: 502.9875\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 457.1301 - mae: 16.3698 - mse: 457.1301 - val_loss: 496.6900 - val_mae: 17.2185 - val_mse: 496.6900\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 459.4128 - mae: 16.4314 - mse: 459.4128 - val_loss: 502.8075 - val_mae: 17.0330 - val_mse: 502.8075\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 454.1688 - mae: 16.2550 - mse: 454.1688 - val_loss: 497.0360 - val_mae: 17.2852 - val_mse: 497.0360\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 455.2572 - mae: 16.3576 - mse: 455.2572 - val_loss: 507.4308 - val_mae: 17.3179 - val_mse: 507.4308\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 452.3672 - mae: 16.2527 - mse: 452.3672 - val_loss: 494.0938 - val_mae: 17.4207 - val_mse: 494.0938\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 450.1534 - mae: 16.2146 - mse: 450.1534 - val_loss: 506.9326 - val_mae: 17.8810 - val_mse: 506.9326\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.2242 - mae: 16.2945 - mse: 451.2242 - val_loss: 500.4746 - val_mae: 17.6965 - val_mse: 500.4746\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 447.5209 - mae: 16.1485 - mse: 447.5209 - val_loss: 508.4296 - val_mae: 18.0848 - val_mse: 508.4296\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 447.8461 - mae: 16.2137 - mse: 447.8461 - val_loss: 510.0475 - val_mae: 17.8509 - val_mse: 510.0475\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 447.6882 - mae: 16.1755 - mse: 447.6882 - val_loss: 515.8989 - val_mae: 18.3052 - val_mse: 515.8989\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 442.2391 - mae: 16.1399 - mse: 442.2391 - val_loss: 506.1750 - val_mae: 17.1819 - val_mse: 506.1750\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.5520 - mae: 15.9899 - mse: 438.5520 - val_loss: 510.4245 - val_mae: 17.2800 - val_mse: 510.4245\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 436.8173 - mae: 15.9844 - mse: 436.8173 - val_loss: 508.9857 - val_mae: 18.0230 - val_mse: 508.9857\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 436.6111 - mae: 15.9807 - mse: 436.6111 - val_loss: 513.3489 - val_mae: 18.0142 - val_mse: 513.3489\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 436.0714 - mae: 15.9555 - mse: 436.0714 - val_loss: 508.2621 - val_mae: 17.4788 - val_mse: 508.2621\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 434.0513 - mae: 15.9039 - mse: 434.0513 - val_loss: 513.2465 - val_mae: 17.4503 - val_mse: 513.2465\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.7212 - mae: 15.9133 - mse: 434.7212 - val_loss: 511.5938 - val_mae: 17.9016 - val_mse: 511.5938\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 432.2248 - mae: 15.9067 - mse: 432.2248 - val_loss: 525.3815 - val_mae: 17.3754 - val_mse: 525.3815\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 430.1569 - mae: 15.8171 - mse: 430.1569 - val_loss: 515.8960 - val_mae: 18.0224 - val_mse: 515.8960\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 430.4926 - mae: 15.9089 - mse: 430.4926 - val_loss: 518.7918 - val_mae: 17.9219 - val_mse: 518.7918\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.6432 - mae: 15.7649 - mse: 426.6432 - val_loss: 515.3190 - val_mae: 17.4910 - val_mse: 515.3190\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.5134 - mae: 15.8229 - mse: 426.5134 - val_loss: 516.7847 - val_mae: 17.7470 - val_mse: 516.7847\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 426.6697 - mae: 15.8196 - mse: 426.6697 - val_loss: 545.5805 - val_mae: 17.3761 - val_mse: 545.5805\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 423.8633 - mae: 15.7151 - mse: 423.8633 - val_loss: 524.4741 - val_mae: 18.1706 - val_mse: 524.4741\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 423.3539 - mae: 15.7032 - mse: 423.3539 - val_loss: 523.2936 - val_mae: 17.6593 - val_mse: 523.2936\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 421.9861 - mae: 15.7878 - mse: 421.9861 - val_loss: 525.7878 - val_mae: 17.7972 - val_mse: 525.7878\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 418.5110 - mae: 15.6692 - mse: 418.5110 - val_loss: 521.7631 - val_mae: 17.9073 - val_mse: 521.7631\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 416.9155 - mae: 15.6447 - mse: 416.9155 - val_loss: 532.5114 - val_mae: 17.8788 - val_mse: 532.5114\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 415.7471 - mae: 15.6018 - mse: 415.7471 - val_loss: 526.9938 - val_mae: 18.0233 - val_mse: 526.9938\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.1190 - mae: 15.4875 - mse: 412.1190 - val_loss: 533.2522 - val_mae: 18.3125 - val_mse: 533.2522\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 414.5464 - mae: 15.5918 - mse: 414.5464 - val_loss: 527.8963 - val_mae: 18.0504 - val_mse: 527.8963\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 411.2368 - mae: 15.5448 - mse: 411.2368 - val_loss: 533.0829 - val_mae: 17.7895 - val_mse: 533.0829\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.0792 - mae: 15.4613 - mse: 409.0792 - val_loss: 539.5701 - val_mae: 17.7955 - val_mse: 539.5701\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 407.0265 - mae: 15.4026 - mse: 407.0265 - val_loss: 546.9268 - val_mae: 17.7916 - val_mse: 546.9268\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 411.2872 - mae: 15.5100 - mse: 411.2872 - val_loss: 543.2707 - val_mae: 17.9467 - val_mse: 543.2707\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 409.4622 - mae: 15.5657 - mse: 409.4622 - val_loss: 545.8762 - val_mae: 17.7985 - val_mse: 545.8762\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.8742 - mae: 15.3437 - mse: 404.8742 - val_loss: 545.9336 - val_mae: 18.4481 - val_mse: 545.9336\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.7788 - mae: 15.3833 - mse: 404.7788 - val_loss: 547.0612 - val_mae: 17.7472 - val_mse: 547.0612\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 404.1299 - mae: 15.4497 - mse: 404.1299 - val_loss: 540.1212 - val_mae: 18.2801 - val_mse: 540.1212\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.4113 - mae: 15.2785 - mse: 399.4113 - val_loss: 552.5936 - val_mae: 18.7343 - val_mse: 552.5936\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 398.2041 - mae: 15.3230 - mse: 398.2041 - val_loss: 538.3664 - val_mae: 17.9450 - val_mse: 538.3664\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.7830 - mae: 15.3380 - mse: 399.7830 - val_loss: 543.1089 - val_mae: 18.3602 - val_mse: 543.1089\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 401.6050 - mae: 15.3529 - mse: 401.6050 - val_loss: 546.5948 - val_mae: 17.9844 - val_mse: 546.5948\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 398.7794 - mae: 15.2802 - mse: 398.7794 - val_loss: 544.4263 - val_mae: 18.2400 - val_mse: 544.4263\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.3988 - mae: 15.2791 - mse: 397.3988 - val_loss: 554.1939 - val_mae: 17.9493 - val_mse: 554.1939\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 393.7537 - mae: 15.1798 - mse: 393.7537 - val_loss: 550.9030 - val_mae: 18.3655 - val_mse: 550.9030\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.1619 - mae: 15.1724 - mse: 392.1619 - val_loss: 552.3148 - val_mae: 18.3455 - val_mse: 552.3148\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 389.9302 - mae: 15.1217 - mse: 389.9302 - val_loss: 557.1672 - val_mae: 18.3695 - val_mse: 557.1672\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.8891 - mae: 15.1914 - mse: 392.8891 - val_loss: 558.3782 - val_mae: 18.2113 - val_mse: 558.3782\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 388.0539 - mae: 15.0538 - mse: 388.0539 - val_loss: 565.5303 - val_mae: 18.7598 - val_mse: 565.5303\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 389.9147 - mae: 15.2011 - mse: 389.9147 - val_loss: 554.7290 - val_mae: 18.1745 - val_mse: 554.7290\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 384.1323 - mae: 15.0548 - mse: 384.1323 - val_loss: 555.0225 - val_mae: 18.5653 - val_mse: 555.0225\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 386.8987 - mae: 15.1158 - mse: 386.8987 - val_loss: 557.1505 - val_mae: 18.1024 - val_mse: 557.1505\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 388.1218 - mae: 15.0916 - mse: 388.1218 - val_loss: 563.1410 - val_mae: 18.3664 - val_mse: 563.1410\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 384.0925 - mae: 14.9991 - mse: 384.0925 - val_loss: 562.3793 - val_mae: 18.5614 - val_mse: 562.3793\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.8701 - mae: 15.0815 - mse: 385.8701 - val_loss: 557.7654 - val_mae: 18.4775 - val_mse: 557.7654\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.0906 - mae: 14.9543 - mse: 381.0906 - val_loss: 564.2411 - val_mae: 18.6923 - val_mse: 564.2411\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.4917 - mae: 14.9239 - mse: 379.4917 - val_loss: 559.4780 - val_mae: 18.7443 - val_mse: 559.4780\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 379.8032 - mae: 15.0226 - mse: 379.8032 - val_loss: 567.9104 - val_mae: 18.2340 - val_mse: 567.9104\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.5663 - mae: 14.9266 - mse: 379.5663 - val_loss: 575.2344 - val_mae: 18.7882 - val_mse: 575.2344\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.1804 - mae: 14.9150 - mse: 376.1804 - val_loss: 565.7868 - val_mae: 18.6739 - val_mse: 565.7868\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.5493 - mae: 14.8226 - mse: 373.5493 - val_loss: 571.8291 - val_mae: 18.8567 - val_mse: 571.8291\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 373.8908 - mae: 14.8477 - mse: 373.8908 - val_loss: 574.3768 - val_mae: 18.8456 - val_mse: 574.3768\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5851.8906 - mae: 72.6476 - mse: 5851.8906 - val_loss: 5651.5874 - val_mae: 71.4733 - val_mse: 5651.5874\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4666.3667 - mae: 64.3989 - mse: 4666.3667 - val_loss: 3954.0635 - val_mae: 59.0888 - val_mse: 3954.0635\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3064.9041 - mae: 51.3851 - mse: 3064.9041 - val_loss: 2353.0398 - val_mae: 44.6987 - val_mse: 2353.0398\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1626.5095 - mae: 36.4668 - mse: 1626.5095 - val_loss: 1146.4127 - val_mae: 30.0209 - val_mse: 1146.4127\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 827.4645 - mae: 24.9593 - mse: 827.4645 - val_loss: 643.7032 - val_mae: 21.6736 - val_mse: 643.7032\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 574.0120 - mae: 19.6001 - mse: 574.0120 - val_loss: 542.1612 - val_mae: 19.1383 - val_mse: 542.1612\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 517.4794 - mae: 17.9477 - mse: 517.4794 - val_loss: 489.2751 - val_mae: 17.4206 - val_mse: 489.2751\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 510.0293 - mae: 17.3691 - mse: 510.0293 - val_loss: 486.8300 - val_mae: 17.3675 - val_mse: 486.8300\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 503.0972 - mae: 17.2335 - mse: 503.0972 - val_loss: 488.8733 - val_mae: 17.3222 - val_mse: 488.8733\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 497.4265 - mae: 17.0943 - mse: 497.4265 - val_loss: 487.3111 - val_mae: 17.2283 - val_mse: 487.3111\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 496.1514 - mae: 17.0929 - mse: 496.1514 - val_loss: 491.9576 - val_mae: 17.2409 - val_mse: 491.9576\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 491.6809 - mae: 16.9804 - mse: 491.6809 - val_loss: 492.3288 - val_mae: 16.9923 - val_mse: 492.3288\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 482.7526 - mae: 16.7466 - mse: 482.7526 - val_loss: 494.0729 - val_mae: 17.6306 - val_mse: 494.0729\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 480.6911 - mae: 16.7360 - mse: 480.6911 - val_loss: 491.8650 - val_mae: 17.3832 - val_mse: 491.8650\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 474.4710 - mae: 16.6212 - mse: 474.4710 - val_loss: 493.5953 - val_mae: 17.2509 - val_mse: 493.5953\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 474.4846 - mae: 16.5760 - mse: 474.4846 - val_loss: 505.0648 - val_mae: 17.4732 - val_mse: 505.0648\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 473.2977 - mae: 16.6698 - mse: 473.2977 - val_loss: 505.7195 - val_mae: 17.9222 - val_mse: 505.7195\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 464.8788 - mae: 16.4491 - mse: 464.8788 - val_loss: 502.1259 - val_mae: 17.6153 - val_mse: 502.1259\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 463.1366 - mae: 16.4096 - mse: 463.1366 - val_loss: 497.7792 - val_mae: 17.4499 - val_mse: 497.7792\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 458.4832 - mae: 16.3408 - mse: 458.4832 - val_loss: 501.3792 - val_mae: 17.1890 - val_mse: 501.3792\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 460.0415 - mae: 16.3418 - mse: 460.0415 - val_loss: 499.8820 - val_mae: 17.3720 - val_mse: 499.8820\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.6199 - mae: 16.1744 - mse: 451.6199 - val_loss: 513.5532 - val_mae: 17.1330 - val_mse: 513.5532\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.7422 - mae: 16.1587 - mse: 451.7422 - val_loss: 509.7841 - val_mae: 17.4475 - val_mse: 509.7841\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 443.7116 - mae: 16.0591 - mse: 443.7116 - val_loss: 515.0828 - val_mae: 17.7905 - val_mse: 515.0828\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 449.9147 - mae: 16.2115 - mse: 449.9147 - val_loss: 512.6425 - val_mae: 17.3624 - val_mse: 512.6425\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 444.7753 - mae: 16.0769 - mse: 444.7753 - val_loss: 517.2388 - val_mae: 17.8782 - val_mse: 517.2388\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 447.0614 - mae: 16.0617 - mse: 447.0614 - val_loss: 511.5176 - val_mae: 17.7023 - val_mse: 511.5176\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 441.8242 - mae: 15.9474 - mse: 441.8242 - val_loss: 514.6925 - val_mae: 17.7256 - val_mse: 514.6925\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.8530 - mae: 15.8245 - mse: 434.8530 - val_loss: 521.7326 - val_mae: 17.2641 - val_mse: 521.7326\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 432.8844 - mae: 15.8290 - mse: 432.8844 - val_loss: 520.5737 - val_mae: 17.8681 - val_mse: 520.5737\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 424.4903 - mae: 15.6312 - mse: 424.4903 - val_loss: 526.0905 - val_mae: 17.7782 - val_mse: 526.0905\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.3234 - mae: 15.8358 - mse: 434.3234 - val_loss: 529.7477 - val_mae: 17.3516 - val_mse: 529.7477\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 429.9236 - mae: 15.7594 - mse: 429.9236 - val_loss: 524.9996 - val_mae: 17.8921 - val_mse: 524.9996\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 427.0891 - mae: 15.6821 - mse: 427.0891 - val_loss: 528.3973 - val_mae: 17.6463 - val_mse: 528.3973\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 425.5359 - mae: 15.5754 - mse: 425.5359 - val_loss: 515.7779 - val_mae: 17.7935 - val_mse: 515.7779\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 428.0937 - mae: 15.7617 - mse: 428.0937 - val_loss: 525.2448 - val_mae: 17.6638 - val_mse: 525.2448\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.5238 - mae: 15.5518 - mse: 419.5238 - val_loss: 521.1468 - val_mae: 17.7045 - val_mse: 521.1468\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.1915 - mae: 15.5597 - mse: 419.1915 - val_loss: 525.3145 - val_mae: 17.8965 - val_mse: 525.3145\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 414.3619 - mae: 15.5778 - mse: 414.3619 - val_loss: 527.6876 - val_mae: 17.4749 - val_mse: 527.6876\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 413.3690 - mae: 15.3675 - mse: 413.3690 - val_loss: 526.1476 - val_mae: 17.9665 - val_mse: 526.1476\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.3266 - mae: 15.5896 - mse: 419.3266 - val_loss: 529.5512 - val_mae: 18.0028 - val_mse: 529.5512\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 410.9309 - mae: 15.4720 - mse: 410.9309 - val_loss: 537.4049 - val_mae: 17.8429 - val_mse: 537.4049\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.1388 - mae: 15.3905 - mse: 409.1388 - val_loss: 538.3312 - val_mae: 17.8216 - val_mse: 538.3312\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 403.8900 - mae: 15.2370 - mse: 403.8900 - val_loss: 531.2563 - val_mae: 17.9443 - val_mse: 531.2563\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.3340 - mae: 15.2030 - mse: 405.3340 - val_loss: 540.5571 - val_mae: 17.9567 - val_mse: 540.5571\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 408.0271 - mae: 15.2985 - mse: 408.0271 - val_loss: 545.4421 - val_mae: 17.9337 - val_mse: 545.4421\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 399.3531 - mae: 15.1505 - mse: 399.3531 - val_loss: 542.5694 - val_mae: 17.9608 - val_mse: 542.5694\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 404.3011 - mae: 15.2841 - mse: 404.3011 - val_loss: 540.3690 - val_mae: 17.8100 - val_mse: 540.3690\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 398.5748 - mae: 15.2058 - mse: 398.5748 - val_loss: 533.5903 - val_mae: 17.8229 - val_mse: 533.5903\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 395.3835 - mae: 15.1408 - mse: 395.3835 - val_loss: 540.4743 - val_mae: 18.1131 - val_mse: 540.4743\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 401.2710 - mae: 15.2145 - mse: 401.2710 - val_loss: 544.1772 - val_mae: 18.1012 - val_mse: 544.1772\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 391.0296 - mae: 15.0302 - mse: 391.0296 - val_loss: 543.8338 - val_mae: 18.0552 - val_mse: 543.8338\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.4317 - mae: 15.1154 - mse: 394.4317 - val_loss: 565.3024 - val_mae: 18.1627 - val_mse: 565.3024\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.4927 - mae: 15.1004 - mse: 397.4927 - val_loss: 557.1964 - val_mae: 18.5762 - val_mse: 557.1964\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 396.5296 - mae: 15.1524 - mse: 396.5296 - val_loss: 543.5672 - val_mae: 17.9871 - val_mse: 543.5672\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.1523 - mae: 15.0774 - mse: 394.1523 - val_loss: 546.2386 - val_mae: 18.2674 - val_mse: 546.2386\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.4361 - mae: 14.8390 - mse: 383.4361 - val_loss: 539.5917 - val_mae: 17.9089 - val_mse: 539.5917\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 389.5795 - mae: 15.0152 - mse: 389.5795 - val_loss: 555.7221 - val_mae: 18.1397 - val_mse: 555.7221\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 387.9057 - mae: 14.9026 - mse: 387.9057 - val_loss: 545.2907 - val_mae: 17.7533 - val_mse: 545.2907\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 390.6403 - mae: 15.0576 - mse: 390.6403 - val_loss: 551.6979 - val_mae: 17.9138 - val_mse: 551.6979\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 382.3562 - mae: 14.8456 - mse: 382.3562 - val_loss: 554.2005 - val_mae: 18.5094 - val_mse: 554.2005\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.6459 - mae: 14.9305 - mse: 383.6459 - val_loss: 558.4500 - val_mae: 18.4947 - val_mse: 558.4500\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.8488 - mae: 14.7736 - mse: 379.8488 - val_loss: 551.4978 - val_mae: 18.0294 - val_mse: 551.4978\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.9655 - mae: 14.6613 - mse: 376.9655 - val_loss: 559.6479 - val_mae: 18.3410 - val_mse: 559.6479\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 380.7325 - mae: 14.7738 - mse: 380.7325 - val_loss: 560.1947 - val_mae: 18.3413 - val_mse: 560.1947\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 380.7882 - mae: 14.7935 - mse: 380.7882 - val_loss: 560.7771 - val_mae: 18.2944 - val_mse: 560.7771\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.7351 - mae: 14.7160 - mse: 373.7351 - val_loss: 565.0800 - val_mae: 18.1687 - val_mse: 565.0800\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 374.8674 - mae: 14.7094 - mse: 374.8674 - val_loss: 554.6207 - val_mae: 18.0937 - val_mse: 554.6207\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.5134 - mae: 14.9012 - mse: 383.5134 - val_loss: 556.4305 - val_mae: 18.2633 - val_mse: 556.4305\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.2375 - mae: 14.6628 - mse: 372.2375 - val_loss: 562.2667 - val_mae: 18.3973 - val_mse: 562.2667\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 375.1579 - mae: 14.6821 - mse: 375.1579 - val_loss: 568.1216 - val_mae: 18.3824 - val_mse: 568.1216\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 378.9480 - mae: 14.7438 - mse: 378.9480 - val_loss: 555.2978 - val_mae: 17.7532 - val_mse: 555.2978\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 368.0993 - mae: 14.6170 - mse: 368.0993 - val_loss: 563.8961 - val_mae: 18.0530 - val_mse: 563.8961\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 368.0417 - mae: 14.6307 - mse: 368.0417 - val_loss: 559.6123 - val_mae: 18.0026 - val_mse: 559.6123\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 370.7229 - mae: 14.5916 - mse: 370.7229 - val_loss: 571.9691 - val_mae: 18.6153 - val_mse: 571.9691\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.3111 - mae: 14.5475 - mse: 364.3111 - val_loss: 571.9350 - val_mae: 18.2136 - val_mse: 571.9350\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.6209 - mae: 14.5813 - mse: 372.6209 - val_loss: 563.7224 - val_mae: 18.3297 - val_mse: 563.7224\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 367.5572 - mae: 14.5395 - mse: 367.5572 - val_loss: 565.8137 - val_mae: 18.2724 - val_mse: 565.8137\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 367.5726 - mae: 14.6770 - mse: 367.5726 - val_loss: 565.5074 - val_mae: 18.1168 - val_mse: 565.5074\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 361.0623 - mae: 14.3681 - mse: 361.0623 - val_loss: 567.7952 - val_mae: 18.2103 - val_mse: 567.7952\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 367.5349 - mae: 14.7083 - mse: 367.5349 - val_loss: 562.0535 - val_mae: 18.1528 - val_mse: 562.0535\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.8636 - mae: 14.4632 - mse: 365.8636 - val_loss: 564.8405 - val_mae: 18.1964 - val_mse: 564.8405\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 366.3882 - mae: 14.4953 - mse: 366.3882 - val_loss: 558.6450 - val_mae: 18.2422 - val_mse: 558.6450\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 358.7378 - mae: 14.3667 - mse: 358.7378 - val_loss: 572.7955 - val_mae: 18.4485 - val_mse: 572.7955\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 363.0776 - mae: 14.5322 - mse: 363.0776 - val_loss: 565.3776 - val_mae: 18.6274 - val_mse: 565.3776\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 357.7409 - mae: 14.3861 - mse: 357.7409 - val_loss: 566.4020 - val_mae: 18.4724 - val_mse: 566.4020\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.1135 - mae: 14.2723 - mse: 355.1135 - val_loss: 565.4093 - val_mae: 18.2539 - val_mse: 565.4093\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 362.9678 - mae: 14.4768 - mse: 362.9678 - val_loss: 571.3088 - val_mae: 18.4072 - val_mse: 571.3088\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 359.4512 - mae: 14.3689 - mse: 359.4512 - val_loss: 569.0195 - val_mae: 18.4416 - val_mse: 569.0195\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.3615 - mae: 14.2406 - mse: 354.3615 - val_loss: 567.9642 - val_mae: 18.3156 - val_mse: 567.9642\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.6206 - mae: 14.4722 - mse: 364.6206 - val_loss: 573.1206 - val_mae: 18.4740 - val_mse: 573.1206\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 358.3999 - mae: 14.4585 - mse: 358.3999 - val_loss: 568.8878 - val_mae: 18.6986 - val_mse: 568.8878\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.5077 - mae: 14.3190 - mse: 353.5077 - val_loss: 583.4858 - val_mae: 18.7183 - val_mse: 583.4858\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.8593 - mae: 14.4067 - mse: 354.8593 - val_loss: 568.5494 - val_mae: 18.3843 - val_mse: 568.5494\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 351.9129 - mae: 14.1956 - mse: 351.9129 - val_loss: 583.3265 - val_mae: 18.9989 - val_mse: 583.3265\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.4090 - mae: 14.3351 - mse: 353.4090 - val_loss: 570.2659 - val_mae: 18.2667 - val_mse: 570.2659\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.9775 - mae: 14.1881 - mse: 355.9775 - val_loss: 570.0823 - val_mae: 18.3321 - val_mse: 570.0823\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 344.4247 - mae: 14.0831 - mse: 344.4247 - val_loss: 572.7032 - val_mae: 18.4377 - val_mse: 572.7032\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.4384 - mae: 14.2003 - mse: 349.4384 - val_loss: 602.7197 - val_mae: 18.8389 - val_mse: 602.7197\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.2115 - mae: 14.2141 - mse: 349.2115 - val_loss: 581.7924 - val_mae: 18.6678 - val_mse: 581.7924\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2979.2764 - mae: 45.9555 - mse: 2979.2581 - val_loss: 760.0155 - val_mae: 23.2844 - val_mse: 759.9960\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1018.5153 - mae: 26.2043 - mse: 1018.4959 - val_loss: 623.7423 - val_mae: 20.3517 - val_mse: 623.7227\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 934.3148 - mae: 24.6846 - mse: 934.2952 - val_loss: 601.3236 - val_mae: 20.3922 - val_mse: 601.3040\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 877.2440 - mae: 23.9785 - mse: 877.2250 - val_loss: 592.9407 - val_mae: 20.3693 - val_mse: 592.9212\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 836.6983 - mae: 23.2846 - mse: 836.6787 - val_loss: 572.1729 - val_mae: 19.9360 - val_mse: 572.1534\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 821.0781 - mae: 23.0374 - mse: 821.0589 - val_loss: 559.3003 - val_mae: 19.6092 - val_mse: 559.2808\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 806.1334 - mae: 22.8558 - mse: 806.1142 - val_loss: 544.3677 - val_mae: 19.2702 - val_mse: 544.3486\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 776.7045 - mae: 22.5178 - mse: 776.6854 - val_loss: 550.8495 - val_mae: 19.5213 - val_mse: 550.8304\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 768.0920 - mae: 22.1586 - mse: 768.0729 - val_loss: 526.5245 - val_mae: 18.6983 - val_mse: 526.5056\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 770.5010 - mae: 22.3797 - mse: 770.4822 - val_loss: 519.7021 - val_mae: 18.5630 - val_mse: 519.6832\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 753.9324 - mae: 22.0247 - mse: 753.9139 - val_loss: 523.7183 - val_mae: 18.6775 - val_mse: 523.6995\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 735.2782 - mae: 21.6171 - mse: 735.2596 - val_loss: 519.2223 - val_mae: 18.4538 - val_mse: 519.2037\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 748.9566 - mae: 22.0350 - mse: 748.9380 - val_loss: 520.3136 - val_mae: 18.6805 - val_mse: 520.2950\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 724.2542 - mae: 21.5849 - mse: 724.2358 - val_loss: 526.8956 - val_mae: 18.8379 - val_mse: 526.8771\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 717.4550 - mae: 21.5043 - mse: 717.4366 - val_loss: 519.7739 - val_mae: 18.6673 - val_mse: 519.7557\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 711.3146 - mae: 21.4150 - mse: 711.2961 - val_loss: 517.8089 - val_mae: 18.6040 - val_mse: 517.7908\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.2398 - mae: 21.3918 - mse: 708.2217 - val_loss: 503.9768 - val_mae: 17.8387 - val_mse: 503.9586\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 704.8459 - mae: 21.2271 - mse: 704.8275 - val_loss: 516.2507 - val_mae: 18.6273 - val_mse: 516.2328\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 695.4225 - mae: 21.1489 - mse: 695.4048 - val_loss: 548.2101 - val_mae: 19.6681 - val_mse: 548.1923\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.8582 - mae: 21.2970 - mse: 708.8407 - val_loss: 518.3754 - val_mae: 18.6525 - val_mse: 518.3578\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 707.7458 - mae: 21.2505 - mse: 707.7278 - val_loss: 514.2116 - val_mae: 18.4471 - val_mse: 514.1940\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 703.5203 - mae: 21.2367 - mse: 703.5027 - val_loss: 514.5049 - val_mae: 18.4452 - val_mse: 514.4877\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.9591 - mae: 21.3248 - mse: 709.9417 - val_loss: 518.5211 - val_mae: 18.6794 - val_mse: 518.5040\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 699.7354 - mae: 21.1090 - mse: 699.7183 - val_loss: 515.6614 - val_mae: 18.5527 - val_mse: 515.6442\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 672.5071 - mae: 20.6031 - mse: 672.4901 - val_loss: 509.0794 - val_mae: 18.2691 - val_mse: 509.0622\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 683.7155 - mae: 20.8241 - mse: 683.6986 - val_loss: 501.4780 - val_mae: 17.9621 - val_mse: 501.4611\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.2922 - mae: 20.5321 - mse: 672.2754 - val_loss: 497.3922 - val_mae: 17.5977 - val_mse: 497.3753\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.0279 - mae: 20.9348 - mse: 690.0112 - val_loss: 513.1105 - val_mae: 18.6228 - val_mse: 513.0937\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.2975 - mae: 20.4690 - mse: 666.2807 - val_loss: 509.4179 - val_mae: 18.4811 - val_mse: 509.4013\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.2653 - mae: 20.5390 - mse: 667.2491 - val_loss: 506.4560 - val_mae: 18.2824 - val_mse: 506.4395\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 670.2034 - mae: 20.5357 - mse: 670.1870 - val_loss: 494.7745 - val_mae: 17.5756 - val_mse: 494.7582\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 671.0449 - mae: 20.6021 - mse: 671.0289 - val_loss: 496.1466 - val_mae: 17.8793 - val_mse: 496.1304\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 681.2925 - mae: 20.7500 - mse: 681.2764 - val_loss: 497.1060 - val_mae: 17.3704 - val_mse: 497.0900\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.3893 - mae: 20.3109 - mse: 660.3736 - val_loss: 496.2045 - val_mae: 17.8604 - val_mse: 496.1886\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.4667 - mae: 20.2947 - mse: 661.4509 - val_loss: 493.3372 - val_mae: 17.7755 - val_mse: 493.3215\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.3923 - mae: 20.3543 - mse: 658.3766 - val_loss: 497.7395 - val_mae: 18.0620 - val_mse: 497.7238\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.5251 - mae: 20.3144 - mse: 659.5094 - val_loss: 494.3217 - val_mae: 17.8446 - val_mse: 494.3062\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.1866 - mae: 20.3447 - mse: 668.1710 - val_loss: 496.9356 - val_mae: 17.9837 - val_mse: 496.9202\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.2998 - mae: 20.3543 - mse: 657.2845 - val_loss: 496.3510 - val_mae: 17.8985 - val_mse: 496.3359\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.7859 - mae: 20.5862 - mse: 674.7708 - val_loss: 525.7047 - val_mae: 19.1041 - val_mse: 525.6898\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.3507 - mae: 20.4625 - mse: 668.3357 - val_loss: 495.3698 - val_mae: 17.8186 - val_mse: 495.3549\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 652.2626 - mae: 20.0914 - mse: 652.2478 - val_loss: 518.5317 - val_mae: 18.8385 - val_mse: 518.5172\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.4525 - mae: 20.4866 - mse: 666.4382 - val_loss: 495.2185 - val_mae: 17.7371 - val_mse: 495.2039\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.5675 - mae: 20.1339 - mse: 648.5532 - val_loss: 494.9854 - val_mae: 17.8104 - val_mse: 494.9709\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 659.3745 - mae: 20.4038 - mse: 659.3597 - val_loss: 503.1967 - val_mae: 18.2368 - val_mse: 503.1824\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 655.7101 - mae: 20.2510 - mse: 655.6957 - val_loss: 512.1959 - val_mae: 18.6474 - val_mse: 512.1817\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 641.2416 - mae: 20.1509 - mse: 641.2274 - val_loss: 490.6578 - val_mae: 17.3708 - val_mse: 490.6435\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 661.7751 - mae: 20.3721 - mse: 661.7610 - val_loss: 492.2108 - val_mae: 17.4911 - val_mse: 492.1966\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 649.1249 - mae: 20.1591 - mse: 649.1104 - val_loss: 493.8874 - val_mae: 17.7969 - val_mse: 493.8734\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.1970 - mae: 19.9864 - mse: 643.1832 - val_loss: 501.4378 - val_mae: 18.2201 - val_mse: 501.4241\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 644.4419 - mae: 20.0724 - mse: 644.4282 - val_loss: 497.7058 - val_mae: 18.0963 - val_mse: 497.6922\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.8116 - mae: 20.2390 - mse: 659.7975 - val_loss: 492.2253 - val_mae: 17.7632 - val_mse: 492.2117\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.9926 - mae: 20.2051 - mse: 659.9789 - val_loss: 495.4863 - val_mae: 17.9426 - val_mse: 495.4729\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.7786 - mae: 20.1318 - mse: 653.7650 - val_loss: 491.8537 - val_mae: 17.8329 - val_mse: 491.8404\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.3760 - mae: 19.7275 - mse: 628.3632 - val_loss: 501.4107 - val_mae: 18.2445 - val_mse: 501.3975\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 670.1246 - mae: 20.3161 - mse: 670.1116 - val_loss: 491.6684 - val_mae: 17.7594 - val_mse: 491.6552\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.0873 - mae: 20.4440 - mse: 663.0746 - val_loss: 498.4047 - val_mae: 18.1014 - val_mse: 498.3918\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.6885 - mae: 20.0216 - mse: 646.6754 - val_loss: 499.3006 - val_mae: 18.1404 - val_mse: 499.2878\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6576 - mae: 19.9996 - mse: 635.6450 - val_loss: 490.7336 - val_mae: 17.5091 - val_mse: 490.7207\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.2973 - mae: 20.0600 - mse: 643.2846 - val_loss: 495.2721 - val_mae: 17.9187 - val_mse: 495.2596\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 640.9224 - mae: 19.9115 - mse: 640.9097 - val_loss: 491.7971 - val_mae: 17.3375 - val_mse: 491.7845\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.2477 - mae: 19.8768 - mse: 633.2352 - val_loss: 490.1575 - val_mae: 17.6339 - val_mse: 490.1450\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.1706 - mae: 19.8267 - mse: 630.1583 - val_loss: 498.9841 - val_mae: 18.1309 - val_mse: 498.9719\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 634.6945 - mae: 19.9994 - mse: 634.6818 - val_loss: 520.0391 - val_mae: 18.9224 - val_mse: 520.0269\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 630.9132 - mae: 19.8434 - mse: 630.9012 - val_loss: 489.7959 - val_mae: 17.6502 - val_mse: 489.7836\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.9176 - mae: 19.7468 - mse: 631.9055 - val_loss: 497.0493 - val_mae: 18.0803 - val_mse: 497.0373\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.0549 - mae: 19.9461 - mse: 638.0431 - val_loss: 490.9205 - val_mae: 17.4489 - val_mse: 490.9085\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.1712 - mae: 19.8778 - mse: 634.1593 - val_loss: 494.7243 - val_mae: 17.8887 - val_mse: 494.7123\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.4377 - mae: 19.7861 - mse: 629.4260 - val_loss: 487.8039 - val_mae: 17.3340 - val_mse: 487.7921\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 634.1630 - mae: 19.7549 - mse: 634.1511 - val_loss: 493.2359 - val_mae: 17.7813 - val_mse: 493.2243\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.6912 - mae: 20.0142 - mse: 636.6794 - val_loss: 503.5572 - val_mae: 18.3203 - val_mse: 503.5457\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.2803 - mae: 19.8405 - mse: 633.2685 - val_loss: 518.1147 - val_mae: 18.8417 - val_mse: 518.1035\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.3307 - mae: 19.8649 - mse: 627.3191 - val_loss: 492.0692 - val_mae: 17.7525 - val_mse: 492.0579\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.5982 - mae: 19.8460 - mse: 635.5871 - val_loss: 489.6605 - val_mae: 17.5008 - val_mse: 489.6492\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.8899 - mae: 19.7313 - mse: 629.8785 - val_loss: 493.7029 - val_mae: 17.8809 - val_mse: 493.6918\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 630.0901 - mae: 19.7309 - mse: 630.0792 - val_loss: 504.5531 - val_mae: 18.3207 - val_mse: 504.5421\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.5948 - mae: 19.6938 - mse: 628.5837 - val_loss: 499.2320 - val_mae: 18.1421 - val_mse: 499.2211\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.3044 - mae: 19.7256 - mse: 631.2933 - val_loss: 487.7284 - val_mae: 17.3541 - val_mse: 487.7176\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.0092 - mae: 19.9341 - mse: 635.9983 - val_loss: 489.1519 - val_mae: 17.6187 - val_mse: 489.1410\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.1063 - mae: 19.5391 - mse: 619.0954 - val_loss: 486.8118 - val_mae: 17.5678 - val_mse: 486.8010\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.4692 - mae: 19.9663 - mse: 640.4584 - val_loss: 487.1936 - val_mae: 17.6145 - val_mse: 487.1828\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.7944 - mae: 19.6738 - mse: 619.7834 - val_loss: 483.8549 - val_mae: 17.4232 - val_mse: 483.8441\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.5200 - mae: 19.6049 - mse: 623.5091 - val_loss: 489.7200 - val_mae: 17.8365 - val_mse: 489.7093\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.1088 - mae: 19.5665 - mse: 619.0981 - val_loss: 488.3701 - val_mae: 17.7463 - val_mse: 488.3593\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.8768 - mae: 19.3149 - mse: 609.8660 - val_loss: 486.2737 - val_mae: 17.4891 - val_mse: 486.2630\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.6137 - mae: 19.3598 - mse: 607.6030 - val_loss: 489.1529 - val_mae: 17.7987 - val_mse: 489.1423\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.2412 - mae: 19.5942 - mse: 619.2305 - val_loss: 485.0508 - val_mae: 17.5829 - val_mse: 485.0403\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 622.9512 - mae: 19.5730 - mse: 622.9410 - val_loss: 489.8441 - val_mae: 17.8555 - val_mse: 489.8336\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.4948 - mae: 19.6177 - mse: 621.4843 - val_loss: 483.8210 - val_mae: 17.5150 - val_mse: 483.8106\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.6133 - mae: 19.3209 - mse: 613.6030 - val_loss: 494.0031 - val_mae: 18.0461 - val_mse: 493.9926\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.7522 - mae: 19.3968 - mse: 605.7421 - val_loss: 486.5209 - val_mae: 17.6323 - val_mse: 486.5106\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.6124 - mae: 19.2318 - mse: 601.6020 - val_loss: 487.6220 - val_mae: 17.6383 - val_mse: 487.6115\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8701 - mae: 19.5021 - mse: 615.8599 - val_loss: 483.5876 - val_mae: 17.4441 - val_mse: 483.5773\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.0725 - mae: 19.3636 - mse: 610.0623 - val_loss: 486.0204 - val_mae: 17.6914 - val_mse: 486.0102\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 604.5483 - mae: 19.2141 - mse: 604.5380 - val_loss: 481.3989 - val_mae: 17.3295 - val_mse: 481.3887\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 614.1721 - mae: 19.4679 - mse: 614.1617 - val_loss: 487.7609 - val_mae: 17.7212 - val_mse: 487.7506\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.8781 - mae: 19.2992 - mse: 607.8679 - val_loss: 482.3763 - val_mae: 17.2931 - val_mse: 482.3661\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.9138 - mae: 19.0613 - mse: 591.9038 - val_loss: 481.3646 - val_mae: 17.3723 - val_mse: 481.3544\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.1880 - mae: 19.2094 - mse: 599.1775 - val_loss: 490.3348 - val_mae: 17.8573 - val_mse: 490.3246\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.0259 - mae: 19.3297 - mse: 608.0159 - val_loss: 483.4658 - val_mae: 17.4544 - val_mse: 483.4556\n"
     ]
    }
   ],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 14.8111, Train MSE: 359.5782\n",
      "Val   MAE: 18.8456, Val   MSE: 574.3768\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 12.2385, Train MSE: 265.5150\n",
      "Val   MAE: 18.6678, Val   MSE: 581.7924\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 17.3169, Train MSE: 498.1217\n",
      "Val   MAE: 17.4544, Val   MSE: 483.4556\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1892.7213 - mae: 33.4263 - mse: 1892.7213 - val_loss: 520.3790 - val_mae: 18.1290 - val_mse: 520.3790\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.7260 - mae: 16.7923 - mse: 478.7260 - val_loss: 470.7764 - val_mae: 16.9759 - val_mse: 470.7764\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.6718 - mae: 16.1420 - mse: 451.6718 - val_loss: 462.6606 - val_mae: 16.8362 - val_mse: 462.6606\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.1059 - mae: 15.8108 - mse: 438.1059 - val_loss: 458.5118 - val_mae: 16.7691 - val_mse: 458.5118\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 434.0944 - mae: 15.7039 - mse: 434.0944 - val_loss: 456.4850 - val_mae: 16.7472 - val_mse: 456.4850\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 425.9094 - mae: 15.5476 - mse: 425.9094 - val_loss: 457.1035 - val_mae: 16.6385 - val_mse: 457.1035\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 419.2168 - mae: 15.3868 - mse: 419.2168 - val_loss: 451.6199 - val_mae: 16.4680 - val_mse: 451.6199\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.8326 - mae: 15.2631 - mse: 412.8326 - val_loss: 457.9308 - val_mae: 16.9458 - val_mse: 457.9308\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 412.5154 - mae: 15.2607 - mse: 412.5154 - val_loss: 462.5644 - val_mae: 17.1032 - val_mse: 462.5644\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.2845 - mae: 15.1008 - mse: 405.2845 - val_loss: 461.2936 - val_mae: 17.1343 - val_mse: 461.2936\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 400.7457 - mae: 15.0050 - mse: 400.7457 - val_loss: 450.3910 - val_mae: 16.2371 - val_mse: 450.3910\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.6764 - mae: 14.8601 - mse: 395.6764 - val_loss: 457.5585 - val_mae: 16.3147 - val_mse: 457.5585\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.6292 - mae: 14.8355 - mse: 395.6292 - val_loss: 453.7112 - val_mae: 16.4447 - val_mse: 453.7112\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.3511 - mae: 14.8365 - mse: 392.3511 - val_loss: 454.4232 - val_mae: 16.5412 - val_mse: 454.4232\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.4425 - mae: 14.6643 - mse: 385.4425 - val_loss: 455.3342 - val_mae: 16.9172 - val_mse: 455.3342\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 384.5289 - mae: 14.5799 - mse: 384.5289 - val_loss: 456.2061 - val_mae: 16.7527 - val_mse: 456.2061\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 381.1043 - mae: 14.5781 - mse: 381.1043 - val_loss: 453.3718 - val_mae: 16.6571 - val_mse: 453.3718\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 374.3138 - mae: 14.3935 - mse: 374.3138 - val_loss: 460.0337 - val_mae: 16.7442 - val_mse: 460.0337\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 369.3541 - mae: 14.3137 - mse: 369.3541 - val_loss: 455.9283 - val_mae: 16.4409 - val_mse: 455.9283\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 370.6097 - mae: 14.3330 - mse: 370.6097 - val_loss: 470.5733 - val_mae: 16.5976 - val_mse: 470.5733\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 365.4076 - mae: 14.2229 - mse: 365.4076 - val_loss: 459.6632 - val_mae: 16.3948 - val_mse: 459.6632\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 366.1736 - mae: 14.1925 - mse: 366.1736 - val_loss: 464.7560 - val_mae: 16.7702 - val_mse: 464.7560\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 357.0033 - mae: 13.9864 - mse: 357.0033 - val_loss: 467.2335 - val_mae: 17.0623 - val_mse: 467.2335\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 352.4339 - mae: 13.9761 - mse: 352.4339 - val_loss: 468.7139 - val_mae: 16.4632 - val_mse: 468.7139\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 348.3414 - mae: 13.7797 - mse: 348.3414 - val_loss: 476.8320 - val_mae: 17.2379 - val_mse: 476.8320\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 345.1793 - mae: 13.7669 - mse: 345.1793 - val_loss: 470.8262 - val_mae: 16.5245 - val_mse: 470.8262\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 340.0699 - mae: 13.6442 - mse: 340.0699 - val_loss: 471.9616 - val_mae: 16.8797 - val_mse: 471.9616\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 338.1692 - mae: 13.6232 - mse: 338.1692 - val_loss: 474.3118 - val_mae: 16.8708 - val_mse: 474.3118\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 333.3511 - mae: 13.4843 - mse: 333.3511 - val_loss: 485.0847 - val_mae: 16.8765 - val_mse: 485.0847\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 325.9722 - mae: 13.3045 - mse: 325.9722 - val_loss: 477.7078 - val_mae: 16.6771 - val_mse: 477.7078\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 323.1123 - mae: 13.3083 - mse: 323.1123 - val_loss: 491.4530 - val_mae: 17.1689 - val_mse: 491.4530\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 318.3349 - mae: 13.1968 - mse: 318.3349 - val_loss: 488.9095 - val_mae: 16.7596 - val_mse: 488.9095\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 310.2058 - mae: 12.9917 - mse: 310.2058 - val_loss: 488.1793 - val_mae: 17.0778 - val_mse: 488.1793\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 305.8344 - mae: 12.9338 - mse: 305.8344 - val_loss: 490.8407 - val_mae: 16.9126 - val_mse: 490.8407\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 303.0354 - mae: 12.7753 - mse: 303.0354 - val_loss: 497.4867 - val_mae: 17.3696 - val_mse: 497.4867\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 300.0373 - mae: 12.7306 - mse: 300.0373 - val_loss: 520.1455 - val_mae: 18.2407 - val_mse: 520.1455\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 297.0416 - mae: 12.7148 - mse: 297.0416 - val_loss: 506.5608 - val_mae: 17.8004 - val_mse: 506.5608\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 286.6959 - mae: 12.4262 - mse: 286.6959 - val_loss: 520.1334 - val_mae: 18.0876 - val_mse: 520.1334\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 281.5569 - mae: 12.3512 - mse: 281.5569 - val_loss: 509.3662 - val_mae: 17.4594 - val_mse: 509.3662\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 274.1237 - mae: 12.1716 - mse: 274.1237 - val_loss: 506.1709 - val_mae: 17.6599 - val_mse: 506.1709\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 270.8106 - mae: 12.0299 - mse: 270.8106 - val_loss: 531.0521 - val_mae: 17.5972 - val_mse: 531.0521\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 268.4175 - mae: 11.9875 - mse: 268.4175 - val_loss: 519.0777 - val_mae: 17.4131 - val_mse: 519.0777\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 263.4570 - mae: 11.8073 - mse: 263.4570 - val_loss: 530.5804 - val_mae: 17.6679 - val_mse: 530.5804\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 256.9211 - mae: 11.7870 - mse: 256.9211 - val_loss: 527.5912 - val_mae: 17.5916 - val_mse: 527.5912\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 249.3903 - mae: 11.5042 - mse: 249.3903 - val_loss: 539.9401 - val_mae: 17.6758 - val_mse: 539.9401\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 249.5378 - mae: 11.4962 - mse: 249.5378 - val_loss: 543.3564 - val_mae: 17.8698 - val_mse: 543.3564\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 240.2439 - mae: 11.2567 - mse: 240.2439 - val_loss: 540.8874 - val_mae: 18.1939 - val_mse: 540.8874\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 237.7074 - mae: 11.2719 - mse: 237.7074 - val_loss: 558.0477 - val_mae: 18.0345 - val_mse: 558.0477\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.5417 - mae: 11.1816 - mse: 234.5417 - val_loss: 542.7029 - val_mae: 17.5737 - val_mse: 542.7029\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 233.5663 - mae: 11.1447 - mse: 233.5663 - val_loss: 554.2781 - val_mae: 18.0526 - val_mse: 554.2781\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.1102 - mae: 11.1115 - mse: 234.1102 - val_loss: 567.6879 - val_mae: 18.0554 - val_mse: 567.6879\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 224.1743 - mae: 10.8234 - mse: 224.1743 - val_loss: 580.0012 - val_mae: 18.4358 - val_mse: 580.0012\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 218.1941 - mae: 10.7213 - mse: 218.1941 - val_loss: 571.9794 - val_mae: 18.3055 - val_mse: 571.9794\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 220.0957 - mae: 10.6973 - mse: 220.0957 - val_loss: 573.1796 - val_mae: 18.2130 - val_mse: 573.1796\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 214.8078 - mae: 10.7697 - mse: 214.8078 - val_loss: 594.4938 - val_mae: 18.7789 - val_mse: 594.4938\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 209.0017 - mae: 10.5725 - mse: 209.0017 - val_loss: 575.3069 - val_mae: 18.2085 - val_mse: 575.3069\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 205.2209 - mae: 10.3839 - mse: 205.2209 - val_loss: 583.3596 - val_mae: 18.7053 - val_mse: 583.3596\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.3109 - mae: 10.3498 - mse: 202.3109 - val_loss: 584.6224 - val_mae: 18.5512 - val_mse: 584.6224\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 199.1269 - mae: 10.3228 - mse: 199.1269 - val_loss: 581.2922 - val_mae: 18.4172 - val_mse: 581.2922\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.8089 - mae: 10.2021 - mse: 195.8089 - val_loss: 589.9075 - val_mae: 18.6954 - val_mse: 589.9075\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 190.6271 - mae: 10.0456 - mse: 190.6271 - val_loss: 598.8209 - val_mae: 18.8713 - val_mse: 598.8209\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 189.3175 - mae: 10.0006 - mse: 189.3175 - val_loss: 610.1231 - val_mae: 19.3005 - val_mse: 610.1231\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 183.2306 - mae: 9.8317 - mse: 183.2306 - val_loss: 631.8595 - val_mae: 19.0779 - val_mse: 631.8595\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 181.8792 - mae: 9.8090 - mse: 181.8792 - val_loss: 631.9651 - val_mae: 19.0343 - val_mse: 631.9651\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 176.2698 - mae: 9.5989 - mse: 176.2698 - val_loss: 618.3696 - val_mae: 18.8983 - val_mse: 618.3696\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 176.6344 - mae: 9.7076 - mse: 176.6344 - val_loss: 641.5378 - val_mae: 18.8336 - val_mse: 641.5378\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 175.7886 - mae: 9.6094 - mse: 175.7886 - val_loss: 637.1666 - val_mae: 19.2912 - val_mse: 637.1666\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.3987 - mae: 9.5277 - mse: 171.3987 - val_loss: 632.7845 - val_mae: 19.0899 - val_mse: 632.7845\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 168.2045 - mae: 9.4112 - mse: 168.2045 - val_loss: 649.2037 - val_mae: 19.2462 - val_mse: 649.2037\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.0506 - mae: 9.5423 - mse: 171.0506 - val_loss: 675.4341 - val_mae: 19.4216 - val_mse: 675.4341\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 167.1529 - mae: 9.3497 - mse: 167.1529 - val_loss: 661.6721 - val_mae: 19.4914 - val_mse: 661.6721\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 157.8326 - mae: 9.0896 - mse: 157.8326 - val_loss: 662.5737 - val_mae: 19.2938 - val_mse: 662.5737\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 156.7415 - mae: 9.0857 - mse: 156.7415 - val_loss: 672.7690 - val_mae: 20.0315 - val_mse: 672.7690\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 153.3025 - mae: 8.9901 - mse: 153.3025 - val_loss: 687.0602 - val_mae: 19.6466 - val_mse: 687.0602\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 153.8064 - mae: 8.9677 - mse: 153.8064 - val_loss: 660.1406 - val_mae: 19.6931 - val_mse: 660.1406\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.5812 - mae: 8.8227 - mse: 147.5812 - val_loss: 646.2503 - val_mae: 19.2705 - val_mse: 646.2503\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 146.6172 - mae: 8.7100 - mse: 146.6172 - val_loss: 662.7087 - val_mae: 19.5628 - val_mse: 662.7087\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 141.6480 - mae: 8.6112 - mse: 141.6480 - val_loss: 696.4136 - val_mae: 19.8653 - val_mse: 696.4136\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.2308 - mae: 8.5359 - mse: 140.2308 - val_loss: 675.4758 - val_mae: 19.7746 - val_mse: 675.4758\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6542 - mae: 8.4273 - mse: 136.6542 - val_loss: 669.3276 - val_mae: 19.6349 - val_mse: 669.3276\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2565 - mae: 8.3920 - mse: 134.2565 - val_loss: 686.1292 - val_mae: 19.8983 - val_mse: 686.1292\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.7356 - mae: 8.3393 - mse: 132.7356 - val_loss: 718.5313 - val_mae: 20.1434 - val_mse: 718.5313\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.1627 - mae: 8.4911 - mse: 134.1627 - val_loss: 699.2370 - val_mae: 19.9888 - val_mse: 699.2370\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.6164 - mae: 8.5269 - mse: 135.6164 - val_loss: 698.2330 - val_mae: 20.3521 - val_mse: 698.2330\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.7049 - mae: 8.2169 - mse: 126.7049 - val_loss: 706.7504 - val_mae: 20.4258 - val_mse: 706.7504\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7794 - mae: 8.3042 - mse: 128.7794 - val_loss: 706.2249 - val_mae: 19.9442 - val_mse: 706.2249\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4908 - mae: 8.3317 - mse: 128.4908 - val_loss: 697.6802 - val_mae: 20.0852 - val_mse: 697.6802\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.8584 - mae: 8.0513 - mse: 121.8584 - val_loss: 722.1714 - val_mae: 20.2290 - val_mse: 722.1714\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.0711 - mae: 8.0673 - mse: 122.0711 - val_loss: 737.4019 - val_mae: 20.5457 - val_mse: 737.4019\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.1796 - mae: 7.8699 - mse: 118.1796 - val_loss: 751.5908 - val_mae: 20.7820 - val_mse: 751.5908\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.7113 - mae: 7.7076 - mse: 114.7113 - val_loss: 733.6817 - val_mae: 20.5299 - val_mse: 733.6817\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.0477 - mae: 7.7017 - mse: 114.0477 - val_loss: 757.0280 - val_mae: 20.7442 - val_mse: 757.0280\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 120.8091 - mae: 7.9975 - mse: 120.8091 - val_loss: 725.3312 - val_mae: 20.7769 - val_mse: 725.3312\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 116.7880 - mae: 7.8752 - mse: 116.7880 - val_loss: 741.6437 - val_mae: 20.4434 - val_mse: 741.6437\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.9279 - mae: 7.6135 - mse: 112.9279 - val_loss: 757.6684 - val_mae: 20.9498 - val_mse: 757.6684\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.7833 - mae: 7.5082 - mse: 107.7833 - val_loss: 743.4960 - val_mae: 20.3855 - val_mse: 743.4960\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.3735 - mae: 7.5345 - mse: 109.3735 - val_loss: 767.4005 - val_mae: 20.8563 - val_mse: 767.4005\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.2023 - mae: 7.4484 - mse: 106.2023 - val_loss: 735.8165 - val_mae: 20.3734 - val_mse: 735.8165\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.5678 - mae: 7.4372 - mse: 106.5678 - val_loss: 741.6857 - val_mae: 20.7945 - val_mse: 741.6857\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9588 - mae: 7.2521 - mse: 101.9588 - val_loss: 764.1398 - val_mae: 20.9125 - val_mse: 764.1398\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5866.9878 - mae: 72.7818 - mse: 5866.9878 - val_loss: 5441.7329 - val_mae: 70.0525 - val_mse: 5441.7329\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4661.4712 - mae: 64.4526 - mse: 4661.4712 - val_loss: 4027.0215 - val_mae: 59.7014 - val_mse: 4027.0215\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3143.0669 - mae: 52.1421 - mse: 3143.0669 - val_loss: 2472.0081 - val_mae: 46.0076 - val_mse: 2472.0081\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1706.3195 - mae: 37.3661 - mse: 1706.3195 - val_loss: 1213.7183 - val_mae: 31.2431 - val_mse: 1213.7183\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 832.3569 - mae: 24.9782 - mse: 832.3569 - val_loss: 676.0316 - val_mae: 22.1221 - val_mse: 676.0316\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.7731 - mae: 18.3451 - mse: 512.7731 - val_loss: 459.2562 - val_mae: 16.9291 - val_mse: 459.2562\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 441.5845 - mae: 16.2541 - mse: 441.5845 - val_loss: 450.6696 - val_mae: 16.4872 - val_mse: 450.6696\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 424.7069 - mae: 15.4769 - mse: 424.7069 - val_loss: 446.4685 - val_mae: 15.9859 - val_mse: 446.4685\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.9382 - mae: 15.2586 - mse: 412.9382 - val_loss: 449.1614 - val_mae: 16.0142 - val_mse: 449.1614\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 400.3046 - mae: 14.9203 - mse: 400.3046 - val_loss: 451.0849 - val_mae: 16.0207 - val_mse: 451.0849\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.9171 - mae: 14.8369 - mse: 397.9171 - val_loss: 455.1835 - val_mae: 16.2439 - val_mse: 455.1835\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.3323 - mae: 14.8609 - mse: 394.3323 - val_loss: 457.4597 - val_mae: 16.2118 - val_mse: 457.4597\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.4710 - mae: 14.6079 - mse: 385.4710 - val_loss: 465.0157 - val_mae: 16.5770 - val_mse: 465.0157\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 384.9205 - mae: 14.5747 - mse: 384.9205 - val_loss: 458.4398 - val_mae: 16.0008 - val_mse: 458.4398\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.9151 - mae: 14.4564 - mse: 376.9151 - val_loss: 470.0336 - val_mae: 16.3067 - val_mse: 470.0336\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.9337 - mae: 14.4393 - mse: 381.9337 - val_loss: 464.6107 - val_mae: 16.3114 - val_mse: 464.6107\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.0064 - mae: 14.2703 - mse: 372.0064 - val_loss: 464.6444 - val_mae: 16.2317 - val_mse: 464.6444\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.2359 - mae: 14.1109 - mse: 364.2359 - val_loss: 465.2691 - val_mae: 16.4874 - val_mse: 465.2691\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 356.8594 - mae: 14.0007 - mse: 356.8594 - val_loss: 460.3544 - val_mae: 16.1133 - val_mse: 460.3544\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 357.5927 - mae: 13.9377 - mse: 357.5927 - val_loss: 466.6535 - val_mae: 16.5870 - val_mse: 466.6535\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.1353 - mae: 13.8807 - mse: 353.1353 - val_loss: 466.7399 - val_mae: 16.2004 - val_mse: 466.7399\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.0158 - mae: 13.8273 - mse: 353.0158 - val_loss: 463.8322 - val_mae: 16.4347 - val_mse: 463.8322\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.6851 - mae: 13.8015 - mse: 349.6851 - val_loss: 470.5819 - val_mae: 16.7223 - val_mse: 470.5819\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 344.5558 - mae: 13.7877 - mse: 344.5558 - val_loss: 464.3658 - val_mae: 16.0960 - val_mse: 464.3658\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 337.2541 - mae: 13.5315 - mse: 337.2541 - val_loss: 473.1261 - val_mae: 16.5070 - val_mse: 473.1261\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 335.2006 - mae: 13.5423 - mse: 335.2006 - val_loss: 471.8835 - val_mae: 16.4423 - val_mse: 471.8835\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 331.1770 - mae: 13.3746 - mse: 331.1770 - val_loss: 477.9010 - val_mae: 16.4507 - val_mse: 477.9010\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 325.7626 - mae: 13.4064 - mse: 325.7626 - val_loss: 478.7518 - val_mae: 16.6151 - val_mse: 478.7518\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 327.3549 - mae: 13.3480 - mse: 327.3549 - val_loss: 471.8765 - val_mae: 16.2520 - val_mse: 471.8765\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 325.6339 - mae: 13.2860 - mse: 325.6339 - val_loss: 481.2211 - val_mae: 16.6347 - val_mse: 481.2211\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 314.9679 - mae: 13.1025 - mse: 314.9679 - val_loss: 481.1561 - val_mae: 16.5358 - val_mse: 481.1561\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 313.7764 - mae: 13.0008 - mse: 313.7764 - val_loss: 482.3104 - val_mae: 16.5673 - val_mse: 482.3104\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 315.0605 - mae: 13.0688 - mse: 315.0605 - val_loss: 469.6340 - val_mae: 16.3503 - val_mse: 469.6340\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 308.6010 - mae: 12.9189 - mse: 308.6010 - val_loss: 469.8265 - val_mae: 16.3978 - val_mse: 469.8265\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 302.0658 - mae: 12.7850 - mse: 302.0658 - val_loss: 481.9107 - val_mae: 16.8085 - val_mse: 481.9107\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 304.0415 - mae: 12.8768 - mse: 304.0415 - val_loss: 485.9336 - val_mae: 16.4845 - val_mse: 485.9336\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 307.5296 - mae: 12.8157 - mse: 307.5296 - val_loss: 477.2332 - val_mae: 16.5424 - val_mse: 477.2332\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 299.6761 - mae: 12.7099 - mse: 299.6761 - val_loss: 490.7467 - val_mae: 16.9306 - val_mse: 490.7467\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 291.6181 - mae: 12.4962 - mse: 291.6181 - val_loss: 495.7002 - val_mae: 16.6689 - val_mse: 495.7002\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 293.1833 - mae: 12.5437 - mse: 293.1833 - val_loss: 484.8248 - val_mae: 16.6809 - val_mse: 484.8248\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 283.3069 - mae: 12.3855 - mse: 283.3069 - val_loss: 496.7206 - val_mae: 16.6691 - val_mse: 496.7206\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 283.0588 - mae: 12.3464 - mse: 283.0588 - val_loss: 503.1490 - val_mae: 17.2267 - val_mse: 503.1490\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 280.0394 - mae: 12.2234 - mse: 280.0394 - val_loss: 499.8807 - val_mae: 16.8741 - val_mse: 499.8807\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 277.4629 - mae: 12.2164 - mse: 277.4629 - val_loss: 507.8779 - val_mae: 16.8089 - val_mse: 507.8779\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 274.7758 - mae: 12.1680 - mse: 274.7758 - val_loss: 508.6575 - val_mae: 17.1845 - val_mse: 508.6575\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 277.5538 - mae: 12.2126 - mse: 277.5538 - val_loss: 506.0362 - val_mae: 16.9816 - val_mse: 506.0362\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 267.9180 - mae: 11.9835 - mse: 267.9180 - val_loss: 505.0749 - val_mae: 16.9086 - val_mse: 505.0749\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 268.4735 - mae: 11.9260 - mse: 268.4735 - val_loss: 511.1898 - val_mae: 16.8433 - val_mse: 511.1898\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 261.0044 - mae: 11.7558 - mse: 261.0044 - val_loss: 518.0646 - val_mae: 17.1133 - val_mse: 518.0646\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 264.1172 - mae: 11.8962 - mse: 264.1172 - val_loss: 525.9396 - val_mae: 17.3493 - val_mse: 525.9396\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 254.5170 - mae: 11.7996 - mse: 254.5170 - val_loss: 529.6415 - val_mae: 17.5690 - val_mse: 529.6415\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 259.1226 - mae: 11.7465 - mse: 259.1226 - val_loss: 527.3185 - val_mae: 17.5603 - val_mse: 527.3185\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 250.8418 - mae: 11.5431 - mse: 250.8418 - val_loss: 529.4772 - val_mae: 17.3844 - val_mse: 529.4772\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 254.7495 - mae: 11.7982 - mse: 254.7495 - val_loss: 530.1727 - val_mae: 17.2655 - val_mse: 530.1727\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 253.8266 - mae: 11.6086 - mse: 253.8266 - val_loss: 537.9186 - val_mae: 17.4931 - val_mse: 537.9186\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 248.3782 - mae: 11.5386 - mse: 248.3782 - val_loss: 540.6470 - val_mae: 17.8119 - val_mse: 540.6470\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 243.4111 - mae: 11.3685 - mse: 243.4111 - val_loss: 540.0712 - val_mae: 17.9197 - val_mse: 540.0712\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 248.7368 - mae: 11.5906 - mse: 248.7368 - val_loss: 536.9722 - val_mae: 17.7332 - val_mse: 536.9722\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 238.5628 - mae: 11.2956 - mse: 238.5628 - val_loss: 545.8381 - val_mae: 17.7426 - val_mse: 545.8381\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 238.3579 - mae: 11.2783 - mse: 238.3579 - val_loss: 540.4854 - val_mae: 17.7453 - val_mse: 540.4854\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 226.6470 - mae: 10.9682 - mse: 226.6470 - val_loss: 547.1608 - val_mae: 17.6305 - val_mse: 547.1608\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 232.3131 - mae: 11.0624 - mse: 232.3131 - val_loss: 535.6311 - val_mae: 17.4598 - val_mse: 535.6311\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 233.7380 - mae: 11.1734 - mse: 233.7380 - val_loss: 541.5906 - val_mae: 17.4435 - val_mse: 541.5906\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 230.8068 - mae: 11.1507 - mse: 230.8068 - val_loss: 548.8479 - val_mae: 17.7583 - val_mse: 548.8479\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 223.9628 - mae: 10.9801 - mse: 223.9628 - val_loss: 541.9437 - val_mae: 17.5134 - val_mse: 541.9437\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 235.8235 - mae: 11.2977 - mse: 235.8235 - val_loss: 543.1762 - val_mae: 17.4922 - val_mse: 543.1762\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 225.1030 - mae: 10.9145 - mse: 225.1030 - val_loss: 541.0896 - val_mae: 17.4648 - val_mse: 541.0896\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 226.3839 - mae: 11.0066 - mse: 226.3839 - val_loss: 564.9969 - val_mae: 17.9952 - val_mse: 564.9969\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 221.5108 - mae: 10.9441 - mse: 221.5108 - val_loss: 565.8674 - val_mae: 17.9486 - val_mse: 565.8674\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 223.4545 - mae: 10.9491 - mse: 223.4545 - val_loss: 566.2095 - val_mae: 18.0752 - val_mse: 566.2095\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 218.3287 - mae: 10.7970 - mse: 218.3287 - val_loss: 562.3889 - val_mae: 17.9173 - val_mse: 562.3889\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.6367 - mae: 10.7840 - mse: 215.6367 - val_loss: 557.6080 - val_mae: 17.8526 - val_mse: 557.6080\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 221.2101 - mae: 10.8905 - mse: 221.2101 - val_loss: 546.6534 - val_mae: 17.6331 - val_mse: 546.6534\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 216.3825 - mae: 10.7594 - mse: 216.3825 - val_loss: 565.8367 - val_mae: 17.8717 - val_mse: 565.8367\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 210.8029 - mae: 10.6284 - mse: 210.8029 - val_loss: 573.1037 - val_mae: 18.1484 - val_mse: 573.1037\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 212.7604 - mae: 10.6628 - mse: 212.7604 - val_loss: 568.5330 - val_mae: 18.1579 - val_mse: 568.5330\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 208.5023 - mae: 10.6022 - mse: 208.5023 - val_loss: 571.2440 - val_mae: 18.0985 - val_mse: 571.2440\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 211.4844 - mae: 10.7471 - mse: 211.4844 - val_loss: 557.2679 - val_mae: 17.8960 - val_mse: 557.2679\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.2939 - mae: 10.7750 - mse: 215.2939 - val_loss: 566.6571 - val_mae: 18.0758 - val_mse: 566.6571\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 212.6004 - mae: 10.6770 - mse: 212.6004 - val_loss: 567.9473 - val_mae: 17.8868 - val_mse: 567.9473\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.7089 - mae: 10.4998 - mse: 203.7089 - val_loss: 560.5833 - val_mae: 18.1725 - val_mse: 560.5833\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 200.9450 - mae: 10.4617 - mse: 200.9450 - val_loss: 557.6107 - val_mae: 18.0139 - val_mse: 557.6107\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.7611 - mae: 10.3987 - mse: 203.7611 - val_loss: 557.4728 - val_mae: 17.9910 - val_mse: 557.4728\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.0396 - mae: 10.4846 - mse: 202.0396 - val_loss: 569.1538 - val_mae: 18.1557 - val_mse: 569.1538\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.7039 - mae: 10.3752 - mse: 195.7039 - val_loss: 583.2649 - val_mae: 18.2930 - val_mse: 583.2649\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 196.3322 - mae: 10.2210 - mse: 196.3322 - val_loss: 577.3466 - val_mae: 18.0782 - val_mse: 577.3466\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 200.4626 - mae: 10.3867 - mse: 200.4626 - val_loss: 576.5277 - val_mae: 18.1372 - val_mse: 576.5277\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 197.6616 - mae: 10.3311 - mse: 197.6616 - val_loss: 586.8721 - val_mae: 18.4099 - val_mse: 586.8721\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 199.5432 - mae: 10.5151 - mse: 199.5432 - val_loss: 567.8392 - val_mae: 18.1075 - val_mse: 567.8392\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 192.1346 - mae: 10.2349 - mse: 192.1346 - val_loss: 574.4958 - val_mae: 18.0917 - val_mse: 574.4958\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 197.9836 - mae: 10.2845 - mse: 197.9836 - val_loss: 567.5116 - val_mae: 17.9115 - val_mse: 567.5116\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 198.1748 - mae: 10.3916 - mse: 198.1748 - val_loss: 574.7110 - val_mae: 18.3141 - val_mse: 574.7110\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 196.9849 - mae: 10.3517 - mse: 196.9849 - val_loss: 570.9031 - val_mae: 17.9650 - val_mse: 570.9031\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 191.1976 - mae: 10.2090 - mse: 191.1976 - val_loss: 587.0274 - val_mae: 18.4660 - val_mse: 587.0274\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 194.6873 - mae: 10.3398 - mse: 194.6873 - val_loss: 582.4552 - val_mae: 18.1100 - val_mse: 582.4552\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.2665 - mae: 10.3365 - mse: 195.2665 - val_loss: 577.6946 - val_mae: 18.2511 - val_mse: 577.6946\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 191.2505 - mae: 10.1402 - mse: 191.2505 - val_loss: 581.4416 - val_mae: 18.1287 - val_mse: 581.4416\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.5191 - mae: 10.2593 - mse: 195.5191 - val_loss: 573.3727 - val_mae: 18.0882 - val_mse: 573.3727\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 189.0881 - mae: 10.1114 - mse: 189.0881 - val_loss: 569.4839 - val_mae: 18.3654 - val_mse: 569.4839\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 187.1733 - mae: 10.0975 - mse: 187.1733 - val_loss: 573.3924 - val_mae: 18.1703 - val_mse: 573.3924\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2699.9878 - mae: 42.9134 - mse: 2699.9597 - val_loss: 655.1633 - val_mae: 21.8393 - val_mse: 655.1312\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 877.1313 - mae: 24.1740 - mse: 877.0978 - val_loss: 552.1523 - val_mae: 19.5945 - val_mse: 552.1185\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 827.9899 - mae: 23.2400 - mse: 827.9553 - val_loss: 527.2841 - val_mae: 19.1267 - val_mse: 527.2489\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 799.1243 - mae: 22.8544 - mse: 799.0889 - val_loss: 467.1892 - val_mae: 17.3312 - val_mse: 467.1527\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 737.2303 - mae: 21.9505 - mse: 737.1935 - val_loss: 507.0651 - val_mae: 18.5500 - val_mse: 507.0283\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 733.1163 - mae: 21.5839 - mse: 733.0792 - val_loss: 547.1879 - val_mae: 19.6434 - val_mse: 547.1505\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 726.4760 - mae: 21.5847 - mse: 726.4379 - val_loss: 493.8077 - val_mae: 18.1518 - val_mse: 493.7698\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 732.0952 - mae: 21.7908 - mse: 732.0567 - val_loss: 464.4268 - val_mae: 17.2329 - val_mse: 464.3880\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.4677 - mae: 21.4737 - mse: 709.4289 - val_loss: 478.1371 - val_mae: 17.8140 - val_mse: 478.0981\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 703.3143 - mae: 21.3230 - mse: 703.2752 - val_loss: 466.0837 - val_mae: 17.3452 - val_mse: 466.0443\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 679.2150 - mae: 20.9736 - mse: 679.1757 - val_loss: 461.8279 - val_mae: 17.2486 - val_mse: 461.7884\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.6072 - mae: 20.7926 - mse: 674.5679 - val_loss: 480.9797 - val_mae: 17.9876 - val_mse: 480.9402\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.4896 - mae: 20.5273 - mse: 665.4501 - val_loss: 471.7990 - val_mae: 17.7558 - val_mse: 471.7594\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.2434 - mae: 20.6405 - mse: 664.2038 - val_loss: 490.0572 - val_mae: 18.1827 - val_mse: 490.0177\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.0842 - mae: 20.4652 - mse: 653.0446 - val_loss: 494.0870 - val_mae: 18.4085 - val_mse: 494.0478\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.0980 - mae: 20.5665 - mse: 657.0585 - val_loss: 449.6897 - val_mae: 16.9544 - val_mse: 449.6502\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.5643 - mae: 20.7140 - mse: 664.5252 - val_loss: 473.1994 - val_mae: 17.6840 - val_mse: 473.1603\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.7613 - mae: 20.2277 - mse: 646.7220 - val_loss: 502.1192 - val_mae: 18.7093 - val_mse: 502.0802\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.6370 - mae: 20.1777 - mse: 634.5981 - val_loss: 477.1523 - val_mae: 17.8658 - val_mse: 477.1138\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 632.6215 - mae: 20.1404 - mse: 632.5833 - val_loss: 471.9774 - val_mae: 17.6859 - val_mse: 471.9391\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.0890 - mae: 20.0329 - mse: 624.0509 - val_loss: 469.2138 - val_mae: 17.5977 - val_mse: 469.1762\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.9440 - mae: 20.0887 - mse: 630.9061 - val_loss: 471.3345 - val_mae: 17.5622 - val_mse: 471.2968\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.0771 - mae: 20.1625 - mse: 635.0399 - val_loss: 467.8669 - val_mae: 17.5838 - val_mse: 467.8294\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 620.7756 - mae: 19.9544 - mse: 620.7382 - val_loss: 464.7070 - val_mae: 17.5406 - val_mse: 464.6699\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.7949 - mae: 19.7874 - mse: 616.7581 - val_loss: 471.5573 - val_mae: 17.7064 - val_mse: 471.5207\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.8621 - mae: 19.9372 - mse: 626.8256 - val_loss: 457.5323 - val_mae: 17.1159 - val_mse: 457.4958\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.3671 - mae: 19.6887 - mse: 607.3311 - val_loss: 477.3049 - val_mae: 17.8936 - val_mse: 477.2691\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.7769 - mae: 19.8022 - mse: 609.7413 - val_loss: 461.1885 - val_mae: 17.3280 - val_mse: 461.1531\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.3765 - mae: 19.6564 - mse: 606.3416 - val_loss: 491.7515 - val_mae: 18.3382 - val_mse: 491.7165\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.9058 - mae: 19.6416 - mse: 603.8710 - val_loss: 455.6653 - val_mae: 17.1682 - val_mse: 455.6307\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.2947 - mae: 19.7515 - mse: 611.2605 - val_loss: 490.2550 - val_mae: 18.2829 - val_mse: 490.2209\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.5119 - mae: 19.4143 - mse: 588.4781 - val_loss: 448.6614 - val_mae: 16.7411 - val_mse: 448.6277\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.9391 - mae: 19.5670 - mse: 596.9056 - val_loss: 447.4780 - val_mae: 16.8605 - val_mse: 447.4446\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.8622 - mae: 19.6592 - mse: 606.8291 - val_loss: 451.4127 - val_mae: 16.9833 - val_mse: 451.3796\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.9089 - mae: 19.2578 - mse: 582.8762 - val_loss: 462.3209 - val_mae: 17.3709 - val_mse: 462.2881\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.2286 - mae: 19.5939 - mse: 596.1962 - val_loss: 453.9679 - val_mae: 16.8232 - val_mse: 453.9356\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.7500 - mae: 19.4566 - mse: 594.7178 - val_loss: 461.2596 - val_mae: 17.3555 - val_mse: 461.2276\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.2617 - mae: 19.5007 - mse: 599.2297 - val_loss: 462.1499 - val_mae: 17.3968 - val_mse: 462.1181\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.2078 - mae: 19.5109 - mse: 595.1760 - val_loss: 479.7705 - val_mae: 18.0515 - val_mse: 479.7392\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.4805 - mae: 19.2231 - mse: 583.4492 - val_loss: 482.3332 - val_mae: 18.0100 - val_mse: 482.3020\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.2358 - mae: 19.4789 - mse: 586.2047 - val_loss: 450.3031 - val_mae: 16.7396 - val_mse: 450.2722\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 579.7890 - mae: 19.1410 - mse: 579.7586 - val_loss: 444.7226 - val_mae: 16.6006 - val_mse: 444.6920\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.8231 - mae: 19.2911 - mse: 583.7926 - val_loss: 467.8677 - val_mae: 17.6150 - val_mse: 467.8375\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 580.9232 - mae: 19.3041 - mse: 580.8929 - val_loss: 451.1052 - val_mae: 17.0699 - val_mse: 451.0750\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.2396 - mae: 19.3425 - mse: 580.2094 - val_loss: 451.6719 - val_mae: 16.9895 - val_mse: 451.6418\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 575.8730 - mae: 19.1499 - mse: 575.8431 - val_loss: 473.6396 - val_mae: 17.8598 - val_mse: 473.6101\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 578.8672 - mae: 19.2373 - mse: 578.8376 - val_loss: 458.7299 - val_mae: 17.3896 - val_mse: 458.7004\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.2045 - mae: 19.3511 - mse: 582.1750 - val_loss: 449.5809 - val_mae: 16.8661 - val_mse: 449.5516\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.7388 - mae: 18.9605 - mse: 571.7095 - val_loss: 451.4867 - val_mae: 17.0170 - val_mse: 451.4576\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.3334 - mae: 18.9519 - mse: 566.3043 - val_loss: 453.4833 - val_mae: 16.9750 - val_mse: 453.4543\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.6766 - mae: 18.8130 - mse: 555.6478 - val_loss: 444.4706 - val_mae: 16.1283 - val_mse: 444.4417\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 559.4786 - mae: 18.8973 - mse: 559.4500 - val_loss: 447.5214 - val_mae: 16.7026 - val_mse: 447.4927\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.4465 - mae: 19.0880 - mse: 569.4179 - val_loss: 457.4016 - val_mae: 17.2652 - val_mse: 457.3733\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.5757 - mae: 19.1285 - mse: 567.5477 - val_loss: 447.8289 - val_mae: 16.6455 - val_mse: 447.8008\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.4783 - mae: 18.7808 - mse: 555.4499 - val_loss: 443.9866 - val_mae: 16.5430 - val_mse: 443.9584\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.5856 - mae: 18.7910 - mse: 555.5577 - val_loss: 454.2576 - val_mae: 17.0280 - val_mse: 454.2296\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.7063 - mae: 18.8281 - mse: 563.6780 - val_loss: 441.6079 - val_mae: 16.3820 - val_mse: 441.5801\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 568.4222 - mae: 19.0096 - mse: 568.3945 - val_loss: 448.4235 - val_mae: 16.8210 - val_mse: 448.3957\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.7584 - mae: 18.7636 - mse: 557.7306 - val_loss: 459.5228 - val_mae: 17.3076 - val_mse: 459.4951\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.6462 - mae: 18.6563 - mse: 551.6185 - val_loss: 441.4968 - val_mae: 16.2092 - val_mse: 441.4691\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 558.0876 - mae: 18.8012 - mse: 558.0597 - val_loss: 448.4619 - val_mae: 16.5663 - val_mse: 448.4342\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.8842 - mae: 18.6760 - mse: 551.8568 - val_loss: 455.1687 - val_mae: 17.0464 - val_mse: 455.1412\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.5602 - mae: 18.5982 - mse: 544.5328 - val_loss: 451.1304 - val_mae: 17.0004 - val_mse: 451.1030\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 539.9414 - mae: 18.5146 - mse: 539.9139 - val_loss: 447.9300 - val_mae: 16.7539 - val_mse: 447.9026\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 539.1511 - mae: 18.5504 - mse: 539.1235 - val_loss: 454.3408 - val_mae: 16.8057 - val_mse: 454.3134\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.7576 - mae: 18.5255 - mse: 537.7303 - val_loss: 450.9364 - val_mae: 16.9210 - val_mse: 450.9089\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 540.1222 - mae: 18.4738 - mse: 540.0948 - val_loss: 449.8743 - val_mae: 16.8495 - val_mse: 449.8468\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 547.0176 - mae: 18.6083 - mse: 546.9903 - val_loss: 443.7357 - val_mae: 16.1128 - val_mse: 443.7083\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.6927 - mae: 18.5505 - mse: 541.6654 - val_loss: 449.5108 - val_mae: 16.8734 - val_mse: 449.4836\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.0793 - mae: 18.7892 - mse: 548.0521 - val_loss: 451.2350 - val_mae: 16.9946 - val_mse: 451.2080\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 534.1136 - mae: 18.3648 - mse: 534.0862 - val_loss: 447.0436 - val_mae: 16.8070 - val_mse: 447.0166\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 538.7796 - mae: 18.4450 - mse: 538.7526 - val_loss: 442.7869 - val_mae: 16.5145 - val_mse: 442.7600\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.8929 - mae: 18.5843 - mse: 537.8660 - val_loss: 446.4301 - val_mae: 16.7643 - val_mse: 446.4033\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 534.1005 - mae: 18.4162 - mse: 534.0739 - val_loss: 453.8603 - val_mae: 17.1420 - val_mse: 453.8335\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.1382 - mae: 18.6092 - mse: 544.1115 - val_loss: 458.8878 - val_mae: 17.4266 - val_mse: 458.8611\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.6786 - mae: 18.5276 - mse: 541.6519 - val_loss: 443.3396 - val_mae: 16.5242 - val_mse: 443.3129\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 533.8526 - mae: 18.3285 - mse: 533.8256 - val_loss: 446.4863 - val_mae: 16.5450 - val_mse: 446.4596\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.1078 - mae: 18.0338 - mse: 523.0812 - val_loss: 446.5092 - val_mae: 16.8586 - val_mse: 446.4824\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4077 - mae: 18.3821 - mse: 531.3808 - val_loss: 441.8688 - val_mae: 16.6643 - val_mse: 441.8419\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 520.9041 - mae: 18.1942 - mse: 520.8773 - val_loss: 450.2386 - val_mae: 16.9020 - val_mse: 450.2120\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 522.0189 - mae: 18.1962 - mse: 521.9919 - val_loss: 450.0806 - val_mae: 16.7623 - val_mse: 450.0539\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.8525 - mae: 18.1870 - mse: 523.8259 - val_loss: 460.1342 - val_mae: 17.3582 - val_mse: 460.1077\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4272 - mae: 18.3456 - mse: 531.4003 - val_loss: 446.3467 - val_mae: 16.7663 - val_mse: 446.3199\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 519.7271 - mae: 18.2588 - mse: 519.7008 - val_loss: 450.4432 - val_mae: 16.8916 - val_mse: 450.4163\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 522.2610 - mae: 18.1714 - mse: 522.2343 - val_loss: 448.2872 - val_mae: 16.8938 - val_mse: 448.2605\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 521.3271 - mae: 18.1554 - mse: 521.3004 - val_loss: 439.4465 - val_mae: 16.3366 - val_mse: 439.4197\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 515.1349 - mae: 18.0212 - mse: 515.1079 - val_loss: 441.5893 - val_mae: 16.3981 - val_mse: 441.5623\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 520.8240 - mae: 18.1105 - mse: 520.7972 - val_loss: 443.1610 - val_mae: 16.3491 - val_mse: 443.1340\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 516.2036 - mae: 17.9859 - mse: 516.1766 - val_loss: 457.4094 - val_mae: 17.3441 - val_mse: 457.3825\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 504.5481 - mae: 17.7379 - mse: 504.5211 - val_loss: 447.1848 - val_mae: 16.6371 - val_mse: 447.1581\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 518.5012 - mae: 18.1689 - mse: 518.4744 - val_loss: 440.6575 - val_mae: 16.5706 - val_mse: 440.6306\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 505.0982 - mae: 17.8506 - mse: 505.0714 - val_loss: 449.1805 - val_mae: 16.7496 - val_mse: 449.1534\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.4738 - mae: 17.9799 - mse: 512.4469 - val_loss: 472.5655 - val_mae: 17.8911 - val_mse: 472.5386\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.2599 - mae: 17.9938 - mse: 512.2328 - val_loss: 448.1171 - val_mae: 16.5961 - val_mse: 448.0900\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 511.8910 - mae: 17.8440 - mse: 511.8642 - val_loss: 451.9624 - val_mae: 16.9372 - val_mse: 451.9354\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 501.7968 - mae: 17.8233 - mse: 501.7699 - val_loss: 462.8581 - val_mae: 17.3499 - val_mse: 462.8312\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.7311 - mae: 17.9569 - mse: 512.7040 - val_loss: 443.2213 - val_mae: 16.4805 - val_mse: 443.1942\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 498.4146 - mae: 17.6197 - mse: 498.3873 - val_loss: 449.6291 - val_mae: 16.8109 - val_mse: 449.6020\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 497.5008 - mae: 17.6150 - mse: 497.4735 - val_loss: 446.4661 - val_mae: 16.3010 - val_mse: 446.4388\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 496.2195 - mae: 17.6483 - mse: 496.1921 - val_loss: 452.2564 - val_mae: 16.9125 - val_mse: 452.2292\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 6.8788, Train MSE: 90.7993\n",
      "Val   MAE: 20.9125, Val   MSE: 764.1398\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 7.3418, Train MSE: 106.9902\n",
      "Val   MAE: 18.1703, Val   MSE: 573.3924\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 14.8380, Train MSE: 369.1338\n",
      "Val   MAE: 16.9125, Val   MSE: 452.2292\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 163.6994 - mae: 9.1939 - mse: 163.6994 - val_loss: 146.5902 - val_mae: 8.7993 - val_mse: 146.5902\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.6751 - mae: 8.6075 - mse: 139.6751 - val_loss: 142.5868 - val_mae: 8.9208 - val_mse: 142.5868\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.7392 - mae: 8.5368 - mse: 137.7392 - val_loss: 142.4232 - val_mae: 8.6669 - val_mse: 142.4232\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.5552 - mae: 8.5116 - mse: 136.5552 - val_loss: 145.1798 - val_mae: 8.6242 - val_mse: 145.1798\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8448 - mae: 8.4530 - mse: 135.8448 - val_loss: 140.7722 - val_mae: 8.7388 - val_mse: 140.7722\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.6794 - mae: 8.4913 - mse: 135.6794 - val_loss: 140.7099 - val_mae: 8.6030 - val_mse: 140.7099\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4535 - mae: 8.4366 - mse: 134.4535 - val_loss: 141.2067 - val_mae: 8.5922 - val_mse: 141.2067\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.2167 - mae: 8.3747 - mse: 133.2167 - val_loss: 141.4359 - val_mae: 8.6665 - val_mse: 141.4359\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.8638 - mae: 8.3753 - mse: 132.8638 - val_loss: 140.9899 - val_mae: 8.6244 - val_mse: 140.9899\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.1011 - mae: 8.3602 - mse: 132.1011 - val_loss: 144.0760 - val_mae: 8.6456 - val_mse: 144.0760\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5261 - mae: 8.3266 - mse: 131.5261 - val_loss: 140.4108 - val_mae: 8.7875 - val_mse: 140.4108\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.1434 - mae: 8.2949 - mse: 130.1434 - val_loss: 140.5952 - val_mae: 8.6727 - val_mse: 140.5952\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.7185 - mae: 8.2535 - mse: 129.7185 - val_loss: 141.3340 - val_mae: 8.6334 - val_mse: 141.3340\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.5713 - mae: 8.2283 - mse: 128.5713 - val_loss: 141.0575 - val_mae: 8.8117 - val_mse: 141.0575\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 127.3479 - mae: 8.2172 - mse: 127.3479 - val_loss: 144.6642 - val_mae: 8.7645 - val_mse: 144.6642\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.9542 - mae: 8.1597 - mse: 125.9542 - val_loss: 143.3976 - val_mae: 8.9220 - val_mse: 143.3976\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.4509 - mae: 8.1502 - mse: 125.4509 - val_loss: 146.0223 - val_mae: 9.0921 - val_mse: 146.0223\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 123.5753 - mae: 8.0880 - mse: 123.5753 - val_loss: 146.0869 - val_mae: 9.0358 - val_mse: 146.0869\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 123.0955 - mae: 8.1051 - mse: 123.0955 - val_loss: 144.4891 - val_mae: 8.9256 - val_mse: 144.4891\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9696 - mae: 8.0197 - mse: 120.9696 - val_loss: 146.2754 - val_mae: 8.9358 - val_mse: 146.2754\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 120.4910 - mae: 7.9959 - mse: 120.4910 - val_loss: 145.4303 - val_mae: 8.9014 - val_mse: 145.4303\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 119.2536 - mae: 7.9589 - mse: 119.2536 - val_loss: 146.3809 - val_mae: 9.0443 - val_mse: 146.3809\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.6051 - mae: 7.9208 - mse: 117.6051 - val_loss: 149.8065 - val_mae: 9.2909 - val_mse: 149.8065\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 116.6282 - mae: 7.8963 - mse: 116.6282 - val_loss: 147.9337 - val_mae: 9.0718 - val_mse: 147.9337\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 115.8159 - mae: 7.8588 - mse: 115.8159 - val_loss: 149.2712 - val_mae: 9.1155 - val_mse: 149.2712\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 113.8677 - mae: 7.8258 - mse: 113.8677 - val_loss: 149.9238 - val_mae: 9.2684 - val_mse: 149.9238\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 113.4410 - mae: 7.8044 - mse: 113.4410 - val_loss: 151.8965 - val_mae: 9.0818 - val_mse: 151.8965\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.1809 - mae: 7.7742 - mse: 112.1809 - val_loss: 156.7161 - val_mae: 9.0797 - val_mse: 156.7161\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4911 - mae: 7.7190 - mse: 110.4911 - val_loss: 153.5957 - val_mae: 9.3343 - val_mse: 153.5957\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.6604 - mae: 7.6934 - mse: 109.6604 - val_loss: 153.5593 - val_mae: 9.3030 - val_mse: 153.5593\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.2698 - mae: 7.6386 - mse: 108.2698 - val_loss: 155.9758 - val_mae: 9.1564 - val_mse: 155.9758\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.2494 - mae: 7.6403 - mse: 108.2494 - val_loss: 156.8181 - val_mae: 9.4661 - val_mse: 156.8181\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 105.5371 - mae: 7.5436 - mse: 105.5371 - val_loss: 159.3262 - val_mae: 9.6046 - val_mse: 159.3262\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 106.2799 - mae: 7.5966 - mse: 106.2799 - val_loss: 155.6282 - val_mae: 9.5144 - val_mse: 155.6282\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.9642 - mae: 7.5168 - mse: 103.9642 - val_loss: 158.0479 - val_mae: 9.5430 - val_mse: 158.0479\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.5565 - mae: 7.4959 - mse: 103.5565 - val_loss: 159.0953 - val_mae: 9.4178 - val_mse: 159.0953\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 100.9547 - mae: 7.4069 - mse: 100.9547 - val_loss: 159.4695 - val_mae: 9.4688 - val_mse: 159.4695\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.0795 - mae: 7.3972 - mse: 101.0795 - val_loss: 161.3044 - val_mae: 9.6116 - val_mse: 161.3044\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.8931 - mae: 7.3784 - mse: 99.8931 - val_loss: 164.5630 - val_mae: 9.6412 - val_mse: 164.5630\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.1165 - mae: 7.3793 - mse: 99.1165 - val_loss: 161.6770 - val_mae: 9.5400 - val_mse: 161.6770\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 97.7009 - mae: 7.2810 - mse: 97.7009 - val_loss: 162.8642 - val_mae: 9.5352 - val_mse: 162.8642\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 96.4544 - mae: 7.2676 - mse: 96.4544 - val_loss: 165.9525 - val_mae: 9.6618 - val_mse: 165.9525\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.5412 - mae: 7.2361 - mse: 95.5412 - val_loss: 169.7922 - val_mae: 9.7286 - val_mse: 169.7922\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.2807 - mae: 7.2246 - mse: 95.2807 - val_loss: 164.2546 - val_mae: 9.4558 - val_mse: 164.2546\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.5301 - mae: 7.1680 - mse: 93.5301 - val_loss: 169.4503 - val_mae: 9.5071 - val_mse: 169.4503\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.5097 - mae: 7.1862 - mse: 93.5097 - val_loss: 171.2588 - val_mae: 9.6116 - val_mse: 171.2588\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 91.1018 - mae: 7.0943 - mse: 91.1018 - val_loss: 167.5082 - val_mae: 9.6585 - val_mse: 167.5082\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.1476 - mae: 7.0749 - mse: 91.1476 - val_loss: 171.4738 - val_mae: 9.7714 - val_mse: 171.4738\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.8585 - mae: 7.0533 - mse: 89.8585 - val_loss: 169.8099 - val_mae: 9.6920 - val_mse: 169.8099\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.7339 - mae: 7.0436 - mse: 89.7339 - val_loss: 181.8004 - val_mae: 9.8077 - val_mse: 181.8004\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 88.0498 - mae: 6.9543 - mse: 88.0498 - val_loss: 172.7514 - val_mae: 9.7914 - val_mse: 172.7514\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 87.6393 - mae: 6.9405 - mse: 87.6393 - val_loss: 171.0560 - val_mae: 9.7843 - val_mse: 171.0560\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.2995 - mae: 7.0573 - mse: 89.2995 - val_loss: 174.9364 - val_mae: 10.0138 - val_mse: 174.9364\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 85.6382 - mae: 6.8651 - mse: 85.6382 - val_loss: 173.1906 - val_mae: 9.8124 - val_mse: 173.1906\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 85.0885 - mae: 6.8737 - mse: 85.0885 - val_loss: 174.5753 - val_mae: 9.7240 - val_mse: 174.5753\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.6962 - mae: 6.8449 - mse: 84.6962 - val_loss: 177.0687 - val_mae: 9.8937 - val_mse: 177.0687\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.7378 - mae: 6.7886 - mse: 82.7378 - val_loss: 179.1032 - val_mae: 10.0657 - val_mse: 179.1032\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.7162 - mae: 6.7789 - mse: 82.7162 - val_loss: 178.2224 - val_mae: 9.9927 - val_mse: 178.2224\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.9106 - mae: 6.8213 - mse: 82.9106 - val_loss: 178.6161 - val_mae: 10.1090 - val_mse: 178.6161\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 81.0066 - mae: 6.7574 - mse: 81.0066 - val_loss: 183.7741 - val_mae: 9.9509 - val_mse: 183.7741\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 81.2601 - mae: 6.7511 - mse: 81.2601 - val_loss: 176.0390 - val_mae: 9.8469 - val_mse: 176.0390\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 79.2529 - mae: 6.6525 - mse: 79.2529 - val_loss: 179.6586 - val_mae: 9.9941 - val_mse: 179.6586\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 77.6975 - mae: 6.5982 - mse: 77.6975 - val_loss: 183.5479 - val_mae: 10.0085 - val_mse: 183.5479\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 78.1089 - mae: 6.6363 - mse: 78.1089 - val_loss: 189.2261 - val_mae: 9.9719 - val_mse: 189.2261\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 77.2047 - mae: 6.5808 - mse: 77.2047 - val_loss: 183.1960 - val_mae: 10.0293 - val_mse: 183.1960\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.8369 - mae: 6.5034 - mse: 75.8369 - val_loss: 181.6039 - val_mae: 10.0097 - val_mse: 181.6039\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.6088 - mae: 6.5025 - mse: 75.6088 - val_loss: 185.0665 - val_mae: 10.1556 - val_mse: 185.0665\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.8699 - mae: 6.5571 - mse: 75.8699 - val_loss: 188.2630 - val_mae: 10.2544 - val_mse: 188.2630\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.1594 - mae: 6.4087 - mse: 73.1594 - val_loss: 182.7262 - val_mae: 10.0353 - val_mse: 182.7262\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 72.7649 - mae: 6.4306 - mse: 72.7649 - val_loss: 185.5446 - val_mae: 10.1193 - val_mse: 185.5446\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.1563 - mae: 6.3276 - mse: 71.1563 - val_loss: 186.3555 - val_mae: 10.2473 - val_mse: 186.3555\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.2269 - mae: 6.3394 - mse: 71.2269 - val_loss: 189.5376 - val_mae: 10.2179 - val_mse: 189.5376\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 71.4683 - mae: 6.3284 - mse: 71.4683 - val_loss: 187.6027 - val_mae: 10.1683 - val_mse: 187.6027\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 70.5083 - mae: 6.3166 - mse: 70.5083 - val_loss: 187.3468 - val_mae: 10.1728 - val_mse: 187.3468\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 69.5165 - mae: 6.2678 - mse: 69.5165 - val_loss: 188.6333 - val_mae: 10.1744 - val_mse: 188.6333\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.6138 - mae: 6.1575 - mse: 67.6138 - val_loss: 193.6031 - val_mae: 10.3609 - val_mse: 193.6031\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 68.8259 - mae: 6.2901 - mse: 68.8259 - val_loss: 184.5905 - val_mae: 9.9600 - val_mse: 184.5905\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.0607 - mae: 6.1297 - mse: 67.0607 - val_loss: 198.4982 - val_mae: 10.6384 - val_mse: 198.4982\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.2416 - mae: 6.1509 - mse: 67.2416 - val_loss: 198.2409 - val_mae: 10.5752 - val_mse: 198.2409\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 65.9747 - mae: 6.1179 - mse: 65.9747 - val_loss: 197.1183 - val_mae: 10.4870 - val_mse: 197.1183\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.2949 - mae: 6.1631 - mse: 66.2949 - val_loss: 193.3045 - val_mae: 10.2911 - val_mse: 193.3045\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.0802 - mae: 6.1151 - mse: 65.0802 - val_loss: 192.4500 - val_mae: 10.1665 - val_mse: 192.4500\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.9513 - mae: 6.0429 - mse: 63.9513 - val_loss: 194.8931 - val_mae: 10.2247 - val_mse: 194.8931\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.4745 - mae: 6.0489 - mse: 64.4745 - val_loss: 198.8596 - val_mae: 10.3209 - val_mse: 198.8596\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.6069 - mae: 6.0232 - mse: 63.6069 - val_loss: 195.1287 - val_mae: 10.3807 - val_mse: 195.1287\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.8266 - mae: 5.8670 - mse: 60.8266 - val_loss: 206.7375 - val_mae: 10.7635 - val_mse: 206.7375\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 62.8106 - mae: 5.9873 - mse: 62.8106 - val_loss: 195.0881 - val_mae: 10.1797 - val_mse: 195.0881\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.9522 - mae: 5.9313 - mse: 60.9522 - val_loss: 201.1508 - val_mae: 10.4595 - val_mse: 201.1508\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.4340 - mae: 5.8591 - mse: 60.4340 - val_loss: 200.7374 - val_mae: 10.3991 - val_mse: 200.7374\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.3661 - mae: 5.8215 - mse: 59.3661 - val_loss: 206.0656 - val_mae: 10.6274 - val_mse: 206.0656\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.2402 - mae: 5.7472 - mse: 58.2402 - val_loss: 205.1778 - val_mae: 10.6182 - val_mse: 205.1778\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 59.6572 - mae: 5.8316 - mse: 59.6572 - val_loss: 201.8430 - val_mae: 10.4406 - val_mse: 201.8430\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.8766 - mae: 5.7897 - mse: 58.8766 - val_loss: 202.0848 - val_mae: 10.4834 - val_mse: 202.0848\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 56.9950 - mae: 5.7094 - mse: 56.9950 - val_loss: 207.6714 - val_mae: 10.7088 - val_mse: 207.6714\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 57.7192 - mae: 5.7404 - mse: 57.7192 - val_loss: 221.2529 - val_mae: 11.1979 - val_mse: 221.2529\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.5192 - mae: 5.7947 - mse: 58.5192 - val_loss: 203.7799 - val_mae: 10.5138 - val_mse: 203.7799\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 56.5748 - mae: 5.6652 - mse: 56.5748 - val_loss: 202.0991 - val_mae: 10.5297 - val_mse: 202.0991\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.3109 - mae: 5.5820 - mse: 54.3109 - val_loss: 209.4045 - val_mae: 10.6295 - val_mse: 209.4045\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.2451 - mae: 5.5601 - mse: 54.2451 - val_loss: 209.4900 - val_mae: 10.6832 - val_mse: 209.4900\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.0082 - mae: 5.5728 - mse: 54.0082 - val_loss: 206.0707 - val_mae: 10.4803 - val_mse: 206.0707\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 247.5984 - mae: 11.4104 - mse: 247.5984 - val_loss: 235.4777 - val_mae: 11.2651 - val_mse: 235.4777\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 165.1202 - mae: 9.0374 - mse: 165.1202 - val_loss: 159.6051 - val_mae: 8.9333 - val_mse: 159.6051\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3735 - mae: 8.3451 - mse: 136.3735 - val_loss: 142.8192 - val_mae: 8.6185 - val_mse: 142.8192\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5479 - mae: 8.3087 - mse: 131.5479 - val_loss: 143.1145 - val_mae: 8.7279 - val_mse: 143.1145\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.2044 - mae: 8.3153 - mse: 130.2044 - val_loss: 142.1322 - val_mae: 8.7200 - val_mse: 142.1322\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7501 - mae: 8.2483 - mse: 128.7501 - val_loss: 142.3866 - val_mae: 8.8997 - val_mse: 142.3866\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4380 - mae: 8.2627 - mse: 128.4380 - val_loss: 141.1958 - val_mae: 8.7414 - val_mse: 141.1958\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4285 - mae: 8.2114 - mse: 127.4285 - val_loss: 141.9936 - val_mae: 8.8748 - val_mse: 141.9936\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 126.1214 - mae: 8.1568 - mse: 126.1214 - val_loss: 143.2634 - val_mae: 8.7664 - val_mse: 143.2634\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.8694 - mae: 8.1672 - mse: 125.8694 - val_loss: 144.8573 - val_mae: 8.7578 - val_mse: 144.8573\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.6261 - mae: 8.1627 - mse: 124.6261 - val_loss: 142.3284 - val_mae: 8.7864 - val_mse: 142.3284\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1815 - mae: 8.1456 - mse: 124.1815 - val_loss: 145.6023 - val_mae: 8.9521 - val_mse: 145.6023\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.8567 - mae: 8.1262 - mse: 123.8567 - val_loss: 147.0350 - val_mae: 9.0388 - val_mse: 147.0350\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.7364 - mae: 8.1238 - mse: 122.7364 - val_loss: 145.3135 - val_mae: 8.9169 - val_mse: 145.3135\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.3613 - mae: 8.0858 - mse: 122.3613 - val_loss: 145.0243 - val_mae: 8.9683 - val_mse: 145.0243\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9691 - mae: 8.0650 - mse: 120.9691 - val_loss: 145.9814 - val_mae: 8.9570 - val_mse: 145.9814\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.3455 - mae: 8.0598 - mse: 120.3455 - val_loss: 147.5744 - val_mae: 9.0449 - val_mse: 147.5744\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.7127 - mae: 7.9850 - mse: 118.7127 - val_loss: 147.8299 - val_mae: 9.1306 - val_mse: 147.8299\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.2331 - mae: 8.0116 - mse: 119.2331 - val_loss: 147.6138 - val_mae: 9.0588 - val_mse: 147.6138\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.5877 - mae: 8.0290 - mse: 118.5877 - val_loss: 150.8034 - val_mae: 9.0548 - val_mse: 150.8034\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.7278 - mae: 7.9341 - mse: 116.7278 - val_loss: 150.5605 - val_mae: 9.1111 - val_mse: 150.5605\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.6613 - mae: 8.0021 - mse: 117.6613 - val_loss: 149.2334 - val_mae: 9.0985 - val_mse: 149.2334\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.3404 - mae: 7.8846 - mse: 115.3404 - val_loss: 146.9989 - val_mae: 9.0333 - val_mse: 146.9989\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.3909 - mae: 7.9375 - mse: 116.3909 - val_loss: 151.4994 - val_mae: 9.3081 - val_mse: 151.4994\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.8293 - mae: 7.8900 - mse: 113.8293 - val_loss: 155.4615 - val_mae: 9.2647 - val_mse: 155.4615\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.3351 - mae: 7.8563 - mse: 114.3351 - val_loss: 152.1231 - val_mae: 9.2545 - val_mse: 152.1231\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.2566 - mae: 7.8584 - mse: 114.2566 - val_loss: 152.9240 - val_mae: 9.0529 - val_mse: 152.9240\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.5988 - mae: 7.7792 - mse: 112.5988 - val_loss: 153.8217 - val_mae: 9.2339 - val_mse: 153.8217\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.6743 - mae: 7.8058 - mse: 111.6743 - val_loss: 148.7007 - val_mae: 9.1642 - val_mse: 148.7007\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.7598 - mae: 7.7445 - mse: 110.7598 - val_loss: 153.4382 - val_mae: 9.1908 - val_mse: 153.4382\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.5570 - mae: 7.8161 - mse: 111.5570 - val_loss: 157.3539 - val_mae: 9.2684 - val_mse: 157.3539\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4225 - mae: 7.7974 - mse: 110.4225 - val_loss: 157.1826 - val_mae: 9.2503 - val_mse: 157.1826\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.3080 - mae: 7.7633 - mse: 109.3080 - val_loss: 155.3094 - val_mae: 9.2153 - val_mse: 155.3094\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8155 - mae: 7.7179 - mse: 108.8155 - val_loss: 156.8951 - val_mae: 9.3047 - val_mse: 156.8951\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9454 - mae: 7.6932 - mse: 107.9454 - val_loss: 156.2481 - val_mae: 9.3160 - val_mse: 156.2481\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.6165 - mae: 7.6507 - mse: 107.6165 - val_loss: 154.3568 - val_mae: 9.2302 - val_mse: 154.3568\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8985 - mae: 7.6659 - mse: 108.8985 - val_loss: 157.2781 - val_mae: 9.3106 - val_mse: 157.2781\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.1801 - mae: 7.5935 - mse: 105.1801 - val_loss: 157.9465 - val_mae: 9.2510 - val_mse: 157.9465\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.4131 - mae: 7.6273 - mse: 106.4131 - val_loss: 160.1235 - val_mae: 9.4201 - val_mse: 160.1235\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.9819 - mae: 7.5492 - mse: 104.9819 - val_loss: 160.2046 - val_mae: 9.5112 - val_mse: 160.2046\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.3627 - mae: 7.6145 - mse: 104.3627 - val_loss: 158.2147 - val_mae: 9.3068 - val_mse: 158.2147\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.8930 - mae: 7.6262 - mse: 105.8930 - val_loss: 161.2203 - val_mae: 9.4742 - val_mse: 161.2203\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.7386 - mae: 7.5535 - mse: 104.7386 - val_loss: 160.1093 - val_mae: 9.3491 - val_mse: 160.1093\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.3412 - mae: 7.5487 - mse: 103.3412 - val_loss: 159.5777 - val_mae: 9.2731 - val_mse: 159.5777\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.7248 - mae: 7.5077 - mse: 102.7248 - val_loss: 160.8370 - val_mae: 9.5076 - val_mse: 160.8370\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6129 - mae: 7.4856 - mse: 101.6129 - val_loss: 164.6996 - val_mae: 9.4638 - val_mse: 164.6996\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.0041 - mae: 7.4990 - mse: 102.0041 - val_loss: 164.6512 - val_mae: 9.6210 - val_mse: 164.6512\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6507 - mae: 7.5820 - mse: 101.6507 - val_loss: 161.3277 - val_mae: 9.4896 - val_mse: 161.3277\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6515 - mae: 7.4872 - mse: 101.6515 - val_loss: 158.6392 - val_mae: 9.3518 - val_mse: 158.6392\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.7848 - mae: 7.3915 - mse: 98.7848 - val_loss: 164.1393 - val_mae: 9.4980 - val_mse: 164.1393\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.8782 - mae: 7.3414 - mse: 98.8782 - val_loss: 161.2203 - val_mae: 9.4840 - val_mse: 161.2203\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.0479 - mae: 7.3956 - mse: 99.0479 - val_loss: 166.1731 - val_mae: 9.6877 - val_mse: 166.1731\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.8396 - mae: 7.4661 - mse: 100.8396 - val_loss: 164.8414 - val_mae: 9.4956 - val_mse: 164.8414\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.5965 - mae: 7.3776 - mse: 97.5965 - val_loss: 165.0360 - val_mae: 9.5769 - val_mse: 165.0360\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.2586 - mae: 7.2912 - mse: 96.2586 - val_loss: 168.0140 - val_mae: 9.6748 - val_mse: 168.0140\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.5316 - mae: 7.4081 - mse: 98.5316 - val_loss: 165.2034 - val_mae: 9.4899 - val_mse: 165.2034\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.9094 - mae: 7.3947 - mse: 97.9094 - val_loss: 162.8026 - val_mae: 9.4295 - val_mse: 162.8026\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.0997 - mae: 7.2895 - mse: 96.0997 - val_loss: 163.1887 - val_mae: 9.4548 - val_mse: 163.1887\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.1257 - mae: 7.2628 - mse: 95.1257 - val_loss: 165.0116 - val_mae: 9.6234 - val_mse: 165.0116\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.2509 - mae: 7.4064 - mse: 97.2509 - val_loss: 163.5095 - val_mae: 9.4748 - val_mse: 163.5095\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.6674 - mae: 7.2598 - mse: 94.6674 - val_loss: 168.1011 - val_mae: 9.6351 - val_mse: 168.1011\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.7941 - mae: 7.2687 - mse: 95.7941 - val_loss: 170.5729 - val_mae: 9.6862 - val_mse: 170.5729\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.6397 - mae: 7.2156 - mse: 93.6397 - val_loss: 169.7390 - val_mae: 9.6785 - val_mse: 169.7390\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.8529 - mae: 7.2515 - mse: 93.8529 - val_loss: 167.8690 - val_mae: 9.4817 - val_mse: 167.8690\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.6009 - mae: 7.1706 - mse: 92.6009 - val_loss: 169.4846 - val_mae: 9.6368 - val_mse: 169.4846\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.0449 - mae: 7.1864 - mse: 92.0449 - val_loss: 171.1148 - val_mae: 9.6764 - val_mse: 171.1148\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.4900 - mae: 7.2203 - mse: 93.4900 - val_loss: 171.6473 - val_mae: 9.5700 - val_mse: 171.6473\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.9964 - mae: 7.2173 - mse: 92.9964 - val_loss: 174.7949 - val_mae: 9.7926 - val_mse: 174.7949\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.4757 - mae: 7.1443 - mse: 91.4757 - val_loss: 172.0092 - val_mae: 9.7661 - val_mse: 172.0092\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 92.4805 - mae: 7.1835 - mse: 92.4805 - val_loss: 171.4970 - val_mae: 9.7283 - val_mse: 171.4970\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.9706 - mae: 7.1329 - mse: 90.9706 - val_loss: 173.3345 - val_mae: 9.8543 - val_mse: 173.3345\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.7169 - mae: 7.1805 - mse: 91.7169 - val_loss: 166.9821 - val_mae: 9.5171 - val_mse: 166.9821\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.0778 - mae: 7.0974 - mse: 90.0778 - val_loss: 174.3049 - val_mae: 9.7913 - val_mse: 174.3049\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.7824 - mae: 7.1527 - mse: 91.7824 - val_loss: 169.8535 - val_mae: 9.6779 - val_mse: 169.8535\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.1445 - mae: 7.1059 - mse: 90.1445 - val_loss: 174.8537 - val_mae: 9.8768 - val_mse: 174.8537\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.5445 - mae: 6.9904 - mse: 88.5445 - val_loss: 174.6725 - val_mae: 9.7522 - val_mse: 174.6725\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.5072 - mae: 7.0028 - mse: 87.5072 - val_loss: 173.9221 - val_mae: 9.8444 - val_mse: 173.9221\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.3801 - mae: 7.0531 - mse: 90.3801 - val_loss: 174.2487 - val_mae: 9.8130 - val_mse: 174.2487\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.0257 - mae: 7.0663 - mse: 89.0257 - val_loss: 174.3065 - val_mae: 9.8603 - val_mse: 174.3065\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.0637 - mae: 7.1105 - mse: 89.0637 - val_loss: 176.1683 - val_mae: 9.9135 - val_mse: 176.1683\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8796 - mae: 7.0569 - mse: 88.8796 - val_loss: 175.8932 - val_mae: 9.7858 - val_mse: 175.8932\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.7311 - mae: 6.9827 - mse: 87.7311 - val_loss: 173.3930 - val_mae: 9.8080 - val_mse: 173.3930\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.0489 - mae: 6.9569 - mse: 87.0489 - val_loss: 176.5896 - val_mae: 9.8747 - val_mse: 176.5896\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.0093 - mae: 6.9911 - mse: 86.0093 - val_loss: 179.6147 - val_mae: 9.9981 - val_mse: 179.6147\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.7597 - mae: 7.0719 - mse: 89.7597 - val_loss: 177.2987 - val_mae: 9.8647 - val_mse: 177.2987\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.7876 - mae: 7.0468 - mse: 88.7876 - val_loss: 171.1225 - val_mae: 9.7284 - val_mse: 171.1225\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8485 - mae: 7.0940 - mse: 88.8485 - val_loss: 173.4394 - val_mae: 9.7386 - val_mse: 173.4394\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.5924 - mae: 6.9602 - mse: 86.5924 - val_loss: 179.5074 - val_mae: 9.9893 - val_mse: 179.5074\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.4019 - mae: 7.0100 - mse: 86.4019 - val_loss: 178.5220 - val_mae: 9.8964 - val_mse: 178.5220\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.1849 - mae: 6.9364 - mse: 86.1849 - val_loss: 179.9277 - val_mae: 10.0068 - val_mse: 179.9277\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.7952 - mae: 6.9094 - mse: 84.7952 - val_loss: 178.5181 - val_mae: 9.9323 - val_mse: 178.5181\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.7605 - mae: 6.8852 - mse: 83.7605 - val_loss: 176.7695 - val_mae: 9.8616 - val_mse: 176.7695\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.2041 - mae: 6.8537 - mse: 83.2041 - val_loss: 178.1810 - val_mae: 9.9514 - val_mse: 178.1810\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.1400 - mae: 6.8484 - mse: 84.1400 - val_loss: 178.0415 - val_mae: 9.8664 - val_mse: 178.0415\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 85.2397 - mae: 6.9042 - mse: 85.2397 - val_loss: 179.7577 - val_mae: 9.8960 - val_mse: 179.7577\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.3273 - mae: 6.8553 - mse: 83.3273 - val_loss: 175.7792 - val_mae: 9.8158 - val_mse: 175.7792\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.6359 - mae: 6.9136 - mse: 84.6359 - val_loss: 183.0036 - val_mae: 9.9984 - val_mse: 183.0036\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.2593 - mae: 6.9139 - mse: 86.2593 - val_loss: 177.9559 - val_mae: 9.8041 - val_mse: 177.9559\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 81.5784 - mae: 6.7807 - mse: 81.5784 - val_loss: 180.8874 - val_mae: 9.9697 - val_mse: 180.8874\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 82.7775 - mae: 6.8267 - mse: 82.7775 - val_loss: 181.1145 - val_mae: 9.9698 - val_mse: 181.1145\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 184.8586 - mae: 9.7674 - mse: 184.8427 - val_loss: 145.5124 - val_mae: 8.6903 - val_mse: 145.4963\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 153.4001 - mae: 8.9627 - mse: 153.3840 - val_loss: 148.2971 - val_mae: 8.6866 - val_mse: 148.2811\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 149.6286 - mae: 8.8920 - mse: 149.6125 - val_loss: 143.3225 - val_mae: 8.6073 - val_mse: 143.3064\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.4624 - mae: 8.8655 - mse: 147.4463 - val_loss: 143.7707 - val_mae: 8.5932 - val_mse: 143.7546\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.1608 - mae: 8.8135 - mse: 147.1447 - val_loss: 142.5448 - val_mae: 8.5872 - val_mse: 142.5287\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 144.5305 - mae: 8.7481 - mse: 144.5144 - val_loss: 145.4015 - val_mae: 8.5968 - val_mse: 145.3856\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 144.1734 - mae: 8.7255 - mse: 144.1575 - val_loss: 143.0185 - val_mae: 8.5857 - val_mse: 143.0025\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5416 - mae: 8.6301 - mse: 142.5257 - val_loss: 139.6987 - val_mae: 8.5743 - val_mse: 139.6826\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5748 - mae: 8.7054 - mse: 142.5588 - val_loss: 146.4006 - val_mae: 8.6097 - val_mse: 146.3847\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.8417 - mae: 8.6044 - mse: 140.8256 - val_loss: 141.9977 - val_mae: 8.5368 - val_mse: 141.9817\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 143.1664 - mae: 8.6803 - mse: 143.1504 - val_loss: 140.1859 - val_mae: 8.5790 - val_mse: 140.1699\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.9545 - mae: 8.6191 - mse: 140.9386 - val_loss: 141.3959 - val_mae: 8.5268 - val_mse: 141.3799\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.6898 - mae: 8.6635 - mse: 141.6738 - val_loss: 141.3783 - val_mae: 8.5510 - val_mse: 141.3624\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.7580 - mae: 8.5832 - mse: 139.7421 - val_loss: 141.2381 - val_mae: 8.5587 - val_mse: 141.2222\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.4827 - mae: 8.5738 - mse: 138.4668 - val_loss: 140.5972 - val_mae: 8.5632 - val_mse: 140.5813\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.9393 - mae: 8.5529 - mse: 139.9235 - val_loss: 140.5279 - val_mae: 8.5556 - val_mse: 140.5120\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 138.3905 - mae: 8.5504 - mse: 138.3747 - val_loss: 139.7779 - val_mae: 8.5904 - val_mse: 139.7621\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.4282 - mae: 8.6223 - mse: 140.4125 - val_loss: 138.8164 - val_mae: 8.6549 - val_mse: 138.8006\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.0870 - mae: 8.5850 - mse: 140.0712 - val_loss: 140.1086 - val_mae: 8.5473 - val_mse: 140.0928\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 138.1979 - mae: 8.5443 - mse: 138.1822 - val_loss: 140.3788 - val_mae: 8.5598 - val_mse: 140.3630\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.0611 - mae: 8.5868 - mse: 140.0453 - val_loss: 143.6061 - val_mae: 8.5681 - val_mse: 143.5903\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.0822 - mae: 8.5815 - mse: 140.0664 - val_loss: 143.9062 - val_mae: 8.5647 - val_mse: 143.8905\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.5349 - mae: 8.5472 - mse: 139.5191 - val_loss: 140.5637 - val_mae: 8.5544 - val_mse: 140.5478\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.4373 - mae: 8.5373 - mse: 139.4215 - val_loss: 139.4457 - val_mae: 8.5766 - val_mse: 139.4298\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7419 - mae: 8.5551 - mse: 138.7260 - val_loss: 141.9120 - val_mae: 8.5712 - val_mse: 141.8962\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1335 - mae: 8.5233 - mse: 137.1176 - val_loss: 141.5430 - val_mae: 8.5574 - val_mse: 141.5271\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.5289 - mae: 8.4477 - mse: 137.5130 - val_loss: 140.3367 - val_mae: 8.5674 - val_mse: 140.3207\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.0491 - mae: 8.5457 - mse: 138.0331 - val_loss: 140.9082 - val_mae: 8.5650 - val_mse: 140.8922\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.4627 - mae: 8.5007 - mse: 137.4467 - val_loss: 141.5446 - val_mae: 8.5669 - val_mse: 141.5286\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.4302 - mae: 8.5051 - mse: 137.4141 - val_loss: 140.6466 - val_mae: 8.5760 - val_mse: 140.6305\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1166 - mae: 8.5243 - mse: 138.1004 - val_loss: 139.1218 - val_mae: 8.5931 - val_mse: 139.1056\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.9010 - mae: 8.5321 - mse: 136.8847 - val_loss: 143.0862 - val_mae: 8.5584 - val_mse: 143.0700\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.3363 - mae: 8.4590 - mse: 135.3200 - val_loss: 139.3717 - val_mae: 8.5842 - val_mse: 139.3553\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.9167 - mae: 8.4762 - mse: 134.9003 - val_loss: 140.3321 - val_mae: 8.5534 - val_mse: 140.3156\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.0851 - mae: 8.4693 - mse: 136.0687 - val_loss: 140.4544 - val_mae: 8.5733 - val_mse: 140.4378\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.7149 - mae: 8.4490 - mse: 134.6983 - val_loss: 142.1852 - val_mae: 8.5696 - val_mse: 142.1684\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6889 - mae: 8.5095 - mse: 137.6721 - val_loss: 141.9231 - val_mae: 8.5858 - val_mse: 141.9063\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8188 - mae: 8.4628 - mse: 135.8020 - val_loss: 140.3885 - val_mae: 8.6142 - val_mse: 140.3715\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3018 - mae: 8.5004 - mse: 136.2848 - val_loss: 143.8772 - val_mae: 8.5778 - val_mse: 143.8603\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.9806 - mae: 8.4632 - mse: 134.9635 - val_loss: 142.1642 - val_mae: 8.5513 - val_mse: 142.1470\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.5133 - mae: 8.5273 - mse: 136.4962 - val_loss: 143.0370 - val_mae: 8.5762 - val_mse: 143.0198\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.2795 - mae: 8.4572 - mse: 136.2622 - val_loss: 142.7816 - val_mae: 8.5681 - val_mse: 142.7643\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4536 - mae: 8.4633 - mse: 134.4363 - val_loss: 141.4570 - val_mae: 8.5972 - val_mse: 141.4395\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.5393 - mae: 8.4648 - mse: 134.5217 - val_loss: 142.1177 - val_mae: 8.5778 - val_mse: 142.1000\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.8352 - mae: 8.4346 - mse: 134.8175 - val_loss: 144.7634 - val_mae: 8.5806 - val_mse: 144.7457\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.0972 - mae: 8.4387 - mse: 135.0795 - val_loss: 141.7296 - val_mae: 8.6041 - val_mse: 141.7117\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1247 - mae: 8.4693 - mse: 135.1067 - val_loss: 142.9007 - val_mae: 8.5939 - val_mse: 142.8828\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.8335 - mae: 8.4374 - mse: 134.8156 - val_loss: 143.1390 - val_mae: 8.6089 - val_mse: 143.1210\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9144 - mae: 8.4330 - mse: 133.8963 - val_loss: 142.1110 - val_mae: 8.5975 - val_mse: 142.0928\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6674 - mae: 8.4038 - mse: 133.6492 - val_loss: 142.8239 - val_mae: 8.5960 - val_mse: 142.8056\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.3118 - mae: 8.4426 - mse: 134.2935 - val_loss: 141.8967 - val_mae: 8.6124 - val_mse: 141.8783\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.3753 - mae: 8.4417 - mse: 134.3569 - val_loss: 142.4393 - val_mae: 8.6394 - val_mse: 142.4207\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.0813 - mae: 8.4538 - mse: 134.0627 - val_loss: 143.7713 - val_mae: 8.6110 - val_mse: 143.7526\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9356 - mae: 8.4495 - mse: 133.9167 - val_loss: 142.1321 - val_mae: 8.6142 - val_mse: 142.1132\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.6818 - mae: 8.4303 - mse: 132.6628 - val_loss: 144.9672 - val_mae: 8.6126 - val_mse: 144.9481\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.7416 - mae: 8.4263 - mse: 133.7224 - val_loss: 142.4402 - val_mae: 8.6031 - val_mse: 142.4209\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.2498 - mae: 8.4582 - mse: 133.2305 - val_loss: 144.7819 - val_mae: 8.6228 - val_mse: 144.7626\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5761 - mae: 8.3889 - mse: 131.5567 - val_loss: 144.6856 - val_mae: 8.6293 - val_mse: 144.6661\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.4501 - mae: 8.3790 - mse: 133.4306 - val_loss: 141.4031 - val_mae: 8.6410 - val_mse: 141.3834\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6531 - mae: 8.4406 - mse: 133.6333 - val_loss: 144.0704 - val_mae: 8.5892 - val_mse: 144.0506\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.7709 - mae: 8.3430 - mse: 131.7510 - val_loss: 142.8871 - val_mae: 8.6277 - val_mse: 142.8671\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.1493 - mae: 8.4060 - mse: 133.1293 - val_loss: 142.9602 - val_mae: 8.6024 - val_mse: 142.9401\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.4891 - mae: 8.4247 - mse: 133.4689 - val_loss: 142.7030 - val_mae: 8.6125 - val_mse: 142.6826\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3583 - mae: 8.4084 - mse: 132.3378 - val_loss: 144.0322 - val_mae: 8.5958 - val_mse: 144.0116\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.7677 - mae: 8.3890 - mse: 132.7470 - val_loss: 143.2183 - val_mae: 8.5949 - val_mse: 143.1975\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.1645 - mae: 8.4171 - mse: 133.1435 - val_loss: 145.0832 - val_mae: 8.6272 - val_mse: 145.0622\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.0820 - mae: 8.3892 - mse: 132.0608 - val_loss: 143.8825 - val_mae: 8.6275 - val_mse: 143.8611\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.8586 - mae: 8.3722 - mse: 130.8372 - val_loss: 146.1688 - val_mae: 8.6307 - val_mse: 146.1473\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5276 - mae: 8.3918 - mse: 131.5060 - val_loss: 143.8951 - val_mae: 8.6199 - val_mse: 143.8733\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7664 - mae: 8.4156 - mse: 131.7446 - val_loss: 144.6709 - val_mae: 8.6203 - val_mse: 144.6489\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3107 - mae: 8.3857 - mse: 132.2886 - val_loss: 143.2753 - val_mae: 8.6381 - val_mse: 143.2531\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5637 - mae: 8.3922 - mse: 132.5414 - val_loss: 144.3784 - val_mae: 8.6233 - val_mse: 144.3559\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.2849 - mae: 8.4105 - mse: 132.2625 - val_loss: 143.2732 - val_mae: 8.6347 - val_mse: 143.2505\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.6207 - mae: 8.3502 - mse: 130.5979 - val_loss: 143.1509 - val_mae: 8.6387 - val_mse: 143.1281\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.3034 - mae: 8.3989 - mse: 131.2804 - val_loss: 144.2000 - val_mae: 8.6411 - val_mse: 144.1769\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3348 - mae: 8.3830 - mse: 132.3115 - val_loss: 142.7035 - val_mae: 8.6637 - val_mse: 142.6801\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3513 - mae: 8.4256 - mse: 133.3279 - val_loss: 143.6877 - val_mae: 8.6457 - val_mse: 143.6642\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.5668 - mae: 8.3658 - mse: 130.5432 - val_loss: 143.0252 - val_mae: 8.6459 - val_mse: 143.0013\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.2181 - mae: 8.3628 - mse: 130.1940 - val_loss: 144.8686 - val_mae: 8.6624 - val_mse: 144.8445\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.9491 - mae: 8.3763 - mse: 131.9249 - val_loss: 142.8505 - val_mae: 8.6231 - val_mse: 142.8262\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.7681 - mae: 8.3432 - mse: 130.7437 - val_loss: 144.0482 - val_mae: 8.6330 - val_mse: 144.0236\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.6645 - mae: 8.3538 - mse: 129.6398 - val_loss: 144.7215 - val_mae: 8.6566 - val_mse: 144.6967\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.1600 - mae: 8.3730 - mse: 131.1351 - val_loss: 143.9108 - val_mae: 8.6805 - val_mse: 143.8858\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.8605 - mae: 8.3342 - mse: 128.8352 - val_loss: 145.6736 - val_mae: 8.6629 - val_mse: 145.6482\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8574 - mae: 8.3678 - mse: 129.8318 - val_loss: 147.4540 - val_mae: 8.6614 - val_mse: 147.4283\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.7361 - mae: 8.3289 - mse: 130.7103 - val_loss: 144.7823 - val_mae: 8.6700 - val_mse: 144.7564\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.2824 - mae: 8.3851 - mse: 130.2563 - val_loss: 145.1832 - val_mae: 8.6507 - val_mse: 145.1570\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.0860 - mae: 8.2806 - mse: 129.0596 - val_loss: 145.4065 - val_mae: 8.6653 - val_mse: 145.3800\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.5686 - mae: 8.3654 - mse: 129.5418 - val_loss: 145.3940 - val_mae: 8.6410 - val_mse: 145.3671\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.3251 - mae: 8.3463 - mse: 131.2982 - val_loss: 143.7665 - val_mae: 8.6289 - val_mse: 143.7395\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.1049 - mae: 8.3063 - mse: 129.0776 - val_loss: 144.3714 - val_mae: 8.6567 - val_mse: 144.3439\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.2364 - mae: 8.3168 - mse: 129.2087 - val_loss: 146.0615 - val_mae: 8.6389 - val_mse: 146.0337\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.6616 - mae: 8.3683 - mse: 131.6338 - val_loss: 146.0207 - val_mae: 8.6405 - val_mse: 145.9928\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.1987 - mae: 8.2956 - mse: 128.1705 - val_loss: 145.8173 - val_mae: 8.6558 - val_mse: 145.7889\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.0585 - mae: 8.3186 - mse: 129.0300 - val_loss: 145.8145 - val_mae: 8.6639 - val_mse: 145.7858\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.2785 - mae: 8.3463 - mse: 129.2497 - val_loss: 147.4288 - val_mae: 8.7048 - val_mse: 147.3998\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9611 - mae: 8.3355 - mse: 129.9319 - val_loss: 147.0152 - val_mae: 8.6904 - val_mse: 146.9859\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2383 - mae: 8.2814 - mse: 128.2089 - val_loss: 146.3531 - val_mae: 8.6874 - val_mse: 146.3235\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.6755 - mae: 8.3037 - mse: 129.6457 - val_loss: 146.1844 - val_mae: 8.7301 - val_mse: 146.1544\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.8901 - mae: 8.3443 - mse: 127.8599 - val_loss: 146.7063 - val_mae: 8.6852 - val_mse: 146.6760\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 5.3295, Train MSE: 50.0978\n",
      "Val   MAE: 10.4803, Val   MSE: 206.0707\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8031, Train MSE: 58.0731\n",
      "Val   MAE: 9.9698, Val   MSE: 181.1145\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.9646, Train MSE: 124.6105\n",
      "Val   MAE: 8.6852, Val   MSE: 146.6760\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 161.4726 - mae: 9.1934 - mse: 161.4726 - val_loss: 140.9807 - val_mae: 8.5906 - val_mse: 140.9807\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0031 - mae: 8.2058 - mse: 131.0031 - val_loss: 137.8750 - val_mae: 8.5968 - val_mse: 137.8750\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2051 - mae: 8.0609 - mse: 128.2051 - val_loss: 138.8955 - val_mae: 8.6845 - val_mse: 138.8955\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.1012 - mae: 8.0178 - mse: 126.1012 - val_loss: 138.6035 - val_mae: 8.6133 - val_mse: 138.6035\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.9222 - mae: 7.9838 - mse: 124.9222 - val_loss: 139.7602 - val_mae: 8.5596 - val_mse: 139.7602\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.5303 - mae: 7.9078 - mse: 123.5303 - val_loss: 138.7037 - val_mae: 8.6376 - val_mse: 138.7037\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.0633 - mae: 7.8472 - mse: 122.0633 - val_loss: 139.1320 - val_mae: 8.6394 - val_mse: 139.1320\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 121.2353 - mae: 7.8144 - mse: 121.2353 - val_loss: 140.5667 - val_mae: 8.7631 - val_mse: 140.5667\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 119.4185 - mae: 7.7670 - mse: 119.4185 - val_loss: 141.9588 - val_mae: 8.6418 - val_mse: 141.9588\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.9765 - mae: 7.7086 - mse: 117.9765 - val_loss: 140.7823 - val_mae: 8.5916 - val_mse: 140.7823\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.0236 - mae: 7.6430 - mse: 117.0236 - val_loss: 139.5932 - val_mae: 8.7460 - val_mse: 139.5932\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.6427 - mae: 7.5900 - mse: 114.6427 - val_loss: 141.4575 - val_mae: 8.7016 - val_mse: 141.4575\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6479 - mae: 7.5448 - mse: 113.6479 - val_loss: 141.2963 - val_mae: 8.8562 - val_mse: 141.2963\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4678 - mae: 7.4443 - mse: 110.4678 - val_loss: 143.5450 - val_mae: 8.9974 - val_mse: 143.5450\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7563 - mae: 7.3614 - mse: 108.7563 - val_loss: 142.8491 - val_mae: 8.8511 - val_mse: 142.8491\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 106.6163 - mae: 7.3035 - mse: 106.6163 - val_loss: 143.1089 - val_mae: 8.8603 - val_mse: 143.1089\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.9976 - mae: 7.1860 - mse: 103.9976 - val_loss: 146.6412 - val_mae: 8.9852 - val_mse: 146.6412\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 101.5579 - mae: 7.1065 - mse: 101.5579 - val_loss: 147.5126 - val_mae: 8.8336 - val_mse: 147.5126\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.6691 - mae: 7.0436 - mse: 99.6691 - val_loss: 151.5329 - val_mae: 9.1810 - val_mse: 151.5329\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.4079 - mae: 6.9123 - mse: 95.4079 - val_loss: 151.9431 - val_mae: 9.3025 - val_mse: 151.9431\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.9827 - mae: 6.8395 - mse: 93.9827 - val_loss: 152.3937 - val_mae: 9.2086 - val_mse: 152.3937\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.2824 - mae: 6.7228 - mse: 90.2824 - val_loss: 159.0290 - val_mae: 9.2628 - val_mse: 159.0290\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.0609 - mae: 6.6845 - mse: 89.0609 - val_loss: 157.5204 - val_mae: 9.3202 - val_mse: 157.5204\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 84.4606 - mae: 6.4505 - mse: 84.4606 - val_loss: 158.7558 - val_mae: 9.2066 - val_mse: 158.7558\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.6047 - mae: 6.3787 - mse: 82.6047 - val_loss: 157.8629 - val_mae: 9.2331 - val_mse: 157.8629\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 80.0564 - mae: 6.2868 - mse: 80.0564 - val_loss: 165.5187 - val_mae: 9.7654 - val_mse: 165.5187\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 80.2578 - mae: 6.2686 - mse: 80.2578 - val_loss: 165.4089 - val_mae: 9.3981 - val_mse: 165.4089\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 76.1484 - mae: 6.1205 - mse: 76.1484 - val_loss: 166.0064 - val_mae: 9.4336 - val_mse: 166.0064\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 72.7964 - mae: 5.9652 - mse: 72.7964 - val_loss: 174.7423 - val_mae: 9.5493 - val_mse: 174.7423\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.3494 - mae: 5.9682 - mse: 73.3494 - val_loss: 169.4640 - val_mae: 9.5949 - val_mse: 169.4640\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.8724 - mae: 5.7773 - mse: 68.8724 - val_loss: 168.6990 - val_mae: 9.4906 - val_mse: 168.6990\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.0972 - mae: 5.6229 - mse: 66.0972 - val_loss: 172.5446 - val_mae: 9.6460 - val_mse: 172.5446\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.6686 - mae: 5.5937 - mse: 64.6686 - val_loss: 175.3035 - val_mae: 9.6815 - val_mse: 175.3035\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 62.3165 - mae: 5.4798 - mse: 62.3165 - val_loss: 175.3754 - val_mae: 9.7154 - val_mse: 175.3754\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.3155 - mae: 5.3613 - mse: 60.3155 - val_loss: 181.8994 - val_mae: 9.8883 - val_mse: 181.8994\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 59.6393 - mae: 5.3313 - mse: 59.6393 - val_loss: 178.7400 - val_mae: 9.7845 - val_mse: 178.7400\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.4754 - mae: 5.2262 - mse: 58.4754 - val_loss: 176.7740 - val_mae: 9.8613 - val_mse: 176.7740\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.2489 - mae: 5.1032 - mse: 54.2489 - val_loss: 181.5812 - val_mae: 9.8598 - val_mse: 181.5812\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.1621 - mae: 5.1088 - mse: 54.1621 - val_loss: 183.3923 - val_mae: 9.8059 - val_mse: 183.3923\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.0787 - mae: 4.9136 - mse: 51.0787 - val_loss: 207.2942 - val_mae: 10.7947 - val_mse: 207.2942\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 50.5626 - mae: 4.9017 - mse: 50.5626 - val_loss: 181.2331 - val_mae: 9.9564 - val_mse: 181.2331\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 48.3393 - mae: 4.7734 - mse: 48.3393 - val_loss: 192.9110 - val_mae: 10.2519 - val_mse: 192.9110\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 45.8952 - mae: 4.6701 - mse: 45.8952 - val_loss: 189.4877 - val_mae: 10.0818 - val_mse: 189.4877\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 44.9024 - mae: 4.6262 - mse: 44.9024 - val_loss: 190.8255 - val_mae: 10.1355 - val_mse: 190.8255\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 43.5674 - mae: 4.5187 - mse: 43.5674 - val_loss: 193.3514 - val_mae: 10.2865 - val_mse: 193.3514\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.0047 - mae: 4.4352 - mse: 42.0047 - val_loss: 192.7274 - val_mae: 10.1199 - val_mse: 192.7274\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.2410 - mae: 4.4761 - mse: 42.2410 - val_loss: 186.8794 - val_mae: 9.9859 - val_mse: 186.8794\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 41.5240 - mae: 4.4035 - mse: 41.5240 - val_loss: 190.9940 - val_mae: 10.0872 - val_mse: 190.9940\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 41.2658 - mae: 4.4574 - mse: 41.2658 - val_loss: 190.3036 - val_mae: 10.2231 - val_mse: 190.3036\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 38.8430 - mae: 4.2890 - mse: 38.8430 - val_loss: 195.0031 - val_mae: 10.2526 - val_mse: 195.0031\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 37.4284 - mae: 4.1635 - mse: 37.4284 - val_loss: 199.5318 - val_mae: 10.4248 - val_mse: 199.5318\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 36.8557 - mae: 4.1771 - mse: 36.8557 - val_loss: 192.8505 - val_mae: 10.1140 - val_mse: 192.8505\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 36.8438 - mae: 4.1598 - mse: 36.8438 - val_loss: 203.9976 - val_mae: 10.3916 - val_mse: 203.9976\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 35.8060 - mae: 4.0767 - mse: 35.8060 - val_loss: 190.6319 - val_mae: 10.0648 - val_mse: 190.6319\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 33.7931 - mae: 3.9732 - mse: 33.7931 - val_loss: 197.0139 - val_mae: 10.2856 - val_mse: 197.0139\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 34.1264 - mae: 3.9878 - mse: 34.1264 - val_loss: 200.7989 - val_mae: 10.3514 - val_mse: 200.7989\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 32.2341 - mae: 3.8585 - mse: 32.2341 - val_loss: 198.1495 - val_mae: 10.2873 - val_mse: 198.1495\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 31.9303 - mae: 3.8473 - mse: 31.9303 - val_loss: 210.9775 - val_mae: 10.6164 - val_mse: 210.9775\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.2038 - mae: 3.7561 - mse: 30.2038 - val_loss: 202.9928 - val_mae: 10.4315 - val_mse: 202.9928\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.4943 - mae: 3.7457 - mse: 30.4943 - val_loss: 210.9052 - val_mae: 10.6706 - val_mse: 210.9052\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.1650 - mae: 3.7456 - mse: 30.1650 - val_loss: 206.6323 - val_mae: 10.4172 - val_mse: 206.6323\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 29.4358 - mae: 3.7125 - mse: 29.4358 - val_loss: 206.6526 - val_mae: 10.4790 - val_mse: 206.6526\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 27.8217 - mae: 3.5797 - mse: 27.8217 - val_loss: 205.0745 - val_mae: 10.3959 - val_mse: 205.0745\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 27.8769 - mae: 3.5410 - mse: 27.8769 - val_loss: 207.5512 - val_mae: 10.4414 - val_mse: 207.5512\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.7936 - mae: 3.5367 - mse: 26.7936 - val_loss: 206.1088 - val_mae: 10.4904 - val_mse: 206.1088\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 25.6102 - mae: 3.4624 - mse: 25.6102 - val_loss: 217.8742 - val_mae: 10.7538 - val_mse: 217.8742\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 25.7527 - mae: 3.4707 - mse: 25.7527 - val_loss: 211.8038 - val_mae: 10.4651 - val_mse: 211.8038\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 25.1089 - mae: 3.3996 - mse: 25.1089 - val_loss: 210.2323 - val_mae: 10.5837 - val_mse: 210.2323\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.0278 - mae: 3.4125 - mse: 26.0278 - val_loss: 209.0444 - val_mae: 10.6329 - val_mse: 209.0444\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.8155 - mae: 3.4238 - mse: 24.8155 - val_loss: 211.2266 - val_mae: 10.6878 - val_mse: 211.2266\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.4379 - mae: 3.3517 - mse: 24.4379 - val_loss: 214.8246 - val_mae: 10.5790 - val_mse: 214.8246\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.8630 - mae: 3.3798 - mse: 24.8630 - val_loss: 208.7324 - val_mae: 10.4740 - val_mse: 208.7324\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 23.8204 - mae: 3.3143 - mse: 23.8204 - val_loss: 216.2418 - val_mae: 10.6944 - val_mse: 216.2418\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.0728 - mae: 3.2231 - mse: 22.0728 - val_loss: 215.3469 - val_mae: 10.6628 - val_mse: 215.3469\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 23.0477 - mae: 3.2508 - mse: 23.0477 - val_loss: 214.4421 - val_mae: 10.5805 - val_mse: 214.4421\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.3491 - mae: 3.1580 - mse: 21.3491 - val_loss: 218.2379 - val_mae: 10.7007 - val_mse: 218.2379\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.9084 - mae: 3.1750 - mse: 21.9084 - val_loss: 217.6043 - val_mae: 10.7433 - val_mse: 217.6043\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.1702 - mae: 3.0756 - mse: 21.1702 - val_loss: 215.6024 - val_mae: 10.7471 - val_mse: 215.6024\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.7097 - mae: 3.2019 - mse: 22.7097 - val_loss: 207.8883 - val_mae: 10.3474 - val_mse: 207.8883\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.5941 - mae: 3.1698 - mse: 21.5941 - val_loss: 221.2746 - val_mae: 10.8910 - val_mse: 221.2746\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.5705 - mae: 3.0126 - mse: 19.5705 - val_loss: 218.0820 - val_mae: 10.7315 - val_mse: 218.0820\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.8291 - mae: 2.9993 - mse: 19.8291 - val_loss: 227.2062 - val_mae: 10.9708 - val_mse: 227.2062\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.4722 - mae: 3.1353 - mse: 21.4722 - val_loss: 215.6814 - val_mae: 10.7302 - val_mse: 215.6814\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 19.7174 - mae: 3.0210 - mse: 19.7174 - val_loss: 213.9366 - val_mae: 10.6585 - val_mse: 213.9366\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.4368 - mae: 3.0043 - mse: 19.4368 - val_loss: 215.9455 - val_mae: 10.6511 - val_mse: 215.9455\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.9892 - mae: 2.9271 - mse: 18.9892 - val_loss: 215.3099 - val_mae: 10.5833 - val_mse: 215.3099\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.9285 - mae: 3.2095 - mse: 22.9285 - val_loss: 220.5648 - val_mae: 10.8196 - val_mse: 220.5648\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.8599 - mae: 2.9742 - mse: 19.8599 - val_loss: 225.2145 - val_mae: 11.0549 - val_mse: 225.2145\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.0075 - mae: 2.8528 - mse: 18.0075 - val_loss: 218.7548 - val_mae: 10.7770 - val_mse: 218.7548\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.4457 - mae: 2.7891 - mse: 17.4457 - val_loss: 230.3237 - val_mae: 11.1410 - val_mse: 230.3237\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.0110 - mae: 2.8617 - mse: 18.0110 - val_loss: 222.1540 - val_mae: 10.8680 - val_mse: 222.1540\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 17.3777 - mae: 2.7825 - mse: 17.3777 - val_loss: 214.7461 - val_mae: 10.6090 - val_mse: 214.7461\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 20.9071 - mae: 2.9932 - mse: 20.9071 - val_loss: 218.8286 - val_mae: 10.8262 - val_mse: 218.8286\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.6439 - mae: 2.8074 - mse: 17.6439 - val_loss: 223.4378 - val_mae: 10.7986 - val_mse: 223.4378\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.9368 - mae: 2.8958 - mse: 18.9368 - val_loss: 215.9441 - val_mae: 10.6715 - val_mse: 215.9441\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.1077 - mae: 2.6862 - mse: 16.1077 - val_loss: 214.5085 - val_mae: 10.7360 - val_mse: 214.5085\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.4696 - mae: 2.6984 - mse: 16.4696 - val_loss: 225.2644 - val_mae: 10.9313 - val_mse: 225.2644\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 15.2490 - mae: 2.6107 - mse: 15.2490 - val_loss: 230.2008 - val_mae: 11.1243 - val_mse: 230.2008\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.8547 - mae: 2.5916 - mse: 14.8547 - val_loss: 216.5108 - val_mae: 10.7424 - val_mse: 216.5108\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.3457 - mae: 2.5502 - mse: 14.3457 - val_loss: 215.8615 - val_mae: 10.7414 - val_mse: 215.8615\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 228.5887 - mae: 10.8400 - mse: 228.5887 - val_loss: 215.8089 - val_mae: 10.6629 - val_mse: 215.8089\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 150.6140 - mae: 8.5490 - mse: 150.6140 - val_loss: 150.1284 - val_mae: 8.7162 - val_mse: 150.1284\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.8971 - mae: 8.0235 - mse: 128.8971 - val_loss: 140.8296 - val_mae: 8.6181 - val_mse: 140.8296\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.9577 - mae: 8.0057 - mse: 125.9577 - val_loss: 139.2443 - val_mae: 8.6329 - val_mse: 139.2443\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 123.6529 - mae: 7.9166 - mse: 123.6529 - val_loss: 140.9102 - val_mae: 8.6520 - val_mse: 140.9102\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.2403 - mae: 7.8375 - mse: 121.2403 - val_loss: 141.7994 - val_mae: 8.8073 - val_mse: 141.7994\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.2463 - mae: 7.8486 - mse: 121.2463 - val_loss: 141.2703 - val_mae: 8.6385 - val_mse: 141.2703\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.0794 - mae: 7.7744 - mse: 119.0794 - val_loss: 140.9797 - val_mae: 8.7462 - val_mse: 140.9797\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4061 - mae: 7.7590 - mse: 118.4061 - val_loss: 140.8953 - val_mae: 8.7891 - val_mse: 140.8953\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.0918 - mae: 7.7112 - mse: 116.0918 - val_loss: 141.1821 - val_mae: 8.7707 - val_mse: 141.1821\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.8404 - mae: 7.6062 - mse: 113.8404 - val_loss: 143.2003 - val_mae: 8.8444 - val_mse: 143.2003\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.2393 - mae: 7.6193 - mse: 112.2393 - val_loss: 144.1846 - val_mae: 8.8445 - val_mse: 144.1846\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.2765 - mae: 7.4954 - mse: 110.2765 - val_loss: 147.7897 - val_mae: 8.9526 - val_mse: 147.7897\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.4216 - mae: 7.4770 - mse: 109.4216 - val_loss: 148.7707 - val_mae: 8.8872 - val_mse: 148.7707\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.3138 - mae: 7.4479 - mse: 109.3138 - val_loss: 146.1940 - val_mae: 8.9234 - val_mse: 146.1940\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.7955 - mae: 7.3547 - mse: 106.7955 - val_loss: 148.3199 - val_mae: 8.9804 - val_mse: 148.3199\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.7870 - mae: 7.2966 - mse: 104.7870 - val_loss: 147.4205 - val_mae: 8.8806 - val_mse: 147.4205\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.9115 - mae: 7.2472 - mse: 103.9115 - val_loss: 147.8915 - val_mae: 8.9054 - val_mse: 147.8915\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.9126 - mae: 7.2422 - mse: 102.9126 - val_loss: 150.4281 - val_mae: 9.0770 - val_mse: 150.4281\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6428 - mae: 7.1726 - mse: 101.6428 - val_loss: 150.5000 - val_mae: 8.9858 - val_mse: 150.5000\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.8264 - mae: 7.1295 - mse: 99.8264 - val_loss: 151.1312 - val_mae: 8.9835 - val_mse: 151.1312\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.3070 - mae: 7.0191 - mse: 96.3070 - val_loss: 151.5411 - val_mae: 9.0299 - val_mse: 151.5411\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.3641 - mae: 6.9898 - mse: 96.3641 - val_loss: 152.6515 - val_mae: 9.0869 - val_mse: 152.6515\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.7598 - mae: 6.9390 - mse: 94.7598 - val_loss: 150.9450 - val_mae: 9.0547 - val_mse: 150.9450\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.5450 - mae: 6.9799 - mse: 94.5450 - val_loss: 151.5098 - val_mae: 8.8880 - val_mse: 151.5098\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.0264 - mae: 6.7963 - mse: 91.0264 - val_loss: 153.3621 - val_mae: 9.1662 - val_mse: 153.3621\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.6348 - mae: 6.8187 - mse: 90.6348 - val_loss: 151.9318 - val_mae: 9.0334 - val_mse: 151.9318\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8049 - mae: 6.7094 - mse: 88.8049 - val_loss: 158.7911 - val_mae: 9.1994 - val_mse: 158.7911\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.5300 - mae: 6.6723 - mse: 87.5300 - val_loss: 156.9970 - val_mae: 9.2121 - val_mse: 156.9970\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.3825 - mae: 6.6762 - mse: 86.3825 - val_loss: 156.4699 - val_mae: 9.1633 - val_mse: 156.4699\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.6482 - mae: 6.6947 - mse: 86.6482 - val_loss: 157.5306 - val_mae: 9.0986 - val_mse: 157.5306\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.5897 - mae: 6.5470 - mse: 83.5897 - val_loss: 159.1537 - val_mae: 9.2306 - val_mse: 159.1537\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.1537 - mae: 6.5285 - mse: 83.1537 - val_loss: 159.0829 - val_mae: 9.2167 - val_mse: 159.0829\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.0175 - mae: 6.4520 - mse: 80.0175 - val_loss: 166.0707 - val_mae: 9.4977 - val_mse: 166.0707\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.0244 - mae: 6.4300 - mse: 80.0244 - val_loss: 158.3020 - val_mae: 9.2328 - val_mse: 158.3020\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 79.9303 - mae: 6.3537 - mse: 79.9303 - val_loss: 159.7334 - val_mae: 9.2853 - val_mse: 159.7334\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 78.6536 - mae: 6.3439 - mse: 78.6536 - val_loss: 158.3983 - val_mae: 9.2453 - val_mse: 158.3983\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 78.1751 - mae: 6.2545 - mse: 78.1751 - val_loss: 158.0094 - val_mae: 9.2079 - val_mse: 158.0094\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 81.3343 - mae: 6.5214 - mse: 81.3343 - val_loss: 155.4272 - val_mae: 9.2748 - val_mse: 155.4272\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 77.0667 - mae: 6.2610 - mse: 77.0667 - val_loss: 158.2077 - val_mae: 9.2472 - val_mse: 158.2077\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.6951 - mae: 6.1874 - mse: 73.6951 - val_loss: 160.5908 - val_mae: 9.2671 - val_mse: 160.5908\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.3524 - mae: 6.1453 - mse: 73.3524 - val_loss: 161.6114 - val_mae: 9.3862 - val_mse: 161.6114\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 72.7775 - mae: 6.1027 - mse: 72.7775 - val_loss: 158.0458 - val_mae: 9.2080 - val_mse: 158.0458\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 72.0577 - mae: 6.1016 - mse: 72.0577 - val_loss: 163.2254 - val_mae: 9.4679 - val_mse: 163.2254\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 71.8582 - mae: 6.0355 - mse: 71.8582 - val_loss: 162.2205 - val_mae: 9.4003 - val_mse: 162.2205\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 70.9060 - mae: 6.0286 - mse: 70.9060 - val_loss: 161.4151 - val_mae: 9.3810 - val_mse: 161.4151\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.5991 - mae: 5.9371 - mse: 68.5991 - val_loss: 157.6324 - val_mae: 9.1656 - val_mse: 157.6324\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.2235 - mae: 5.9494 - mse: 68.2235 - val_loss: 164.1641 - val_mae: 9.3547 - val_mse: 164.1641\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.1397 - mae: 5.9507 - mse: 68.1397 - val_loss: 158.8388 - val_mae: 9.2215 - val_mse: 158.8388\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.8671 - mae: 5.7452 - mse: 64.8671 - val_loss: 160.0213 - val_mae: 9.2944 - val_mse: 160.0213\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 67.3076 - mae: 5.9231 - mse: 67.3076 - val_loss: 161.5705 - val_mae: 9.3378 - val_mse: 161.5705\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.9358 - mae: 5.7990 - mse: 64.9358 - val_loss: 172.6468 - val_mae: 9.7320 - val_mse: 172.6468\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 66.4746 - mae: 5.8770 - mse: 66.4746 - val_loss: 161.1747 - val_mae: 9.3543 - val_mse: 161.1747\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.4635 - mae: 5.7550 - mse: 64.4635 - val_loss: 154.8273 - val_mae: 9.1430 - val_mse: 154.8273\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 66.2916 - mae: 5.8410 - mse: 66.2916 - val_loss: 166.3676 - val_mae: 9.5250 - val_mse: 166.3676\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.6026 - mae: 5.8040 - mse: 65.6026 - val_loss: 160.9932 - val_mae: 9.3599 - val_mse: 160.9932\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 63.3861 - mae: 5.7096 - mse: 63.3861 - val_loss: 163.5392 - val_mae: 9.3611 - val_mse: 163.5392\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.5200 - mae: 5.6762 - mse: 62.5200 - val_loss: 165.4363 - val_mae: 9.3532 - val_mse: 165.4363\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 63.8047 - mae: 5.7653 - mse: 63.8047 - val_loss: 168.3450 - val_mae: 9.5645 - val_mse: 168.3450\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.2511 - mae: 5.5913 - mse: 60.2511 - val_loss: 167.9265 - val_mae: 9.5169 - val_mse: 167.9265\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.6129 - mae: 5.7125 - mse: 62.6129 - val_loss: 165.3417 - val_mae: 9.4361 - val_mse: 165.3417\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.9369 - mae: 5.6181 - mse: 60.9369 - val_loss: 162.0472 - val_mae: 9.3146 - val_mse: 162.0472\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.7645 - mae: 5.5681 - mse: 59.7645 - val_loss: 168.9467 - val_mae: 9.5939 - val_mse: 168.9467\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.5845 - mae: 5.5548 - mse: 59.5845 - val_loss: 165.9582 - val_mae: 9.4363 - val_mse: 165.9582\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.6505 - mae: 5.6003 - mse: 60.6505 - val_loss: 169.3410 - val_mae: 9.6225 - val_mse: 169.3410\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.6496 - mae: 5.4901 - mse: 58.6496 - val_loss: 170.2224 - val_mae: 9.6935 - val_mse: 170.2224\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.6621 - mae: 5.5535 - mse: 59.6621 - val_loss: 159.8146 - val_mae: 9.3029 - val_mse: 159.8146\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.8749 - mae: 5.5048 - mse: 58.8749 - val_loss: 165.8739 - val_mae: 9.3806 - val_mse: 165.8739\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.0118 - mae: 5.3255 - mse: 56.0118 - val_loss: 164.5335 - val_mae: 9.3751 - val_mse: 164.5335\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.7606 - mae: 5.6286 - mse: 60.7606 - val_loss: 167.7990 - val_mae: 9.4400 - val_mse: 167.7990\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.7523 - mae: 5.4997 - mse: 58.7523 - val_loss: 163.5680 - val_mae: 9.3824 - val_mse: 163.5680\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 55.1109 - mae: 5.3648 - mse: 55.1109 - val_loss: 161.6992 - val_mae: 9.3647 - val_mse: 161.6992\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 55.7196 - mae: 5.3588 - mse: 55.7196 - val_loss: 166.4328 - val_mae: 9.4043 - val_mse: 166.4328\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.1517 - mae: 5.3872 - mse: 56.1517 - val_loss: 166.5158 - val_mae: 9.4248 - val_mse: 166.5158\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.8807 - mae: 5.3445 - mse: 54.8807 - val_loss: 165.6010 - val_mae: 9.4459 - val_mse: 165.6010\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.3155 - mae: 5.3372 - mse: 54.3155 - val_loss: 163.3861 - val_mae: 9.3069 - val_mse: 163.3861\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.6630 - mae: 5.2970 - mse: 53.6630 - val_loss: 169.3163 - val_mae: 9.5078 - val_mse: 169.3163\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.2636 - mae: 5.2637 - mse: 52.2636 - val_loss: 170.2025 - val_mae: 9.5243 - val_mse: 170.2025\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.2807 - mae: 5.2810 - mse: 52.2807 - val_loss: 164.7175 - val_mae: 9.4391 - val_mse: 164.7175\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.9424 - mae: 5.2893 - mse: 53.9424 - val_loss: 164.4215 - val_mae: 9.3843 - val_mse: 164.4215\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.7228 - mae: 5.2974 - mse: 53.7228 - val_loss: 178.5916 - val_mae: 9.7588 - val_mse: 178.5916\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.2288 - mae: 5.2479 - mse: 53.2288 - val_loss: 164.3759 - val_mae: 9.3724 - val_mse: 164.3759\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.8252 - mae: 5.2684 - mse: 53.8252 - val_loss: 170.3187 - val_mae: 9.5161 - val_mse: 170.3187\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.5267 - mae: 5.1088 - mse: 51.5267 - val_loss: 163.5145 - val_mae: 9.2755 - val_mse: 163.5145\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.1963 - mae: 5.2739 - mse: 53.1963 - val_loss: 165.7485 - val_mae: 9.4379 - val_mse: 165.7485\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.3554 - mae: 5.2387 - mse: 52.3554 - val_loss: 175.2908 - val_mae: 9.7283 - val_mse: 175.2908\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.1037 - mae: 5.0836 - mse: 50.1037 - val_loss: 169.9044 - val_mae: 9.4618 - val_mse: 169.9044\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.9829 - mae: 5.2172 - mse: 51.9829 - val_loss: 161.8248 - val_mae: 9.3035 - val_mse: 161.8248\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.8648 - mae: 5.1163 - mse: 50.8648 - val_loss: 168.8173 - val_mae: 9.6260 - val_mse: 168.8173\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.7151 - mae: 5.1629 - mse: 50.7151 - val_loss: 167.6453 - val_mae: 9.4827 - val_mse: 167.6453\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.4314 - mae: 5.0305 - mse: 49.4314 - val_loss: 171.2142 - val_mae: 9.5475 - val_mse: 171.2142\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 50.8307 - mae: 5.1175 - mse: 50.8307 - val_loss: 165.9234 - val_mae: 9.4119 - val_mse: 165.9234\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.6934 - mae: 5.1567 - mse: 50.6934 - val_loss: 167.8779 - val_mae: 9.4675 - val_mse: 167.8779\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.6356 - mae: 5.1539 - mse: 50.6356 - val_loss: 164.6260 - val_mae: 9.4726 - val_mse: 164.6260\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.6492 - mae: 5.1057 - mse: 49.6492 - val_loss: 166.2095 - val_mae: 9.4359 - val_mse: 166.2095\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.3419 - mae: 5.1207 - mse: 50.3419 - val_loss: 166.5183 - val_mae: 9.3597 - val_mse: 166.5183\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 48.4203 - mae: 5.0473 - mse: 48.4203 - val_loss: 168.9840 - val_mae: 9.5448 - val_mse: 168.9840\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.7431 - mae: 5.0188 - mse: 47.7431 - val_loss: 164.2905 - val_mae: 9.3106 - val_mse: 164.2905\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 45.4315 - mae: 4.9093 - mse: 45.4315 - val_loss: 171.1413 - val_mae: 9.4024 - val_mse: 171.1413\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 46.2897 - mae: 4.8958 - mse: 46.2897 - val_loss: 168.2096 - val_mae: 9.4547 - val_mse: 168.2096\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 172.2329 - mae: 9.4199 - mse: 172.2080 - val_loss: 146.2268 - val_mae: 8.5796 - val_mse: 146.2008\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.1977 - mae: 8.7237 - mse: 146.1714 - val_loss: 143.2088 - val_mae: 8.5322 - val_mse: 143.1823\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.8656 - mae: 8.6204 - mse: 142.8390 - val_loss: 141.2887 - val_mae: 8.5337 - val_mse: 141.2619\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.8969 - mae: 8.5458 - mse: 140.8701 - val_loss: 142.4988 - val_mae: 8.5350 - val_mse: 142.4719\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2815 - mae: 8.4815 - mse: 139.2545 - val_loss: 145.3491 - val_mae: 8.5526 - val_mse: 145.3222\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1884 - mae: 8.4563 - mse: 138.1613 - val_loss: 146.3978 - val_mae: 8.6033 - val_mse: 146.3708\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2265 - mae: 8.4130 - mse: 136.1993 - val_loss: 144.6232 - val_mae: 8.5803 - val_mse: 144.5961\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.0520 - mae: 8.3952 - mse: 136.0249 - val_loss: 142.9586 - val_mae: 8.5823 - val_mse: 142.9316\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2320 - mae: 8.2168 - mse: 134.2050 - val_loss: 140.3635 - val_mae: 8.5710 - val_mse: 140.3363\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.4396 - mae: 8.3087 - mse: 134.4127 - val_loss: 142.0224 - val_mae: 8.5956 - val_mse: 141.9953\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3878 - mae: 8.2592 - mse: 132.3607 - val_loss: 144.1301 - val_mae: 8.6063 - val_mse: 144.1032\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.4749 - mae: 8.3059 - mse: 134.4479 - val_loss: 140.4869 - val_mae: 8.5629 - val_mse: 140.4600\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.2868 - mae: 8.2819 - mse: 133.2600 - val_loss: 141.1147 - val_mae: 8.5715 - val_mse: 141.0879\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.6877 - mae: 8.2530 - mse: 132.6608 - val_loss: 143.2128 - val_mae: 8.5772 - val_mse: 143.1860\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.1440 - mae: 8.1826 - mse: 130.1171 - val_loss: 140.7945 - val_mae: 8.5670 - val_mse: 140.7676\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.7963 - mae: 8.1913 - mse: 131.7694 - val_loss: 142.1677 - val_mae: 8.5743 - val_mse: 142.1408\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.3632 - mae: 8.1947 - mse: 130.3364 - val_loss: 141.4183 - val_mae: 8.5572 - val_mse: 141.3914\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.3891 - mae: 8.2005 - mse: 130.3623 - val_loss: 142.9663 - val_mae: 8.5834 - val_mse: 142.9396\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6410 - mae: 8.1477 - mse: 129.6144 - val_loss: 146.7997 - val_mae: 8.6223 - val_mse: 146.7731\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.9724 - mae: 8.0828 - mse: 128.9456 - val_loss: 141.8740 - val_mae: 8.5667 - val_mse: 141.8472\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6548 - mae: 8.1507 - mse: 129.6281 - val_loss: 141.0468 - val_mae: 8.5538 - val_mse: 141.0199\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9468 - mae: 8.0855 - mse: 127.9199 - val_loss: 144.2395 - val_mae: 8.6073 - val_mse: 144.2127\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9381 - mae: 8.1026 - mse: 127.9113 - val_loss: 143.6216 - val_mae: 8.5854 - val_mse: 143.5946\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.3337 - mae: 8.0313 - mse: 127.3067 - val_loss: 142.3750 - val_mae: 8.5783 - val_mse: 142.3478\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6238 - mae: 8.0814 - mse: 127.5967 - val_loss: 140.2678 - val_mae: 8.5524 - val_mse: 140.2405\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.9058 - mae: 8.0248 - mse: 124.8786 - val_loss: 140.7648 - val_mae: 8.5475 - val_mse: 140.7375\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.8342 - mae: 8.0272 - mse: 124.8068 - val_loss: 148.1098 - val_mae: 8.6590 - val_mse: 148.0825\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.7544 - mae: 7.9785 - mse: 124.7270 - val_loss: 143.6451 - val_mae: 8.6217 - val_mse: 143.6175\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.4543 - mae: 8.0066 - mse: 122.4266 - val_loss: 142.6391 - val_mae: 8.6067 - val_mse: 142.6114\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.3346 - mae: 8.0122 - mse: 124.3068 - val_loss: 145.1747 - val_mae: 8.6401 - val_mse: 145.1468\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.5445 - mae: 7.9709 - mse: 122.5165 - val_loss: 143.3630 - val_mae: 8.6240 - val_mse: 143.3350\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1657 - mae: 8.0478 - mse: 124.1376 - val_loss: 144.3729 - val_mae: 8.6530 - val_mse: 144.3447\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.5382 - mae: 7.9138 - mse: 121.5099 - val_loss: 144.3369 - val_mae: 8.6525 - val_mse: 144.3085\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 121.9240 - mae: 7.9363 - mse: 121.8954 - val_loss: 142.4773 - val_mae: 8.5921 - val_mse: 142.4486\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 122.0870 - mae: 7.9437 - mse: 122.0582 - val_loss: 143.4388 - val_mae: 8.5969 - val_mse: 143.4099\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9363 - mae: 7.9370 - mse: 120.9072 - val_loss: 145.2041 - val_mae: 8.6572 - val_mse: 145.1750\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4859 - mae: 7.8833 - mse: 120.4566 - val_loss: 142.9342 - val_mae: 8.6078 - val_mse: 142.9047\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.2653 - mae: 7.9248 - mse: 119.2356 - val_loss: 142.2544 - val_mae: 8.6036 - val_mse: 142.2246\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4596 - mae: 7.8926 - mse: 120.4297 - val_loss: 142.3389 - val_mae: 8.6216 - val_mse: 142.3088\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4369 - mae: 7.8995 - mse: 120.4067 - val_loss: 144.9455 - val_mae: 8.6737 - val_mse: 144.9153\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9876 - mae: 7.9019 - mse: 120.9572 - val_loss: 145.9210 - val_mae: 8.6738 - val_mse: 145.8904\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2576 - mae: 7.8296 - mse: 118.2269 - val_loss: 143.6654 - val_mae: 8.6594 - val_mse: 143.6345\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.5000 - mae: 7.8044 - mse: 116.4689 - val_loss: 143.0874 - val_mae: 8.6257 - val_mse: 143.0561\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.3101 - mae: 7.8003 - mse: 117.2789 - val_loss: 144.5740 - val_mae: 8.6609 - val_mse: 144.5426\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4976 - mae: 7.7961 - mse: 118.4660 - val_loss: 141.6870 - val_mae: 8.6223 - val_mse: 141.6552\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.6169 - mae: 7.7688 - mse: 115.5849 - val_loss: 144.2662 - val_mae: 8.6684 - val_mse: 144.2341\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.7217 - mae: 7.8411 - mse: 117.6894 - val_loss: 144.9301 - val_mae: 8.6414 - val_mse: 144.8977\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.3526 - mae: 7.7748 - mse: 118.3200 - val_loss: 143.1316 - val_mae: 8.6173 - val_mse: 143.0989\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.0795 - mae: 7.7772 - mse: 114.0466 - val_loss: 144.4593 - val_mae: 8.6747 - val_mse: 144.4263\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.5923 - mae: 7.7674 - mse: 114.5591 - val_loss: 144.9316 - val_mae: 8.6437 - val_mse: 144.8982\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6796 - mae: 7.7472 - mse: 113.6460 - val_loss: 144.0377 - val_mae: 8.6480 - val_mse: 144.0040\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.6487 - mae: 7.7941 - mse: 115.6147 - val_loss: 143.5474 - val_mae: 8.6235 - val_mse: 143.5133\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.3232 - mae: 7.7427 - mse: 114.2888 - val_loss: 145.9073 - val_mae: 8.6875 - val_mse: 145.8728\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.9403 - mae: 7.7192 - mse: 112.9057 - val_loss: 142.6536 - val_mae: 8.6436 - val_mse: 142.6188\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.4681 - mae: 7.7054 - mse: 112.4331 - val_loss: 144.9102 - val_mae: 8.7120 - val_mse: 144.8750\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.1936 - mae: 7.7454 - mse: 113.1582 - val_loss: 142.5117 - val_mae: 8.6421 - val_mse: 142.4761\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.7728 - mae: 7.7481 - mse: 113.7370 - val_loss: 146.5298 - val_mae: 8.7014 - val_mse: 146.4939\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 111.5365 - mae: 7.6650 - mse: 111.5004 - val_loss: 143.7491 - val_mae: 8.6547 - val_mse: 143.7128\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 110.3987 - mae: 7.6223 - mse: 110.3622 - val_loss: 146.3931 - val_mae: 8.6871 - val_mse: 146.3565\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.2176 - mae: 7.6724 - mse: 111.1809 - val_loss: 144.0108 - val_mae: 8.6764 - val_mse: 143.9739\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7983 - mae: 7.6248 - mse: 108.7612 - val_loss: 145.4747 - val_mae: 8.7012 - val_mse: 145.4373\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.8878 - mae: 7.6564 - mse: 109.8503 - val_loss: 145.6251 - val_mae: 8.7504 - val_mse: 145.5873\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.1050 - mae: 7.6256 - mse: 108.0670 - val_loss: 148.1740 - val_mae: 8.7959 - val_mse: 148.1359\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.5856 - mae: 7.6502 - mse: 110.5472 - val_loss: 147.4284 - val_mae: 8.7382 - val_mse: 147.3899\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.1474 - mae: 7.5798 - mse: 109.1087 - val_loss: 145.8545 - val_mae: 8.7095 - val_mse: 145.8156\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8896 - mae: 7.5640 - mse: 108.8505 - val_loss: 146.2674 - val_mae: 8.6885 - val_mse: 146.2280\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.5795 - mae: 7.5678 - mse: 108.5400 - val_loss: 146.4636 - val_mae: 8.7062 - val_mse: 146.4239\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9802 - mae: 7.6191 - mse: 107.9403 - val_loss: 148.1125 - val_mae: 8.7296 - val_mse: 148.0723\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.8633 - mae: 7.5059 - mse: 107.8231 - val_loss: 148.3330 - val_mae: 8.7201 - val_mse: 148.2925\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.4574 - mae: 7.6072 - mse: 108.4167 - val_loss: 147.7589 - val_mae: 8.7383 - val_mse: 147.7180\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.2132 - mae: 7.4938 - mse: 105.1721 - val_loss: 146.1678 - val_mae: 8.7342 - val_mse: 146.1265\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6760 - mae: 7.4559 - mse: 103.6345 - val_loss: 147.3804 - val_mae: 8.7514 - val_mse: 147.3387\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.1063 - mae: 7.5759 - mse: 106.0644 - val_loss: 146.7662 - val_mae: 8.7381 - val_mse: 146.7241\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.5234 - mae: 7.5411 - mse: 106.4811 - val_loss: 149.3772 - val_mae: 8.7609 - val_mse: 149.3347\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.7348 - mae: 7.5932 - mse: 107.6921 - val_loss: 146.9877 - val_mae: 8.7236 - val_mse: 146.9447\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.0529 - mae: 7.3913 - mse: 103.0098 - val_loss: 148.5652 - val_mae: 8.7478 - val_mse: 148.5219\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.2480 - mae: 7.4278 - mse: 104.2045 - val_loss: 148.0592 - val_mae: 8.7360 - val_mse: 148.0155\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9610 - mae: 7.4153 - mse: 101.9171 - val_loss: 147.2341 - val_mae: 8.7145 - val_mse: 147.1901\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.1638 - mae: 7.4554 - mse: 104.1196 - val_loss: 147.0531 - val_mae: 8.6978 - val_mse: 147.0086\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.2715 - mae: 7.4100 - mse: 102.2268 - val_loss: 148.8810 - val_mae: 8.7517 - val_mse: 148.8361\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.8687 - mae: 7.4424 - mse: 102.8235 - val_loss: 149.1584 - val_mae: 8.7374 - val_mse: 149.1132\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.1611 - mae: 7.4125 - mse: 104.1157 - val_loss: 145.6856 - val_mae: 8.6807 - val_mse: 145.6399\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.1440 - mae: 7.4424 - mse: 103.0981 - val_loss: 149.1925 - val_mae: 8.7773 - val_mse: 149.1463\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.4559 - mae: 7.4626 - mse: 103.4096 - val_loss: 147.0263 - val_mae: 8.6948 - val_mse: 146.9798\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9912 - mae: 7.4217 - mse: 101.9445 - val_loss: 146.2417 - val_mae: 8.6967 - val_mse: 146.1946\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 104.0916 - mae: 7.4856 - mse: 104.0443 - val_loss: 149.0478 - val_mae: 8.7525 - val_mse: 149.0005\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.9350 - mae: 7.3835 - mse: 100.8874 - val_loss: 148.8420 - val_mae: 8.7696 - val_mse: 148.7943\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 102.1079 - mae: 7.4153 - mse: 102.0599 - val_loss: 150.4041 - val_mae: 8.8059 - val_mse: 150.3560\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.1013 - mae: 7.3627 - mse: 102.0531 - val_loss: 149.4193 - val_mae: 8.7370 - val_mse: 149.3710\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.8585 - mae: 7.4338 - mse: 103.8099 - val_loss: 148.8768 - val_mae: 8.7124 - val_mse: 148.8281\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.6518 - mae: 7.3243 - mse: 99.6028 - val_loss: 148.3242 - val_mae: 8.7366 - val_mse: 148.2751\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.3496 - mae: 7.3526 - mse: 100.3003 - val_loss: 149.2192 - val_mae: 8.7739 - val_mse: 149.1695\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.2299 - mae: 7.4323 - mse: 103.1801 - val_loss: 146.5369 - val_mae: 8.7107 - val_mse: 146.4868\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.0050 - mae: 7.3477 - mse: 99.9547 - val_loss: 149.9954 - val_mae: 8.7692 - val_mse: 149.9450\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 98.4780 - mae: 7.3198 - mse: 98.4273 - val_loss: 150.1174 - val_mae: 8.7660 - val_mse: 150.0664\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 101.4495 - mae: 7.3630 - mse: 101.3984 - val_loss: 151.0921 - val_mae: 8.7646 - val_mse: 151.0409\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.4617 - mae: 7.2355 - mse: 95.4102 - val_loss: 148.1281 - val_mae: 8.7388 - val_mse: 148.0763\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.9246 - mae: 7.3454 - mse: 99.8727 - val_loss: 149.0641 - val_mae: 8.7482 - val_mse: 149.0120\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.1327 - mae: 7.2928 - mse: 98.0804 - val_loss: 147.5014 - val_mae: 8.7082 - val_mse: 147.4488\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.2837 - mae: 7.3076 - mse: 98.2309 - val_loss: 148.3302 - val_mae: 8.7153 - val_mse: 148.2770\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 2.4526, Train MSE: 13.5034\n",
      "Val   MAE: 10.7414, Val   MSE: 215.8615\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8031, Train MSE: 58.0731\n",
      "Val   MAE: 9.9698, Val   MSE: 181.1145\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.0905, Train MSE: 94.4091\n",
      "Val   MAE: 8.7153, Val   MSE: 148.2770\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all versions of the deep learning model, the regularized model has the best combination of low MAE (accuracy) and small difference between training and validation (least overfitting). \n",
    "\n",
    "Additionally, the max peak position model with genre data had a lower MAE score. For max rank change, the two models had essentially the same MAE.\n",
    "\n",
    "Next I'll optimize the regularized, max peak position model with genre and the regularized, max rank change model without genre (in the interest of maximizing model efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position (regularized, with genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2637.5242 - mae: 42.3891 - mse: 2637.5012 - val_loss: 787.6920 - val_mae: 24.0844 - val_mse: 787.6674\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1063.5612 - mae: 26.7792 - mse: 1063.5369 - val_loss: 663.6807 - val_mae: 21.7454 - val_mse: 663.6561\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 986.6328 - mae: 25.7667 - mse: 986.6082 - val_loss: 698.2437 - val_mae: 22.8379 - val_mse: 698.2192\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 898.9559 - mae: 24.4286 - mse: 898.9312 - val_loss: 627.2560 - val_mae: 21.2902 - val_mse: 627.2315\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 840.7985 - mae: 23.6541 - mse: 840.7739 - val_loss: 617.2349 - val_mae: 21.2061 - val_mse: 617.2105\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 828.8493 - mae: 23.2666 - mse: 828.8248 - val_loss: 559.0635 - val_mae: 19.7727 - val_mse: 559.0392\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 805.7038 - mae: 22.8687 - mse: 805.6796 - val_loss: 569.1693 - val_mae: 20.1579 - val_mse: 569.1452\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 791.1954 - mae: 22.6961 - mse: 791.1714 - val_loss: 550.3447 - val_mae: 19.6908 - val_mse: 550.3208\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 762.0412 - mae: 22.0111 - mse: 762.0175 - val_loss: 608.3972 - val_mae: 21.2633 - val_mse: 608.3735\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 779.4661 - mae: 22.6219 - mse: 779.4424 - val_loss: 573.0119 - val_mae: 20.3544 - val_mse: 572.9883\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 769.0422 - mae: 22.2899 - mse: 769.0186 - val_loss: 523.8784 - val_mae: 18.9577 - val_mse: 523.8547\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 725.5492 - mae: 21.5103 - mse: 725.5258 - val_loss: 588.0566 - val_mae: 20.7732 - val_mse: 588.0333\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 744.7244 - mae: 21.9056 - mse: 744.7010 - val_loss: 531.4764 - val_mae: 19.2680 - val_mse: 531.4531\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 701.7850 - mae: 21.1913 - mse: 701.7618 - val_loss: 531.8601 - val_mae: 19.3203 - val_mse: 531.8369\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 718.7404 - mae: 21.5242 - mse: 718.7173 - val_loss: 533.1886 - val_mae: 19.3593 - val_mse: 533.1654\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 719.3492 - mae: 21.4788 - mse: 719.3263 - val_loss: 505.3168 - val_mae: 18.4826 - val_mse: 505.2938\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 723.9532 - mae: 21.4882 - mse: 723.9302 - val_loss: 538.2142 - val_mae: 19.4737 - val_mse: 538.1915\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 692.6707 - mae: 21.0897 - mse: 692.6479 - val_loss: 546.3524 - val_mae: 19.7114 - val_mse: 546.3300\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 685.3653 - mae: 20.9989 - mse: 685.3428 - val_loss: 516.8538 - val_mae: 18.8609 - val_mse: 516.8311\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.6763 - mae: 20.9423 - mse: 687.6538 - val_loss: 533.9154 - val_mae: 19.3988 - val_mse: 533.8930\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 681.6442 - mae: 20.7607 - mse: 681.6219 - val_loss: 499.7081 - val_mae: 18.3070 - val_mse: 499.6859\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.7054 - mae: 20.6236 - mse: 671.6832 - val_loss: 516.1550 - val_mae: 18.8041 - val_mse: 516.1328\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 676.3180 - mae: 20.6502 - mse: 676.2960 - val_loss: 494.1901 - val_mae: 17.9817 - val_mse: 494.1678\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.8241 - mae: 20.5432 - mse: 668.8020 - val_loss: 519.7742 - val_mae: 18.9676 - val_mse: 519.7522\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.7075 - mae: 20.6422 - mse: 673.6857 - val_loss: 524.3220 - val_mae: 19.0950 - val_mse: 524.3002\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.7618 - mae: 20.5057 - mse: 665.7403 - val_loss: 530.2259 - val_mae: 19.2823 - val_mse: 530.2043\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.5175 - mae: 20.4147 - mse: 659.4961 - val_loss: 502.1443 - val_mae: 18.3170 - val_mse: 502.1227\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 656.6533 - mae: 20.2846 - mse: 656.6321 - val_loss: 503.6998 - val_mae: 18.3511 - val_mse: 503.6783\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.1359 - mae: 20.4154 - mse: 663.1141 - val_loss: 529.4603 - val_mae: 19.1812 - val_mse: 529.4390\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.3648 - mae: 20.6512 - mse: 673.3434 - val_loss: 547.8799 - val_mae: 19.7503 - val_mse: 547.8585\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.6510 - mae: 20.3991 - mse: 660.6298 - val_loss: 515.6743 - val_mae: 18.7595 - val_mse: 515.6532\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 654.1020 - mae: 20.2363 - mse: 654.0806 - val_loss: 517.4538 - val_mae: 18.8341 - val_mse: 517.4327\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.4595 - mae: 20.0693 - mse: 643.4385 - val_loss: 507.3864 - val_mae: 18.4492 - val_mse: 507.3652\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.2739 - mae: 20.2869 - mse: 655.2531 - val_loss: 508.5467 - val_mae: 18.4890 - val_mse: 508.5258\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.5474 - mae: 20.4340 - mse: 665.5264 - val_loss: 546.6007 - val_mae: 19.6829 - val_mse: 546.5798\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.3766 - mae: 20.2460 - mse: 655.3553 - val_loss: 515.5179 - val_mae: 18.7632 - val_mse: 515.4972\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 654.8655 - mae: 20.3486 - mse: 654.8451 - val_loss: 500.7202 - val_mae: 18.1622 - val_mse: 500.6997\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.4797 - mae: 20.1701 - mse: 645.4588 - val_loss: 533.5328 - val_mae: 19.3129 - val_mse: 533.5122\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.4099 - mae: 20.1922 - mse: 646.3891 - val_loss: 521.5591 - val_mae: 18.9523 - val_mse: 521.5385\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.9392 - mae: 20.0895 - mse: 641.9186 - val_loss: 508.7217 - val_mae: 18.5331 - val_mse: 508.7012\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.9070 - mae: 20.2031 - mse: 647.8863 - val_loss: 499.3183 - val_mae: 18.0074 - val_mse: 499.2977\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.2271 - mae: 20.2554 - mse: 651.2064 - val_loss: 505.5118 - val_mae: 18.3691 - val_mse: 505.4913\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 650.6272 - mae: 20.1421 - mse: 650.6070 - val_loss: 504.1235 - val_mae: 18.2725 - val_mse: 504.1032\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.5176 - mae: 20.1832 - mse: 647.4971 - val_loss: 537.1807 - val_mae: 19.4067 - val_mse: 537.1605\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 644.1755 - mae: 20.0960 - mse: 644.1553 - val_loss: 497.9360 - val_mae: 17.9893 - val_mse: 497.9157\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 637.2231 - mae: 19.9372 - mse: 637.2029 - val_loss: 507.8427 - val_mae: 18.4236 - val_mse: 507.8226\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.7903 - mae: 20.1500 - mse: 645.7697 - val_loss: 501.8157 - val_mae: 18.1105 - val_mse: 501.7956\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.3427 - mae: 19.8945 - mse: 630.3227 - val_loss: 515.8507 - val_mae: 18.7522 - val_mse: 515.8306\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.6007 - mae: 20.0279 - mse: 633.5807 - val_loss: 498.6514 - val_mae: 17.9871 - val_mse: 498.6313\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.7400 - mae: 20.1366 - mse: 643.7202 - val_loss: 525.0330 - val_mae: 19.0047 - val_mse: 525.0131\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.6149 - mae: 20.0384 - mse: 642.5948 - val_loss: 517.9816 - val_mae: 18.7936 - val_mse: 517.9617\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.5749 - mae: 20.0476 - mse: 642.5549 - val_loss: 499.7908 - val_mae: 18.0682 - val_mse: 499.7709\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.8812 - mae: 20.0251 - mse: 636.8611 - val_loss: 539.8171 - val_mae: 19.4634 - val_mse: 539.7975\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.2925 - mae: 19.9117 - mse: 636.2726 - val_loss: 551.5005 - val_mae: 19.7850 - val_mse: 551.4810\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.4844 - mae: 19.9404 - mse: 634.4646 - val_loss: 529.2404 - val_mae: 19.1510 - val_mse: 529.2209\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 651.5452 - mae: 20.1773 - mse: 651.5261 - val_loss: 526.6722 - val_mae: 19.0857 - val_mse: 526.6528\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.0021 - mae: 20.0271 - mse: 635.9825 - val_loss: 511.1419 - val_mae: 18.4964 - val_mse: 511.1224\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4112 - mae: 19.8982 - mse: 635.3918 - val_loss: 523.0687 - val_mae: 18.9475 - val_mse: 523.0493\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.8182 - mae: 20.1372 - mse: 642.7986 - val_loss: 517.2814 - val_mae: 18.7861 - val_mse: 517.2620\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.4564 - mae: 19.5494 - mse: 619.4371 - val_loss: 518.4246 - val_mae: 18.8715 - val_mse: 518.4053\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5644 - mae: 20.0621 - mse: 636.5451 - val_loss: 498.7797 - val_mae: 18.0344 - val_mse: 498.7603\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.6397 - mae: 19.7749 - mse: 629.6203 - val_loss: 508.1760 - val_mae: 18.4650 - val_mse: 508.1566\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.1559 - mae: 19.9317 - mse: 633.1365 - val_loss: 522.9647 - val_mae: 18.9874 - val_mse: 522.9454\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.0758 - mae: 19.8535 - mse: 626.0565 - val_loss: 522.4543 - val_mae: 18.9987 - val_mse: 522.4350\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.7549 - mae: 19.9939 - mse: 638.7354 - val_loss: 512.5812 - val_mae: 18.6797 - val_mse: 512.5620\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.6474 - mae: 19.9792 - mse: 636.6285 - val_loss: 506.3384 - val_mae: 18.4452 - val_mse: 506.3190\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.2679 - mae: 19.9235 - mse: 629.2484 - val_loss: 506.1094 - val_mae: 18.4488 - val_mse: 506.0901\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.0809 - mae: 19.5790 - mse: 621.0613 - val_loss: 517.5500 - val_mae: 18.8239 - val_mse: 517.5309\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.5034 - mae: 19.7120 - mse: 625.4841 - val_loss: 494.3990 - val_mae: 18.0008 - val_mse: 494.3797\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.7723 - mae: 19.5048 - mse: 609.7534 - val_loss: 501.3704 - val_mae: 18.3097 - val_mse: 501.3512\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.0745 - mae: 19.6606 - mse: 621.0552 - val_loss: 493.1142 - val_mae: 18.0540 - val_mse: 493.0948\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5077 - mae: 19.8044 - mse: 621.4884 - val_loss: 515.4812 - val_mae: 18.8636 - val_mse: 515.4619\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.0442 - mae: 19.5073 - mse: 610.0250 - val_loss: 504.3161 - val_mae: 18.4555 - val_mse: 504.2967\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.0364 - mae: 19.3210 - mse: 604.0172 - val_loss: 497.7630 - val_mae: 18.1735 - val_mse: 497.7437\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.0826 - mae: 19.5191 - mse: 614.0633 - val_loss: 507.3298 - val_mae: 18.5796 - val_mse: 507.3104\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.4748 - mae: 19.2772 - mse: 600.4555 - val_loss: 493.5315 - val_mae: 18.0484 - val_mse: 493.5120\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.4202 - mae: 19.5886 - mse: 610.4006 - val_loss: 494.6381 - val_mae: 18.0752 - val_mse: 494.6187\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.7144 - mae: 19.5084 - mse: 605.6953 - val_loss: 503.9870 - val_mae: 18.4488 - val_mse: 503.9676\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.5853 - mae: 19.6346 - mse: 620.5658 - val_loss: 521.4331 - val_mae: 19.0363 - val_mse: 521.4138\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.8193 - mae: 19.3898 - mse: 599.7997 - val_loss: 513.2712 - val_mae: 18.7813 - val_mse: 513.2517\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.8187 - mae: 19.5117 - mse: 611.7991 - val_loss: 514.7736 - val_mae: 18.8461 - val_mse: 514.7542\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.7655 - mae: 19.2373 - mse: 600.7458 - val_loss: 505.4450 - val_mae: 18.5522 - val_mse: 505.4255\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.4352 - mae: 19.4259 - mse: 603.4156 - val_loss: 484.6104 - val_mae: 17.6683 - val_mse: 484.5908\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.2169 - mae: 19.4034 - mse: 609.1973 - val_loss: 491.2191 - val_mae: 18.0218 - val_mse: 491.1994\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.2939 - mae: 19.1560 - mse: 597.2740 - val_loss: 499.3791 - val_mae: 18.3193 - val_mse: 499.3595\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.4918 - mae: 19.4893 - mse: 614.4719 - val_loss: 503.0061 - val_mae: 18.4178 - val_mse: 502.9863\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.9703 - mae: 19.1757 - mse: 595.9506 - val_loss: 501.2331 - val_mae: 18.3544 - val_mse: 501.2134\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.3693 - mae: 19.4699 - mse: 608.3497 - val_loss: 510.7640 - val_mae: 18.7066 - val_mse: 510.7442\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.1156 - mae: 19.4193 - mse: 605.0958 - val_loss: 522.8132 - val_mae: 19.1099 - val_mse: 522.7935\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.6736 - mae: 19.0966 - mse: 591.6536 - val_loss: 511.4246 - val_mae: 18.7206 - val_mse: 511.4048\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.5034 - mae: 19.3851 - mse: 609.4835 - val_loss: 517.4868 - val_mae: 18.9055 - val_mse: 517.4667\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.4907 - mae: 19.2595 - mse: 595.4707 - val_loss: 494.7609 - val_mae: 18.0711 - val_mse: 494.7408\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.1793 - mae: 19.4053 - mse: 603.1591 - val_loss: 493.3702 - val_mae: 18.0409 - val_mse: 493.3501\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.3113 - mae: 18.9860 - mse: 586.2912 - val_loss: 488.6969 - val_mae: 17.8457 - val_mse: 488.6768\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.6484 - mae: 19.1151 - mse: 595.6282 - val_loss: 493.4080 - val_mae: 18.0841 - val_mse: 493.3878\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.0280 - mae: 19.1957 - mse: 597.0079 - val_loss: 496.4456 - val_mae: 18.2021 - val_mse: 496.4254\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.1672 - mae: 19.1335 - mse: 590.1471 - val_loss: 495.1434 - val_mae: 18.1295 - val_mse: 495.1230\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.4115 - mae: 19.2551 - mse: 594.3913 - val_loss: 490.4645 - val_mae: 17.9729 - val_mse: 490.4441\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.5989 - mae: 19.3759 - mse: 603.5790 - val_loss: 492.8329 - val_mae: 18.0911 - val_mse: 492.8127\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.5779 - mae: 18.9267 - mse: 583.5576 - val_loss: 511.7914 - val_mae: 18.7369 - val_mse: 511.7708\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_1.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding an additional deep layer\n",
      "Train MAE: 18.3038, Train MSE: 498.7998\n",
      "Val   MAE: 18.7369, Val   MSE: 511.7708\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2063.6829 - mae: 36.3444 - mse: 2063.6509 - val_loss: 689.2535 - val_mae: 22.0733 - val_mse: 689.2202\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 826.1360 - mae: 23.2645 - mse: 826.1028 - val_loss: 593.9785 - val_mae: 19.9290 - val_mse: 593.9451\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 814.2825 - mae: 22.9193 - mse: 814.2485 - val_loss: 562.1478 - val_mae: 19.3306 - val_mse: 562.1143\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 766.1361 - mae: 22.2481 - mse: 766.1028 - val_loss: 551.6454 - val_mae: 19.1931 - val_mse: 551.6119\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 720.8075 - mae: 21.6047 - mse: 720.7739 - val_loss: 540.4574 - val_mae: 18.5377 - val_mse: 540.4238\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 716.8436 - mae: 21.5159 - mse: 716.8102 - val_loss: 529.9467 - val_mae: 18.6217 - val_mse: 529.9132\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 704.5610 - mae: 21.1665 - mse: 704.5273 - val_loss: 540.5358 - val_mae: 19.3349 - val_mse: 540.5023\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.4141 - mae: 20.9290 - mse: 690.3810 - val_loss: 525.1923 - val_mae: 18.6695 - val_mse: 525.1588\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.4059 - mae: 20.5534 - mse: 674.3724 - val_loss: 510.6354 - val_mae: 17.7198 - val_mse: 510.6020\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.2300 - mae: 20.5712 - mse: 667.1963 - val_loss: 521.4208 - val_mae: 18.6187 - val_mse: 521.3874\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.0100 - mae: 20.3180 - mse: 660.9769 - val_loss: 541.0872 - val_mae: 19.3431 - val_mse: 541.0541\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.3366 - mae: 20.1949 - mse: 653.3036 - val_loss: 506.7249 - val_mae: 18.1787 - val_mse: 506.6917\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 640.4178 - mae: 20.1199 - mse: 640.3845 - val_loss: 504.3607 - val_mae: 18.0174 - val_mse: 504.3275\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4736 - mae: 19.8075 - mse: 635.4406 - val_loss: 497.6169 - val_mae: 17.8358 - val_mse: 497.5838\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.8004 - mae: 19.8562 - mse: 635.7673 - val_loss: 521.9502 - val_mae: 18.9723 - val_mse: 521.9172\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.4069 - mae: 20.0055 - mse: 639.3740 - val_loss: 509.9736 - val_mae: 18.3243 - val_mse: 509.9404\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6353 - mae: 19.7719 - mse: 635.6024 - val_loss: 521.1716 - val_mae: 18.8760 - val_mse: 521.1384\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.8526 - mae: 19.8068 - mse: 629.8189 - val_loss: 489.0791 - val_mae: 17.4591 - val_mse: 489.0457\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.1804 - mae: 19.6546 - mse: 624.1472 - val_loss: 487.7478 - val_mae: 17.3666 - val_mse: 487.7143\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.7040 - mae: 19.5317 - mse: 616.6709 - val_loss: 490.1374 - val_mae: 17.7633 - val_mse: 490.1041\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.8480 - mae: 19.3442 - mse: 613.8148 - val_loss: 490.2246 - val_mae: 17.7977 - val_mse: 490.1913\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.1722 - mae: 19.2415 - mse: 606.1391 - val_loss: 496.8153 - val_mae: 18.1447 - val_mse: 496.7819\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.0172 - mae: 19.5272 - mse: 617.9839 - val_loss: 490.0059 - val_mae: 17.8076 - val_mse: 489.9726\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.3236 - mae: 19.3385 - mse: 606.2903 - val_loss: 491.7262 - val_mae: 17.7017 - val_mse: 491.6928\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.1060 - mae: 19.4716 - mse: 613.0730 - val_loss: 489.3952 - val_mae: 17.8854 - val_mse: 489.3619\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.0970 - mae: 19.4754 - mse: 618.0638 - val_loss: 505.4539 - val_mae: 18.4565 - val_mse: 505.4208\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 606.0465 - mae: 19.2575 - mse: 606.0137 - val_loss: 495.0088 - val_mae: 18.0628 - val_mse: 494.9757\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8764 - mae: 19.4395 - mse: 615.8434 - val_loss: 483.6725 - val_mae: 17.3013 - val_mse: 483.6393\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.2694 - mae: 19.1591 - mse: 611.2365 - val_loss: 484.9034 - val_mae: 17.3457 - val_mse: 484.8702\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.6193 - mae: 19.0162 - mse: 596.5858 - val_loss: 515.6871 - val_mae: 18.7712 - val_mse: 515.6540\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 587.8530 - mae: 18.9259 - mse: 587.8202 - val_loss: 498.0406 - val_mae: 18.2178 - val_mse: 498.0076\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0739 - mae: 19.1231 - mse: 601.0406 - val_loss: 498.7513 - val_mae: 18.1641 - val_mse: 498.7184\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 594.5905 - mae: 19.0746 - mse: 594.5576 - val_loss: 486.6208 - val_mae: 17.7439 - val_mse: 486.5878\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.8818 - mae: 19.1108 - mse: 592.8489 - val_loss: 482.6638 - val_mae: 17.2366 - val_mse: 482.6308\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0429 - mae: 19.0356 - mse: 596.0098 - val_loss: 484.8214 - val_mae: 17.4956 - val_mse: 484.7884\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.4198 - mae: 18.9672 - mse: 593.3869 - val_loss: 492.8668 - val_mae: 18.0092 - val_mse: 492.8339\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.6039 - mae: 19.0538 - mse: 595.5710 - val_loss: 482.6855 - val_mae: 17.3479 - val_mse: 482.6524\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.7720 - mae: 18.8500 - mse: 592.7392 - val_loss: 487.7218 - val_mae: 17.7466 - val_mse: 487.6888\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.6913 - mae: 19.2397 - mse: 598.6585 - val_loss: 487.2433 - val_mae: 17.6640 - val_mse: 487.2102\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 602.4775 - mae: 19.0450 - mse: 602.4445 - val_loss: 481.3237 - val_mae: 17.4564 - val_mse: 481.2908\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.5073 - mae: 18.7169 - mse: 580.4745 - val_loss: 488.9723 - val_mae: 17.7244 - val_mse: 488.9394\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.3596 - mae: 18.8683 - mse: 583.3268 - val_loss: 503.5258 - val_mae: 18.4629 - val_mse: 503.4931\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.3982 - mae: 18.9759 - mse: 591.3657 - val_loss: 493.7786 - val_mae: 18.1133 - val_mse: 493.7460\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.1117 - mae: 18.9606 - mse: 591.0793 - val_loss: 505.6937 - val_mae: 18.5484 - val_mse: 505.6612\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.2828 - mae: 18.8860 - mse: 582.2501 - val_loss: 483.1662 - val_mae: 17.4713 - val_mse: 483.1336\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 579.1772 - mae: 18.6986 - mse: 579.1448 - val_loss: 482.8763 - val_mae: 17.5999 - val_mse: 482.8437\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 589.3599 - mae: 18.9190 - mse: 589.3273 - val_loss: 485.8301 - val_mae: 17.7447 - val_mse: 485.7976\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.3544 - mae: 18.8262 - mse: 585.3218 - val_loss: 482.2225 - val_mae: 17.3220 - val_mse: 482.1901\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.6801 - mae: 18.9135 - mse: 592.6476 - val_loss: 484.6383 - val_mae: 17.5806 - val_mse: 484.6059\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.1219 - mae: 18.9306 - mse: 590.0895 - val_loss: 485.7797 - val_mae: 17.8399 - val_mse: 485.7474\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.0735 - mae: 18.8575 - mse: 588.0412 - val_loss: 480.4764 - val_mae: 17.3719 - val_mse: 480.4442\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 578.6832 - mae: 18.7766 - mse: 578.6511 - val_loss: 493.9861 - val_mae: 18.1682 - val_mse: 493.9540\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.4916 - mae: 18.9476 - mse: 590.4597 - val_loss: 473.9874 - val_mae: 16.9695 - val_mse: 473.9552\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 578.5526 - mae: 18.7244 - mse: 578.5204 - val_loss: 488.8027 - val_mae: 18.0312 - val_mse: 488.7705\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 575.3963 - mae: 18.6701 - mse: 575.3640 - val_loss: 477.5664 - val_mae: 17.2039 - val_mse: 477.5341\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.2013 - mae: 18.4085 - mse: 566.1693 - val_loss: 477.0541 - val_mae: 17.0290 - val_mse: 477.0218\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 561.7394 - mae: 18.4777 - mse: 561.7072 - val_loss: 478.0927 - val_mae: 17.0780 - val_mse: 478.0604\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.0107 - mae: 18.7890 - mse: 576.9786 - val_loss: 481.3262 - val_mae: 17.6549 - val_mse: 481.2939\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 579.5758 - mae: 18.6505 - mse: 579.5435 - val_loss: 493.7932 - val_mae: 18.1582 - val_mse: 493.7609\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.6399 - mae: 18.5772 - mse: 567.6077 - val_loss: 489.0762 - val_mae: 18.0386 - val_mse: 489.0440\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 574.9600 - mae: 18.6202 - mse: 574.9274 - val_loss: 484.0294 - val_mae: 17.7431 - val_mse: 483.9970\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.5184 - mae: 18.5897 - mse: 567.4857 - val_loss: 480.2362 - val_mae: 17.6287 - val_mse: 480.2040\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.8011 - mae: 18.7486 - mse: 577.7687 - val_loss: 489.0372 - val_mae: 17.9650 - val_mse: 489.0049\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.8035 - mae: 18.6299 - mse: 571.7712 - val_loss: 478.3602 - val_mae: 17.3808 - val_mse: 478.3278\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.3093 - mae: 18.5521 - mse: 569.2769 - val_loss: 477.0341 - val_mae: 17.2017 - val_mse: 477.0016\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.2319 - mae: 18.6109 - mse: 571.1995 - val_loss: 488.2876 - val_mae: 18.0145 - val_mse: 488.2553\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 568.1151 - mae: 18.6261 - mse: 568.0825 - val_loss: 474.6282 - val_mae: 17.1484 - val_mse: 474.5958\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.6682 - mae: 18.6274 - mse: 569.6357 - val_loss: 475.8278 - val_mae: 17.3527 - val_mse: 475.7954\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.9432 - mae: 18.5724 - mse: 566.9108 - val_loss: 478.0833 - val_mae: 17.5742 - val_mse: 478.0506\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.2289 - mae: 18.5980 - mse: 571.1957 - val_loss: 491.9922 - val_mae: 18.1580 - val_mse: 491.9597\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.5851 - mae: 18.5325 - mse: 563.5527 - val_loss: 474.6701 - val_mae: 16.9458 - val_mse: 474.6373\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.9595 - mae: 18.5407 - mse: 567.9268 - val_loss: 476.4854 - val_mae: 17.2172 - val_mse: 476.4528\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.2883 - mae: 18.6916 - mse: 577.2557 - val_loss: 491.8947 - val_mae: 18.1575 - val_mse: 491.8620\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 572.3086 - mae: 18.5347 - mse: 572.2759 - val_loss: 471.3790 - val_mae: 17.2950 - val_mse: 471.3463\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 561.6018 - mae: 18.3063 - mse: 561.5685 - val_loss: 472.3777 - val_mae: 16.8421 - val_mse: 472.3448\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 565.2369 - mae: 18.4742 - mse: 565.2039 - val_loss: 491.8140 - val_mae: 18.2074 - val_mse: 491.7812\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 564.7375 - mae: 18.4219 - mse: 564.7045 - val_loss: 477.3441 - val_mae: 17.3961 - val_mse: 477.3110\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.4319 - mae: 18.3369 - mse: 552.3987 - val_loss: 481.1107 - val_mae: 17.6875 - val_mse: 481.0775\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 558.9868 - mae: 18.3541 - mse: 558.9532 - val_loss: 475.7746 - val_mae: 17.4473 - val_mse: 475.7412\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 553.8135 - mae: 18.4379 - mse: 553.7802 - val_loss: 476.8531 - val_mae: 16.7508 - val_mse: 476.8194\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.1340 - mae: 18.2214 - mse: 551.1006 - val_loss: 471.6800 - val_mae: 16.9736 - val_mse: 471.6460\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.0787 - mae: 18.2012 - mse: 552.0449 - val_loss: 477.5336 - val_mae: 17.4844 - val_mse: 477.4996\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.2136 - mae: 18.2206 - mse: 549.1796 - val_loss: 471.9548 - val_mae: 17.1773 - val_mse: 471.9204\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.6270 - mae: 18.2514 - mse: 549.5929 - val_loss: 475.8790 - val_mae: 17.3385 - val_mse: 475.8447\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.9986 - mae: 18.2985 - mse: 552.9641 - val_loss: 474.8471 - val_mae: 16.7697 - val_mse: 474.8124\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 556.0417 - mae: 18.3448 - mse: 556.0071 - val_loss: 472.0791 - val_mae: 16.9819 - val_mse: 472.0442\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.9942 - mae: 18.0864 - mse: 548.9596 - val_loss: 473.7958 - val_mae: 17.2211 - val_mse: 473.7608\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.5380 - mae: 18.1561 - mse: 549.5029 - val_loss: 483.1813 - val_mae: 17.8658 - val_mse: 483.1463\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.2013 - mae: 18.3096 - mse: 554.1661 - val_loss: 476.0331 - val_mae: 17.2323 - val_mse: 475.9977\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.3806 - mae: 18.1607 - mse: 551.3451 - val_loss: 485.6524 - val_mae: 17.9194 - val_mse: 485.6172\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.4897 - mae: 18.2159 - mse: 555.4540 - val_loss: 489.9293 - val_mae: 18.0352 - val_mse: 489.8935\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 549.8048 - mae: 18.2680 - mse: 549.7688 - val_loss: 479.9194 - val_mae: 17.5817 - val_mse: 479.8834\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.1467 - mae: 17.9981 - mse: 544.1105 - val_loss: 484.8286 - val_mae: 17.8535 - val_mse: 484.7925\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 545.8397 - mae: 18.1333 - mse: 545.8032 - val_loss: 481.0097 - val_mae: 17.6744 - val_mse: 480.9730\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 547.1404 - mae: 18.1408 - mse: 547.1041 - val_loss: 487.7214 - val_mae: 17.9139 - val_mse: 487.6846\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 529.8124 - mae: 17.8488 - mse: 529.7758 - val_loss: 488.6172 - val_mae: 17.9323 - val_mse: 488.5800\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 543.0217 - mae: 18.0920 - mse: 542.9844 - val_loss: 476.3470 - val_mae: 17.1372 - val_mse: 476.3095\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.9143 - mae: 17.9622 - mse: 537.8768 - val_loss: 482.8395 - val_mae: 17.5574 - val_mse: 482.8018\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.5068 - mae: 18.1021 - mse: 548.4690 - val_loss: 482.3041 - val_mae: 17.6814 - val_mse: 482.2663\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 550.2599 - mae: 18.2865 - mse: 550.2221 - val_loss: 478.9218 - val_mae: 17.1875 - val_mse: 478.8835\n"
     ]
    }
   ],
   "source": [
    "reg_model_maxpeak_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_2.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    3 deep layers, 128 kernels per layer\n",
      "Train MAE: 16.2949, Train MSE: 454.0456\n",
      "Val   MAE: 17.1875, Val   MSE: 478.8835\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model but increased overfitting a little bit. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2370.4817 - mae: 39.8835 - mse: 2370.4497 - val_loss: 707.7922 - val_mae: 22.4548 - val_mse: 707.7590\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1031.9341 - mae: 26.3930 - mse: 1031.9010 - val_loss: 596.6560 - val_mae: 19.8617 - val_mse: 596.6224\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 966.9283 - mae: 25.4896 - mse: 966.8953 - val_loss: 643.0316 - val_mae: 21.4751 - val_mse: 642.9983\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 905.8526 - mae: 24.4060 - mse: 905.8193 - val_loss: 573.0706 - val_mae: 19.8401 - val_mse: 573.0372\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 893.2500 - mae: 24.2132 - mse: 893.2168 - val_loss: 623.0960 - val_mae: 21.1503 - val_mse: 623.0629\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 870.3020 - mae: 24.0005 - mse: 870.2688 - val_loss: 557.1016 - val_mae: 19.5457 - val_mse: 557.0682\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 854.3907 - mae: 23.5718 - mse: 854.3577 - val_loss: 538.7375 - val_mae: 18.9882 - val_mse: 538.7041\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 844.8894 - mae: 23.4831 - mse: 844.8561 - val_loss: 562.2311 - val_mae: 19.7859 - val_mse: 562.1980\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 802.4953 - mae: 22.8672 - mse: 802.4619 - val_loss: 551.2480 - val_mae: 19.5142 - val_mse: 551.2148\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 794.9246 - mae: 22.8921 - mse: 794.8914 - val_loss: 533.1952 - val_mae: 18.9642 - val_mse: 533.1620\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 797.9401 - mae: 22.8359 - mse: 797.9071 - val_loss: 566.0074 - val_mae: 20.0079 - val_mse: 565.9745\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 777.1449 - mae: 22.5327 - mse: 777.1119 - val_loss: 513.5373 - val_mae: 18.3665 - val_mse: 513.5042\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 790.2823 - mae: 22.5970 - mse: 790.2498 - val_loss: 569.4741 - val_mae: 20.1819 - val_mse: 569.4413\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 757.5637 - mae: 22.2042 - mse: 757.5308 - val_loss: 579.7106 - val_mae: 20.4741 - val_mse: 579.6780\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 755.5649 - mae: 22.0759 - mse: 755.5323 - val_loss: 537.5193 - val_mae: 19.2939 - val_mse: 537.4868\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 745.4221 - mae: 22.0480 - mse: 745.3898 - val_loss: 555.8333 - val_mae: 19.8769 - val_mse: 555.8010\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 737.8729 - mae: 21.6997 - mse: 737.8408 - val_loss: 513.9667 - val_mae: 18.4935 - val_mse: 513.9344\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 741.1009 - mae: 21.7715 - mse: 741.0690 - val_loss: 514.5392 - val_mae: 18.5344 - val_mse: 514.5071\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 741.4383 - mae: 21.7390 - mse: 741.4062 - val_loss: 521.2806 - val_mae: 18.8260 - val_mse: 521.2487\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 743.7592 - mae: 21.7819 - mse: 743.7276 - val_loss: 513.8451 - val_mae: 18.5683 - val_mse: 513.8133\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 710.1010 - mae: 21.3589 - mse: 710.0692 - val_loss: 564.0231 - val_mae: 20.1432 - val_mse: 563.9917\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 728.2238 - mae: 21.6471 - mse: 728.1923 - val_loss: 517.0915 - val_mae: 18.6641 - val_mse: 517.0602\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 711.6091 - mae: 21.3805 - mse: 711.5779 - val_loss: 515.7766 - val_mae: 18.6934 - val_mse: 515.7454\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 718.7947 - mae: 21.5033 - mse: 718.7635 - val_loss: 511.7367 - val_mae: 18.5750 - val_mse: 511.7055\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 713.5811 - mae: 21.3877 - mse: 713.5500 - val_loss: 513.3278 - val_mae: 18.6715 - val_mse: 513.2968\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.7830 - mae: 21.3000 - mse: 708.7523 - val_loss: 496.7324 - val_mae: 17.9501 - val_mse: 496.7014\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 719.8051 - mae: 21.2720 - mse: 719.7742 - val_loss: 511.8014 - val_mae: 18.6154 - val_mse: 511.7708\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.8479 - mae: 21.2594 - mse: 708.8172 - val_loss: 490.9307 - val_mae: 17.7041 - val_mse: 490.9000\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 720.3104 - mae: 21.4057 - mse: 720.2799 - val_loss: 503.9372 - val_mae: 18.2918 - val_mse: 503.9066\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 698.5232 - mae: 21.0462 - mse: 698.4930 - val_loss: 506.8420 - val_mae: 18.4600 - val_mse: 506.8118\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 703.0370 - mae: 20.9897 - mse: 703.0067 - val_loss: 503.2796 - val_mae: 18.3218 - val_mse: 503.2494\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 691.7380 - mae: 20.9063 - mse: 691.7078 - val_loss: 489.9079 - val_mae: 17.4940 - val_mse: 489.8777\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 695.0565 - mae: 20.9082 - mse: 695.0264 - val_loss: 489.8180 - val_mae: 17.6209 - val_mse: 489.7877\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 698.6441 - mae: 20.9108 - mse: 698.6138 - val_loss: 486.9463 - val_mae: 17.4825 - val_mse: 486.9163\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 689.9433 - mae: 20.8928 - mse: 689.9135 - val_loss: 490.2141 - val_mae: 17.6608 - val_mse: 490.1843\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 678.9495 - mae: 20.7361 - mse: 678.9194 - val_loss: 491.6341 - val_mae: 17.8498 - val_mse: 491.6044\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 695.8645 - mae: 21.0201 - mse: 695.8348 - val_loss: 518.3657 - val_mae: 18.8653 - val_mse: 518.3364\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.1659 - mae: 20.8057 - mse: 687.1367 - val_loss: 497.3813 - val_mae: 18.0993 - val_mse: 497.3520\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 686.4155 - mae: 20.6745 - mse: 686.3865 - val_loss: 520.7607 - val_mae: 18.9433 - val_mse: 520.7315\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 696.3968 - mae: 20.9450 - mse: 696.3677 - val_loss: 487.9097 - val_mae: 17.4476 - val_mse: 487.8805\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 693.1377 - mae: 20.8443 - mse: 693.1088 - val_loss: 489.3266 - val_mae: 17.7114 - val_mse: 489.2975\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.9763 - mae: 20.7167 - mse: 677.9471 - val_loss: 484.3979 - val_mae: 17.4469 - val_mse: 484.3688\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 669.1261 - mae: 20.4164 - mse: 669.0970 - val_loss: 496.7319 - val_mae: 18.1407 - val_mse: 496.7032\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 680.4749 - mae: 20.7381 - mse: 680.4462 - val_loss: 490.6148 - val_mae: 17.8961 - val_mse: 490.5858\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.2322 - mae: 20.3606 - mse: 659.2034 - val_loss: 481.7736 - val_mae: 17.4203 - val_mse: 481.7447\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 664.7844 - mae: 20.3866 - mse: 664.7556 - val_loss: 498.0884 - val_mae: 18.2144 - val_mse: 498.0598\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.8715 - mae: 20.3182 - mse: 659.8427 - val_loss: 490.5023 - val_mae: 17.9175 - val_mse: 490.4739\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 670.8020 - mae: 20.5636 - mse: 670.7737 - val_loss: 481.8839 - val_mae: 17.4094 - val_mse: 481.8555\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.2516 - mae: 20.4513 - mse: 677.2231 - val_loss: 484.8761 - val_mae: 17.6780 - val_mse: 484.8478\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.0927 - mae: 20.4247 - mse: 671.0650 - val_loss: 487.6601 - val_mae: 17.8072 - val_mse: 487.6320\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.8250 - mae: 20.7011 - mse: 677.7968 - val_loss: 486.7062 - val_mae: 17.7193 - val_mse: 486.6781\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.2039 - mae: 20.3201 - mse: 665.1757 - val_loss: 484.3996 - val_mae: 17.5805 - val_mse: 484.3715\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.0454 - mae: 20.4325 - mse: 662.0170 - val_loss: 482.1376 - val_mae: 17.5276 - val_mse: 482.1096\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.0865 - mae: 20.2359 - mse: 653.0589 - val_loss: 488.1039 - val_mae: 17.8716 - val_mse: 488.0760\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.3917 - mae: 20.3824 - mse: 667.3642 - val_loss: 498.6682 - val_mae: 18.2573 - val_mse: 498.6405\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 655.4000 - mae: 20.2152 - mse: 655.3726 - val_loss: 488.4238 - val_mae: 17.8672 - val_mse: 488.3961\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.0429 - mae: 20.2100 - mse: 649.0149 - val_loss: 493.4773 - val_mae: 18.0856 - val_mse: 493.4497\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.5632 - mae: 20.4506 - mse: 665.5358 - val_loss: 496.5226 - val_mae: 18.2152 - val_mse: 496.4952\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.0017 - mae: 20.3352 - mse: 656.9742 - val_loss: 486.4055 - val_mae: 17.8098 - val_mse: 486.3781\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.4045 - mae: 19.9747 - mse: 640.3768 - val_loss: 489.5039 - val_mae: 17.9322 - val_mse: 489.4765\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.2098 - mae: 20.0409 - mse: 649.1822 - val_loss: 478.7143 - val_mae: 17.2445 - val_mse: 478.6869\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.8448 - mae: 19.9278 - mse: 647.8175 - val_loss: 482.9593 - val_mae: 17.6334 - val_mse: 482.9320\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 648.1691 - mae: 20.0164 - mse: 648.1416 - val_loss: 496.0704 - val_mae: 18.2108 - val_mse: 496.0433\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.3317 - mae: 20.0606 - mse: 648.3044 - val_loss: 485.0473 - val_mae: 17.7449 - val_mse: 485.0201\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.0553 - mae: 19.9766 - mse: 639.0279 - val_loss: 484.5574 - val_mae: 17.6937 - val_mse: 484.5302\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.6790 - mae: 20.0066 - mse: 642.6519 - val_loss: 482.9801 - val_mae: 17.6219 - val_mse: 482.9529\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.2778 - mae: 19.7229 - mse: 629.2505 - val_loss: 488.0178 - val_mae: 17.9015 - val_mse: 487.9907\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.7576 - mae: 19.8970 - mse: 640.7301 - val_loss: 479.1344 - val_mae: 17.4168 - val_mse: 479.1073\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.5248 - mae: 19.8142 - mse: 634.4976 - val_loss: 490.6411 - val_mae: 18.0173 - val_mse: 490.6139\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.8738 - mae: 20.3833 - mse: 658.8469 - val_loss: 481.0026 - val_mae: 17.5946 - val_mse: 480.9754\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 648.3746 - mae: 20.1422 - mse: 648.3475 - val_loss: 485.2260 - val_mae: 17.7416 - val_mse: 485.1987\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6077 - mae: 19.7911 - mse: 635.5804 - val_loss: 485.2171 - val_mae: 17.7743 - val_mse: 485.1898\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5091 - mae: 19.8978 - mse: 636.4820 - val_loss: 496.4600 - val_mae: 18.1811 - val_mse: 496.4329\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.7330 - mae: 19.7818 - mse: 628.7059 - val_loss: 488.4967 - val_mae: 17.9166 - val_mse: 488.4696\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.3363 - mae: 19.7873 - mse: 633.3091 - val_loss: 490.0869 - val_mae: 17.9737 - val_mse: 490.0597\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.6699 - mae: 19.6026 - mse: 634.6428 - val_loss: 487.9421 - val_mae: 17.9305 - val_mse: 487.9149\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.4261 - mae: 19.5634 - mse: 617.3990 - val_loss: 486.0007 - val_mae: 17.7886 - val_mse: 485.9734\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.0632 - mae: 19.6455 - mse: 620.0357 - val_loss: 484.1146 - val_mae: 17.6957 - val_mse: 484.0871\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 628.3881 - mae: 19.6155 - mse: 628.3602 - val_loss: 486.2995 - val_mae: 17.8432 - val_mse: 486.2719\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.5321 - mae: 19.3312 - mse: 613.5043 - val_loss: 484.3665 - val_mae: 17.7828 - val_mse: 484.3387\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4154 - mae: 19.8486 - mse: 635.3877 - val_loss: 475.6969 - val_mae: 17.2685 - val_mse: 475.6692\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.7649 - mae: 19.6513 - mse: 623.7369 - val_loss: 478.0510 - val_mae: 17.3644 - val_mse: 478.0229\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.1729 - mae: 19.4859 - mse: 616.1447 - val_loss: 484.4632 - val_mae: 17.7986 - val_mse: 484.4350\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.3753 - mae: 19.4800 - mse: 610.3473 - val_loss: 483.6250 - val_mae: 17.7543 - val_mse: 483.5967\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.8707 - mae: 19.4377 - mse: 616.8427 - val_loss: 474.3017 - val_mae: 17.1186 - val_mse: 474.2733\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.1155 - mae: 19.8856 - mse: 635.0873 - val_loss: 479.4889 - val_mae: 17.4790 - val_mse: 479.4603\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.2742 - mae: 19.4196 - mse: 614.2454 - val_loss: 475.7259 - val_mae: 17.2529 - val_mse: 475.6974\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.1202 - mae: 19.4561 - mse: 613.0916 - val_loss: 483.0739 - val_mae: 17.7150 - val_mse: 483.0451\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.0538 - mae: 19.3359 - mse: 609.0245 - val_loss: 478.5358 - val_mae: 17.5016 - val_mse: 478.5069\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.2075 - mae: 19.5229 - mse: 616.1786 - val_loss: 498.6602 - val_mae: 18.2924 - val_mse: 498.6313\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.1732 - mae: 19.6631 - mse: 626.1437 - val_loss: 474.4667 - val_mae: 17.1515 - val_mse: 474.4373\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.7549 - mae: 19.6208 - mse: 620.7253 - val_loss: 480.5599 - val_mae: 17.5718 - val_mse: 480.5302\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.8147 - mae: 19.4336 - mse: 600.7848 - val_loss: 474.3376 - val_mae: 17.1914 - val_mse: 474.3077\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.8564 - mae: 19.4106 - mse: 614.8265 - val_loss: 479.1705 - val_mae: 17.4530 - val_mse: 479.1406\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.9748 - mae: 19.6012 - mse: 621.9449 - val_loss: 487.0316 - val_mae: 17.9372 - val_mse: 487.0014\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.9178 - mae: 19.4602 - mse: 614.8876 - val_loss: 478.7530 - val_mae: 17.5391 - val_mse: 478.7224\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.4213 - mae: 19.3787 - mse: 605.3906 - val_loss: 484.9724 - val_mae: 17.7589 - val_mse: 484.9415\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.2260 - mae: 19.5059 - mse: 615.1950 - val_loss: 477.7089 - val_mae: 17.3433 - val_mse: 477.6777\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.5020 - mae: 19.2249 - mse: 602.4709 - val_loss: 478.5802 - val_mae: 17.3325 - val_mse: 478.5489\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.6281 - mae: 19.2195 - mse: 601.5967 - val_loss: 477.0312 - val_mae: 17.3380 - val_mse: 476.9997\n"
     ]
    }
   ],
   "source": [
    "# increasing dropout rate\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_3 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_3.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    3 deep layers, 128 kernels per layer\n",
      "Train MAE: 17.0017, Train MSE: 482.7560\n",
      "Val   MAE: 17.3380, Val   MSE: 476.9997\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the dropout rate brought the training and validation MAEs closer together but the result is similar to the model before optimization. Trying another layer again, keeping the increased kernels and higher dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2154.9417 - mae: 37.8534 - mse: 2154.8975 - val_loss: 866.1996 - val_mae: 25.5505 - val_mse: 866.1552\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1137.0728 - mae: 27.6327 - mse: 1137.0280 - val_loss: 860.4778 - val_mae: 25.8384 - val_mse: 860.4332\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1024.1622 - mae: 26.1851 - mse: 1024.1172 - val_loss: 795.1439 - val_mae: 24.6692 - val_mse: 795.0990\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 976.2333 - mae: 25.5275 - mse: 976.1888 - val_loss: 794.0362 - val_mae: 24.7748 - val_mse: 793.9913\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 949.3926 - mae: 25.0022 - mse: 949.3472 - val_loss: 821.3254 - val_mae: 25.4429 - val_mse: 821.2805\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 906.3994 - mae: 24.5855 - mse: 906.3545 - val_loss: 629.3447 - val_mae: 21.5867 - val_mse: 629.2996\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 867.0803 - mae: 23.9324 - mse: 867.0353 - val_loss: 734.7047 - val_mae: 23.8791 - val_mse: 734.6599\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 874.6105 - mae: 23.9893 - mse: 874.5656 - val_loss: 635.2013 - val_mae: 21.7867 - val_mse: 635.1565\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 832.3945 - mae: 23.4804 - mse: 832.3498 - val_loss: 625.8828 - val_mae: 21.6204 - val_mse: 625.8381\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 834.0496 - mae: 23.4565 - mse: 834.0052 - val_loss: 646.4335 - val_mae: 22.0875 - val_mse: 646.3888\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 824.6174 - mae: 23.1036 - mse: 824.5726 - val_loss: 750.8664 - val_mae: 24.3312 - val_mse: 750.8220\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 818.6053 - mae: 23.2537 - mse: 818.5607 - val_loss: 641.1705 - val_mae: 22.0328 - val_mse: 641.1260\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 799.1076 - mae: 22.8414 - mse: 799.0632 - val_loss: 627.8763 - val_mae: 21.6786 - val_mse: 627.8318\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 780.1438 - mae: 22.5605 - mse: 780.0992 - val_loss: 700.4547 - val_mae: 23.3380 - val_mse: 700.4104\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 751.4824 - mae: 22.1496 - mse: 751.4378 - val_loss: 591.4529 - val_mae: 20.8409 - val_mse: 591.4083\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 760.1786 - mae: 22.0283 - mse: 760.1346 - val_loss: 626.7730 - val_mae: 21.7044 - val_mse: 626.7288\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 742.0889 - mae: 21.9682 - mse: 742.0447 - val_loss: 612.7074 - val_mae: 21.4077 - val_mse: 612.6631\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 734.3730 - mae: 21.8063 - mse: 734.3290 - val_loss: 572.7029 - val_mae: 20.3444 - val_mse: 572.6586\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 738.5729 - mae: 21.8140 - mse: 738.5287 - val_loss: 594.1309 - val_mae: 20.9489 - val_mse: 594.0868\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 722.6121 - mae: 21.5739 - mse: 722.5682 - val_loss: 589.8914 - val_mae: 20.8310 - val_mse: 589.8475\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 726.9529 - mae: 21.5877 - mse: 726.9090 - val_loss: 611.0002 - val_mae: 21.3770 - val_mse: 610.9562\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 720.4545 - mae: 21.4948 - mse: 720.4105 - val_loss: 580.3383 - val_mae: 20.6102 - val_mse: 580.2946\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 726.3301 - mae: 21.5814 - mse: 726.2864 - val_loss: 652.4115 - val_mae: 22.3682 - val_mse: 652.3679\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.0776 - mae: 21.2808 - mse: 709.0335 - val_loss: 561.3838 - val_mae: 20.0718 - val_mse: 561.3400\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 708.0976 - mae: 21.2893 - mse: 708.0536 - val_loss: 582.0026 - val_mae: 20.6647 - val_mse: 581.9589\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 706.0010 - mae: 21.1757 - mse: 705.9574 - val_loss: 553.0450 - val_mae: 19.8914 - val_mse: 553.0012\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 691.4828 - mae: 21.0597 - mse: 691.4391 - val_loss: 515.3265 - val_mae: 18.7642 - val_mse: 515.2828\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 693.5110 - mae: 20.9171 - mse: 693.4673 - val_loss: 553.4686 - val_mae: 19.9429 - val_mse: 553.4249\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.8601 - mae: 21.2148 - mse: 709.8164 - val_loss: 556.6267 - val_mae: 20.0002 - val_mse: 556.5832\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 694.7885 - mae: 20.9559 - mse: 694.7446 - val_loss: 566.1856 - val_mae: 20.2767 - val_mse: 566.1420\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.1754 - mae: 20.4129 - mse: 661.1313 - val_loss: 528.8629 - val_mae: 19.2432 - val_mse: 528.8190\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 682.6413 - mae: 20.8466 - mse: 682.5972 - val_loss: 521.7114 - val_mae: 19.0066 - val_mse: 521.6674\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.3447 - mae: 20.4964 - mse: 668.3007 - val_loss: 512.7366 - val_mae: 18.6749 - val_mse: 512.6926\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.9238 - mae: 20.5391 - mse: 665.8797 - val_loss: 536.8917 - val_mae: 19.4548 - val_mse: 536.8477\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 669.7986 - mae: 20.5144 - mse: 669.7540 - val_loss: 497.2353 - val_mae: 18.1167 - val_mse: 497.1910\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 669.8341 - mae: 20.5306 - mse: 669.7902 - val_loss: 499.3320 - val_mae: 18.2661 - val_mse: 499.2876\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.6592 - mae: 20.4028 - mse: 662.6149 - val_loss: 500.4725 - val_mae: 18.2873 - val_mse: 500.4282\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.6938 - mae: 20.3837 - mse: 665.6494 - val_loss: 495.1460 - val_mae: 18.1375 - val_mse: 495.1016\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.1176 - mae: 20.4769 - mse: 672.0734 - val_loss: 506.5137 - val_mae: 18.5039 - val_mse: 506.4693\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.6395 - mae: 20.4631 - mse: 672.5951 - val_loss: 499.2678 - val_mae: 18.2162 - val_mse: 499.2232\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 656.4976 - mae: 20.1560 - mse: 656.4532 - val_loss: 550.8789 - val_mae: 19.8270 - val_mse: 550.8346\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.3124 - mae: 20.5062 - mse: 671.2677 - val_loss: 502.0875 - val_mae: 18.3084 - val_mse: 502.0428\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.9319 - mae: 20.4180 - mse: 666.8868 - val_loss: 507.6384 - val_mae: 18.5031 - val_mse: 507.5936\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 663.4995 - mae: 20.2984 - mse: 663.4547 - val_loss: 504.1674 - val_mae: 18.4142 - val_mse: 504.1225\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 660.1348 - mae: 20.3224 - mse: 660.0897 - val_loss: 522.7314 - val_mae: 18.9916 - val_mse: 522.6865\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 642.0040 - mae: 20.0064 - mse: 641.9590 - val_loss: 496.1809 - val_mae: 18.0375 - val_mse: 496.1359\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 648.7177 - mae: 20.1878 - mse: 648.6729 - val_loss: 521.4437 - val_mae: 18.9493 - val_mse: 521.3986\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.5725 - mae: 20.2354 - mse: 651.5274 - val_loss: 512.5760 - val_mae: 18.6214 - val_mse: 512.5308\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.0632 - mae: 20.1314 - mse: 651.0177 - val_loss: 500.2267 - val_mae: 18.2550 - val_mse: 500.1812\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.9503 - mae: 20.0613 - mse: 641.9049 - val_loss: 499.4483 - val_mae: 18.2255 - val_mse: 499.4026\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.7725 - mae: 19.9390 - mse: 639.7268 - val_loss: 506.2184 - val_mae: 18.4163 - val_mse: 506.1729\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.6553 - mae: 19.8917 - mse: 641.6094 - val_loss: 501.0995 - val_mae: 18.2436 - val_mse: 501.0536\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.0501 - mae: 19.9380 - mse: 641.0043 - val_loss: 509.7984 - val_mae: 18.5709 - val_mse: 509.7525\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 653.0039 - mae: 20.2009 - mse: 652.9579 - val_loss: 517.8643 - val_mae: 18.8392 - val_mse: 517.8183\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 642.6546 - mae: 19.9452 - mse: 642.6085 - val_loss: 488.6543 - val_mae: 17.7068 - val_mse: 488.6078\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 639.3681 - mae: 19.8847 - mse: 639.3217 - val_loss: 513.5941 - val_mae: 18.6720 - val_mse: 513.5477\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.1001 - mae: 20.0636 - mse: 641.0535 - val_loss: 499.6726 - val_mae: 18.1407 - val_mse: 499.6259\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 640.8742 - mae: 19.8167 - mse: 640.8274 - val_loss: 499.8205 - val_mae: 18.1987 - val_mse: 499.7736\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 639.1833 - mae: 19.8990 - mse: 639.1362 - val_loss: 493.5721 - val_mae: 17.8873 - val_mse: 493.5248\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 634.3763 - mae: 19.8093 - mse: 634.3292 - val_loss: 510.4951 - val_mae: 18.5111 - val_mse: 510.4479\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 632.4431 - mae: 19.8234 - mse: 632.3957 - val_loss: 501.0948 - val_mae: 18.1606 - val_mse: 501.0472\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5638 - mae: 19.5484 - mse: 621.5162 - val_loss: 500.7289 - val_mae: 18.1388 - val_mse: 500.6813\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.9314 - mae: 19.6593 - mse: 623.8836 - val_loss: 498.4855 - val_mae: 18.1260 - val_mse: 498.4374\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.4550 - mae: 19.8172 - mse: 630.4067 - val_loss: 496.4584 - val_mae: 17.9957 - val_mse: 496.4101\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5922 - mae: 19.5725 - mse: 621.5441 - val_loss: 495.8011 - val_mae: 17.9290 - val_mse: 495.7524\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 630.1591 - mae: 19.8147 - mse: 630.1103 - val_loss: 502.8672 - val_mae: 18.2376 - val_mse: 502.8184\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.2939 - mae: 19.7686 - mse: 626.2451 - val_loss: 506.2541 - val_mae: 18.4039 - val_mse: 506.2051\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.8990 - mae: 20.0727 - mse: 643.8499 - val_loss: 493.4131 - val_mae: 17.8551 - val_mse: 493.3638\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.6761 - mae: 19.6660 - mse: 624.6265 - val_loss: 490.5192 - val_mae: 17.5994 - val_mse: 490.4695\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.4990 - mae: 19.5090 - mse: 620.4492 - val_loss: 502.5094 - val_mae: 18.1541 - val_mse: 502.4596\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.3821 - mae: 19.6353 - mse: 621.3320 - val_loss: 502.7688 - val_mae: 18.2471 - val_mse: 502.7187\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.1290 - mae: 19.5394 - mse: 622.0783 - val_loss: 498.7411 - val_mae: 18.1238 - val_mse: 498.6906\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 598.9103 - mae: 19.1758 - mse: 598.8597 - val_loss: 493.1137 - val_mae: 17.7813 - val_mse: 493.0628\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 629.6525 - mae: 19.7937 - mse: 629.6014 - val_loss: 485.8556 - val_mae: 17.3152 - val_mse: 485.8043\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.0275 - mae: 19.4620 - mse: 613.9763 - val_loss: 498.8578 - val_mae: 18.0670 - val_mse: 498.8064\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 617.7936 - mae: 19.5666 - mse: 617.7422 - val_loss: 496.8887 - val_mae: 18.0111 - val_mse: 496.8369\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.7433 - mae: 19.7060 - mse: 625.6911 - val_loss: 488.1200 - val_mae: 17.4149 - val_mse: 488.0677\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.9290 - mae: 19.3170 - mse: 604.8766 - val_loss: 498.8098 - val_mae: 18.0322 - val_mse: 498.7571\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.5310 - mae: 19.6814 - mse: 623.4781 - val_loss: 496.2546 - val_mae: 17.8974 - val_mse: 496.2018\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.3730 - mae: 19.4609 - mse: 614.3202 - val_loss: 494.1033 - val_mae: 17.8210 - val_mse: 494.0502\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.7858 - mae: 19.5133 - mse: 614.7327 - val_loss: 503.5404 - val_mae: 18.2465 - val_mse: 503.4871\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.9753 - mae: 19.3881 - mse: 612.9220 - val_loss: 499.7571 - val_mae: 18.0416 - val_mse: 499.7033\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.5526 - mae: 19.4963 - mse: 613.4985 - val_loss: 493.4690 - val_mae: 17.8828 - val_mse: 493.4147\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.0770 - mae: 19.1776 - mse: 598.0228 - val_loss: 497.1225 - val_mae: 17.9222 - val_mse: 497.0681\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.1599 - mae: 19.1631 - mse: 604.1053 - val_loss: 495.5492 - val_mae: 17.8792 - val_mse: 495.4945\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.0568 - mae: 19.3420 - mse: 608.0020 - val_loss: 500.0040 - val_mae: 18.0439 - val_mse: 499.9489\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.8804 - mae: 19.1036 - mse: 598.8253 - val_loss: 492.1984 - val_mae: 17.5782 - val_mse: 492.1428\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 609.5414 - mae: 19.3778 - mse: 609.4859 - val_loss: 504.1074 - val_mae: 18.1326 - val_mse: 504.0516\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0741 - mae: 19.1296 - mse: 601.0181 - val_loss: 511.7747 - val_mae: 18.4640 - val_mse: 511.7184\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0271 - mae: 19.1866 - mse: 600.9705 - val_loss: 488.9930 - val_mae: 17.1257 - val_mse: 488.9363\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.0302 - mae: 19.1785 - mse: 604.9736 - val_loss: 508.4807 - val_mae: 18.3530 - val_mse: 508.4238\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.5561 - mae: 19.3024 - mse: 604.4990 - val_loss: 500.9649 - val_mae: 18.0183 - val_mse: 500.9077\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.2015 - mae: 19.2291 - mse: 604.1444 - val_loss: 494.8755 - val_mae: 17.5464 - val_mse: 494.8181\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0103 - mae: 19.1861 - mse: 595.9526 - val_loss: 519.1091 - val_mae: 18.6407 - val_mse: 519.0513\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 597.4541 - mae: 19.2732 - mse: 597.3961 - val_loss: 511.5906 - val_mae: 18.4944 - val_mse: 511.5327\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.9136 - mae: 19.2602 - mse: 597.8554 - val_loss: 496.8972 - val_mae: 17.8510 - val_mse: 496.8387\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.8374 - mae: 19.3442 - mse: 602.7787 - val_loss: 500.0653 - val_mae: 17.9994 - val_mse: 500.0065\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.3226 - mae: 19.0373 - mse: 593.2635 - val_loss: 493.0575 - val_mae: 17.6811 - val_mse: 492.9982\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 589.5729 - mae: 18.9771 - mse: 589.5134 - val_loss: 501.1159 - val_mae: 18.0700 - val_mse: 501.0563\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.2280 - mae: 19.2664 - mse: 599.1681 - val_loss: 490.5354 - val_mae: 17.3484 - val_mse: 490.4753\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_4 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_4.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_4.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    4 deep layers, 128 kernels per layer\n",
      "Train MAE: 16.8372, Train MSE: 476.2779\n",
      "Val   MAE: 17.3484, Val   MSE: 490.4753\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model continues to have the best combination of train/val MAE and limited overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_maxpeak_model = reg_model_maxpeak_nogenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change (regularized, no genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 185.6404 - mae: 9.9279 - mse: 185.6183 - val_loss: 153.0399 - val_mae: 8.8688 - val_mse: 153.0175\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 154.9768 - mae: 9.0846 - mse: 154.9545 - val_loss: 149.9039 - val_mae: 8.7539 - val_mse: 149.8815\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 149.5989 - mae: 8.9075 - mse: 149.5766 - val_loss: 147.7548 - val_mae: 8.7104 - val_mse: 147.7323\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 148.4724 - mae: 8.8419 - mse: 148.4500 - val_loss: 150.3177 - val_mae: 8.7239 - val_mse: 150.2953\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.3181 - mae: 8.8303 - mse: 147.2956 - val_loss: 145.1816 - val_mae: 8.6158 - val_mse: 145.1592\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 146.1712 - mae: 8.7592 - mse: 146.1489 - val_loss: 146.9011 - val_mae: 8.6395 - val_mse: 146.8788\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.6194 - mae: 8.7570 - mse: 144.5971 - val_loss: 144.3659 - val_mae: 8.6022 - val_mse: 144.3436\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5418 - mae: 8.6809 - mse: 142.5195 - val_loss: 145.1015 - val_mae: 8.5977 - val_mse: 145.0792\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.3413 - mae: 8.7341 - mse: 144.3189 - val_loss: 144.2987 - val_mae: 8.5695 - val_mse: 144.2764\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.9597 - mae: 8.6952 - mse: 142.9374 - val_loss: 146.7326 - val_mae: 8.6001 - val_mse: 146.7104\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.3297 - mae: 8.6129 - mse: 141.3076 - val_loss: 145.7192 - val_mae: 8.5828 - val_mse: 145.6970\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.5415 - mae: 8.6021 - mse: 141.5193 - val_loss: 140.4306 - val_mae: 8.5545 - val_mse: 140.4085\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.1872 - mae: 8.6050 - mse: 140.1651 - val_loss: 147.2967 - val_mae: 8.6165 - val_mse: 147.2745\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.3527 - mae: 8.6061 - mse: 140.3304 - val_loss: 142.4047 - val_mae: 8.5292 - val_mse: 142.3825\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5134 - mae: 8.6051 - mse: 140.4913 - val_loss: 141.6037 - val_mae: 8.5277 - val_mse: 141.5816\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.1872 - mae: 8.5976 - mse: 140.1650 - val_loss: 143.9831 - val_mae: 8.5564 - val_mse: 143.9610\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.0159 - mae: 8.5808 - mse: 139.9938 - val_loss: 147.0474 - val_mae: 8.6022 - val_mse: 147.0253\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.4542 - mae: 8.5936 - mse: 140.4320 - val_loss: 144.5712 - val_mae: 8.5610 - val_mse: 144.5491\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.3758 - mae: 8.5102 - mse: 138.3537 - val_loss: 140.7147 - val_mae: 8.5383 - val_mse: 140.6925\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.2290 - mae: 8.5564 - mse: 138.2067 - val_loss: 140.1227 - val_mae: 8.5321 - val_mse: 140.1005\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5728 - mae: 8.5804 - mse: 140.5506 - val_loss: 144.8643 - val_mae: 8.5566 - val_mse: 144.8421\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.0222 - mae: 8.5032 - mse: 138.0000 - val_loss: 141.0152 - val_mae: 8.5142 - val_mse: 140.9929\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.4387 - mae: 8.5008 - mse: 137.4164 - val_loss: 141.1543 - val_mae: 8.5177 - val_mse: 141.1319\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.3318 - mae: 8.5621 - mse: 137.3093 - val_loss: 142.2667 - val_mae: 8.5228 - val_mse: 142.2443\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.0850 - mae: 8.5149 - mse: 137.0624 - val_loss: 139.5884 - val_mae: 8.5447 - val_mse: 139.5658\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1284 - mae: 8.5639 - mse: 138.1058 - val_loss: 141.1020 - val_mae: 8.5190 - val_mse: 141.0794\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7666 - mae: 8.5343 - mse: 138.7439 - val_loss: 140.8071 - val_mae: 8.5149 - val_mse: 140.7844\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.1392 - mae: 8.4919 - mse: 137.1165 - val_loss: 139.4441 - val_mae: 8.5469 - val_mse: 139.4213\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1032 - mae: 8.5429 - mse: 138.0804 - val_loss: 138.8628 - val_mae: 8.5433 - val_mse: 138.8399\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.8159 - mae: 8.5366 - mse: 137.7930 - val_loss: 140.4274 - val_mae: 8.5120 - val_mse: 140.4044\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.2311 - mae: 8.5034 - mse: 137.2081 - val_loss: 140.1375 - val_mae: 8.5201 - val_mse: 140.1145\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.5889 - mae: 8.4956 - mse: 137.5657 - val_loss: 139.8516 - val_mae: 8.5407 - val_mse: 139.8284\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.4123 - mae: 8.5041 - mse: 136.3890 - val_loss: 139.4075 - val_mae: 8.5223 - val_mse: 139.3841\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.7919 - mae: 8.5145 - mse: 137.7685 - val_loss: 141.8356 - val_mae: 8.5142 - val_mse: 141.8123\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.4682 - mae: 8.4565 - mse: 136.4448 - val_loss: 140.4936 - val_mae: 8.5311 - val_mse: 140.4701\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.4084 - mae: 8.4675 - mse: 136.3849 - val_loss: 141.2007 - val_mae: 8.5117 - val_mse: 141.1770\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2537 - mae: 8.4405 - mse: 136.2299 - val_loss: 138.8529 - val_mae: 8.5409 - val_mse: 138.8291\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4799 - mae: 8.4970 - mse: 135.4559 - val_loss: 141.8116 - val_mae: 8.5029 - val_mse: 141.7878\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9371 - mae: 8.4277 - mse: 135.9132 - val_loss: 139.6461 - val_mae: 8.5128 - val_mse: 139.6221\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5671 - mae: 8.4611 - mse: 135.5428 - val_loss: 139.2698 - val_mae: 8.5149 - val_mse: 139.2455\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.1739 - mae: 8.5080 - mse: 136.1497 - val_loss: 139.2148 - val_mae: 8.5417 - val_mse: 139.1904\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5615 - mae: 8.4748 - mse: 136.5371 - val_loss: 138.8166 - val_mae: 8.5368 - val_mse: 138.7921\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1186 - mae: 8.5075 - mse: 135.0941 - val_loss: 139.9830 - val_mae: 8.5254 - val_mse: 139.9584\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.0293 - mae: 8.4761 - mse: 136.0046 - val_loss: 139.7086 - val_mae: 8.5604 - val_mse: 139.6838\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.3847 - mae: 8.4762 - mse: 135.3598 - val_loss: 141.9727 - val_mae: 8.5047 - val_mse: 141.9478\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9017 - mae: 8.4905 - mse: 135.8768 - val_loss: 139.0233 - val_mae: 8.5729 - val_mse: 138.9981\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6533 - mae: 8.4476 - mse: 134.6281 - val_loss: 140.3326 - val_mae: 8.5431 - val_mse: 140.3073\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1310 - mae: 8.4362 - mse: 135.1056 - val_loss: 139.8172 - val_mae: 8.5506 - val_mse: 139.7917\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2646 - mae: 8.4847 - mse: 136.2391 - val_loss: 140.1552 - val_mae: 8.5542 - val_mse: 140.1295\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.6565 - mae: 8.4702 - mse: 135.6308 - val_loss: 140.9275 - val_mae: 8.5268 - val_mse: 140.9017\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3149 - mae: 8.4684 - mse: 134.2890 - val_loss: 141.0142 - val_mae: 8.5382 - val_mse: 140.9882\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.0630 - mae: 8.4872 - mse: 135.0370 - val_loss: 142.3656 - val_mae: 8.5106 - val_mse: 142.3396\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5713 - mae: 8.4673 - mse: 135.5451 - val_loss: 139.7197 - val_mae: 8.5305 - val_mse: 139.6936\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.1850 - mae: 8.4374 - mse: 134.1588 - val_loss: 139.9125 - val_mae: 8.5183 - val_mse: 139.8862\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.5234 - mae: 8.4796 - mse: 134.4970 - val_loss: 139.5965 - val_mae: 8.5400 - val_mse: 139.5700\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0437 - mae: 8.4471 - mse: 134.0170 - val_loss: 139.7214 - val_mae: 8.5424 - val_mse: 139.6947\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0918 - mae: 8.3616 - mse: 132.0649 - val_loss: 141.0916 - val_mae: 8.5161 - val_mse: 141.0647\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0291 - mae: 8.4365 - mse: 134.0020 - val_loss: 139.9357 - val_mae: 8.5430 - val_mse: 139.9086\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6928 - mae: 8.4193 - mse: 133.6654 - val_loss: 141.0993 - val_mae: 8.5114 - val_mse: 141.0719\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3661 - mae: 8.4102 - mse: 133.3385 - val_loss: 141.4367 - val_mae: 8.5090 - val_mse: 141.4090\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3040 - mae: 8.3966 - mse: 134.2761 - val_loss: 139.3528 - val_mae: 8.5394 - val_mse: 139.3248\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.2141 - mae: 8.4942 - mse: 135.1861 - val_loss: 142.3911 - val_mae: 8.5116 - val_mse: 142.3631\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.4249 - mae: 8.3706 - mse: 131.3967 - val_loss: 140.1526 - val_mae: 8.5340 - val_mse: 140.1242\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.5224 - mae: 8.4469 - mse: 134.4939 - val_loss: 139.6628 - val_mae: 8.5317 - val_mse: 139.6341\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.2102 - mae: 8.3879 - mse: 131.1814 - val_loss: 140.3748 - val_mae: 8.5344 - val_mse: 140.3459\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3113 - mae: 8.4649 - mse: 133.2822 - val_loss: 141.3019 - val_mae: 8.5070 - val_mse: 141.2726\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5851 - mae: 8.3777 - mse: 132.5558 - val_loss: 140.8809 - val_mae: 8.5427 - val_mse: 140.8514\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0728 - mae: 8.3976 - mse: 131.0432 - val_loss: 141.1810 - val_mae: 8.5354 - val_mse: 141.1512\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5413 - mae: 8.4215 - mse: 132.5116 - val_loss: 139.3936 - val_mae: 8.5528 - val_mse: 139.3636\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.8450 - mae: 8.4131 - mse: 132.8149 - val_loss: 140.4918 - val_mae: 8.5381 - val_mse: 140.4616\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0323 - mae: 8.4411 - mse: 132.0021 - val_loss: 140.6239 - val_mae: 8.5193 - val_mse: 140.5934\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5534 - mae: 8.4047 - mse: 132.5227 - val_loss: 142.1323 - val_mae: 8.5230 - val_mse: 142.1017\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.2930 - mae: 8.3214 - mse: 131.2621 - val_loss: 141.0707 - val_mae: 8.5368 - val_mse: 141.0397\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.6532 - mae: 8.3885 - mse: 132.6220 - val_loss: 140.9773 - val_mae: 8.5364 - val_mse: 140.9460\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.6695 - mae: 8.3549 - mse: 131.6381 - val_loss: 141.2665 - val_mae: 8.5173 - val_mse: 141.2349\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0108 - mae: 8.3569 - mse: 131.9792 - val_loss: 140.7110 - val_mae: 8.5657 - val_mse: 140.6790\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.9874 - mae: 8.3808 - mse: 131.9555 - val_loss: 141.2395 - val_mae: 8.5235 - val_mse: 141.2073\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.1205 - mae: 8.3877 - mse: 131.0882 - val_loss: 139.7785 - val_mae: 8.5629 - val_mse: 139.7460\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0206 - mae: 8.3856 - mse: 129.9882 - val_loss: 140.6365 - val_mae: 8.5506 - val_mse: 140.6038\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.6348 - mae: 8.3941 - mse: 130.6019 - val_loss: 141.9392 - val_mae: 8.5278 - val_mse: 141.9062\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9631 - mae: 8.3449 - mse: 129.9300 - val_loss: 141.1113 - val_mae: 8.5177 - val_mse: 141.0780\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0258 - mae: 8.3632 - mse: 129.9923 - val_loss: 141.6522 - val_mae: 8.5292 - val_mse: 141.6185\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8562 - mae: 8.3276 - mse: 129.8224 - val_loss: 140.5197 - val_mae: 8.5239 - val_mse: 140.4856\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0236 - mae: 8.3668 - mse: 129.9893 - val_loss: 141.2959 - val_mae: 8.5350 - val_mse: 141.2617\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.2784 - mae: 8.3357 - mse: 129.2439 - val_loss: 142.6967 - val_mae: 8.5438 - val_mse: 142.6622\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0056 - mae: 8.3316 - mse: 129.9711 - val_loss: 140.2737 - val_mae: 8.5505 - val_mse: 140.2389\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0963 - mae: 8.3527 - mse: 130.0613 - val_loss: 140.7341 - val_mae: 8.5247 - val_mse: 140.6991\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.3027 - mae: 8.3517 - mse: 129.2675 - val_loss: 140.0058 - val_mae: 8.5361 - val_mse: 139.9704\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8526 - mae: 8.3593 - mse: 129.8173 - val_loss: 140.4399 - val_mae: 8.5344 - val_mse: 140.4043\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7123 - mae: 8.3107 - mse: 128.6767 - val_loss: 140.7914 - val_mae: 8.5447 - val_mse: 140.7556\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8034 - mae: 8.3495 - mse: 129.7675 - val_loss: 141.1859 - val_mae: 8.5150 - val_mse: 141.1499\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.5850 - mae: 8.2940 - mse: 127.5488 - val_loss: 140.6683 - val_mae: 8.5271 - val_mse: 140.6319\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3728 - mae: 8.3511 - mse: 128.3361 - val_loss: 142.0614 - val_mae: 8.5248 - val_mse: 142.0246\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4046 - mae: 8.3369 - mse: 128.3675 - val_loss: 142.1432 - val_mae: 8.5241 - val_mse: 142.1059\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.7517 - mae: 8.3114 - mse: 127.7141 - val_loss: 142.1891 - val_mae: 8.5324 - val_mse: 142.1515\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.0766 - mae: 8.2900 - mse: 128.0390 - val_loss: 140.8091 - val_mae: 8.5365 - val_mse: 140.7713\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.0006 - mae: 8.2918 - mse: 127.9627 - val_loss: 141.9430 - val_mae: 8.5441 - val_mse: 141.9049\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.5049 - mae: 8.2782 - mse: 128.4667 - val_loss: 140.7656 - val_mae: 8.5828 - val_mse: 140.7271\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2908 - mae: 8.3149 - mse: 128.2522 - val_loss: 140.5533 - val_mae: 8.5605 - val_mse: 140.5144\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.8713 - mae: 8.2875 - mse: 128.8325 - val_loss: 141.6661 - val_mae: 8.5604 - val_mse: 141.6273\n"
     ]
    }
   ],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_1.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding a layer\n",
      "Train MAE: 8.0288, Train MSE: 123.9662\n",
      "Val   MAE: 8.5604, Val   MSE: 141.6273\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very nearly identical to the initial configurations. Doubling the kernels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 167.2343 - mae: 9.3601 - mse: 167.1924 - val_loss: 142.3192 - val_mae: 8.6937 - val_mse: 142.2769\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.9986 - mae: 8.8403 - mse: 146.9561 - val_loss: 143.6896 - val_mae: 8.6442 - val_mse: 143.6469\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.8293 - mae: 8.8142 - mse: 146.7865 - val_loss: 142.2674 - val_mae: 8.6695 - val_mse: 142.2244\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.7292 - mae: 8.7351 - mse: 142.6861 - val_loss: 148.5261 - val_mae: 8.6831 - val_mse: 148.4829\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 142.2989 - mae: 8.7025 - mse: 142.2555 - val_loss: 145.1137 - val_mae: 8.6189 - val_mse: 145.0701\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 143.5256 - mae: 8.7281 - mse: 143.4818 - val_loss: 145.0394 - val_mae: 8.5886 - val_mse: 144.9955\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5378 - mae: 8.6921 - mse: 142.4936 - val_loss: 148.0239 - val_mae: 8.6391 - val_mse: 147.9796\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.7583 - mae: 8.6883 - mse: 141.7138 - val_loss: 146.7876 - val_mae: 8.6268 - val_mse: 146.7430\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.8806 - mae: 8.5904 - mse: 139.8357 - val_loss: 141.9741 - val_mae: 8.5532 - val_mse: 141.9289\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.4467 - mae: 8.5163 - mse: 138.4015 - val_loss: 142.3516 - val_mae: 8.5186 - val_mse: 142.3063\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.6581 - mae: 8.5976 - mse: 138.6126 - val_loss: 143.3703 - val_mae: 8.5701 - val_mse: 143.3246\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9484 - mae: 8.5278 - mse: 137.9025 - val_loss: 143.4153 - val_mae: 8.5852 - val_mse: 143.3693\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 138.1365 - mae: 8.5443 - mse: 138.0903 - val_loss: 144.6257 - val_mae: 8.5770 - val_mse: 144.5794\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.1040 - mae: 8.5595 - mse: 139.0575 - val_loss: 141.5736 - val_mae: 8.5273 - val_mse: 141.5269\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.8542 - mae: 8.4963 - mse: 135.8073 - val_loss: 142.2528 - val_mae: 8.5657 - val_mse: 142.2057\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9015 - mae: 8.5317 - mse: 137.8542 - val_loss: 143.4198 - val_mae: 8.5549 - val_mse: 143.3724\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.7767 - mae: 8.5380 - mse: 137.7291 - val_loss: 141.6530 - val_mae: 8.5109 - val_mse: 141.6052\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6791 - mae: 8.5568 - mse: 136.6310 - val_loss: 142.7033 - val_mae: 8.5489 - val_mse: 142.6550\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5706 - mae: 8.4977 - mse: 136.5221 - val_loss: 140.1360 - val_mae: 8.5392 - val_mse: 140.0872\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 134.9641 - mae: 8.4559 - mse: 134.9152 - val_loss: 139.2952 - val_mae: 8.5437 - val_mse: 139.2460\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7619 - mae: 8.5309 - mse: 135.7125 - val_loss: 139.8416 - val_mae: 8.5255 - val_mse: 139.7920\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4162 - mae: 8.4779 - mse: 135.3664 - val_loss: 144.0311 - val_mae: 8.5329 - val_mse: 143.9810\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7253 - mae: 8.4024 - mse: 135.6751 - val_loss: 137.8393 - val_mae: 8.5547 - val_mse: 137.7887\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.9588 - mae: 8.4673 - mse: 134.9080 - val_loss: 139.6904 - val_mae: 8.5463 - val_mse: 139.6392\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9683 - mae: 8.4899 - mse: 135.9169 - val_loss: 139.4721 - val_mae: 8.5090 - val_mse: 139.4205\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.4305 - mae: 8.4107 - mse: 133.3785 - val_loss: 141.4364 - val_mae: 8.5308 - val_mse: 141.3842\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.0132 - mae: 8.4213 - mse: 132.9607 - val_loss: 143.8673 - val_mae: 8.5166 - val_mse: 143.8147\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 134.8278 - mae: 8.4693 - mse: 134.7749 - val_loss: 141.6443 - val_mae: 8.5036 - val_mse: 141.5913\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.8729 - mae: 8.3675 - mse: 132.8193 - val_loss: 139.0052 - val_mae: 8.5789 - val_mse: 138.9514\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3246 - mae: 8.4146 - mse: 133.2705 - val_loss: 139.8519 - val_mae: 8.5917 - val_mse: 139.7974\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3360 - mae: 8.4220 - mse: 132.2812 - val_loss: 140.3040 - val_mae: 8.5418 - val_mse: 140.2487\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.8846 - mae: 8.4455 - mse: 133.8291 - val_loss: 141.3894 - val_mae: 8.5181 - val_mse: 141.3336\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.8174 - mae: 8.4420 - mse: 132.7612 - val_loss: 143.1489 - val_mae: 8.4935 - val_mse: 143.0927\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7212 - mae: 8.3805 - mse: 131.6645 - val_loss: 142.6922 - val_mae: 8.5238 - val_mse: 142.6352\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.4975 - mae: 8.3838 - mse: 132.4403 - val_loss: 143.6801 - val_mae: 8.5531 - val_mse: 143.6228\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 130.9808 - mae: 8.4080 - mse: 130.9229 - val_loss: 143.4050 - val_mae: 8.5860 - val_mse: 143.3468\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0766 - mae: 8.3692 - mse: 131.0181 - val_loss: 142.1825 - val_mae: 8.5578 - val_mse: 142.1237\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.9281 - mae: 8.3781 - mse: 131.8689 - val_loss: 144.2332 - val_mae: 8.5767 - val_mse: 144.1739\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.4735 - mae: 8.3273 - mse: 129.4137 - val_loss: 142.1961 - val_mae: 8.5680 - val_mse: 142.1360\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5219 - mae: 8.3848 - mse: 131.4615 - val_loss: 142.1891 - val_mae: 8.5514 - val_mse: 142.1282\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0459 - mae: 8.3502 - mse: 129.9849 - val_loss: 143.1303 - val_mae: 8.5454 - val_mse: 143.0690\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9299 - mae: 8.3293 - mse: 127.8679 - val_loss: 142.3903 - val_mae: 8.5558 - val_mse: 142.3279\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6933 - mae: 8.3736 - mse: 129.6305 - val_loss: 144.4853 - val_mae: 8.5685 - val_mse: 144.4222\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 130.2331 - mae: 8.3830 - mse: 130.1696 - val_loss: 143.3303 - val_mae: 8.5560 - val_mse: 143.2665\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3790 - mae: 8.3095 - mse: 128.3148 - val_loss: 141.1558 - val_mae: 8.5748 - val_mse: 141.0911\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.8945 - mae: 8.2859 - mse: 125.8292 - val_loss: 148.7896 - val_mae: 8.6283 - val_mse: 148.7239\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.2449 - mae: 8.2599 - mse: 127.1786 - val_loss: 142.7781 - val_mae: 8.5782 - val_mse: 142.7115\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3915 - mae: 8.3311 - mse: 128.3246 - val_loss: 142.1332 - val_mae: 8.5774 - val_mse: 142.0658\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.8932 - mae: 8.2957 - mse: 126.8253 - val_loss: 142.5808 - val_mae: 8.6131 - val_mse: 142.5125\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4585 - mae: 8.3014 - mse: 127.3897 - val_loss: 142.0307 - val_mae: 8.5772 - val_mse: 141.9615\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 125.3658 - mae: 8.2324 - mse: 125.2961 - val_loss: 142.2643 - val_mae: 8.5864 - val_mse: 142.1942\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.4737 - mae: 8.2313 - mse: 126.4031 - val_loss: 141.6476 - val_mae: 8.5615 - val_mse: 141.5765\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.0957 - mae: 8.2242 - mse: 125.0241 - val_loss: 144.8544 - val_mae: 8.5502 - val_mse: 144.7825\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.5320 - mae: 8.2880 - mse: 127.4597 - val_loss: 142.9434 - val_mae: 8.5553 - val_mse: 142.8706\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1300 - mae: 8.1972 - mse: 124.0567 - val_loss: 141.6886 - val_mae: 8.5606 - val_mse: 141.6147\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.3518 - mae: 8.2462 - mse: 125.2774 - val_loss: 143.1861 - val_mae: 8.5661 - val_mse: 143.1111\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.0992 - mae: 8.2291 - mse: 124.0236 - val_loss: 142.3804 - val_mae: 8.5474 - val_mse: 142.3043\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 123.9983 - mae: 8.2476 - mse: 123.9217 - val_loss: 147.0419 - val_mae: 8.5826 - val_mse: 146.9649\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 125.2617 - mae: 8.2452 - mse: 125.1843 - val_loss: 142.9872 - val_mae: 8.5335 - val_mse: 142.9093\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 124.1462 - mae: 8.2328 - mse: 124.0676 - val_loss: 143.7397 - val_mae: 8.5639 - val_mse: 143.6607\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.3945 - mae: 8.2399 - mse: 124.3150 - val_loss: 143.1200 - val_mae: 8.5599 - val_mse: 143.0401\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.5267 - mae: 8.2182 - mse: 124.4465 - val_loss: 144.3668 - val_mae: 8.5402 - val_mse: 144.2862\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.4436 - mae: 8.2121 - mse: 124.3625 - val_loss: 144.6765 - val_mae: 8.5492 - val_mse: 144.5949\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 124.0374 - mae: 8.2037 - mse: 123.9556 - val_loss: 142.8904 - val_mae: 8.5322 - val_mse: 142.8083\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.1619 - mae: 8.1572 - mse: 121.0793 - val_loss: 141.5815 - val_mae: 8.6053 - val_mse: 141.4982\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.2630 - mae: 8.1516 - mse: 120.1794 - val_loss: 140.6527 - val_mae: 8.6211 - val_mse: 140.5681\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6034 - mae: 8.1723 - mse: 120.5185 - val_loss: 144.6385 - val_mae: 8.5692 - val_mse: 144.5532\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9402 - mae: 8.1227 - mse: 120.8546 - val_loss: 143.4853 - val_mae: 8.5932 - val_mse: 143.3990\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6059 - mae: 8.1439 - mse: 120.5192 - val_loss: 143.5187 - val_mae: 8.5557 - val_mse: 143.4315\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.9470 - mae: 8.0636 - mse: 117.8592 - val_loss: 144.4667 - val_mae: 8.6474 - val_mse: 144.3784\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.9726 - mae: 8.1714 - mse: 121.8844 - val_loss: 142.7219 - val_mae: 8.5928 - val_mse: 142.6335\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 120.1376 - mae: 8.1479 - mse: 120.0485 - val_loss: 144.1719 - val_mae: 8.5664 - val_mse: 144.0824\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2621 - mae: 8.0513 - mse: 118.1720 - val_loss: 145.5330 - val_mae: 8.5838 - val_mse: 145.4428\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.7539 - mae: 8.1283 - mse: 121.6630 - val_loss: 145.5918 - val_mae: 8.5583 - val_mse: 145.5007\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.5285 - mae: 8.1143 - mse: 120.4370 - val_loss: 143.1920 - val_mae: 8.5901 - val_mse: 143.0999\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.1266 - mae: 8.1329 - mse: 119.0342 - val_loss: 142.7547 - val_mae: 8.5803 - val_mse: 142.6619\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.6071 - mae: 8.1491 - mse: 119.5139 - val_loss: 143.6204 - val_mae: 8.5820 - val_mse: 143.5264\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.4226 - mae: 8.1369 - mse: 119.3280 - val_loss: 146.5015 - val_mae: 8.5855 - val_mse: 146.4067\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 117.6746 - mae: 8.0619 - mse: 117.5791 - val_loss: 145.6877 - val_mae: 8.5810 - val_mse: 145.5917\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4643 - mae: 8.0976 - mse: 118.3677 - val_loss: 145.3576 - val_mae: 8.6285 - val_mse: 145.2605\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 116.1383 - mae: 8.0324 - mse: 116.0407 - val_loss: 147.1670 - val_mae: 8.6217 - val_mse: 147.0690\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 120.1077 - mae: 8.1593 - mse: 120.0093 - val_loss: 144.7285 - val_mae: 8.6745 - val_mse: 144.6296\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.9701 - mae: 8.0076 - mse: 115.8707 - val_loss: 145.2923 - val_mae: 8.6285 - val_mse: 145.1924\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.7361 - mae: 8.0149 - mse: 115.6357 - val_loss: 146.9341 - val_mae: 8.6500 - val_mse: 146.8333\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 115.9289 - mae: 8.0171 - mse: 115.8276 - val_loss: 145.7986 - val_mae: 8.6626 - val_mse: 145.6969\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 117.7450 - mae: 8.0315 - mse: 117.6426 - val_loss: 144.6985 - val_mae: 8.6306 - val_mse: 144.5953\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.9291 - mae: 8.0375 - mse: 117.8258 - val_loss: 144.3044 - val_mae: 8.6536 - val_mse: 144.2004\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.9409 - mae: 7.9505 - mse: 113.8365 - val_loss: 145.9460 - val_mae: 8.6339 - val_mse: 145.8409\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.6720 - mae: 7.9751 - mse: 114.5663 - val_loss: 146.9050 - val_mae: 8.6889 - val_mse: 146.7985\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.3056 - mae: 7.9143 - mse: 113.1987 - val_loss: 147.6449 - val_mae: 8.6230 - val_mse: 147.5374\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 113.9317 - mae: 7.9802 - mse: 113.8237 - val_loss: 146.0756 - val_mae: 8.5843 - val_mse: 145.9674\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.9066 - mae: 7.9147 - mse: 113.7979 - val_loss: 146.6966 - val_mae: 8.6101 - val_mse: 146.5875\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1304 - mae: 7.8935 - mse: 113.0206 - val_loss: 147.4060 - val_mae: 8.6228 - val_mse: 147.2956\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1933 - mae: 7.9547 - mse: 113.0825 - val_loss: 146.3798 - val_mae: 8.6741 - val_mse: 146.2683\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 114.4721 - mae: 7.9588 - mse: 114.3603 - val_loss: 145.5726 - val_mae: 8.6007 - val_mse: 145.4605\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 115.3004 - mae: 7.9820 - mse: 115.1878 - val_loss: 146.2556 - val_mae: 8.6051 - val_mse: 146.1426\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 112.6000 - mae: 7.9371 - mse: 112.4864 - val_loss: 147.5810 - val_mae: 8.6040 - val_mse: 147.4670\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.9418 - mae: 7.9294 - mse: 112.8273 - val_loss: 146.5905 - val_mae: 8.6325 - val_mse: 146.4753\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1401 - mae: 7.9114 - mse: 113.0246 - val_loss: 148.1808 - val_mae: 8.6684 - val_mse: 148.0648\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.5969 - mae: 7.9860 - mse: 117.4809 - val_loss: 145.3819 - val_mae: 8.5919 - val_mse: 145.2657\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_2.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      " 4 deep layers, 128 kernels per layer\n",
      "Train MAE: 8.0288, Train MSE: 123.9662\n",
      "Val   MAE: 8.5604, Val   MSE: 141.6273\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\" 4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model has the best MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_maxrankchange_model = reg_model_maxrank_nogenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "The XGBoost model did not return meaningful results for this dataset. The highest r<sup>2</sup> value XGB achieved was 0.271, not high enough to indicate that the model strongly fits the underlying data.\n",
    "\n",
    " The best metrics for XGBoost were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- RMSE: 21.607\n",
    "- r<sup>2</sup>: 0.271\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- RMSE: 11.764\n",
    "- r<sup>2</sup>: 0.045\n",
    "\n",
    "**k-Nearest Neighbors**\n",
    "\n",
    "This model also did not perform well on this data set--its accuracy was not above 22% in any scenario.\n",
    "\n",
    "Its best metrics were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- Accuracy: 0.035\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- Accuracy: 0.217\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "The deep learning model returned the most promising results and are described in the next section.\n",
    "\n",
    "### Final Models\n",
    "\n",
    "The final models are both based on a deep learning architecture. \n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "The best model for the Maximum Peak Position is a 3-layer, regularized model with 64 kernels per layer. It was trained a song dataset with genre information.\n",
    "\n",
    "The final metrics for the best model were:\n",
    "\n",
    "- Training mean absolute error: 14.84\n",
    "- Validation mean absolute error: 16.91\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "The best model for the Maximum Rank Increase is named reg_model4_2. It is also a 3-layer, regularized model trained on the song dataset without genre information.\n",
    "Its final metrics:\n",
    "\n",
    "- Training mean absolute error: 7.96\n",
    "- Validation mean absolute error: 8.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for maximum peak position provides a reasonable estimate of the highest position a song will reach on the Billboard Hot 100, giving FutureProduct Advisors consultants an easy way to predict a song's popularity.\n",
    "\n",
    "The mean absolute error (MAE) of the model is 17, which is less accurate than the initial target of 10. However, this MAE still means that the model will typically predict whether a song ends up in the top third, middle third, or bottom third of the Hot 100 list. This gives enough information about the predicted popularity to help FPA consultants estimate if the song will become more expensive to license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for maximum rank change gives FutureProduct Advisors consultants a way to estimate whether a song's popularity is likely to grow more or less quickly in the future: \n",
    "\n",
    "- If the estimated maximum rank change is greater than the largest rank change to date, the song is likely to increase in popularity in the future\n",
    "- If a song's historical maximum rank change is greater than the estimated maximum rank change, the song's popularity has either peaked or its increase is slowing.\n",
    "\n",
    "The mean absolute error of the model's predictions is 9. Since the mean value of the maximum rank change for the songs in the dataset is 13, this is notably above the target accuracy of 90% (at 90% accuracy the MAE should be less than 2). This means that estimates using this metric are less predictive than the maximum peak position.\n",
    "\n",
    "To account for this uncertainty, FPA consultants should be cautious in drawing conclusions based on the predicted values of maximum rank change, and should not base critical decisions on this metric alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has demonstrated the difficulty of predicting the popularity of music. Even though I started with a dataset of more than 20 features (and I engineered more), I was not able to create models that predicted the maximum position and maximum rank change with 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The models to predict the highest ranking on the Billboard Hot 100 and to predict largest week over week ranking increase are ready for initial deployment to FutureProduct Advisors. \n",
    "\n",
    "The high-level deployment plan would be as follows:\n",
    "\n",
    "1. I and my colleagues will build a data pipeline to ingest the Billboard Hot 100 list from Spotify each week and these two models will be run on that data\n",
    "2. Each week's predictions will be published to the FutureProduct Advisors internal message board for use by their consultants.\n",
    "3. We will provide a 30 minute training to orient the FPA consultants to the models and how to use their outputs.\n",
    "\n",
    "There is also an additional opportunity to built on and refine these models by indluding additional data and experimenting with other modeling approaches. If the FutureProduct Advisors consultants have positive feedback on these prototype tools, we will be happy to partner on future enhancements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
