{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project seeks to build machine learning models to achieve two things:\n",
    "\n",
    "1. Predict the highest ranking a song will achieve on the Billboard Hot 100 list.\n",
    "2. Predict the largest week over week increase in a given song's ranking on the Hot 100 list.\n",
    "\n",
    "Key Insights:\n",
    "\n",
    "- A song's genre has a minimal impact on its ranking on the Billboard Hot 100 list.\n",
    "- Song characteristics such as tempo, danceability, etc. appear to be the strongest drivers of their performance on the Hot 100 list.\n",
    "- Analyzing music can quickly lead to an overwhelming number of features, so thoughtful and careful data selection is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer of this project is FutureProduct Advisors, a consultancy that helps their customers develop innovative and new consumer products. FutureProduct’s customers are increasingly seeking help from their consultants in go-to-market activities. \n",
    "\n",
    "FutureProduct’s consultants can support these go-to-market activities, but the business does not have all the infrastructure needed to support it. Their biggest ask is for a tool to help them find interesting, up-and-coming music to accompany social posts and online ads for go-to-market promotions. \n",
    "\n",
    "**Stakeholders**\n",
    "\n",
    "- FutureProduct Managing Director: oversees their consulting practice and is sponsoring this project.\n",
    "- FutureProduct Senior Consultants: the actual users of the prospective tool. A small subset of the consultants will pilot the prototype tool.\n",
    "- My consulting leadership: sponsors of this effort; will provide oversight and technical input of the project as needed.\n",
    "\n",
    "**Primary Goals**\n",
    "\n",
    "1.\tBuild a data tool that can evaluate any song in the Billboard Hot 100 list and make predictions about:\n",
    "    -\tThe song’s position on the Hot 100 list 4 weeks in the future\n",
    "    -\tThe song’s highest position on the list in the next 6 months\n",
    "2.\tCreate a rubric that lists the 3 most important factors for songs’ placement on the Hot 100 list for each hear from 2000 to 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Billboard Hot 100 weekly charts (Kaggle): https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n",
    "\n",
    "I’ve chosen this dataset because it has a direct measurement of song popularity (the Hot 100 list) and because its long history gives significant context to a song’s positioning in a given week.\n",
    "The features list gives a wide range of song attributes to explore and enables me to determine what features most significantly contribute to a song’s popularity and how that changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mean_squared_error, r2_score, pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hotlist_all = pd.read_csv('Data/Hot Stuff.csv')\n",
    "df_features_all = pd.read_csv('Data/Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring hotlist df\n",
    "df_hotlist_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring features df\n",
    "df_features_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveness_dist = df_cleaned_genre['liveness'].value_counts()\n",
    "df_liveness_dist = pd.DataFrame(liveness_dist)\n",
    "df_liveness_dist = df_liveness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_liveness_dist['liveness'], df_liveness_dist['count'], color='orange')\n",
    "plt.xlabel('Liveness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Liveness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "danceability_dist = df_cleaned_genre['danceability'].value_counts()\n",
    "df_danceability_dist = pd.DataFrame(danceability_dist)\n",
    "df_danceability_dist = df_danceability_dist.reset_index()\n",
    "\n",
    "plt.bar(df_danceability_dist['danceability'], df_danceability_dist['count'], color='skyblue')\n",
    "plt.xlabel('Danceability Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Danceability Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acousticness_dist = df_cleaned_genre['acousticness'].value_counts()\n",
    "df_acousticness_dist = pd.DataFrame(acousticness_dist)\n",
    "df_acousticness_dist = df_acousticness_dist.reset_index()\n",
    "\n",
    "plt.bar(df_acousticness_dist['acousticness'], df_acousticness_dist['count'], color='orange')\n",
    "plt.xlabel('Acousticness Rating')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Distribution of Acousticness Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_pos_dist = df_cleaned_genre['Max_Peak_Position'].value_counts()\n",
    "df_peak_pos_dist = pd.DataFrame(peak_pos_dist)\n",
    "df_peak_pos_dist = df_peak_pos_dist.reset_index()\n",
    "print(f\"Mean Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Highest Ranking: {df_cleaned_genre['Max_Peak_Position'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_peak_pos_dist['Max_Peak_Position'], df_peak_pos_dist['count'], color='skyblue')\n",
    "plt.xlabel('Highest Ranking')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Peak Rankings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank_change = df_cleaned_genre['Max_Rank_Change'].value_counts()\n",
    "df_max_rank_change = pd.DataFrame(max_rank_change)\n",
    "df_max_rank_change = df_max_rank_change.reset_index()\n",
    "df_max_rank_change = df_max_rank_change[df_max_rank_change['Max_Rank_Change'] > 0]\n",
    "print(f\"Mean Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].mean():.0f}\")\n",
    "print(f\"Standard Deviation of Largest Week over Week Rank Change: {df_cleaned_genre['Max_Rank_Change'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_max_rank_change['Max_Rank_Change'], df_max_rank_change['count'], color='orange')\n",
    "plt.xlabel('Largest Week over Week Rank Change')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Frequency of Largest Week over Week Rank Change')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Initial Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_18008\\3177740419.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
      "C:\\Users\\marha\\AppData\\Local\\Temp\\ipykernel_18008\\3177740419.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# removing hotlist df attributes that will not be used in cleaning or analysis\n",
    "df_hotlist_all = df_hotlist_all.drop(['index', 'url', 'Song', 'Performer', 'Instance'], axis=1)\n",
    "# converting WeekID to datetime\n",
    "df_hotlist_all['WeekID'] = pd.to_datetime(df_hotlist_all['WeekID'], errors='coerce')\n",
    "df_hotlist_all = df_hotlist_all.sort_values(by='WeekID')\n",
    "\n",
    "# creating a new hotlist df with only complete year data from 2000 - 2020, the time period being studied\n",
    "df_hotlist_2000s = df_hotlist_all.loc[(df_hotlist_all['WeekID'] > '1999-12-31') & (df_hotlist_all['WeekID'] < '2021-01-01')]\n",
    "\n",
    "# adding a column to calculate the week over week change in rank\n",
    "def diff(a, b):\n",
    "    return a - b\n",
    "\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s.apply(lambda x: diff(x['Week Position'], x['Previous Week Position']), axis=1)\n",
    "# replacing NaNs with 0\n",
    "df_hotlist_2000s['Rank_Change'] = df_hotlist_2000s['Rank_Change'].fillna(0)\n",
    "\n",
    "# removing features df attributes that will not be used in cleaning or analysis\n",
    "df_features_all = df_features_all.drop(['index', 'Performer', 'Song', 'spotify_track_album', \n",
    "                                        'spotify_track_id', 'spotify_track_preview_url',  \n",
    "                                        'spotify_track_explicit', 'spotify_track_popularity'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112098 entries, 0 to 112097\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   WeekID                     112098 non-null  datetime64[ns]\n",
      " 1   Week Position              112098 non-null  int64         \n",
      " 2   SongID                     112098 non-null  object        \n",
      " 3   Previous Week Position     101571 non-null  float64       \n",
      " 4   Peak Position              112098 non-null  int64         \n",
      " 5   Weeks on Chart             112098 non-null  int64         \n",
      " 6   Rank_Change                112098 non-null  float64       \n",
      " 7   spotify_genre              108091 non-null  object        \n",
      " 8   spotify_track_duration_ms  104590 non-null  float64       \n",
      " 9   danceability               104287 non-null  float64       \n",
      " 10  energy                     104287 non-null  float64       \n",
      " 11  key                        104287 non-null  float64       \n",
      " 12  loudness                   104287 non-null  float64       \n",
      " 13  mode                       104287 non-null  float64       \n",
      " 14  speechiness                104287 non-null  float64       \n",
      " 15  acousticness               104287 non-null  float64       \n",
      " 16  instrumentalness           104287 non-null  float64       \n",
      " 17  liveness                   104287 non-null  float64       \n",
      " 18  valence                    104287 non-null  float64       \n",
      " 19  tempo                      104287 non-null  float64       \n",
      " 20  time_signature             104287 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(3), object(2)\n",
      "memory usage: 18.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# combining the hotlist and features into one dataframe\n",
    "\n",
    "df_hotlist_and_features_2000s = pd.merge(df_hotlist_2000s, df_features_all, on='SongID', how='left')\n",
    "df_hotlist_and_features_2000s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset has genre in a single column and the entry for each song has a variety of genres listed in that single column. \n",
    "This does not allow me to explore genre in a systematic way.\n",
    "I'll need to break genre out so that each genre has its own column with a 1 or 0 to indicate whether each song is tagged with that genre \n",
    "(ending with a one-hot encoded structure).\n",
    "\"\"\"\n",
    "\n",
    "# generating a df with unique genre names\n",
    "unique_genres = list(set(\n",
    "    genre \n",
    "    for genre_string in df_hotlist_and_features_2000s['spotify_genre'] \n",
    "    if pd.notna(genre_string)\n",
    "    for genre in ast.literal_eval(genre_string)\n",
    "))\n",
    "\n",
    "df_unique_genres = pd.DataFrame(unique_genres, columns=['genre'])\n",
    "\n",
    "# adding counts of each unique genre name\n",
    "# Extract all genres (with duplicates) and count them\n",
    "all_genres_list = []\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre']:\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        all_genres_list.extend(genre_list)\n",
    "\n",
    "# Count occurrences\n",
    "genre_counts = Counter(all_genres_list)\n",
    "\n",
    "# Map counts to genres dataframe\n",
    "df_unique_genres['count'] = df_unique_genres['genre'].map(genre_counts)\n",
    "df_unique_genres = df_unique_genres.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to csv for easier review of the data\n",
    "df_unique_genres.to_csv('genre_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the full set of genre counts, I'm only including genres that appear in 100 or more songs (i.e. at least 0.1% of songs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading list of genres with 100 or more instances in df_cleaned\n",
    "df_genres_100_up = pd.read_csv('genre_counts_100+inst.csv')\n",
    "\n",
    "# converting df to list\n",
    "final_genres_list = df_genres_100_up['genre'].tolist()\n",
    "\n",
    "# manually one-hot encoding each genre\n",
    "\n",
    "# creating a list of genres and counts\n",
    "genre_data = []\n",
    "\n",
    "for genre_string in df_hotlist_and_features_2000s['spotify_genre'] :\n",
    "    if pd.notna(genre_string):\n",
    "        genre_list = ast.literal_eval(genre_string)\n",
    "        row_dict = {genre: (1 if genre in genre_list else 0) for genre in final_genres_list} # dict with 1 if genre exists in list, 0 if not\n",
    "    else:\n",
    "         row_dict = {genre: 0 for genre in final_genres_list} # 0 of genre does not exist in list\n",
    "    genre_data.append(row_dict)\n",
    "\n",
    "# creating a df with the list of dicts\n",
    "genre_df = pd.DataFrame(genre_data)\n",
    "\n",
    "# concatenating genre data into df_clean\n",
    "df_hotlist_and_features_2000s = pd.concat([df_hotlist_and_features_2000s, genre_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  \n",
       "0           0                 0           0  \n",
       "1           0                 0           0  \n",
       "2           0                 0           0  \n",
       "3           0                 0           0  \n",
       "4           0                 0           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing spotify_genre and spot-checking resulting df \n",
    "pd.set_option('display.max_columns', None)\n",
    "df_hotlist_and_features_2000s = df_hotlist_and_features_2000s.drop(['spotify_genre'], axis=1)\n",
    "df_hotlist_and_features_2000s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new df with most 5 most popular genres by week\n",
    "\n",
    "# Get all genre column names\n",
    "genre_start_idx = df_hotlist_and_features_2000s.columns.get_loc('pop')\n",
    "genre_cols = df_hotlist_and_features_2000s.columns[genre_start_idx:].tolist()\n",
    "\n",
    "# Group by WeekID and sum the genre columns to get counts\n",
    "genre_counts = df_hotlist_and_features_2000s.groupby('WeekID')[genre_cols].sum()\n",
    "\n",
    "# For each week, find the top 5 genres\n",
    "top_genres = []\n",
    "for week_id in genre_counts.index:\n",
    "    # Get the genre counts for this week and sort them\n",
    "    week_genres = genre_counts.loc[week_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Get the top 3 genre names\n",
    "    top_5 = week_genres.head(5).index.tolist()\n",
    "    \n",
    "    # Pad with None if there are fewer than 5 genres\n",
    "    while len(top_5) < 5:\n",
    "        top_5.append(None)\n",
    "    \n",
    "    top_genres.append({\n",
    "        'WeekID': week_id,\n",
    "        'Most_Popular_Genre': top_5[0],\n",
    "        '2nd_Most_Popular_Genre': top_5[1],\n",
    "        '3rd_Most_Popular_Genre': top_5[2],\n",
    "        '4th_Most_Popular_Genre': top_5[3],\n",
    "        '5th_Most_Popular_Genre': top_5[4]\n",
    "    })\n",
    "\n",
    "# Create the new dataframe\n",
    "df_top_genres = pd.DataFrame(top_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column to the main df indicating whether each song is in a genre that's in the top 5 genres for a given week\n",
    "\n",
    "def is_in_top5_genres(row, df_top_genres):\n",
    "    week = row['WeekID']\n",
    "\n",
    "    top_genres = df_top_genres[df_top_genres['WeekID'] == week]\n",
    "\n",
    "    if len(top_genres) == 0:\n",
    "        return 0\n",
    "    \n",
    "    top_5 = [\n",
    "        top_genres.iloc[0]['Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['2nd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['3rd_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['4th_Most_Popular_Genre'],\n",
    "        top_genres.iloc[0]['5th_Most_Popular_Genre'],\n",
    "        ]\n",
    "\n",
    "    for genre in top_5:\n",
    "        if genre in row.index and row[genre] == 1:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "df_hotlist_and_features_2000s['in_top5_genres'] = df_hotlist_and_features_2000s.apply(\n",
    "    lambda row: is_in_top5_genres(row, df_top_genres), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new df with mean score of appearance in top 5 weekly genres\n",
    "df_mean_genre_match = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['in_top5_genres'].mean()\n",
    "df_mean_genre_match.rename(columns={'in_top5_genres': 'In_Top5genres_Mean'}, inplace=True)\n",
    "df_mean_genre_match.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max weekly rank change for each song \n",
    "df_max_rank_change = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Rank_Change'].max()\n",
    "df_max_rank_change.rename(columns={'Rank_Change': 'Max_Rank_Change'}, inplace=True)\n",
    "df_max_rank_change.set_index('SongID', inplace=True)\n",
    "\n",
    "# new df with the max peak rank for each song \n",
    "df_max_peak_pos = df_hotlist_and_features_2000s.groupby('SongID', as_index=False)['Peak Position'].max()\n",
    "df_max_peak_pos.rename(columns={'Peak Position': 'Max_Peak_Position'}, inplace=True)\n",
    "df_max_peak_pos.set_index('SongID', inplace=True)\n",
    "\n",
    "# ensuring these new dfs have no null values\n",
    "df_max_rank_change['Max_Rank_Change'].isna().sum(), df_max_peak_pos['Max_Peak_Position'].isna().sum(), df_mean_genre_match['In_Top5genres_Mean'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekID</th>\n",
       "      <th>Week Position</th>\n",
       "      <th>SongID</th>\n",
       "      <th>Previous Week Position</th>\n",
       "      <th>Peak Position</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "      <th>Rank_Change</th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>in_top5_genres</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>69</td>\n",
       "      <td>Deck The HallsSHeDAISY</td>\n",
       "      <td>97.0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>229773.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.444</td>\n",
       "      <td>118.827</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>83</td>\n",
       "      <td>Guerrilla RadioRage Against The Machine</td>\n",
       "      <td>87.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>206200.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-5.764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.489</td>\n",
       "      <td>103.680</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>60</td>\n",
       "      <td>HeartbreakerMariah Carey Featuring Jay-Z</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285706.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.789</td>\n",
       "      <td>200.031</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>97</td>\n",
       "      <td>Gotta ManEve</td>\n",
       "      <td>66.0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>31.0</td>\n",
       "      <td>264733.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-4.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.903</td>\n",
       "      <td>90.496</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>19</td>\n",
       "      <td>Dancin'Guy</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>248626.0</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.792</td>\n",
       "      <td>101.015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekID  Week Position                                    SongID  \\\n",
       "0 2000-01-01             69                    Deck The HallsSHeDAISY   \n",
       "1 2000-01-01             83   Guerrilla RadioRage Against The Machine   \n",
       "2 2000-01-01             60  HeartbreakerMariah Carey Featuring Jay-Z   \n",
       "3 2000-01-01             97                              Gotta ManEve   \n",
       "4 2000-01-01             19                                Dancin'Guy   \n",
       "\n",
       "   Previous Week Position  Peak Position  Weeks on Chart  Rank_Change  \\\n",
       "0                    97.0             69               2        -28.0   \n",
       "1                    87.0             69              10         -4.0   \n",
       "2                    51.0              1              18          9.0   \n",
       "3                    66.0             26              16         31.0   \n",
       "4                    29.0             19               3        -10.0   \n",
       "\n",
       "   spotify_track_duration_ms  danceability  energy   key  loudness  mode  \\\n",
       "0                   229773.0         0.575   0.837   1.0    -7.141   0.0   \n",
       "1                   206200.0         0.599   0.957  11.0    -5.764   1.0   \n",
       "2                   285706.0         0.524   0.816   1.0    -5.872   1.0   \n",
       "3                   264733.0         0.796   0.871   5.0    -4.135   0.0   \n",
       "4                   248626.0         0.798   0.703  10.0    -4.050   0.0   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0406        0.0195          0.000009     0.144    0.444  118.827   \n",
       "1       0.1880        0.0129          0.000071     0.155    0.489  103.680   \n",
       "2       0.3700        0.3840          0.000000     0.349    0.789  200.031   \n",
       "3       0.1270        0.1480          0.000199     0.113    0.903   90.496   \n",
       "4       0.1350        0.1170          0.000000     0.121    0.792  101.015   \n",
       "\n",
       "   time_signature  pop  dance pop  pop rap  rap  contemporary country  \\\n",
       "0             4.0    0          0        0    0                     1   \n",
       "1             4.0    0          0        0    0                     0   \n",
       "2             4.0    1          1        0    0                     0   \n",
       "3             4.0    0          1        1    1                     0   \n",
       "4             4.0    0          0        0    0                     0   \n",
       "\n",
       "   country  country road  post-teen pop  hip hop  r&b  urban contemporary  \\\n",
       "0        1             1              0        0    0                   0   \n",
       "1        0             0              0        0    0                   0   \n",
       "2        0             0              0        0    1                   1   \n",
       "3        0             0              0        1    1                   1   \n",
       "4        0             0              0        0    0                   0   \n",
       "\n",
       "   trap  southern hip hop  pop rock  hip pop  modern country rock  neo mellow  \\\n",
       "0     0                 0         0        0                    0           0   \n",
       "1     0                 0         0        0                    0           0   \n",
       "2     0                 0         0        0                    0           0   \n",
       "3     0                 0         0        1                    0           0   \n",
       "4     0                 0         0        0                    0           0   \n",
       "\n",
       "   post-grunge  gangster rap  atl hip hop  alternative metal  neo soul  \\\n",
       "0            0             0            0                  0         0   \n",
       "1            1             0            0                  1         0   \n",
       "2            0             0            0                  0         0   \n",
       "3            0             0            0                  0         0   \n",
       "4            0             0            0                  0         0   \n",
       "\n",
       "   dirty south rap  country dawn  modern rock  nu metal  canadian pop  rock  \\\n",
       "0                0             1            0         0             0     0   \n",
       "1                0             0            0         1             0     1   \n",
       "2                0             0            0         0             0     0   \n",
       "3                0             0            0         0             0     0   \n",
       "4                0             0            0         0             0     0   \n",
       "\n",
       "   melodic rap  deep pop r&b  edm  new jack swing  permanent wave  \\\n",
       "0            0             0    0               0               0   \n",
       "1            0             0    0               0               0   \n",
       "2            0             0    0               0               0   \n",
       "3            0             0    0               0               0   \n",
       "4            0             0    0               0               0   \n",
       "\n",
       "   miami hip hop  country pop  oklahoma country  latin  tropical house  \\\n",
       "0              0            0                 0      0               0   \n",
       "1              0            0                 0      0               0   \n",
       "2              0            0                 0      0               0   \n",
       "3              0            0                 0      0               0   \n",
       "4              0            0                 0      0               0   \n",
       "\n",
       "   electropop  uk pop  east coast hip hop  alternative rock  viral pop  \\\n",
       "0           0       0                   0                 0          0   \n",
       "1           0       0                   0                 1          0   \n",
       "2           0       0                   0                 0          0   \n",
       "3           0       0                   0                 0          0   \n",
       "4           0       0                   0                 0          0   \n",
       "\n",
       "   quiet storm  chicago rap  redneck  pop punk  crunk  country rock  \\\n",
       "0            0            0        0         0      0             0   \n",
       "1            0            0        0         0      0             0   \n",
       "2            0            0        0         0      0             0   \n",
       "3            0            0        0         0      0             0   \n",
       "4            0            0        0         0      0             0   \n",
       "\n",
       "   toronto rap  canadian hip hop  boy band  hardcore hip hop  queens hip hop  \\\n",
       "0            0                 0         0                 0               0   \n",
       "1            0                 0         0                 0               0   \n",
       "2            0                 0         0                 0               0   \n",
       "3            0                 0         0                 0               0   \n",
       "4            0                 0         0                 0               0   \n",
       "\n",
       "   talent show  emo  europop  acoustic pop  alternative r&b  \\\n",
       "0            0    0        0             0                0   \n",
       "1            0    0        0             0                0   \n",
       "2            0    0        0             0                0   \n",
       "3            0    0        0             0                0   \n",
       "4            0    0        0             0                0   \n",
       "\n",
       "   conscious hip hop  australian pop  detroit hip hop  rap rock  latin pop  \\\n",
       "0                  0               0                0         0          0   \n",
       "1                  1               0                0         1          0   \n",
       "2                  0               0                0         0          0   \n",
       "3                  0               0                0         0          0   \n",
       "4                  0               0                0         0          0   \n",
       "\n",
       "   barbadian pop  g funk  girl group  canadian rock  tropical  piano rock  \\\n",
       "0              0       0           0              0         0           0   \n",
       "1              0       0           0              0         0           0   \n",
       "2              0       0           0              0         0           0   \n",
       "3              0       0           0              0         0           0   \n",
       "4              0       0           0              0         0           0   \n",
       "\n",
       "   indie pop  electro house  indie poptimism  rap metal  west coast rap  \\\n",
       "0          0              0                0          0               0   \n",
       "1          0              0                0          1               0   \n",
       "2          0              0                0          0               0   \n",
       "3          0              0                0          0               0   \n",
       "4          0              0                0          0               0   \n",
       "\n",
       "   new orleans rap  metropopolis  candy pop  lilith  australian country  \\\n",
       "0                0             0          0       0                   0   \n",
       "1                0             0          0       0                   0   \n",
       "2                0             0          0       0                   0   \n",
       "3                0             0          0       0                   0   \n",
       "4                0             0          0       0                   0   \n",
       "\n",
       "   philly rap  funk metal  reggaeton  dfw rap  canadian contemporary r&b  \\\n",
       "0           0           0          0        0                          0   \n",
       "1           0           1          0        0                          0   \n",
       "2           0           0          0        0                          0   \n",
       "3           1           0          0        0                          0   \n",
       "4           0           0          0        0                          0   \n",
       "\n",
       "   soul  mexican pop  adult standards  nc hip hop  british soul  trap queen  \\\n",
       "0     0            0                0           0             0           0   \n",
       "1     0            0                0           0             0           0   \n",
       "2     0            0                0           0             0           0   \n",
       "3     0            0                0           0             0           0   \n",
       "4     0            0                0           0             0           0   \n",
       "\n",
       "   hollywood  arkansas country  atl trap  underground hip hop  texas country  \\\n",
       "0          0                 0         0                    0              0   \n",
       "1          0                 0         0                    0              0   \n",
       "2          0                 0         0                    0              0   \n",
       "3          0                 0         0                    0              0   \n",
       "4          0                 0         0                    0              0   \n",
       "\n",
       "   uk dance  house  new wave pop  brostep  dancehall  progressive house  funk  \\\n",
       "0         0      0             0        0          0                  0     0   \n",
       "1         0      0             0        0          0                  0     0   \n",
       "2         0      0             0        0          0                  0     0   \n",
       "3         0      0             0        0          0                  0     0   \n",
       "4         0      0             0        0          0                  0     0   \n",
       "\n",
       "   singer-songwriter  latin hip hop  idol  garage rock  mellow gold  \\\n",
       "0                  0              0     0            0            0   \n",
       "1                  0              0     0            0            0   \n",
       "2                  0              0     0            0            0   \n",
       "3                  0              0     0            0            0   \n",
       "4                  0              0     0            0            0   \n",
       "\n",
       "   baroque pop  big room  art pop  reggae fusion  cali rap  bronx hip hop  \\\n",
       "0            0         0        0              0         0              0   \n",
       "1            0         0        0              0         0              0   \n",
       "2            0         0        0              0         0              0   \n",
       "3            0         0        0              0         0              0   \n",
       "4            0         0        0              0         0              0   \n",
       "\n",
       "   folk-pop  country rap  stomp and holler  neon pop punk  emo rap  punk  \\\n",
       "0         0            0                 0              0        0     0   \n",
       "1         0            0                 0              0        0     0   \n",
       "2         0            0                 0              0        0     0   \n",
       "3         0            0                 0              0        0     0   \n",
       "4         0            0                 0              0        0     0   \n",
       "\n",
       "   indie rock  funk rock  memphis hip hop  modern alternative rock  \\\n",
       "0           0          0                0                        0   \n",
       "1           0          0                0                        0   \n",
       "2           0          0                0                        0   \n",
       "3           0          0                0                        0   \n",
       "4           0          0                0                        0   \n",
       "\n",
       "   lgbtq+ hip hop  progressive electro house  alternative hip hop  blues rock  \\\n",
       "0               0                          0                    0           0   \n",
       "1               0                          0                    0           0   \n",
       "2               0                          0                    0           0   \n",
       "3               0                          0                    0           0   \n",
       "4               0                          0                    0           0   \n",
       "\n",
       "   colombian pop  eurodance  classic rock  baton rouge rap  australian dance  \\\n",
       "0              0          0             0                0                 0   \n",
       "1              0          0             0                0                 0   \n",
       "2              0          0             0                0                 0   \n",
       "3              0          0             0                0                 0   \n",
       "4              0          0             0                0                 0   \n",
       "\n",
       "   folk  pop emo  soft rock  motown  pixie  canadian country  wrestling  \\\n",
       "0     0        0          0       0      0                 0          0   \n",
       "1     0        0          0       0      0                 0          0   \n",
       "2     0        0          0       0      0                 0          0   \n",
       "3     0        0          0       0      0                 0          0   \n",
       "4     0        0          0       0      0                 0          0   \n",
       "\n",
       "   glee club  complextro  vapor trap  etherpop  pittsburgh rap  escape room  \\\n",
       "0          0           0           0         0               0            0   \n",
       "1          0           0           0         0               0            0   \n",
       "2          0           0           0         0               0            0   \n",
       "3          0           0           0         0               0            0   \n",
       "4          0           0           0         0               0            0   \n",
       "\n",
       "   indietronica  comic  german techno  new jersey rap  trap latino  \\\n",
       "0             0      0              0               0            0   \n",
       "1             0      0              0               0            0   \n",
       "2             0      0              0               0            0   \n",
       "3             0      0              0               0            0   \n",
       "4             0      0              0               0            0   \n",
       "\n",
       "   houston rap  social media pop  puerto rican pop  deep southern trap  \\\n",
       "0            0                 0                 0                   0   \n",
       "1            0                 0                 0                   0   \n",
       "2            0                 0                 0                   0   \n",
       "3            0                 0                 0                   0   \n",
       "4            0                 0                 0                   0   \n",
       "\n",
       "   heartland rock  alternative dance  bubblegum dance  alberta country  \\\n",
       "0               0                  0                0                0   \n",
       "1               0                  0                0                0   \n",
       "2               0                  0                0                0   \n",
       "3               0                  0                0                0   \n",
       "4               0                  0                0                0   \n",
       "\n",
       "   outlaw country  country gospel  florida rap  hard rock  canadian metal  \\\n",
       "0               0               0            0          0               0   \n",
       "1               0               0            0          0               0   \n",
       "2               0               0            0          0               0   \n",
       "3               0               0            0          0               0   \n",
       "4               0               0            0          0               0   \n",
       "\n",
       "   christian rock  soca  indiecoustica  harlem hip hop  new rave  \\\n",
       "0               0     0              0               0         0   \n",
       "1               0     0              0               0         0   \n",
       "2               0     0              0               0         0   \n",
       "3               0     0              0               0         0   \n",
       "4               0     0              0               0         0   \n",
       "\n",
       "   electronic trap  christian music  grunge  show tunes  viral trap  la indie  \\\n",
       "0                0                0       0           0           0         0   \n",
       "1                0                0       0           0           0         0   \n",
       "2                0                0       0           0           0         0   \n",
       "3                0                0       0           0           0         0   \n",
       "4                0                0       0           0           0         0   \n",
       "\n",
       "   swedish pop  swedish electropop  reggaeton flow  dance-punk  celtic rock  \\\n",
       "0            0                   0               0           0            0   \n",
       "1            0                   0               0           0            0   \n",
       "2            0                   0               0           0            0   \n",
       "3            0                   0               0           0            0   \n",
       "4            0                   0               0           0            0   \n",
       "\n",
       "   socal pop punk  lounge  chicano rap  stomp pop  ccm  vocal jazz  \\\n",
       "0               0       0            0          0    0           0   \n",
       "1               0       0            0          0    0           0   \n",
       "2               0       0            0          0    0           0   \n",
       "3               0       0            0          0    0           0   \n",
       "4               0       0            0          0    0           0   \n",
       "\n",
       "   glam metal  worship  irish rock  electropowerpop  electro  indie pop rap  \\\n",
       "0           0        0           0                0        0              0   \n",
       "1           0        0           0                0        0              0   \n",
       "2           0        0           0                0        0              0   \n",
       "3           0        0           0                0        0              0   \n",
       "4           0        0           0                0        0              0   \n",
       "\n",
       "   canadian contemporary country  bounce  christian alternative rock  \\\n",
       "0                              0       0                           0   \n",
       "1                              0       0                           0   \n",
       "2                              0       0                           0   \n",
       "3                              0       0                           0   \n",
       "4                              0       0                           0   \n",
       "\n",
       "   south african rock  deep talent show  disco  hyphy  disco house  \\\n",
       "0                   0                 0      0      0            0   \n",
       "1                   0                 0      0      0            0   \n",
       "2                   0                 0      0      0            0   \n",
       "3                   0                 0      0      0            0   \n",
       "4                   0                 0      0      0            0   \n",
       "\n",
       "   canadian latin  australian hip hop  nyc rap  brill building pop  k-pop  \\\n",
       "0               0                   0        0                   0      0   \n",
       "1               0                   0        0                   0      0   \n",
       "2               0                   0        0                   0      0   \n",
       "3               0                   0        0                   0      0   \n",
       "4               0                   0        0                   0      0   \n",
       "\n",
       "   nz pop  minnesota hip hop  modern blues rock  album rock  modern folk rock  \\\n",
       "0       0                  0                  0           0                 0   \n",
       "1       0                  0                  0           0                 0   \n",
       "2       0                  0                  0           0                 0   \n",
       "3       0                  0                  0           0                 0   \n",
       "4       0                  0                  0           0                 0   \n",
       "\n",
       "   uk americana  old school hip hop  punk blues  dmv rap  industrial metal  \\\n",
       "0             0                   0           0        0                 0   \n",
       "1             0                   0           0        0                 0   \n",
       "2             0                   0           0        0                 0   \n",
       "3             0                   0           0        0                 0   \n",
       "4             0                   0           0        0                 0   \n",
       "\n",
       "   skate punk  swedish synthpop  moombahton  in_top5_genres  \\\n",
       "0           0                 0           0               0   \n",
       "1           0                 0           0               0   \n",
       "2           0                 0           0               1   \n",
       "3           0                 0           0               1   \n",
       "4           0                 0           0               0   \n",
       "\n",
       "   Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "0                 69             -8.0                 0.0  \n",
       "1                 69             10.0                 0.0  \n",
       "2                  1              9.0                 1.0  \n",
       "3                 26             31.0                 1.0  \n",
       "4                 19             19.0                 0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding max peak position to main df\n",
    "df_2000s_data = df_hotlist_and_features_2000s.join(df_max_peak_pos, on='SongID')\n",
    "\n",
    "# adding max rank change to main df\n",
    "df_2000s_data = df_2000s_data.join(df_max_rank_change, on='SongID')\n",
    "\n",
    "# adding in top5genres mean to main df\n",
    "df_2000s_data = df_2000s_data.join(df_mean_genre_match, on='SongID')\n",
    "\n",
    "df_2000s_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103420, 112098)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a clean df with genre information\n",
    "df_clean_withgenre = df_2000s_data.drop(['WeekID', 'Week Position', 'Previous Week Position', 'Peak Position',\n",
    "                                         'Weeks on Chart', 'Rank_Change', 'in_top5_genres'], axis=1)\n",
    "\n",
    "# counting duplicates and all rows\n",
    "df_clean_withgenre.duplicated().sum(), len(df_clean_withgenre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 8678\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 820\n"
     ]
    }
   ],
   "source": [
    "# removing duplicate rows\n",
    "df_clean_withgenre = df_clean_withgenre.drop_duplicates()\n",
    "\n",
    "# checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7858\n",
      "Duplicate rows: 0\n",
      "Rows with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# dropping rows with missing values (unfortunately there is no reliable way to infer or estimate song characteristics)\n",
    "df_clean_withgenre = df_clean_withgenre.dropna()\n",
    "\n",
    "# re-checking duplicate rows and rows with missing values\n",
    "print(f\"Total rows: {len(df_clean_withgenre)}\")\n",
    "print(f\"Duplicate rows: {df_clean_withgenre.duplicated().sum()}\")\n",
    "print(f\"Rows with missing values: {df_clean_withgenre.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a clean df with no genre\n",
    "nogenre_cols = ['spotify_track_duration_ms', 'danceability', 'energy', 'key', 'loudness',\n",
    "                'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                'time_signature', 'Max_Peak_Position', 'Max_Rank_Change', 'In_Top5genres_Mean']\n",
    "df_clean_nogenre = df_clean_withgenre[nogenre_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have two datasets: one containing genre and one without. This will allow me to model this data with and without genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>pop</th>\n",
       "      <th>dance pop</th>\n",
       "      <th>pop rap</th>\n",
       "      <th>rap</th>\n",
       "      <th>contemporary country</th>\n",
       "      <th>country</th>\n",
       "      <th>country road</th>\n",
       "      <th>post-teen pop</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>r&amp;b</th>\n",
       "      <th>urban contemporary</th>\n",
       "      <th>trap</th>\n",
       "      <th>southern hip hop</th>\n",
       "      <th>pop rock</th>\n",
       "      <th>hip pop</th>\n",
       "      <th>modern country rock</th>\n",
       "      <th>neo mellow</th>\n",
       "      <th>post-grunge</th>\n",
       "      <th>gangster rap</th>\n",
       "      <th>atl hip hop</th>\n",
       "      <th>alternative metal</th>\n",
       "      <th>neo soul</th>\n",
       "      <th>dirty south rap</th>\n",
       "      <th>country dawn</th>\n",
       "      <th>modern rock</th>\n",
       "      <th>nu metal</th>\n",
       "      <th>canadian pop</th>\n",
       "      <th>rock</th>\n",
       "      <th>melodic rap</th>\n",
       "      <th>deep pop r&amp;b</th>\n",
       "      <th>edm</th>\n",
       "      <th>new jack swing</th>\n",
       "      <th>permanent wave</th>\n",
       "      <th>miami hip hop</th>\n",
       "      <th>country pop</th>\n",
       "      <th>oklahoma country</th>\n",
       "      <th>latin</th>\n",
       "      <th>tropical house</th>\n",
       "      <th>electropop</th>\n",
       "      <th>uk pop</th>\n",
       "      <th>east coast hip hop</th>\n",
       "      <th>alternative rock</th>\n",
       "      <th>viral pop</th>\n",
       "      <th>quiet storm</th>\n",
       "      <th>chicago rap</th>\n",
       "      <th>redneck</th>\n",
       "      <th>pop punk</th>\n",
       "      <th>crunk</th>\n",
       "      <th>country rock</th>\n",
       "      <th>toronto rap</th>\n",
       "      <th>canadian hip hop</th>\n",
       "      <th>boy band</th>\n",
       "      <th>hardcore hip hop</th>\n",
       "      <th>queens hip hop</th>\n",
       "      <th>talent show</th>\n",
       "      <th>emo</th>\n",
       "      <th>europop</th>\n",
       "      <th>acoustic pop</th>\n",
       "      <th>alternative r&amp;b</th>\n",
       "      <th>conscious hip hop</th>\n",
       "      <th>australian pop</th>\n",
       "      <th>detroit hip hop</th>\n",
       "      <th>rap rock</th>\n",
       "      <th>latin pop</th>\n",
       "      <th>barbadian pop</th>\n",
       "      <th>g funk</th>\n",
       "      <th>girl group</th>\n",
       "      <th>canadian rock</th>\n",
       "      <th>tropical</th>\n",
       "      <th>piano rock</th>\n",
       "      <th>indie pop</th>\n",
       "      <th>electro house</th>\n",
       "      <th>indie poptimism</th>\n",
       "      <th>rap metal</th>\n",
       "      <th>west coast rap</th>\n",
       "      <th>new orleans rap</th>\n",
       "      <th>metropopolis</th>\n",
       "      <th>candy pop</th>\n",
       "      <th>lilith</th>\n",
       "      <th>australian country</th>\n",
       "      <th>philly rap</th>\n",
       "      <th>funk metal</th>\n",
       "      <th>reggaeton</th>\n",
       "      <th>dfw rap</th>\n",
       "      <th>canadian contemporary r&amp;b</th>\n",
       "      <th>soul</th>\n",
       "      <th>mexican pop</th>\n",
       "      <th>adult standards</th>\n",
       "      <th>nc hip hop</th>\n",
       "      <th>british soul</th>\n",
       "      <th>trap queen</th>\n",
       "      <th>hollywood</th>\n",
       "      <th>arkansas country</th>\n",
       "      <th>atl trap</th>\n",
       "      <th>underground hip hop</th>\n",
       "      <th>texas country</th>\n",
       "      <th>uk dance</th>\n",
       "      <th>house</th>\n",
       "      <th>new wave pop</th>\n",
       "      <th>brostep</th>\n",
       "      <th>dancehall</th>\n",
       "      <th>progressive house</th>\n",
       "      <th>funk</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>latin hip hop</th>\n",
       "      <th>idol</th>\n",
       "      <th>garage rock</th>\n",
       "      <th>mellow gold</th>\n",
       "      <th>baroque pop</th>\n",
       "      <th>big room</th>\n",
       "      <th>art pop</th>\n",
       "      <th>reggae fusion</th>\n",
       "      <th>cali rap</th>\n",
       "      <th>bronx hip hop</th>\n",
       "      <th>folk-pop</th>\n",
       "      <th>country rap</th>\n",
       "      <th>stomp and holler</th>\n",
       "      <th>neon pop punk</th>\n",
       "      <th>emo rap</th>\n",
       "      <th>punk</th>\n",
       "      <th>indie rock</th>\n",
       "      <th>funk rock</th>\n",
       "      <th>memphis hip hop</th>\n",
       "      <th>modern alternative rock</th>\n",
       "      <th>lgbtq+ hip hop</th>\n",
       "      <th>progressive electro house</th>\n",
       "      <th>alternative hip hop</th>\n",
       "      <th>blues rock</th>\n",
       "      <th>colombian pop</th>\n",
       "      <th>eurodance</th>\n",
       "      <th>classic rock</th>\n",
       "      <th>baton rouge rap</th>\n",
       "      <th>australian dance</th>\n",
       "      <th>folk</th>\n",
       "      <th>pop emo</th>\n",
       "      <th>soft rock</th>\n",
       "      <th>motown</th>\n",
       "      <th>pixie</th>\n",
       "      <th>canadian country</th>\n",
       "      <th>wrestling</th>\n",
       "      <th>glee club</th>\n",
       "      <th>complextro</th>\n",
       "      <th>vapor trap</th>\n",
       "      <th>etherpop</th>\n",
       "      <th>pittsburgh rap</th>\n",
       "      <th>escape room</th>\n",
       "      <th>indietronica</th>\n",
       "      <th>comic</th>\n",
       "      <th>german techno</th>\n",
       "      <th>new jersey rap</th>\n",
       "      <th>trap latino</th>\n",
       "      <th>houston rap</th>\n",
       "      <th>social media pop</th>\n",
       "      <th>puerto rican pop</th>\n",
       "      <th>deep southern trap</th>\n",
       "      <th>heartland rock</th>\n",
       "      <th>alternative dance</th>\n",
       "      <th>bubblegum dance</th>\n",
       "      <th>alberta country</th>\n",
       "      <th>outlaw country</th>\n",
       "      <th>country gospel</th>\n",
       "      <th>florida rap</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>canadian metal</th>\n",
       "      <th>christian rock</th>\n",
       "      <th>soca</th>\n",
       "      <th>indiecoustica</th>\n",
       "      <th>harlem hip hop</th>\n",
       "      <th>new rave</th>\n",
       "      <th>electronic trap</th>\n",
       "      <th>christian music</th>\n",
       "      <th>grunge</th>\n",
       "      <th>show tunes</th>\n",
       "      <th>viral trap</th>\n",
       "      <th>la indie</th>\n",
       "      <th>swedish pop</th>\n",
       "      <th>swedish electropop</th>\n",
       "      <th>reggaeton flow</th>\n",
       "      <th>dance-punk</th>\n",
       "      <th>celtic rock</th>\n",
       "      <th>socal pop punk</th>\n",
       "      <th>lounge</th>\n",
       "      <th>chicano rap</th>\n",
       "      <th>stomp pop</th>\n",
       "      <th>ccm</th>\n",
       "      <th>vocal jazz</th>\n",
       "      <th>glam metal</th>\n",
       "      <th>worship</th>\n",
       "      <th>irish rock</th>\n",
       "      <th>electropowerpop</th>\n",
       "      <th>electro</th>\n",
       "      <th>indie pop rap</th>\n",
       "      <th>canadian contemporary country</th>\n",
       "      <th>bounce</th>\n",
       "      <th>christian alternative rock</th>\n",
       "      <th>south african rock</th>\n",
       "      <th>deep talent show</th>\n",
       "      <th>disco</th>\n",
       "      <th>hyphy</th>\n",
       "      <th>disco house</th>\n",
       "      <th>canadian latin</th>\n",
       "      <th>australian hip hop</th>\n",
       "      <th>nyc rap</th>\n",
       "      <th>brill building pop</th>\n",
       "      <th>k-pop</th>\n",
       "      <th>nz pop</th>\n",
       "      <th>minnesota hip hop</th>\n",
       "      <th>modern blues rock</th>\n",
       "      <th>album rock</th>\n",
       "      <th>modern folk rock</th>\n",
       "      <th>uk americana</th>\n",
       "      <th>old school hip hop</th>\n",
       "      <th>punk blues</th>\n",
       "      <th>dmv rap</th>\n",
       "      <th>industrial metal</th>\n",
       "      <th>skate punk</th>\n",
       "      <th>swedish synthpop</th>\n",
       "      <th>moombahton</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.00000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>0.313566</td>\n",
       "      <td>0.278315</td>\n",
       "      <td>0.282260</td>\n",
       "      <td>0.252354</td>\n",
       "      <td>0.162001</td>\n",
       "      <td>0.153092</td>\n",
       "      <td>0.145839</td>\n",
       "      <td>0.181598</td>\n",
       "      <td>0.187580</td>\n",
       "      <td>0.113006</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.147875</td>\n",
       "      <td>0.116187</td>\n",
       "      <td>0.072410</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.068465</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.049631</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.034869</td>\n",
       "      <td>0.037414</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.032706</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.021125</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.017943</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.018962</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.00789</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.010562</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>0.463972</td>\n",
       "      <td>0.448198</td>\n",
       "      <td>0.450128</td>\n",
       "      <td>0.434391</td>\n",
       "      <td>0.368475</td>\n",
       "      <td>0.360099</td>\n",
       "      <td>0.352967</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.390401</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0.304414</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.320470</td>\n",
       "      <td>0.259183</td>\n",
       "      <td>0.268604</td>\n",
       "      <td>0.252559</td>\n",
       "      <td>0.219815</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.233815</td>\n",
       "      <td>0.234296</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>0.189786</td>\n",
       "      <td>0.190096</td>\n",
       "      <td>0.176870</td>\n",
       "      <td>0.168216</td>\n",
       "      <td>0.160538</td>\n",
       "      <td>0.199717</td>\n",
       "      <td>0.177876</td>\n",
       "      <td>0.183136</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>0.134133</td>\n",
       "      <td>0.144655</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.133675</td>\n",
       "      <td>0.127562</td>\n",
       "      <td>0.148392</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.130422</td>\n",
       "      <td>0.116964</td>\n",
       "      <td>0.124136</td>\n",
       "      <td>0.132755</td>\n",
       "      <td>0.128043</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>0.136398</td>\n",
       "      <td>0.119578</td>\n",
       "      <td>0.115365</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.138623</td>\n",
       "      <td>0.138181</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>0.107574</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.101625</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.092627</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>0.115901</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.104644</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.08848</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.095941</td>\n",
       "      <td>0.077111</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.089185</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.084865</td>\n",
       "      <td>0.076291</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.094630</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.087053</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.078725</td>\n",
       "      <td>0.064672</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.071169</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.073776</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.072049</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.062689</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.063688</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.057430</td>\n",
       "      <td>0.075462</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.159028</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.052841</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.040643</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>0.035653</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature          pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003     0.313566   \n",
       "std       0.140660     0.223494    29.558487        0.273062     0.463972   \n",
       "min       0.020000     0.034900    48.718000        0.000000     0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000     0.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000     0.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000     1.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000     1.000000   \n",
       "\n",
       "         dance pop      pop rap          rap  contemporary country  \\\n",
       "count  7858.000000  7858.000000  7858.000000           7858.000000   \n",
       "mean      0.278315     0.282260     0.252354              0.162001   \n",
       "std       0.448198     0.450128     0.434391              0.368475   \n",
       "min       0.000000     0.000000     0.000000              0.000000   \n",
       "25%       0.000000     0.000000     0.000000              0.000000   \n",
       "50%       0.000000     0.000000     0.000000              0.000000   \n",
       "75%       1.000000     1.000000     1.000000              0.000000   \n",
       "max       1.000000     1.000000     1.000000              1.000000   \n",
       "\n",
       "           country  country road  post-teen pop      hip hop          r&b  \\\n",
       "count  7858.000000   7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean      0.153092      0.145839       0.181598     0.187580     0.113006   \n",
       "std       0.360099      0.352967       0.385538     0.390401     0.316620   \n",
       "min       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.000000       0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       urban contemporary         trap  southern hip hop     pop rock  \\\n",
       "count         7858.000000  7858.000000       7858.000000  7858.000000   \n",
       "mean             0.103334     0.147875          0.116187     0.072410   \n",
       "std              0.304414     0.354998          0.320470     0.259183   \n",
       "min              0.000000     0.000000          0.000000     0.000000   \n",
       "25%              0.000000     0.000000          0.000000     0.000000   \n",
       "50%              0.000000     0.000000          0.000000     0.000000   \n",
       "75%              0.000000     0.000000          0.000000     0.000000   \n",
       "max              1.000000     1.000000          1.000000     1.000000   \n",
       "\n",
       "           hip pop  modern country rock   neo mellow  post-grunge  \\\n",
       "count  7858.000000          7858.000000  7858.000000  7858.000000   \n",
       "mean      0.078264             0.068465     0.050904     0.049631   \n",
       "std       0.268604             0.252559     0.219815     0.217195   \n",
       "min       0.000000             0.000000     0.000000     0.000000   \n",
       "25%       0.000000             0.000000     0.000000     0.000000   \n",
       "50%       0.000000             0.000000     0.000000     0.000000   \n",
       "75%       0.000000             0.000000     0.000000     0.000000   \n",
       "max       1.000000             1.000000     1.000000     1.000000   \n",
       "\n",
       "       gangster rap  atl hip hop  alternative metal     neo soul  \\\n",
       "count   7858.000000  7858.000000        7858.000000  7858.000000   \n",
       "mean       0.058030     0.058285           0.034869     0.037414   \n",
       "std        0.233815     0.234296           0.183459     0.189786   \n",
       "min        0.000000     0.000000           0.000000     0.000000   \n",
       "25%        0.000000     0.000000           0.000000     0.000000   \n",
       "50%        0.000000     0.000000           0.000000     0.000000   \n",
       "75%        0.000000     0.000000           0.000000     0.000000   \n",
       "max        1.000000     1.000000           1.000000     1.000000   \n",
       "\n",
       "       dirty south rap  country dawn  modern rock     nu metal  canadian pop  \\\n",
       "count      7858.000000   7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean          0.037541      0.032324     0.029142     0.026470      0.041614   \n",
       "std           0.190096      0.176870     0.168216     0.160538      0.199717   \n",
       "min           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "75%           0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "max           1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "              rock  melodic rap  deep pop r&b          edm  new jack swing  \\\n",
       "count  7858.000000  7858.000000   7858.000000  7858.000000     7858.000000   \n",
       "mean      0.032706     0.034742      0.019598     0.019725        0.018325   \n",
       "std       0.177876     0.183136      0.138623     0.139063        0.134133   \n",
       "min       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "25%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "50%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "75%       0.000000     0.000000      0.000000     0.000000        0.000000   \n",
       "max       1.000000     1.000000      1.000000     1.000000        1.000000   \n",
       "\n",
       "       permanent wave  miami hip hop  country pop  oklahoma country  \\\n",
       "count     7858.000000    7858.000000  7858.000000       7858.000000   \n",
       "mean         0.021379       0.021125     0.018198          0.016544   \n",
       "std          0.144655       0.143810     0.133675          0.127562   \n",
       "min          0.000000       0.000000     0.000000          0.000000   \n",
       "25%          0.000000       0.000000     0.000000          0.000000   \n",
       "50%          0.000000       0.000000     0.000000          0.000000   \n",
       "75%          0.000000       0.000000     0.000000          0.000000   \n",
       "max          1.000000       1.000000     1.000000          1.000000   \n",
       "\n",
       "             latin  tropical house   electropop       uk pop  \\\n",
       "count  7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.022525        0.016671     0.017307     0.013871   \n",
       "std       0.148392        0.128043     0.130422     0.116964   \n",
       "min       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       east coast hip hop  alternative rock    viral pop  quiet storm  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.015653          0.017943     0.016671     0.015271   \n",
       "std              0.124136          0.132755     0.128043     0.122637   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       chicago rap      redneck     pop punk        crunk  country rock  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean      0.018962     0.014508     0.013489     0.012344      0.012471   \n",
       "std       0.136398     0.119578     0.115365     0.110423      0.110984   \n",
       "min       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       toronto rap  canadian hip hop     boy band  hardcore hip hop  \\\n",
       "count  7858.000000       7858.000000  7858.000000       7858.000000   \n",
       "mean      0.019598          0.019471     0.014762          0.012090   \n",
       "std       0.138623          0.138181     0.120607          0.109293   \n",
       "min       0.000000          0.000000     0.000000          0.000000   \n",
       "25%       0.000000          0.000000     0.000000          0.000000   \n",
       "50%       0.000000          0.000000     0.000000          0.000000   \n",
       "75%       0.000000          0.000000     0.000000          0.000000   \n",
       "max       1.000000          1.000000     1.000000          1.000000   \n",
       "\n",
       "       queens hip hop  talent show          emo      europop  acoustic pop  \\\n",
       "count     7858.000000  7858.000000  7858.000000  7858.000000   7858.000000   \n",
       "mean         0.011708     0.010435     0.010435     0.009799      0.008654   \n",
       "std          0.107574     0.101625     0.101625     0.098510      0.092627   \n",
       "min          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "25%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "50%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "75%          0.000000     0.000000     0.000000     0.000000      0.000000   \n",
       "max          1.000000     1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       alternative r&b  conscious hip hop  australian pop  detroit hip hop  \\\n",
       "count      7858.000000        7858.000000     7858.000000      7858.000000   \n",
       "mean          0.010817           0.015526        0.008145         0.013617   \n",
       "std           0.103447           0.123639        0.089885         0.115901   \n",
       "min           0.000000           0.000000        0.000000         0.000000   \n",
       "25%           0.000000           0.000000        0.000000         0.000000   \n",
       "50%           0.000000           0.000000        0.000000         0.000000   \n",
       "75%           0.000000           0.000000        0.000000         0.000000   \n",
       "max           1.000000           1.000000        1.000000         1.000000   \n",
       "\n",
       "          rap rock    latin pop  barbadian pop       g funk  girl group  \\\n",
       "count  7858.000000  7858.000000    7858.000000  7858.000000  7858.00000   \n",
       "mean      0.007126     0.011072       0.005218     0.010817     0.00789   \n",
       "std       0.084123     0.104644       0.072049     0.103447     0.08848   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "50%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "75%       0.000000     0.000000       0.000000     0.000000     0.00000   \n",
       "max       1.000000     1.000000       1.000000     1.000000     1.00000   \n",
       "\n",
       "       canadian rock     tropical   piano rock    indie pop  electro house  \\\n",
       "count    7858.000000  7858.000000  7858.000000  7858.000000    7858.000000   \n",
       "mean        0.005218     0.009290     0.005981     0.007126       0.007126   \n",
       "std         0.072049     0.095941     0.077111     0.084123       0.084123   \n",
       "min         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "25%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "50%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "75%         0.000000     0.000000     0.000000     0.000000       0.000000   \n",
       "max         1.000000     1.000000     1.000000     1.000000       1.000000   \n",
       "\n",
       "       indie poptimism    rap metal  west coast rap  new orleans rap  \\\n",
       "count      7858.000000  7858.000000     7858.000000      7858.000000   \n",
       "mean          0.005727     0.007126        0.008017         0.010562   \n",
       "std           0.075462     0.084123        0.089185         0.102236   \n",
       "min           0.000000     0.000000        0.000000         0.000000   \n",
       "25%           0.000000     0.000000        0.000000         0.000000   \n",
       "50%           0.000000     0.000000        0.000000         0.000000   \n",
       "75%           0.000000     0.000000        0.000000         0.000000   \n",
       "max           1.000000     1.000000        1.000000         1.000000   \n",
       "\n",
       "       metropopolis    candy pop       lilith  australian country  \\\n",
       "count   7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean       0.004581     0.007254     0.005854            0.005090   \n",
       "std        0.067535     0.084865     0.076291            0.071169   \n",
       "min        0.000000     0.000000     0.000000            0.000000   \n",
       "25%        0.000000     0.000000     0.000000            0.000000   \n",
       "50%        0.000000     0.000000     0.000000            0.000000   \n",
       "75%        0.000000     0.000000     0.000000            0.000000   \n",
       "max        1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "        philly rap   funk metal    reggaeton      dfw rap  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.012471     0.005218     0.009035     0.005090   \n",
       "std       0.110984     0.072049     0.094630     0.071169   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       canadian contemporary r&b         soul  mexican pop  adult standards  \\\n",
       "count                7858.000000  7858.000000  7858.000000      7858.000000   \n",
       "mean                    0.007636     0.006108     0.006872         0.006108   \n",
       "std                     0.087053     0.077922     0.082617         0.077922   \n",
       "min                     0.000000     0.000000     0.000000         0.000000   \n",
       "25%                     0.000000     0.000000     0.000000         0.000000   \n",
       "50%                     0.000000     0.000000     0.000000         0.000000   \n",
       "75%                     0.000000     0.000000     0.000000         0.000000   \n",
       "max                     1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "        nc hip hop  british soul   trap queen    hollywood  arkansas country  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean      0.007126      0.004327     0.004709     0.029906          0.004072   \n",
       "std       0.084123      0.065640     0.068462     0.170338          0.063688   \n",
       "min       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000          0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "          atl trap  underground hip hop  texas country     uk dance  \\\n",
       "count  7858.000000          7858.000000    7858.000000  7858.000000   \n",
       "mean      0.007126             0.006236       0.004200     0.003181   \n",
       "std       0.084123             0.078725       0.064672     0.056318   \n",
       "min       0.000000             0.000000       0.000000     0.000000   \n",
       "25%       0.000000             0.000000       0.000000     0.000000   \n",
       "50%       0.000000             0.000000       0.000000     0.000000   \n",
       "75%       0.000000             0.000000       0.000000     0.000000   \n",
       "max       1.000000             1.000000       1.000000     1.000000   \n",
       "\n",
       "             house  new wave pop      brostep    dancehall  progressive house  \\\n",
       "count  7858.000000   7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean      0.003309      0.004327     0.002800     0.003181           0.003818   \n",
       "std       0.057430      0.065640     0.052841     0.056318           0.061674   \n",
       "min       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000           0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "              funk  singer-songwriter  latin hip hop         idol  \\\n",
       "count  7858.000000        7858.000000    7858.000000  7858.000000   \n",
       "mean      0.004581           0.005090       0.004327     0.005472   \n",
       "std       0.067535           0.071169       0.065640     0.073776   \n",
       "min       0.000000           0.000000       0.000000     0.000000   \n",
       "25%       0.000000           0.000000       0.000000     0.000000   \n",
       "50%       0.000000           0.000000       0.000000     0.000000   \n",
       "75%       0.000000           0.000000       0.000000     0.000000   \n",
       "max       1.000000           1.000000       1.000000     1.000000   \n",
       "\n",
       "       garage rock  mellow gold  baroque pop     big room      art pop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.004709     0.005218     0.002927     0.003945     0.004581   \n",
       "std       0.068462     0.072049     0.054026     0.062689     0.067535   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       reggae fusion     cali rap  bronx hip hop     folk-pop  country rap  \\\n",
       "count    7858.000000  7858.000000    7858.000000  7858.000000  7858.000000   \n",
       "mean        0.003309     0.003054       0.002800     0.002418     0.002418   \n",
       "std         0.057430     0.055184       0.052841     0.049116     0.049116   \n",
       "min         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "75%         0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "max         1.000000     1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "       stomp and holler  neon pop punk      emo rap         punk   indie rock  \\\n",
       "count       7858.000000    7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.003563       0.002418     0.006363     0.003054     0.004072   \n",
       "std            0.059590       0.049116     0.079519     0.055184     0.063688   \n",
       "min            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000       0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000       1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         funk rock  memphis hip hop  modern alternative rock  lgbtq+ hip hop  \\\n",
       "count  7858.000000      7858.000000              7858.000000     7858.000000   \n",
       "mean      0.002927         0.003436                 0.001782        0.003436   \n",
       "std       0.054026         0.058520                 0.042174        0.058520   \n",
       "min       0.000000         0.000000                 0.000000        0.000000   \n",
       "25%       0.000000         0.000000                 0.000000        0.000000   \n",
       "50%       0.000000         0.000000                 0.000000        0.000000   \n",
       "75%       0.000000         0.000000                 0.000000        0.000000   \n",
       "max       1.000000         1.000000                 1.000000        1.000000   \n",
       "\n",
       "       progressive electro house  alternative hip hop   blues rock  \\\n",
       "count                7858.000000          7858.000000  7858.000000   \n",
       "mean                    0.001654             0.003563     0.002545   \n",
       "std                     0.040643             0.059590     0.050389   \n",
       "min                     0.000000             0.000000     0.000000   \n",
       "25%                     0.000000             0.000000     0.000000   \n",
       "50%                     0.000000             0.000000     0.000000   \n",
       "75%                     0.000000             0.000000     0.000000   \n",
       "max                     1.000000             1.000000     1.000000   \n",
       "\n",
       "       colombian pop    eurodance  classic rock  baton rouge rap  \\\n",
       "count    7858.000000  7858.000000   7858.000000      7858.000000   \n",
       "mean        0.002800     0.002163      0.003309         0.005727   \n",
       "std         0.052841     0.046465      0.057430         0.075462   \n",
       "min         0.000000     0.000000      0.000000         0.000000   \n",
       "25%         0.000000     0.000000      0.000000         0.000000   \n",
       "50%         0.000000     0.000000      0.000000         0.000000   \n",
       "75%         0.000000     0.000000      0.000000         0.000000   \n",
       "max         1.000000     1.000000      1.000000         1.000000   \n",
       "\n",
       "       australian dance         folk      pop emo    soft rock       motown  \\\n",
       "count       7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean           0.001909     0.002927     0.002927     0.003818     0.002800   \n",
       "std            0.043652     0.054026     0.054026     0.061674     0.052841   \n",
       "min            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%            0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max            1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             pixie  canadian country    wrestling    glee club   complextro  \\\n",
       "count  7858.000000       7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.002545          0.002418     0.001909     0.025961     0.002036   \n",
       "std       0.050389          0.049116     0.043652     0.159028     0.045081   \n",
       "min       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000          0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        vapor trap     etherpop  pittsburgh rap  escape room  indietronica  \\\n",
       "count  7858.000000  7858.000000     7858.000000  7858.000000   7858.000000   \n",
       "mean      0.002036     0.001273        0.004836     0.002036      0.002800   \n",
       "std       0.045081     0.035653        0.069376     0.045081      0.052841   \n",
       "min       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "25%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "50%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "75%       0.000000     0.000000        0.000000     0.000000      0.000000   \n",
       "max       1.000000     1.000000        1.000000     1.000000      1.000000   \n",
       "\n",
       "             comic  german techno  new jersey rap  trap latino  houston rap  \\\n",
       "count  7858.000000    7858.000000     7858.000000  7858.000000  7858.000000   \n",
       "mean      0.003054       0.001782        0.001527     0.003818     0.002291   \n",
       "std       0.055184       0.042174        0.039051     0.061674     0.047809   \n",
       "min       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "25%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "50%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "75%       0.000000       0.000000        0.000000     0.000000     0.000000   \n",
       "max       1.000000       1.000000        1.000000     1.000000     1.000000   \n",
       "\n",
       "       social media pop  puerto rican pop  deep southern trap  heartland rock  \\\n",
       "count       7858.000000       7858.000000         7858.000000     7858.000000   \n",
       "mean           0.006108          0.002418            0.001527        0.002036   \n",
       "std            0.077922          0.049116            0.039051        0.045081   \n",
       "min            0.000000          0.000000            0.000000        0.000000   \n",
       "25%            0.000000          0.000000            0.000000        0.000000   \n",
       "50%            0.000000          0.000000            0.000000        0.000000   \n",
       "75%            0.000000          0.000000            0.000000        0.000000   \n",
       "max            1.000000          1.000000            1.000000        1.000000   \n",
       "\n",
       "       alternative dance  bubblegum dance  alberta country  outlaw country  \\\n",
       "count        7858.000000      7858.000000      7858.000000     7858.000000   \n",
       "mean            0.002163         0.001273         0.001527        0.002036   \n",
       "std             0.046465         0.035653         0.039051        0.045081   \n",
       "min             0.000000         0.000000         0.000000        0.000000   \n",
       "25%             0.000000         0.000000         0.000000        0.000000   \n",
       "50%             0.000000         0.000000         0.000000        0.000000   \n",
       "75%             0.000000         0.000000         0.000000        0.000000   \n",
       "max             1.000000         1.000000         1.000000        1.000000   \n",
       "\n",
       "       country gospel  florida rap    hard rock  canadian metal  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean         0.001400     0.003691     0.002291        0.001273   \n",
       "std          0.037391     0.060641     0.047809        0.035653   \n",
       "min          0.000000     0.000000     0.000000        0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "       christian rock         soca  indiecoustica  harlem hip hop  \\\n",
       "count     7858.000000  7858.000000    7858.000000     7858.000000   \n",
       "mean         0.001909     0.001400       0.002418        0.000891   \n",
       "std          0.043652     0.037391       0.049116        0.029835   \n",
       "min          0.000000     0.000000       0.000000        0.000000   \n",
       "25%          0.000000     0.000000       0.000000        0.000000   \n",
       "50%          0.000000     0.000000       0.000000        0.000000   \n",
       "75%          0.000000     0.000000       0.000000        0.000000   \n",
       "max          1.000000     1.000000       1.000000        1.000000   \n",
       "\n",
       "          new rave  electronic trap  christian music       grunge  \\\n",
       "count  7858.000000      7858.000000      7858.000000  7858.000000   \n",
       "mean      0.002036         0.001145         0.002036     0.002163   \n",
       "std       0.045081         0.033825         0.045081     0.046465   \n",
       "min       0.000000         0.000000         0.000000     0.000000   \n",
       "25%       0.000000         0.000000         0.000000     0.000000   \n",
       "50%       0.000000         0.000000         0.000000     0.000000   \n",
       "75%       0.000000         0.000000         0.000000     0.000000   \n",
       "max       1.000000         1.000000         1.000000     1.000000   \n",
       "\n",
       "        show tunes   viral trap     la indie  swedish pop  swedish electropop  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000         7858.000000   \n",
       "mean      0.001909     0.001145     0.000891     0.001145            0.001145   \n",
       "std       0.043652     0.033825     0.029835     0.033825            0.033825   \n",
       "min       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000            0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000            1.000000   \n",
       "\n",
       "       reggaeton flow   dance-punk  celtic rock  socal pop punk       lounge  \\\n",
       "count     7858.000000  7858.000000  7858.000000     7858.000000  7858.000000   \n",
       "mean         0.001654     0.002291     0.000891        0.001782     0.002036   \n",
       "std          0.040643     0.047809     0.029835        0.042174     0.045081   \n",
       "min          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "25%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "50%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "75%          0.000000     0.000000     0.000000        0.000000     0.000000   \n",
       "max          1.000000     1.000000     1.000000        1.000000     1.000000   \n",
       "\n",
       "       chicano rap    stomp pop          ccm   vocal jazz   glam metal  \\\n",
       "count  7858.000000  7858.000000  7858.000000  7858.000000  7858.000000   \n",
       "mean      0.001400     0.000382     0.001654     0.001654     0.001400   \n",
       "std       0.037391     0.019537     0.040643     0.040643     0.037391   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "           worship   irish rock  electropowerpop      electro  indie pop rap  \\\n",
       "count  7858.000000  7858.000000      7858.000000  7858.000000    7858.000000   \n",
       "mean      0.001527     0.001654         0.001654     0.001018       0.002036   \n",
       "std       0.039051     0.040643         0.040643     0.031893       0.045081   \n",
       "min       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "25%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "50%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "75%       0.000000     0.000000         0.000000     0.000000       0.000000   \n",
       "max       1.000000     1.000000         1.000000     1.000000       1.000000   \n",
       "\n",
       "       canadian contemporary country       bounce  christian alternative rock  \\\n",
       "count                    7858.000000  7858.000000                 7858.000000   \n",
       "mean                        0.001018     0.001018                    0.001909   \n",
       "std                         0.031893     0.031893                    0.043652   \n",
       "min                         0.000000     0.000000                    0.000000   \n",
       "25%                         0.000000     0.000000                    0.000000   \n",
       "50%                         0.000000     0.000000                    0.000000   \n",
       "75%                         0.000000     0.000000                    0.000000   \n",
       "max                         1.000000     1.000000                    1.000000   \n",
       "\n",
       "       south african rock  deep talent show        disco        hyphy  \\\n",
       "count         7858.000000       7858.000000  7858.000000  7858.000000   \n",
       "mean             0.001018          0.004836     0.001145     0.001145   \n",
       "std              0.031893          0.069376     0.033825     0.033825   \n",
       "min              0.000000          0.000000     0.000000     0.000000   \n",
       "25%              0.000000          0.000000     0.000000     0.000000   \n",
       "50%              0.000000          0.000000     0.000000     0.000000   \n",
       "75%              0.000000          0.000000     0.000000     0.000000   \n",
       "max              1.000000          1.000000     1.000000     1.000000   \n",
       "\n",
       "       disco house  canadian latin  australian hip hop      nyc rap  \\\n",
       "count  7858.000000     7858.000000         7858.000000  7858.000000   \n",
       "mean      0.000891        0.000891            0.001273     0.001273   \n",
       "std       0.029835        0.029835            0.035653     0.035653   \n",
       "min       0.000000        0.000000            0.000000     0.000000   \n",
       "25%       0.000000        0.000000            0.000000     0.000000   \n",
       "50%       0.000000        0.000000            0.000000     0.000000   \n",
       "75%       0.000000        0.000000            0.000000     0.000000   \n",
       "max       1.000000        1.000000            1.000000     1.000000   \n",
       "\n",
       "       brill building pop        k-pop       nz pop  minnesota hip hop  \\\n",
       "count         7858.000000  7858.000000  7858.000000        7858.000000   \n",
       "mean             0.001018     0.003436     0.001018           0.000382   \n",
       "std              0.031893     0.058520     0.031893           0.019537   \n",
       "min              0.000000     0.000000     0.000000           0.000000   \n",
       "25%              0.000000     0.000000     0.000000           0.000000   \n",
       "50%              0.000000     0.000000     0.000000           0.000000   \n",
       "75%              0.000000     0.000000     0.000000           0.000000   \n",
       "max              1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "       modern blues rock   album rock  modern folk rock  uk americana  \\\n",
       "count        7858.000000  7858.000000       7858.000000   7858.000000   \n",
       "mean            0.001273     0.002418          0.001400      0.001400   \n",
       "std             0.035653     0.049116          0.037391      0.037391   \n",
       "min             0.000000     0.000000          0.000000      0.000000   \n",
       "25%             0.000000     0.000000          0.000000      0.000000   \n",
       "50%             0.000000     0.000000          0.000000      0.000000   \n",
       "75%             0.000000     0.000000          0.000000      0.000000   \n",
       "max             1.000000     1.000000          1.000000      1.000000   \n",
       "\n",
       "       old school hip hop   punk blues      dmv rap  industrial metal  \\\n",
       "count         7858.000000  7858.000000  7858.000000       7858.000000   \n",
       "mean             0.001145     0.001273     0.002418          0.001273   \n",
       "std              0.033825     0.035653     0.049116          0.035653   \n",
       "min              0.000000     0.000000     0.000000          0.000000   \n",
       "25%              0.000000     0.000000     0.000000          0.000000   \n",
       "50%              0.000000     0.000000     0.000000          0.000000   \n",
       "75%              0.000000     0.000000     0.000000          0.000000   \n",
       "max              1.000000     1.000000     1.000000          1.000000   \n",
       "\n",
       "        skate punk  swedish synthpop   moombahton  Max_Peak_Position  \\\n",
       "count  7858.000000       7858.000000  7858.000000        7858.000000   \n",
       "mean      0.001145          0.000509     0.001018          75.775261   \n",
       "std       0.033825          0.022558     0.031893          24.561479   \n",
       "min       0.000000          0.000000     0.000000           1.000000   \n",
       "25%       0.000000          0.000000     0.000000          66.000000   \n",
       "50%       0.000000          0.000000     0.000000          84.000000   \n",
       "75%       0.000000          0.000000     0.000000          95.000000   \n",
       "max       1.000000          1.000000     1.000000         100.000000   \n",
       "\n",
       "       Max_Rank_Change  In_Top5genres_Mean  \n",
       "count      7858.000000         7858.000000  \n",
       "mean         12.531815            0.383706  \n",
       "std          11.760207            0.466419  \n",
       "min          -8.000000            0.000000  \n",
       "25%           3.000000            0.000000  \n",
       "50%          11.000000            0.000000  \n",
       "75%          17.000000            1.000000  \n",
       "max          79.000000            1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_withgenre = df_clean_withgenre.drop(['SongID'], axis=1)\n",
    "df_clean_withgenre.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_track_duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Max_Peak_Position</th>\n",
       "      <th>Max_Rank_Change</th>\n",
       "      <th>In_Top5genres_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "      <td>7858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225877.547340</td>\n",
       "      <td>0.634195</td>\n",
       "      <td>0.683831</td>\n",
       "      <td>5.258590</td>\n",
       "      <td>-5.955137</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.173873</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.184399</td>\n",
       "      <td>0.506799</td>\n",
       "      <td>122.323709</td>\n",
       "      <td>3.972003</td>\n",
       "      <td>75.775261</td>\n",
       "      <td>12.531815</td>\n",
       "      <td>0.383706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>46659.091277</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.172011</td>\n",
       "      <td>3.589907</td>\n",
       "      <td>2.228035</td>\n",
       "      <td>0.472709</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.215344</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.223494</td>\n",
       "      <td>29.558487</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>24.561479</td>\n",
       "      <td>11.760207</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37013.000000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.023000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>48.718000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198219.250000</td>\n",
       "      <td>0.533250</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.079000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>97.943500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>221798.500000</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>121.070000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248459.500000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.406500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>142.397250</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>992160.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>213.737000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spotify_track_duration_ms  danceability       energy          key  \\\n",
       "count                7858.000000   7858.000000  7858.000000  7858.000000   \n",
       "mean               225877.547340      0.634195     0.683831     5.258590   \n",
       "std                 46659.091277      0.149508     0.172011     3.589907   \n",
       "min                 37013.000000      0.113000     0.031600     0.000000   \n",
       "25%                198219.250000      0.533250     0.571000     2.000000   \n",
       "50%                221798.500000      0.638000     0.704000     5.000000   \n",
       "75%                248459.500000      0.740000     0.819000     8.000000   \n",
       "max                992160.000000      0.986000     0.996000    11.000000   \n",
       "\n",
       "          loudness         mode  speechiness  acousticness  instrumentalness  \\\n",
       "count  7858.000000  7858.000000  7858.000000   7858.000000       7858.000000   \n",
       "mean     -5.955137     0.663019     0.110003      0.173873          0.008682   \n",
       "std       2.228035     0.472709     0.110535      0.215344          0.066326   \n",
       "min     -23.023000     0.000000     0.022400      0.000003          0.000000   \n",
       "25%      -7.079000     0.000000     0.036200      0.019225          0.000000   \n",
       "50%      -5.640000     1.000000     0.057250      0.081900          0.000000   \n",
       "75%      -4.406500     1.000000     0.143000      0.247000          0.000017   \n",
       "max       0.175000     1.000000     0.951000      0.987000          0.982000   \n",
       "\n",
       "          liveness      valence        tempo  time_signature  \\\n",
       "count  7858.000000  7858.000000  7858.000000     7858.000000   \n",
       "mean      0.184399     0.506799   122.323709        3.972003   \n",
       "std       0.140660     0.223494    29.558487        0.273062   \n",
       "min       0.020000     0.034900    48.718000        0.000000   \n",
       "25%       0.095900     0.330000    97.943500        4.000000   \n",
       "50%       0.128000     0.505000   121.070000        4.000000   \n",
       "75%       0.234000     0.679000   142.397250        4.000000   \n",
       "max       0.986000     0.976000   213.737000        5.000000   \n",
       "\n",
       "       Max_Peak_Position  Max_Rank_Change  In_Top5genres_Mean  \n",
       "count        7858.000000      7858.000000         7858.000000  \n",
       "mean           75.775261        12.531815            0.383706  \n",
       "std            24.561479        11.760207            0.466419  \n",
       "min             1.000000        -8.000000            0.000000  \n",
       "25%            66.000000         3.000000            0.000000  \n",
       "50%            84.000000        11.000000            0.000000  \n",
       "75%            95.000000        17.000000            1.000000  \n",
       "max           100.000000        79.000000            1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_nogenre.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Variables\n",
    "\n",
    "I'm prepping 4 versions for XGBoost and k-NN:\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre__1 variables)\n",
    "2. Max Peak Position, with genre (withgenre_1 variables)\n",
    "3. Max Rank Change, no genre (nogenre_2 variables)\n",
    "4. Max Rank Change, with genre (withgenre_2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, no genre\n",
    "X_nogenre_1 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_1 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_1_train, X_nogenre_1_test, y_nogenre_1_train, y_nogenre_1_test = train_test_split(X_nogenre_1, y_nogenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_1_train_scaled = scaler.fit_transform(X_nogenre_1_train)\n",
    "X_nogenre_1_test_scaled = scaler.fit_transform(X_nogenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_peak_position analysis, including genre\n",
    "X_withgenre_1 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_1 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_1_train, X_withgenre_1_test, y_withgenre_1_train, y_withgenre_1_test = train_test_split(X_withgenre_1, y_withgenre_1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_1_train_scaled = scaler.fit_transform(X_withgenre_1_train)\n",
    "X_withgenre_1_test_scaled = scaler.fit_transform(X_withgenre_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, no genre\n",
    "X_nogenre_2 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_2 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_nogenre_2_train, X_nogenre_2_test, y_nogenre_2_train, y_nogenre_2_test = train_test_split(X_nogenre_2, y_nogenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_nogenre_2_train_scaled = scaler.fit_transform(X_nogenre_2_train)\n",
    "X_nogenre_2_test_scaled = scaler.fit_transform(X_nogenre_2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for XGBoost and k-NN max_rank_change analysis, including genre\n",
    "X_withgenre_2 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_2 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets (75-25 split and random_state of 42)\n",
    "X_withgenre_2_train, X_withgenre_2_test, y_withgenre_2_train, y_withgenre_2_test = train_test_split(X_withgenre_2, y_withgenre_2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_withgenre_2_train_scaled = scaler.fit_transform(X_withgenre_2_train)\n",
    "X_withgenre_2_test_scaled = scaler.fit_transform(X_withgenre_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another 4 versions of the data for the deep learning model\n",
    "\n",
    "1. Max Peak Position, no genre (nogenre_3 variables)\n",
    "2. Max Peak Position, with genre (withgenre_3 variables)\n",
    "3. Max Rank Change, no genre (nogenre_4 variables)\n",
    "4. Max Rank Change, with genre (withgenre_4 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, no genre\n",
    "X_nogenre_3 = df_clean_nogenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_nogenre_3 = df_clean_nogenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_3_train, X_nogenre_3_test, y_nogenre_3_train, y_nogenre_3_test = train_test_split(X_nogenre_3, y_nogenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_3_train_final, X_nogenre_3_val, y_nogenre_3_train_final, y_nogenre_3_val = train_test_split(X_nogenre_3_train, y_nogenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing \n",
    "scaler.fit(X_nogenre_3_train_final)\n",
    "X_nogenre_3_train_scaled = scaler.transform(X_nogenre_3_train_final)\n",
    "X_nogenre_3_val_scaled = scaler.transform(X_nogenre_3_val)\n",
    "X_nogenre_3_test_scaled = scaler.transform(X_nogenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_peak_position analysis, including genre\n",
    "X_withgenre_3 = df_clean_withgenre.drop(['Max_Peak_Position'], axis=1)\n",
    "y_withgenre_3 = df_clean_withgenre['Max_Peak_Position']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_3_train, X_withgenre_3_test, y_withgenre_3_train, y_withgenre_3_test = train_test_split(X_withgenre_3, y_withgenre_3, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_3_train_final, X_withgenre_3_val, y_withgenre_3_train_final, y_withgenre_3_val = train_test_split(X_withgenre_3_train, y_withgenre_3_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_3_train_final)\n",
    "X_withgenre_3_train_scaled = scaler.transform(X_withgenre_3_train_final)\n",
    "X_withgenre_3_val_scaled = scaler.transform(X_withgenre_3_val)\n",
    "X_withgenre_3_test_scaled = scaler.transform(X_withgenre_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target for deep learning max_rank_change analysis, no genre\n",
    "X_nogenre_4 = df_clean_nogenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_nogenre_4 = df_clean_nogenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_nogenre_4_train, X_nogenre_4_test, y_nogenre_4_train, y_nogenre_4_test = train_test_split(X_nogenre_4, y_nogenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_nogenre_4_train_final, X_nogenre_4_val, y_nogenre_4_train_final, y_nogenre_4_val = train_test_split(X_nogenre_4_train, y_nogenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_nogenre_4_train_final)\n",
    "X_nogenre_4_train_scaled = scaler.transform(X_nogenre_4_train_final)\n",
    "X_nogenre_4_val_scaled = scaler.transform(X_nogenre_4_val)\n",
    "X_nogenre_4_test_scaled = scaler.transform(X_nogenre_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for simple deep learning max_rank_change analysis, including genre\n",
    "X_withgenre_4 = df_clean_withgenre.drop(['Max_Peak_Position', 'Max_Rank_Change'], axis=1)\n",
    "y_withgenre_4 = df_clean_withgenre['Max_Rank_Change']\n",
    "\n",
    "# Splitting the data into training and testing sets \n",
    "X_withgenre_4_train, X_withgenre_4_test, y_withgenre_4_train, y_withgenre_4_test = train_test_split(X_withgenre_4, y_withgenre_4, test_size=0.2, random_state=42)\n",
    "\n",
    "# splitting training data into training and validiation\n",
    "X_withgenre_4_train_final, X_withgenre_4_val, y_withgenre_4_train_final, y_withgenre_4_val = train_test_split(X_withgenre_4_train, y_withgenre_4_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalizing\n",
    "scaler.fit(X_withgenre_4_train_final)\n",
    "X_withgenre_4_train_scaled = scaler.transform(X_withgenre_4_train_final)\n",
    "X_withgenre_4_val_scaled = scaler.transform(X_withgenre_4_val)\n",
    "X_withgenre_4_test_scaled = scaler.transform(X_withgenre_4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "**XGBoost | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 25.189\n",
      "R²: 0.009\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, no genre\n",
    "\n",
    "xgb_maxpeak_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "y_nogenre_1_pred = xgb_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "y_nogenre_1_pred = np.clip(np.round(y_nogenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_nogenre_1 = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred))\n",
    "r2_nogenre_1 = r2_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1:.3f}')\n",
    "print(f'R²: {r2_nogenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_1 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_1.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.14928596 0.1497933  0.14933232 0.1519076  0.1538763  0.15317112\n",
      " 0.14956039 0.14933785 0.15139946 0.15320263 0.15275146 0.1530459\n",
      " 0.14947191 0.14825419 0.14977659 0.13353322 0.13985333 0.13783634\n",
      " 0.15055884 0.14790804 0.15182688 0.13723545 0.1396863  0.13820686\n",
      " 0.12488542 0.12580568 0.13303351 0.14951099 0.14986331 0.15052954\n",
      " 0.15149311 0.15313528 0.15195441 0.15239897 0.15245192 0.15317686\n",
      " 0.15170226 0.15120819 0.15521759 0.1480341  0.14980488 0.15103229\n",
      " 0.1380379  0.14412221 0.14086841 0.14997244 0.15279588 0.15103062\n",
      " 0.13215508 0.13735918 0.14146951 0.11673868 0.12705541 0.12914339\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.15,],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.9, 1.0, 1.1]\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_2 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid2,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_2.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid3 = {\n",
    "    'max_depth': [2],\n",
    "    'learning_rate': [0.07, 0.1, 0.13],\n",
    "    'subsample': [0.85],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_nogenre_3 = GridSearchCV(estimator=xgb_maxpeak_nogenre,\n",
    "                            param_grid=param_grid3,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_nogenre_3.fit(X_nogenre_1_train, y_nogenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_nogenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.851\n",
      "R²: 0.185\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model for max_peak_position\n",
    "best_xgb1_1 = grid_search_xgb_nogenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_nogenre_1_pred_best = best_xgb1_1.predict(X_nogenre_1_test)\n",
    "y_nogenre_1_pred_best = np.clip(np.round(y_nogenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_nogenre_1_best = np.sqrt(mean_squared_error(y_nogenre_1_test, y_nogenre_1_pred_best))\n",
    "r2_nogenre_1_best = r2_score(y_nogenre_1_test, y_nogenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_nogenre_1_best:.3f}')\n",
    "print(f'R²: {r2_nogenre_1_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 22.751\n",
      "R²: 0.192\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_peak_position, with genre\n",
    "\n",
    "xgb_maxpeak_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxpeak_withgenre.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "y_withgenre_1_pred = xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred = np.clip(np.round(y_withgenre_1_pred), 1, 100)\n",
    "\n",
    "rmse_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred))\n",
    "r2_withgenre_1 = r2_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_1 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_1.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 405.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "135 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.22290783 0.22476627        nan 0.22961248 0.23208994        nan\n",
      " 0.22492174 0.22395395        nan 0.22358264 0.22480186        nan\n",
      " 0.22946347 0.22976409        nan 0.22246672 0.22280164        nan\n",
      " 0.22506559 0.22599194        nan 0.22510593 0.22972054        nan\n",
      " 0.21419413 0.21712435        nan 0.22480632 0.22179151        nan\n",
      " 0.22939501 0.23055565        nan 0.22415408 0.22779516        nan\n",
      " 0.22766875 0.22672504        nan 0.2268309  0.22682267        nan\n",
      " 0.22377956 0.22375016        nan 0.22684716 0.22516409        nan\n",
      " 0.2178275  0.21981466        nan 0.20304011 0.21110715        nan\n",
      " 0.22454622 0.22334156        nan 0.22969245 0.2287244         nan\n",
      " 0.2288182  0.22858987        nan 0.22661289 0.22694678        nan\n",
      " 0.22954861 0.22734727        nan 0.21356106 0.22451347        nan\n",
      " 0.22868726 0.22631047        nan 0.21676042 0.22319322        nan\n",
      " 0.21143588 0.20957587        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.15, 0.2, 0.25],\n",
    "    'subsample': [0.9, 1.0, 1.1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_2 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_2.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.15, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid5 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.12, 0.15, 0.17],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.5, 0.6],\n",
    "}\n",
    "\n",
    "grid_search_xgb_withgenre_3 = GridSearchCV(estimator=xgb_maxpeak_withgenre,\n",
    "                            param_grid=param_grid5,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_withgenre_3.fit(X_withgenre_1_train, y_withgenre_1_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21.607\n",
      "R²: 0.271\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model \n",
    "best_xgb_maxpeak_withgenre = grid_search_xgb_withgenre_3.best_estimator_\n",
    "\n",
    "# predictions\n",
    "y_withgenre_1_pred_best = best_xgb_maxpeak_withgenre.predict(X_withgenre_1_test)\n",
    "y_withgenre_1_pred_best = np.clip(np.round(y_withgenre_1_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_1_test, y_withgenre_1_pred_best))\n",
    "r2_withgenre_2_best = r2_score(y_withgenre_1_test, y_withgenre_1_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.605\n",
      "R²: -0.097\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_nogenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_nogenre.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "y_nogenre_2_pred = xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred = np.clip(np.round(y_nogenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxrank_nogenre_2 = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred))\n",
    "r2_maxrank_nogenre_2 = r2_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1 for max_rank_change\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_1 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_1.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 3, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "135 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.00972437        nan 0.01147871        nan 0.0099106         nan\n",
      " 0.01288679        nan 0.01492339        nan 0.0120229         nan\n",
      " 0.01355836        nan 0.01520104        nan 0.011404          nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2 for max_rank_change\n",
    "\n",
    "param_grid4 = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [1.0, 1.1],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_nogenre_2 = GridSearchCV(estimator=xgb_maxrank_nogenre,\n",
    "                            param_grid=param_grid4,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_nogenre_2.fit(X_nogenre_2_train, y_nogenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_nogenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.918\n",
      "R²: 0.020\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_nogenre = grid_search_xgb_maxrank_nogenre_2.best_estimator_\n",
    "\n",
    "y_nogenre_2_pred_best = best_xgb_maxrank_nogenre.predict(X_nogenre_2_test)\n",
    "y_nogenre_2_pred_best = np.clip(np.round(y_nogenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxrank_nogenre_2_best = np.sqrt(mean_squared_error(y_nogenre_2_test, y_nogenre_2_pred_best))\n",
    "r2_maxrank_nogenre_2_best = r2_score(y_nogenre_2_test, y_nogenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxrank_nogenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxrank_nogenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.342\n",
      "R²: -0.051\n"
     ]
    }
   ],
   "source": [
    "# XGBoost for max_rank_change\n",
    "\n",
    "xgb_maxrank_withgenre = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_maxrank_withgenre.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "y_withgenre_2_pred = xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred = np.clip(np.round(y_withgenre_2_pred), 1, 100)\n",
    "\n",
    "rmse_maxpeak_withgenre_1 = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred))\n",
    "r2_maxpeak_withgenre_1 = r2_score(y_withgenre_2_test, y_withgenre_2_pred)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_1:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 1\n",
    "\n",
    "param_grid1 = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_1 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid1,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_1.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.1 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.0224043  0.0229799  0.02310325 0.023507   0.02370381 0.02407839\n",
      " 0.03057899 0.03026649 0.0313546  0.03141223 0.0324422  0.03149776\n",
      " 0.03260914 0.03307942 0.03341982 0.0323358  0.03272635 0.03061318\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 2\n",
    "\n",
    "param_grid6 = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_2 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid6,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_2.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.015, 'max_depth': 6, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning 3\n",
    "\n",
    "param_grid7 = {\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.013, 0.015, 0.017],\n",
    "    'subsample': [0.5, 0.6, 0.7],\n",
    "    'colsample_bytree': [1.0],\n",
    "}\n",
    "\n",
    "grid_search_xgb_maxrank_withgenre_3 = GridSearchCV(estimator=xgb_maxrank_withgenre,\n",
    "                            param_grid=param_grid7,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=1)\n",
    "grid_search_xgb_maxrank_withgenre_3.fit(X_withgenre_2_train, y_withgenre_2_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_xgb_maxrank_withgenre_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.764\n",
      "R²: 0.045\n"
     ]
    }
   ],
   "source": [
    "# Extract best/final model  for max_rank_change\n",
    "best_xgb_maxrank_withgenre = grid_search_xgb_maxrank_withgenre_3.best_estimator_\n",
    "\n",
    "y_withgenre_2_pred_best = best_xgb_maxrank_withgenre.predict(X_withgenre_2_test)\n",
    "y_withgenre_2_pred_best = np.clip(np.round(y_withgenre_2_pred_best), 1, 100)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_maxpeak_withgenre_2_best = np.sqrt(mean_squared_error(y_withgenre_2_test, y_withgenre_2_pred_best))\n",
    "r2_maxpeak_withgenre_2_best = r2_score(y_withgenre_2_test, y_withgenre_2_pred_best)\n",
    "\n",
    "print(f'RMSE: {rmse_maxpeak_withgenre_2_best:.3f}')\n",
    "print(f'R²: {r2_maxpeak_withgenre_2_best:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Summary\n",
    "\n",
    "Using XGBoost, including the genre features slightly improved model performance. However, the Max Rank Change models both had an r<sup>2</sup> value less than 0.001, essentially indicating no fit of the model to the test data. Max Peak Position performed better, but the highest r<sup>2</sup> value was 0.271 so their predictive value is low.\n",
    "\n",
    "Given the lack of predictive power in these outcomes, I'm shifting focus to the other two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 0.0439\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_nogenre.fit(X_nogenre_1_train_scaled, y_nogenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_params_\n",
    "standard_best_score_params_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0326\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_nogenre = grid_search_maxpeak_nogenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_nogenre_1_pred = final_model_maxpeak_nogenre.predict(X_nogenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_nogenre_1 = accuracy_score(y_nogenre_1_test, y_nogenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_nogenre_1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n",
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'euclidean', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.0409\n"
     ]
    }
   ],
   "source": [
    "# parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxpeak_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxpeak_withgenre.fit(X_withgenre_1_train_scaled, y_withgenre_1_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_params_\n",
    "standard_best_score_params_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxpeak_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_params_maxpeak_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.0351\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxpeak_withgenre = grid_search_maxpeak_withgenre.best_estimator_\n",
    "\n",
    "# predictions on test set\n",
    "y_withgenre_1_pred = final_model_maxpeak_withgenre.predict(X_withgenre_1_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxpeak_withgenre = accuracy_score(y_withgenre_1_test, y_withgenre_1_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxpeak_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2350\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_nogenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_nogenre.fit(X_nogenre_2_train_scaled, y_nogenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_nogenre = grid_search_maxrank_nogenre.best_params_\n",
    "standard_best_score_maxrank_nogenre = grid_search_maxrank_nogenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_nogenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_nogenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2163\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_nogenre = grid_search_maxrank_nogenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_nogenre_2_pred = final_model_maxrank_nogenre.predict(X_nogenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_nogenre = accuracy_score(y_nogenre_2_test, y_nogenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_nogenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for standard metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (standard metrics): {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'uniform'}\n",
      "Best cross-validation accuracy: 0.2379\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for standard metrics\n",
    "param_grid8 = {\n",
    "    'n_neighbors': list(range(1, 31, 2)),  # Odd values to avoid ties\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "# Create standard KNN model for grid search\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting grid search for standard metrics...\")\n",
    "grid_search_maxrank_withgenre = GridSearchCV(knn, param_grid8, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_maxrank_withgenre.fit(X_withgenre_2_train_scaled, y_withgenre_2_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "standard_best_params_maxrank_withgenre = grid_search_maxrank_withgenre.best_params_\n",
    "standard_best_score_maxrank_withgenre = grid_search_maxrank_withgenre.best_score_\n",
    "print(f\"Best parameters (standard metrics): {standard_best_params_maxrank_withgenre}\")\n",
    "print(f\"Best cross-validation accuracy: {standard_best_score_maxrank_withgenre:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.2173\n"
     ]
    }
   ],
   "source": [
    "# final model with best parameters\n",
    "final_model_maxrank_withgenre = grid_search_maxrank_withgenre.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on test set\n",
    "y_maxrank_withgenre_2_pred = final_model_maxrank_withgenre.predict(X_withgenre_2_test_scaled)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_maxrank_withgenre = accuracy_score(y_withgenre_2_test, y_maxrank_withgenre_2_pred)\n",
    "\n",
    "print(f\"Best accuracy: {accuracy_maxrank_withgenre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Summary\n",
    "\n",
    "The k-NN models performed poorly on the Max Peak Position data, with a maximum accuracy of 0.035. The performance on the Max Rank Change was better, but the maximum accuracy was still just 0.217.\n",
    "\n",
    "I will not explore k-NN further and instead focus on the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "**Deep Learning | Max Peak Position - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2419.2927 - mae: 39.6480 - mse: 2419.2927 - val_loss: 689.4341 - val_mae: 21.1786 - val_mse: 689.4341\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 665.7994 - mae: 20.4834 - mse: 665.7994 - val_loss: 615.3640 - val_mae: 19.6692 - val_mse: 615.3640\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 608.4424 - mae: 19.4169 - mse: 608.4424 - val_loss: 568.2308 - val_mae: 19.1297 - val_mse: 568.2308\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.5310 - mae: 18.8835 - mse: 580.5310 - val_loss: 552.3713 - val_mae: 18.7825 - val_mse: 552.3713\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 563.8795 - mae: 18.5870 - mse: 563.8795 - val_loss: 532.4030 - val_mae: 18.4636 - val_mse: 532.4030\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 552.4023 - mae: 18.3202 - mse: 552.4023 - val_loss: 531.0229 - val_mae: 18.1369 - val_mse: 531.0229\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 543.6500 - mae: 18.0884 - mse: 543.6500 - val_loss: 524.1937 - val_mae: 18.6026 - val_mse: 524.1937\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 538.1226 - mae: 18.0059 - mse: 538.1226 - val_loss: 513.3173 - val_mae: 18.1826 - val_mse: 513.3173\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4422 - mae: 17.8144 - mse: 531.4422 - val_loss: 509.8279 - val_mae: 17.9694 - val_mse: 509.8279\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 527.5864 - mae: 17.7839 - mse: 527.5864 - val_loss: 503.0971 - val_mae: 18.0760 - val_mse: 503.0971\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 521.5410 - mae: 17.6273 - mse: 521.5410 - val_loss: 510.4745 - val_mae: 18.4716 - val_mse: 510.4745\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 515.6498 - mae: 17.4958 - mse: 515.6498 - val_loss: 504.4003 - val_mae: 18.2854 - val_mse: 504.4003\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 509.2001 - mae: 17.3392 - mse: 509.2001 - val_loss: 492.7363 - val_mae: 17.6454 - val_mse: 492.7363\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 506.5855 - mae: 17.3311 - mse: 506.5855 - val_loss: 494.3658 - val_mae: 17.1993 - val_mse: 494.3658\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 503.0619 - mae: 17.1726 - mse: 503.0619 - val_loss: 489.0531 - val_mae: 17.2873 - val_mse: 489.0531\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 502.0815 - mae: 17.2147 - mse: 502.0815 - val_loss: 490.0863 - val_mae: 17.6323 - val_mse: 490.0863\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 498.6240 - mae: 17.1130 - mse: 498.6240 - val_loss: 488.2263 - val_mae: 17.5757 - val_mse: 488.2263\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 494.3361 - mae: 17.0147 - mse: 494.3361 - val_loss: 493.1820 - val_mae: 17.7805 - val_mse: 493.1820\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 494.9956 - mae: 17.0423 - mse: 494.9956 - val_loss: 491.9977 - val_mae: 17.4102 - val_mse: 491.9977\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 491.8429 - mae: 17.0051 - mse: 491.8429 - val_loss: 501.6566 - val_mae: 16.9908 - val_mse: 501.6566\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 487.2634 - mae: 16.9084 - mse: 487.2634 - val_loss: 490.1104 - val_mae: 17.3877 - val_mse: 490.1104\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 486.1548 - mae: 16.9138 - mse: 486.1548 - val_loss: 490.3252 - val_mae: 17.5433 - val_mse: 490.3252\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 486.3818 - mae: 16.9306 - mse: 486.3818 - val_loss: 496.2074 - val_mae: 17.0037 - val_mse: 496.2074\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 482.7021 - mae: 16.7403 - mse: 482.7021 - val_loss: 489.6503 - val_mae: 17.2741 - val_mse: 489.6503\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.2343 - mae: 16.7226 - mse: 478.2343 - val_loss: 499.0989 - val_mae: 18.0239 - val_mse: 499.0989\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 480.3737 - mae: 16.7965 - mse: 480.3737 - val_loss: 515.3365 - val_mae: 18.6585 - val_mse: 515.3365\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.9332 - mae: 16.7417 - mse: 478.9332 - val_loss: 486.3153 - val_mae: 17.4821 - val_mse: 486.3153\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 474.6300 - mae: 16.6473 - mse: 474.6300 - val_loss: 486.3873 - val_mae: 17.5012 - val_mse: 486.3873\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 473.7096 - mae: 16.6590 - mse: 473.7096 - val_loss: 485.6329 - val_mae: 17.4391 - val_mse: 485.6329\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 473.0536 - mae: 16.6453 - mse: 473.0536 - val_loss: 488.7411 - val_mae: 17.4251 - val_mse: 488.7411\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 470.8673 - mae: 16.6039 - mse: 470.8673 - val_loss: 517.8712 - val_mae: 16.9943 - val_mse: 517.8712\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 468.8740 - mae: 16.5972 - mse: 468.8740 - val_loss: 490.3929 - val_mae: 17.4073 - val_mse: 490.3929\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 466.5359 - mae: 16.4863 - mse: 466.5359 - val_loss: 490.4259 - val_mae: 17.5247 - val_mse: 490.4259\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.0333 - mae: 16.5200 - mse: 465.0333 - val_loss: 496.1299 - val_mae: 17.7819 - val_mse: 496.1299\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 465.6708 - mae: 16.4928 - mse: 465.6708 - val_loss: 495.2829 - val_mae: 17.6568 - val_mse: 495.2829\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 463.1415 - mae: 16.4833 - mse: 463.1415 - val_loss: 501.4291 - val_mae: 18.0463 - val_mse: 501.4291\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 460.4989 - mae: 16.4622 - mse: 460.4989 - val_loss: 502.9875 - val_mae: 18.1109 - val_mse: 502.9875\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 457.1301 - mae: 16.3698 - mse: 457.1301 - val_loss: 496.6900 - val_mae: 17.2185 - val_mse: 496.6900\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 459.4128 - mae: 16.4314 - mse: 459.4128 - val_loss: 502.8075 - val_mae: 17.0330 - val_mse: 502.8075\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 454.1688 - mae: 16.2550 - mse: 454.1688 - val_loss: 497.0360 - val_mae: 17.2852 - val_mse: 497.0360\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 455.2572 - mae: 16.3576 - mse: 455.2572 - val_loss: 507.4308 - val_mae: 17.3179 - val_mse: 507.4308\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 452.3672 - mae: 16.2527 - mse: 452.3672 - val_loss: 494.0938 - val_mae: 17.4207 - val_mse: 494.0938\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 450.1534 - mae: 16.2146 - mse: 450.1534 - val_loss: 506.9326 - val_mae: 17.8810 - val_mse: 506.9326\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.2242 - mae: 16.2945 - mse: 451.2242 - val_loss: 500.4746 - val_mae: 17.6965 - val_mse: 500.4746\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 447.5209 - mae: 16.1485 - mse: 447.5209 - val_loss: 508.4296 - val_mae: 18.0848 - val_mse: 508.4296\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 447.8461 - mae: 16.2137 - mse: 447.8461 - val_loss: 510.0475 - val_mae: 17.8509 - val_mse: 510.0475\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 447.6882 - mae: 16.1755 - mse: 447.6882 - val_loss: 515.8989 - val_mae: 18.3052 - val_mse: 515.8989\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 442.2391 - mae: 16.1399 - mse: 442.2391 - val_loss: 506.1750 - val_mae: 17.1819 - val_mse: 506.1750\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.5520 - mae: 15.9899 - mse: 438.5520 - val_loss: 510.4245 - val_mae: 17.2800 - val_mse: 510.4245\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 436.8173 - mae: 15.9844 - mse: 436.8173 - val_loss: 508.9857 - val_mae: 18.0230 - val_mse: 508.9857\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 436.6111 - mae: 15.9807 - mse: 436.6111 - val_loss: 513.3489 - val_mae: 18.0142 - val_mse: 513.3489\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 436.0714 - mae: 15.9555 - mse: 436.0714 - val_loss: 508.2621 - val_mae: 17.4788 - val_mse: 508.2621\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 434.0513 - mae: 15.9039 - mse: 434.0513 - val_loss: 513.2465 - val_mae: 17.4503 - val_mse: 513.2465\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.7212 - mae: 15.9133 - mse: 434.7212 - val_loss: 511.5938 - val_mae: 17.9016 - val_mse: 511.5938\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 432.2248 - mae: 15.9067 - mse: 432.2248 - val_loss: 525.3815 - val_mae: 17.3754 - val_mse: 525.3815\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 430.1569 - mae: 15.8171 - mse: 430.1569 - val_loss: 515.8960 - val_mae: 18.0224 - val_mse: 515.8960\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 430.4926 - mae: 15.9089 - mse: 430.4926 - val_loss: 518.7918 - val_mae: 17.9219 - val_mse: 518.7918\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.6432 - mae: 15.7649 - mse: 426.6432 - val_loss: 515.3190 - val_mae: 17.4910 - val_mse: 515.3190\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 426.5134 - mae: 15.8229 - mse: 426.5134 - val_loss: 516.7847 - val_mae: 17.7470 - val_mse: 516.7847\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 426.6697 - mae: 15.8196 - mse: 426.6697 - val_loss: 545.5805 - val_mae: 17.3761 - val_mse: 545.5805\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 423.8633 - mae: 15.7151 - mse: 423.8633 - val_loss: 524.4741 - val_mae: 18.1706 - val_mse: 524.4741\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 423.3539 - mae: 15.7032 - mse: 423.3539 - val_loss: 523.2936 - val_mae: 17.6593 - val_mse: 523.2936\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 421.9861 - mae: 15.7878 - mse: 421.9861 - val_loss: 525.7878 - val_mae: 17.7972 - val_mse: 525.7878\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 418.5110 - mae: 15.6692 - mse: 418.5110 - val_loss: 521.7631 - val_mae: 17.9073 - val_mse: 521.7631\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 416.9155 - mae: 15.6447 - mse: 416.9155 - val_loss: 532.5114 - val_mae: 17.8788 - val_mse: 532.5114\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 415.7471 - mae: 15.6018 - mse: 415.7471 - val_loss: 526.9938 - val_mae: 18.0233 - val_mse: 526.9938\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.1190 - mae: 15.4875 - mse: 412.1190 - val_loss: 533.2522 - val_mae: 18.3125 - val_mse: 533.2522\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 414.5464 - mae: 15.5918 - mse: 414.5464 - val_loss: 527.8963 - val_mae: 18.0504 - val_mse: 527.8963\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 411.2368 - mae: 15.5448 - mse: 411.2368 - val_loss: 533.0829 - val_mae: 17.7895 - val_mse: 533.0829\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.0792 - mae: 15.4613 - mse: 409.0792 - val_loss: 539.5701 - val_mae: 17.7955 - val_mse: 539.5701\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 407.0265 - mae: 15.4026 - mse: 407.0265 - val_loss: 546.9268 - val_mae: 17.7916 - val_mse: 546.9268\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 411.2872 - mae: 15.5100 - mse: 411.2872 - val_loss: 543.2707 - val_mae: 17.9467 - val_mse: 543.2707\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 409.4622 - mae: 15.5657 - mse: 409.4622 - val_loss: 545.8762 - val_mae: 17.7985 - val_mse: 545.8762\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.8742 - mae: 15.3437 - mse: 404.8742 - val_loss: 545.9336 - val_mae: 18.4481 - val_mse: 545.9336\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 404.7788 - mae: 15.3833 - mse: 404.7788 - val_loss: 547.0612 - val_mae: 17.7472 - val_mse: 547.0612\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 404.1299 - mae: 15.4497 - mse: 404.1299 - val_loss: 540.1212 - val_mae: 18.2801 - val_mse: 540.1212\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.4113 - mae: 15.2785 - mse: 399.4113 - val_loss: 552.5936 - val_mae: 18.7343 - val_mse: 552.5936\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 398.2041 - mae: 15.3230 - mse: 398.2041 - val_loss: 538.3664 - val_mae: 17.9450 - val_mse: 538.3664\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 399.7830 - mae: 15.3380 - mse: 399.7830 - val_loss: 543.1089 - val_mae: 18.3602 - val_mse: 543.1089\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 401.6050 - mae: 15.3529 - mse: 401.6050 - val_loss: 546.5948 - val_mae: 17.9844 - val_mse: 546.5948\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 398.7794 - mae: 15.2802 - mse: 398.7794 - val_loss: 544.4263 - val_mae: 18.2400 - val_mse: 544.4263\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.3988 - mae: 15.2791 - mse: 397.3988 - val_loss: 554.1939 - val_mae: 17.9493 - val_mse: 554.1939\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 393.7537 - mae: 15.1798 - mse: 393.7537 - val_loss: 550.9030 - val_mae: 18.3655 - val_mse: 550.9030\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.1619 - mae: 15.1724 - mse: 392.1619 - val_loss: 552.3148 - val_mae: 18.3455 - val_mse: 552.3148\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 389.9302 - mae: 15.1217 - mse: 389.9302 - val_loss: 557.1672 - val_mae: 18.3695 - val_mse: 557.1672\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.8891 - mae: 15.1914 - mse: 392.8891 - val_loss: 558.3782 - val_mae: 18.2113 - val_mse: 558.3782\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 388.0539 - mae: 15.0538 - mse: 388.0539 - val_loss: 565.5303 - val_mae: 18.7598 - val_mse: 565.5303\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 389.9147 - mae: 15.2011 - mse: 389.9147 - val_loss: 554.7290 - val_mae: 18.1745 - val_mse: 554.7290\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 384.1323 - mae: 15.0548 - mse: 384.1323 - val_loss: 555.0225 - val_mae: 18.5653 - val_mse: 555.0225\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 386.8987 - mae: 15.1158 - mse: 386.8987 - val_loss: 557.1505 - val_mae: 18.1024 - val_mse: 557.1505\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 388.1218 - mae: 15.0916 - mse: 388.1218 - val_loss: 563.1410 - val_mae: 18.3664 - val_mse: 563.1410\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 384.0925 - mae: 14.9991 - mse: 384.0925 - val_loss: 562.3793 - val_mae: 18.5614 - val_mse: 562.3793\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.8701 - mae: 15.0815 - mse: 385.8701 - val_loss: 557.7654 - val_mae: 18.4775 - val_mse: 557.7654\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.0906 - mae: 14.9543 - mse: 381.0906 - val_loss: 564.2411 - val_mae: 18.6923 - val_mse: 564.2411\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.4917 - mae: 14.9239 - mse: 379.4917 - val_loss: 559.4780 - val_mae: 18.7443 - val_mse: 559.4780\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 379.8032 - mae: 15.0226 - mse: 379.8032 - val_loss: 567.9104 - val_mae: 18.2340 - val_mse: 567.9104\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.5663 - mae: 14.9266 - mse: 379.5663 - val_loss: 575.2344 - val_mae: 18.7882 - val_mse: 575.2344\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.1804 - mae: 14.9150 - mse: 376.1804 - val_loss: 565.7868 - val_mae: 18.6739 - val_mse: 565.7868\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.5493 - mae: 14.8226 - mse: 373.5493 - val_loss: 571.8291 - val_mae: 18.8567 - val_mse: 571.8291\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 373.8908 - mae: 14.8477 - mse: 373.8908 - val_loss: 574.3768 - val_mae: 18.8456 - val_mse: 574.3768\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_1 = baseline_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5851.8906 - mae: 72.6476 - mse: 5851.8906 - val_loss: 5651.5874 - val_mae: 71.4733 - val_mse: 5651.5874\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4666.3667 - mae: 64.3989 - mse: 4666.3667 - val_loss: 3954.0635 - val_mae: 59.0888 - val_mse: 3954.0635\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3064.9041 - mae: 51.3851 - mse: 3064.9041 - val_loss: 2353.0398 - val_mae: 44.6987 - val_mse: 2353.0398\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1626.5095 - mae: 36.4668 - mse: 1626.5095 - val_loss: 1146.4127 - val_mae: 30.0209 - val_mse: 1146.4127\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 827.4645 - mae: 24.9593 - mse: 827.4645 - val_loss: 643.7032 - val_mae: 21.6736 - val_mse: 643.7032\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 574.0120 - mae: 19.6001 - mse: 574.0120 - val_loss: 542.1612 - val_mae: 19.1383 - val_mse: 542.1612\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 517.4794 - mae: 17.9477 - mse: 517.4794 - val_loss: 489.2751 - val_mae: 17.4206 - val_mse: 489.2751\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 510.0293 - mae: 17.3691 - mse: 510.0293 - val_loss: 486.8300 - val_mae: 17.3675 - val_mse: 486.8300\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 503.0972 - mae: 17.2335 - mse: 503.0972 - val_loss: 488.8733 - val_mae: 17.3222 - val_mse: 488.8733\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 497.4265 - mae: 17.0943 - mse: 497.4265 - val_loss: 487.3111 - val_mae: 17.2283 - val_mse: 487.3111\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 496.1514 - mae: 17.0929 - mse: 496.1514 - val_loss: 491.9576 - val_mae: 17.2409 - val_mse: 491.9576\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 491.6809 - mae: 16.9804 - mse: 491.6809 - val_loss: 492.3288 - val_mae: 16.9923 - val_mse: 492.3288\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 482.7526 - mae: 16.7466 - mse: 482.7526 - val_loss: 494.0729 - val_mae: 17.6306 - val_mse: 494.0729\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 480.6911 - mae: 16.7360 - mse: 480.6911 - val_loss: 491.8650 - val_mae: 17.3832 - val_mse: 491.8650\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 474.4710 - mae: 16.6212 - mse: 474.4710 - val_loss: 493.5953 - val_mae: 17.2509 - val_mse: 493.5953\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 474.4846 - mae: 16.5760 - mse: 474.4846 - val_loss: 505.0648 - val_mae: 17.4732 - val_mse: 505.0648\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 473.2977 - mae: 16.6698 - mse: 473.2977 - val_loss: 505.7195 - val_mae: 17.9222 - val_mse: 505.7195\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 464.8788 - mae: 16.4491 - mse: 464.8788 - val_loss: 502.1259 - val_mae: 17.6153 - val_mse: 502.1259\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 463.1366 - mae: 16.4096 - mse: 463.1366 - val_loss: 497.7792 - val_mae: 17.4499 - val_mse: 497.7792\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 458.4832 - mae: 16.3408 - mse: 458.4832 - val_loss: 501.3792 - val_mae: 17.1890 - val_mse: 501.3792\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 460.0415 - mae: 16.3418 - mse: 460.0415 - val_loss: 499.8820 - val_mae: 17.3720 - val_mse: 499.8820\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.6199 - mae: 16.1744 - mse: 451.6199 - val_loss: 513.5532 - val_mae: 17.1330 - val_mse: 513.5532\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.7422 - mae: 16.1587 - mse: 451.7422 - val_loss: 509.7841 - val_mae: 17.4475 - val_mse: 509.7841\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 443.7116 - mae: 16.0591 - mse: 443.7116 - val_loss: 515.0828 - val_mae: 17.7905 - val_mse: 515.0828\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 449.9147 - mae: 16.2115 - mse: 449.9147 - val_loss: 512.6425 - val_mae: 17.3624 - val_mse: 512.6425\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 444.7753 - mae: 16.0769 - mse: 444.7753 - val_loss: 517.2388 - val_mae: 17.8782 - val_mse: 517.2388\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 447.0614 - mae: 16.0617 - mse: 447.0614 - val_loss: 511.5176 - val_mae: 17.7023 - val_mse: 511.5176\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 441.8242 - mae: 15.9474 - mse: 441.8242 - val_loss: 514.6925 - val_mae: 17.7256 - val_mse: 514.6925\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.8530 - mae: 15.8245 - mse: 434.8530 - val_loss: 521.7326 - val_mae: 17.2641 - val_mse: 521.7326\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 432.8844 - mae: 15.8290 - mse: 432.8844 - val_loss: 520.5737 - val_mae: 17.8681 - val_mse: 520.5737\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 424.4903 - mae: 15.6312 - mse: 424.4903 - val_loss: 526.0905 - val_mae: 17.7782 - val_mse: 526.0905\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 434.3234 - mae: 15.8358 - mse: 434.3234 - val_loss: 529.7477 - val_mae: 17.3516 - val_mse: 529.7477\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 429.9236 - mae: 15.7594 - mse: 429.9236 - val_loss: 524.9996 - val_mae: 17.8921 - val_mse: 524.9996\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 427.0891 - mae: 15.6821 - mse: 427.0891 - val_loss: 528.3973 - val_mae: 17.6463 - val_mse: 528.3973\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 425.5359 - mae: 15.5754 - mse: 425.5359 - val_loss: 515.7779 - val_mae: 17.7935 - val_mse: 515.7779\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 428.0937 - mae: 15.7617 - mse: 428.0937 - val_loss: 525.2448 - val_mae: 17.6638 - val_mse: 525.2448\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.5238 - mae: 15.5518 - mse: 419.5238 - val_loss: 521.1468 - val_mae: 17.7045 - val_mse: 521.1468\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.1915 - mae: 15.5597 - mse: 419.1915 - val_loss: 525.3145 - val_mae: 17.8965 - val_mse: 525.3145\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 414.3619 - mae: 15.5778 - mse: 414.3619 - val_loss: 527.6876 - val_mae: 17.4749 - val_mse: 527.6876\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 413.3690 - mae: 15.3675 - mse: 413.3690 - val_loss: 526.1476 - val_mae: 17.9665 - val_mse: 526.1476\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 419.3266 - mae: 15.5896 - mse: 419.3266 - val_loss: 529.5512 - val_mae: 18.0028 - val_mse: 529.5512\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 410.9309 - mae: 15.4720 - mse: 410.9309 - val_loss: 537.4049 - val_mae: 17.8429 - val_mse: 537.4049\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 409.1388 - mae: 15.3905 - mse: 409.1388 - val_loss: 538.3312 - val_mae: 17.8216 - val_mse: 538.3312\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 403.8900 - mae: 15.2370 - mse: 403.8900 - val_loss: 531.2563 - val_mae: 17.9443 - val_mse: 531.2563\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.3340 - mae: 15.2030 - mse: 405.3340 - val_loss: 540.5571 - val_mae: 17.9567 - val_mse: 540.5571\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 408.0271 - mae: 15.2985 - mse: 408.0271 - val_loss: 545.4421 - val_mae: 17.9337 - val_mse: 545.4421\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 399.3531 - mae: 15.1505 - mse: 399.3531 - val_loss: 542.5694 - val_mae: 17.9608 - val_mse: 542.5694\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 404.3011 - mae: 15.2841 - mse: 404.3011 - val_loss: 540.3690 - val_mae: 17.8100 - val_mse: 540.3690\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 398.5748 - mae: 15.2058 - mse: 398.5748 - val_loss: 533.5903 - val_mae: 17.8229 - val_mse: 533.5903\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 395.3835 - mae: 15.1408 - mse: 395.3835 - val_loss: 540.4743 - val_mae: 18.1131 - val_mse: 540.4743\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 401.2710 - mae: 15.2145 - mse: 401.2710 - val_loss: 544.1772 - val_mae: 18.1012 - val_mse: 544.1772\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 391.0296 - mae: 15.0302 - mse: 391.0296 - val_loss: 543.8338 - val_mae: 18.0552 - val_mse: 543.8338\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.4317 - mae: 15.1154 - mse: 394.4317 - val_loss: 565.3024 - val_mae: 18.1627 - val_mse: 565.3024\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.4927 - mae: 15.1004 - mse: 397.4927 - val_loss: 557.1964 - val_mae: 18.5762 - val_mse: 557.1964\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 396.5296 - mae: 15.1524 - mse: 396.5296 - val_loss: 543.5672 - val_mae: 17.9871 - val_mse: 543.5672\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.1523 - mae: 15.0774 - mse: 394.1523 - val_loss: 546.2386 - val_mae: 18.2674 - val_mse: 546.2386\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.4361 - mae: 14.8390 - mse: 383.4361 - val_loss: 539.5917 - val_mae: 17.9089 - val_mse: 539.5917\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 389.5795 - mae: 15.0152 - mse: 389.5795 - val_loss: 555.7221 - val_mae: 18.1397 - val_mse: 555.7221\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 387.9057 - mae: 14.9026 - mse: 387.9057 - val_loss: 545.2907 - val_mae: 17.7533 - val_mse: 545.2907\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 390.6403 - mae: 15.0576 - mse: 390.6403 - val_loss: 551.6979 - val_mae: 17.9138 - val_mse: 551.6979\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 382.3562 - mae: 14.8456 - mse: 382.3562 - val_loss: 554.2005 - val_mae: 18.5094 - val_mse: 554.2005\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.6459 - mae: 14.9305 - mse: 383.6459 - val_loss: 558.4500 - val_mae: 18.4947 - val_mse: 558.4500\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 379.8488 - mae: 14.7736 - mse: 379.8488 - val_loss: 551.4978 - val_mae: 18.0294 - val_mse: 551.4978\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.9655 - mae: 14.6613 - mse: 376.9655 - val_loss: 559.6479 - val_mae: 18.3410 - val_mse: 559.6479\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 380.7325 - mae: 14.7738 - mse: 380.7325 - val_loss: 560.1947 - val_mae: 18.3413 - val_mse: 560.1947\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 380.7882 - mae: 14.7935 - mse: 380.7882 - val_loss: 560.7771 - val_mae: 18.2944 - val_mse: 560.7771\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 373.7351 - mae: 14.7160 - mse: 373.7351 - val_loss: 565.0800 - val_mae: 18.1687 - val_mse: 565.0800\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 374.8674 - mae: 14.7094 - mse: 374.8674 - val_loss: 554.6207 - val_mae: 18.0937 - val_mse: 554.6207\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 383.5134 - mae: 14.9012 - mse: 383.5134 - val_loss: 556.4305 - val_mae: 18.2633 - val_mse: 556.4305\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.2375 - mae: 14.6628 - mse: 372.2375 - val_loss: 562.2667 - val_mae: 18.3973 - val_mse: 562.2667\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 375.1579 - mae: 14.6821 - mse: 375.1579 - val_loss: 568.1216 - val_mae: 18.3824 - val_mse: 568.1216\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 378.9480 - mae: 14.7438 - mse: 378.9480 - val_loss: 555.2978 - val_mae: 17.7532 - val_mse: 555.2978\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 368.0993 - mae: 14.6170 - mse: 368.0993 - val_loss: 563.8961 - val_mae: 18.0530 - val_mse: 563.8961\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 368.0417 - mae: 14.6307 - mse: 368.0417 - val_loss: 559.6123 - val_mae: 18.0026 - val_mse: 559.6123\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 370.7229 - mae: 14.5916 - mse: 370.7229 - val_loss: 571.9691 - val_mae: 18.6153 - val_mse: 571.9691\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.3111 - mae: 14.5475 - mse: 364.3111 - val_loss: 571.9350 - val_mae: 18.2136 - val_mse: 571.9350\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.6209 - mae: 14.5813 - mse: 372.6209 - val_loss: 563.7224 - val_mae: 18.3297 - val_mse: 563.7224\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 367.5572 - mae: 14.5395 - mse: 367.5572 - val_loss: 565.8137 - val_mae: 18.2724 - val_mse: 565.8137\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 367.5726 - mae: 14.6770 - mse: 367.5726 - val_loss: 565.5074 - val_mae: 18.1168 - val_mse: 565.5074\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 361.0623 - mae: 14.3681 - mse: 361.0623 - val_loss: 567.7952 - val_mae: 18.2103 - val_mse: 567.7952\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 367.5349 - mae: 14.7083 - mse: 367.5349 - val_loss: 562.0535 - val_mae: 18.1528 - val_mse: 562.0535\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 365.8636 - mae: 14.4632 - mse: 365.8636 - val_loss: 564.8405 - val_mae: 18.1964 - val_mse: 564.8405\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 366.3882 - mae: 14.4953 - mse: 366.3882 - val_loss: 558.6450 - val_mae: 18.2422 - val_mse: 558.6450\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 358.7378 - mae: 14.3667 - mse: 358.7378 - val_loss: 572.7955 - val_mae: 18.4485 - val_mse: 572.7955\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 363.0776 - mae: 14.5322 - mse: 363.0776 - val_loss: 565.3776 - val_mae: 18.6274 - val_mse: 565.3776\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 357.7409 - mae: 14.3861 - mse: 357.7409 - val_loss: 566.4020 - val_mae: 18.4724 - val_mse: 566.4020\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.1135 - mae: 14.2723 - mse: 355.1135 - val_loss: 565.4093 - val_mae: 18.2539 - val_mse: 565.4093\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 362.9678 - mae: 14.4768 - mse: 362.9678 - val_loss: 571.3088 - val_mae: 18.4072 - val_mse: 571.3088\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 359.4512 - mae: 14.3689 - mse: 359.4512 - val_loss: 569.0195 - val_mae: 18.4416 - val_mse: 569.0195\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.3615 - mae: 14.2406 - mse: 354.3615 - val_loss: 567.9642 - val_mae: 18.3156 - val_mse: 567.9642\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.6206 - mae: 14.4722 - mse: 364.6206 - val_loss: 573.1206 - val_mae: 18.4740 - val_mse: 573.1206\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 358.3999 - mae: 14.4585 - mse: 358.3999 - val_loss: 568.8878 - val_mae: 18.6986 - val_mse: 568.8878\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.5077 - mae: 14.3190 - mse: 353.5077 - val_loss: 583.4858 - val_mae: 18.7183 - val_mse: 583.4858\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 354.8593 - mae: 14.4067 - mse: 354.8593 - val_loss: 568.5494 - val_mae: 18.3843 - val_mse: 568.5494\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 351.9129 - mae: 14.1956 - mse: 351.9129 - val_loss: 583.3265 - val_mae: 18.9989 - val_mse: 583.3265\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.4090 - mae: 14.3351 - mse: 353.4090 - val_loss: 570.2659 - val_mae: 18.2667 - val_mse: 570.2659\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 355.9775 - mae: 14.1881 - mse: 355.9775 - val_loss: 570.0823 - val_mae: 18.3321 - val_mse: 570.0823\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 344.4247 - mae: 14.0831 - mse: 344.4247 - val_loss: 572.7032 - val_mae: 18.4377 - val_mse: 572.7032\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.4384 - mae: 14.2003 - mse: 349.4384 - val_loss: 602.7197 - val_mae: 18.8389 - val_mse: 602.7197\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.2115 - mae: 14.2141 - mse: 349.2115 - val_loss: 581.7924 - val_mae: 18.6678 - val_mse: 581.7924\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_1 = bnorm_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2979.2764 - mae: 45.9555 - mse: 2979.2581 - val_loss: 760.0155 - val_mae: 23.2844 - val_mse: 759.9960\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1018.5153 - mae: 26.2043 - mse: 1018.4959 - val_loss: 623.7423 - val_mae: 20.3517 - val_mse: 623.7227\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 934.3148 - mae: 24.6846 - mse: 934.2952 - val_loss: 601.3236 - val_mae: 20.3922 - val_mse: 601.3040\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 877.2440 - mae: 23.9785 - mse: 877.2250 - val_loss: 592.9407 - val_mae: 20.3693 - val_mse: 592.9212\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 836.6983 - mae: 23.2846 - mse: 836.6787 - val_loss: 572.1729 - val_mae: 19.9360 - val_mse: 572.1534\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 821.0781 - mae: 23.0374 - mse: 821.0589 - val_loss: 559.3003 - val_mae: 19.6092 - val_mse: 559.2808\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 806.1334 - mae: 22.8558 - mse: 806.1142 - val_loss: 544.3677 - val_mae: 19.2702 - val_mse: 544.3486\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 776.7045 - mae: 22.5178 - mse: 776.6854 - val_loss: 550.8495 - val_mae: 19.5213 - val_mse: 550.8304\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 768.0920 - mae: 22.1586 - mse: 768.0729 - val_loss: 526.5245 - val_mae: 18.6983 - val_mse: 526.5056\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 770.5010 - mae: 22.3797 - mse: 770.4822 - val_loss: 519.7021 - val_mae: 18.5630 - val_mse: 519.6832\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 753.9324 - mae: 22.0247 - mse: 753.9139 - val_loss: 523.7183 - val_mae: 18.6775 - val_mse: 523.6995\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 735.2782 - mae: 21.6171 - mse: 735.2596 - val_loss: 519.2223 - val_mae: 18.4538 - val_mse: 519.2037\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 748.9566 - mae: 22.0350 - mse: 748.9380 - val_loss: 520.3136 - val_mae: 18.6805 - val_mse: 520.2950\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 724.2542 - mae: 21.5849 - mse: 724.2358 - val_loss: 526.8956 - val_mae: 18.8379 - val_mse: 526.8771\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 717.4550 - mae: 21.5043 - mse: 717.4366 - val_loss: 519.7739 - val_mae: 18.6673 - val_mse: 519.7557\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 711.3146 - mae: 21.4150 - mse: 711.2961 - val_loss: 517.8089 - val_mae: 18.6040 - val_mse: 517.7908\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.2398 - mae: 21.3918 - mse: 708.2217 - val_loss: 503.9768 - val_mae: 17.8387 - val_mse: 503.9586\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 704.8459 - mae: 21.2271 - mse: 704.8275 - val_loss: 516.2507 - val_mae: 18.6273 - val_mse: 516.2328\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 695.4225 - mae: 21.1489 - mse: 695.4048 - val_loss: 548.2101 - val_mae: 19.6681 - val_mse: 548.1923\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.8582 - mae: 21.2970 - mse: 708.8407 - val_loss: 518.3754 - val_mae: 18.6525 - val_mse: 518.3578\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 707.7458 - mae: 21.2505 - mse: 707.7278 - val_loss: 514.2116 - val_mae: 18.4471 - val_mse: 514.1940\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 703.5203 - mae: 21.2367 - mse: 703.5027 - val_loss: 514.5049 - val_mae: 18.4452 - val_mse: 514.4877\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.9591 - mae: 21.3248 - mse: 709.9417 - val_loss: 518.5211 - val_mae: 18.6794 - val_mse: 518.5040\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 699.7354 - mae: 21.1090 - mse: 699.7183 - val_loss: 515.6614 - val_mae: 18.5527 - val_mse: 515.6442\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 672.5071 - mae: 20.6031 - mse: 672.4901 - val_loss: 509.0794 - val_mae: 18.2691 - val_mse: 509.0622\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 683.7155 - mae: 20.8241 - mse: 683.6986 - val_loss: 501.4780 - val_mae: 17.9621 - val_mse: 501.4611\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.2922 - mae: 20.5321 - mse: 672.2754 - val_loss: 497.3922 - val_mae: 17.5977 - val_mse: 497.3753\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.0279 - mae: 20.9348 - mse: 690.0112 - val_loss: 513.1105 - val_mae: 18.6228 - val_mse: 513.0937\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.2975 - mae: 20.4690 - mse: 666.2807 - val_loss: 509.4179 - val_mae: 18.4811 - val_mse: 509.4013\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.2653 - mae: 20.5390 - mse: 667.2491 - val_loss: 506.4560 - val_mae: 18.2824 - val_mse: 506.4395\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 670.2034 - mae: 20.5357 - mse: 670.1870 - val_loss: 494.7745 - val_mae: 17.5756 - val_mse: 494.7582\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 671.0449 - mae: 20.6021 - mse: 671.0289 - val_loss: 496.1466 - val_mae: 17.8793 - val_mse: 496.1304\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 681.2925 - mae: 20.7500 - mse: 681.2764 - val_loss: 497.1060 - val_mae: 17.3704 - val_mse: 497.0900\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.3893 - mae: 20.3109 - mse: 660.3736 - val_loss: 496.2045 - val_mae: 17.8604 - val_mse: 496.1886\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.4667 - mae: 20.2947 - mse: 661.4509 - val_loss: 493.3372 - val_mae: 17.7755 - val_mse: 493.3215\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.3923 - mae: 20.3543 - mse: 658.3766 - val_loss: 497.7395 - val_mae: 18.0620 - val_mse: 497.7238\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.5251 - mae: 20.3144 - mse: 659.5094 - val_loss: 494.3217 - val_mae: 17.8446 - val_mse: 494.3062\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.1866 - mae: 20.3447 - mse: 668.1710 - val_loss: 496.9356 - val_mae: 17.9837 - val_mse: 496.9202\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.2998 - mae: 20.3543 - mse: 657.2845 - val_loss: 496.3510 - val_mae: 17.8985 - val_mse: 496.3359\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.7859 - mae: 20.5862 - mse: 674.7708 - val_loss: 525.7047 - val_mae: 19.1041 - val_mse: 525.6898\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.3507 - mae: 20.4625 - mse: 668.3357 - val_loss: 495.3698 - val_mae: 17.8186 - val_mse: 495.3549\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 652.2626 - mae: 20.0914 - mse: 652.2478 - val_loss: 518.5317 - val_mae: 18.8385 - val_mse: 518.5172\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.4525 - mae: 20.4866 - mse: 666.4382 - val_loss: 495.2185 - val_mae: 17.7371 - val_mse: 495.2039\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.5675 - mae: 20.1339 - mse: 648.5532 - val_loss: 494.9854 - val_mae: 17.8104 - val_mse: 494.9709\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 659.3745 - mae: 20.4038 - mse: 659.3597 - val_loss: 503.1967 - val_mae: 18.2368 - val_mse: 503.1824\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 655.7101 - mae: 20.2510 - mse: 655.6957 - val_loss: 512.1959 - val_mae: 18.6474 - val_mse: 512.1817\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 641.2416 - mae: 20.1509 - mse: 641.2274 - val_loss: 490.6578 - val_mae: 17.3708 - val_mse: 490.6435\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 661.7751 - mae: 20.3721 - mse: 661.7610 - val_loss: 492.2108 - val_mae: 17.4911 - val_mse: 492.1966\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 649.1249 - mae: 20.1591 - mse: 649.1104 - val_loss: 493.8874 - val_mae: 17.7969 - val_mse: 493.8734\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.1970 - mae: 19.9864 - mse: 643.1832 - val_loss: 501.4378 - val_mae: 18.2201 - val_mse: 501.4241\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 644.4419 - mae: 20.0724 - mse: 644.4282 - val_loss: 497.7058 - val_mae: 18.0963 - val_mse: 497.6922\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.8116 - mae: 20.2390 - mse: 659.7975 - val_loss: 492.2253 - val_mae: 17.7632 - val_mse: 492.2117\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.9926 - mae: 20.2051 - mse: 659.9789 - val_loss: 495.4863 - val_mae: 17.9426 - val_mse: 495.4729\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.7786 - mae: 20.1318 - mse: 653.7650 - val_loss: 491.8537 - val_mae: 17.8329 - val_mse: 491.8404\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.3760 - mae: 19.7275 - mse: 628.3632 - val_loss: 501.4107 - val_mae: 18.2445 - val_mse: 501.3975\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 670.1246 - mae: 20.3161 - mse: 670.1116 - val_loss: 491.6684 - val_mae: 17.7594 - val_mse: 491.6552\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.0873 - mae: 20.4440 - mse: 663.0746 - val_loss: 498.4047 - val_mae: 18.1014 - val_mse: 498.3918\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.6885 - mae: 20.0216 - mse: 646.6754 - val_loss: 499.3006 - val_mae: 18.1404 - val_mse: 499.2878\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6576 - mae: 19.9996 - mse: 635.6450 - val_loss: 490.7336 - val_mae: 17.5091 - val_mse: 490.7207\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.2973 - mae: 20.0600 - mse: 643.2846 - val_loss: 495.2721 - val_mae: 17.9187 - val_mse: 495.2596\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 640.9224 - mae: 19.9115 - mse: 640.9097 - val_loss: 491.7971 - val_mae: 17.3375 - val_mse: 491.7845\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.2477 - mae: 19.8768 - mse: 633.2352 - val_loss: 490.1575 - val_mae: 17.6339 - val_mse: 490.1450\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.1706 - mae: 19.8267 - mse: 630.1583 - val_loss: 498.9841 - val_mae: 18.1309 - val_mse: 498.9719\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 634.6945 - mae: 19.9994 - mse: 634.6818 - val_loss: 520.0391 - val_mae: 18.9224 - val_mse: 520.0269\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 630.9132 - mae: 19.8434 - mse: 630.9012 - val_loss: 489.7959 - val_mae: 17.6502 - val_mse: 489.7836\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.9176 - mae: 19.7468 - mse: 631.9055 - val_loss: 497.0493 - val_mae: 18.0803 - val_mse: 497.0373\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.0549 - mae: 19.9461 - mse: 638.0431 - val_loss: 490.9205 - val_mae: 17.4489 - val_mse: 490.9085\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.1712 - mae: 19.8778 - mse: 634.1593 - val_loss: 494.7243 - val_mae: 17.8887 - val_mse: 494.7123\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.4377 - mae: 19.7861 - mse: 629.4260 - val_loss: 487.8039 - val_mae: 17.3340 - val_mse: 487.7921\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 634.1630 - mae: 19.7549 - mse: 634.1511 - val_loss: 493.2359 - val_mae: 17.7813 - val_mse: 493.2243\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.6912 - mae: 20.0142 - mse: 636.6794 - val_loss: 503.5572 - val_mae: 18.3203 - val_mse: 503.5457\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.2803 - mae: 19.8405 - mse: 633.2685 - val_loss: 518.1147 - val_mae: 18.8417 - val_mse: 518.1035\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 627.3307 - mae: 19.8649 - mse: 627.3191 - val_loss: 492.0692 - val_mae: 17.7525 - val_mse: 492.0579\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.5982 - mae: 19.8460 - mse: 635.5871 - val_loss: 489.6605 - val_mae: 17.5008 - val_mse: 489.6492\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.8899 - mae: 19.7313 - mse: 629.8785 - val_loss: 493.7029 - val_mae: 17.8809 - val_mse: 493.6918\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 630.0901 - mae: 19.7309 - mse: 630.0792 - val_loss: 504.5531 - val_mae: 18.3207 - val_mse: 504.5421\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.5948 - mae: 19.6938 - mse: 628.5837 - val_loss: 499.2320 - val_mae: 18.1421 - val_mse: 499.2211\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 631.3044 - mae: 19.7256 - mse: 631.2933 - val_loss: 487.7284 - val_mae: 17.3541 - val_mse: 487.7176\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.0092 - mae: 19.9341 - mse: 635.9983 - val_loss: 489.1519 - val_mae: 17.6187 - val_mse: 489.1410\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.1063 - mae: 19.5391 - mse: 619.0954 - val_loss: 486.8118 - val_mae: 17.5678 - val_mse: 486.8010\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.4692 - mae: 19.9663 - mse: 640.4584 - val_loss: 487.1936 - val_mae: 17.6145 - val_mse: 487.1828\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.7944 - mae: 19.6738 - mse: 619.7834 - val_loss: 483.8549 - val_mae: 17.4232 - val_mse: 483.8441\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.5200 - mae: 19.6049 - mse: 623.5091 - val_loss: 489.7200 - val_mae: 17.8365 - val_mse: 489.7093\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.1088 - mae: 19.5665 - mse: 619.0981 - val_loss: 488.3701 - val_mae: 17.7463 - val_mse: 488.3593\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.8768 - mae: 19.3149 - mse: 609.8660 - val_loss: 486.2737 - val_mae: 17.4891 - val_mse: 486.2630\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.6137 - mae: 19.3598 - mse: 607.6030 - val_loss: 489.1529 - val_mae: 17.7987 - val_mse: 489.1423\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.2412 - mae: 19.5942 - mse: 619.2305 - val_loss: 485.0508 - val_mae: 17.5829 - val_mse: 485.0403\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 622.9512 - mae: 19.5730 - mse: 622.9410 - val_loss: 489.8441 - val_mae: 17.8555 - val_mse: 489.8336\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.4948 - mae: 19.6177 - mse: 621.4843 - val_loss: 483.8210 - val_mae: 17.5150 - val_mse: 483.8106\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.6133 - mae: 19.3209 - mse: 613.6030 - val_loss: 494.0031 - val_mae: 18.0461 - val_mse: 493.9926\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.7522 - mae: 19.3968 - mse: 605.7421 - val_loss: 486.5209 - val_mae: 17.6323 - val_mse: 486.5106\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.6124 - mae: 19.2318 - mse: 601.6020 - val_loss: 487.6220 - val_mae: 17.6383 - val_mse: 487.6115\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8701 - mae: 19.5021 - mse: 615.8599 - val_loss: 483.5876 - val_mae: 17.4441 - val_mse: 483.5773\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.0725 - mae: 19.3636 - mse: 610.0623 - val_loss: 486.0204 - val_mae: 17.6914 - val_mse: 486.0102\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 604.5483 - mae: 19.2141 - mse: 604.5380 - val_loss: 481.3989 - val_mae: 17.3295 - val_mse: 481.3887\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 614.1721 - mae: 19.4679 - mse: 614.1617 - val_loss: 487.7609 - val_mae: 17.7212 - val_mse: 487.7506\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.8781 - mae: 19.2992 - mse: 607.8679 - val_loss: 482.3763 - val_mae: 17.2931 - val_mse: 482.3661\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.9138 - mae: 19.0613 - mse: 591.9038 - val_loss: 481.3646 - val_mae: 17.3723 - val_mse: 481.3544\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.1880 - mae: 19.2094 - mse: 599.1775 - val_loss: 490.3348 - val_mae: 17.8573 - val_mse: 490.3246\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.0259 - mae: 19.3297 - mse: 608.0159 - val_loss: 483.4658 - val_mae: 17.4544 - val_mse: 483.4556\n"
     ]
    }
   ],
   "source": [
    "# deep learning model regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 14.8111, Train MSE: 359.5782\n",
      "Val   MAE: 18.8456, Val   MSE: 574.3768\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 12.2385, Train MSE: 265.5150\n",
      "Val   MAE: 18.6678, Val   MSE: 581.7924\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 17.3169, Train MSE: 498.1217\n",
      "Val   MAE: 17.4544, Val   MSE: 483.4556\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_1 = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores3_1   = baseline_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_1[1]:.4f}, Train MSE: {train_scores3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_1[1]:.4f}, Val   MSE: {val_scores3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_1 = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_1   = bnorm_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_1[1]:.4f}, Train MSE: {train_scores_bn3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_1[1]:.4f}, Val   MSE: {val_scores_bn3_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Peak Position - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 1892.7213 - mae: 33.4263 - mse: 1892.7213 - val_loss: 520.3790 - val_mae: 18.1290 - val_mse: 520.3790\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 478.7260 - mae: 16.7923 - mse: 478.7260 - val_loss: 470.7764 - val_mae: 16.9759 - val_mse: 470.7764\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 451.6718 - mae: 16.1420 - mse: 451.6718 - val_loss: 462.6606 - val_mae: 16.8362 - val_mse: 462.6606\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 438.1059 - mae: 15.8108 - mse: 438.1059 - val_loss: 458.5118 - val_mae: 16.7691 - val_mse: 458.5118\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 434.0944 - mae: 15.7039 - mse: 434.0944 - val_loss: 456.4850 - val_mae: 16.7472 - val_mse: 456.4850\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 425.9094 - mae: 15.5476 - mse: 425.9094 - val_loss: 457.1035 - val_mae: 16.6385 - val_mse: 457.1035\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 419.2168 - mae: 15.3868 - mse: 419.2168 - val_loss: 451.6199 - val_mae: 16.4680 - val_mse: 451.6199\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.8326 - mae: 15.2631 - mse: 412.8326 - val_loss: 457.9308 - val_mae: 16.9458 - val_mse: 457.9308\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 412.5154 - mae: 15.2607 - mse: 412.5154 - val_loss: 462.5644 - val_mae: 17.1032 - val_mse: 462.5644\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 405.2845 - mae: 15.1008 - mse: 405.2845 - val_loss: 461.2936 - val_mae: 17.1343 - val_mse: 461.2936\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 400.7457 - mae: 15.0050 - mse: 400.7457 - val_loss: 450.3910 - val_mae: 16.2371 - val_mse: 450.3910\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.6764 - mae: 14.8601 - mse: 395.6764 - val_loss: 457.5585 - val_mae: 16.3147 - val_mse: 457.5585\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 395.6292 - mae: 14.8355 - mse: 395.6292 - val_loss: 453.7112 - val_mae: 16.4447 - val_mse: 453.7112\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 392.3511 - mae: 14.8365 - mse: 392.3511 - val_loss: 454.4232 - val_mae: 16.5412 - val_mse: 454.4232\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.4425 - mae: 14.6643 - mse: 385.4425 - val_loss: 455.3342 - val_mae: 16.9172 - val_mse: 455.3342\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 384.5289 - mae: 14.5799 - mse: 384.5289 - val_loss: 456.2061 - val_mae: 16.7527 - val_mse: 456.2061\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 381.1043 - mae: 14.5781 - mse: 381.1043 - val_loss: 453.3718 - val_mae: 16.6571 - val_mse: 453.3718\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 374.3138 - mae: 14.3935 - mse: 374.3138 - val_loss: 460.0337 - val_mae: 16.7442 - val_mse: 460.0337\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 369.3541 - mae: 14.3137 - mse: 369.3541 - val_loss: 455.9283 - val_mae: 16.4409 - val_mse: 455.9283\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 370.6097 - mae: 14.3330 - mse: 370.6097 - val_loss: 470.5733 - val_mae: 16.5976 - val_mse: 470.5733\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 365.4076 - mae: 14.2229 - mse: 365.4076 - val_loss: 459.6632 - val_mae: 16.3948 - val_mse: 459.6632\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 366.1736 - mae: 14.1925 - mse: 366.1736 - val_loss: 464.7560 - val_mae: 16.7702 - val_mse: 464.7560\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 357.0033 - mae: 13.9864 - mse: 357.0033 - val_loss: 467.2335 - val_mae: 17.0623 - val_mse: 467.2335\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 352.4339 - mae: 13.9761 - mse: 352.4339 - val_loss: 468.7139 - val_mae: 16.4632 - val_mse: 468.7139\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 348.3414 - mae: 13.7797 - mse: 348.3414 - val_loss: 476.8320 - val_mae: 17.2379 - val_mse: 476.8320\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 345.1793 - mae: 13.7669 - mse: 345.1793 - val_loss: 470.8262 - val_mae: 16.5245 - val_mse: 470.8262\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 340.0699 - mae: 13.6442 - mse: 340.0699 - val_loss: 471.9616 - val_mae: 16.8797 - val_mse: 471.9616\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 338.1692 - mae: 13.6232 - mse: 338.1692 - val_loss: 474.3118 - val_mae: 16.8708 - val_mse: 474.3118\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 333.3511 - mae: 13.4843 - mse: 333.3511 - val_loss: 485.0847 - val_mae: 16.8765 - val_mse: 485.0847\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 325.9722 - mae: 13.3045 - mse: 325.9722 - val_loss: 477.7078 - val_mae: 16.6771 - val_mse: 477.7078\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 323.1123 - mae: 13.3083 - mse: 323.1123 - val_loss: 491.4530 - val_mae: 17.1689 - val_mse: 491.4530\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 318.3349 - mae: 13.1968 - mse: 318.3349 - val_loss: 488.9095 - val_mae: 16.7596 - val_mse: 488.9095\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 310.2058 - mae: 12.9917 - mse: 310.2058 - val_loss: 488.1793 - val_mae: 17.0778 - val_mse: 488.1793\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 305.8344 - mae: 12.9338 - mse: 305.8344 - val_loss: 490.8407 - val_mae: 16.9126 - val_mse: 490.8407\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 303.0354 - mae: 12.7753 - mse: 303.0354 - val_loss: 497.4867 - val_mae: 17.3696 - val_mse: 497.4867\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 300.0373 - mae: 12.7306 - mse: 300.0373 - val_loss: 520.1455 - val_mae: 18.2407 - val_mse: 520.1455\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 297.0416 - mae: 12.7148 - mse: 297.0416 - val_loss: 506.5608 - val_mae: 17.8004 - val_mse: 506.5608\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 286.6959 - mae: 12.4262 - mse: 286.6959 - val_loss: 520.1334 - val_mae: 18.0876 - val_mse: 520.1334\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 281.5569 - mae: 12.3512 - mse: 281.5569 - val_loss: 509.3662 - val_mae: 17.4594 - val_mse: 509.3662\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 274.1237 - mae: 12.1716 - mse: 274.1237 - val_loss: 506.1709 - val_mae: 17.6599 - val_mse: 506.1709\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 270.8106 - mae: 12.0299 - mse: 270.8106 - val_loss: 531.0521 - val_mae: 17.5972 - val_mse: 531.0521\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 268.4175 - mae: 11.9875 - mse: 268.4175 - val_loss: 519.0777 - val_mae: 17.4131 - val_mse: 519.0777\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 263.4570 - mae: 11.8073 - mse: 263.4570 - val_loss: 530.5804 - val_mae: 17.6679 - val_mse: 530.5804\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 256.9211 - mae: 11.7870 - mse: 256.9211 - val_loss: 527.5912 - val_mae: 17.5916 - val_mse: 527.5912\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 249.3903 - mae: 11.5042 - mse: 249.3903 - val_loss: 539.9401 - val_mae: 17.6758 - val_mse: 539.9401\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 249.5378 - mae: 11.4962 - mse: 249.5378 - val_loss: 543.3564 - val_mae: 17.8698 - val_mse: 543.3564\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 240.2439 - mae: 11.2567 - mse: 240.2439 - val_loss: 540.8874 - val_mae: 18.1939 - val_mse: 540.8874\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 237.7074 - mae: 11.2719 - mse: 237.7074 - val_loss: 558.0477 - val_mae: 18.0345 - val_mse: 558.0477\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.5417 - mae: 11.1816 - mse: 234.5417 - val_loss: 542.7029 - val_mae: 17.5737 - val_mse: 542.7029\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 233.5663 - mae: 11.1447 - mse: 233.5663 - val_loss: 554.2781 - val_mae: 18.0526 - val_mse: 554.2781\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 234.1102 - mae: 11.1115 - mse: 234.1102 - val_loss: 567.6879 - val_mae: 18.0554 - val_mse: 567.6879\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 224.1743 - mae: 10.8234 - mse: 224.1743 - val_loss: 580.0012 - val_mae: 18.4358 - val_mse: 580.0012\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 218.1941 - mae: 10.7213 - mse: 218.1941 - val_loss: 571.9794 - val_mae: 18.3055 - val_mse: 571.9794\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 220.0957 - mae: 10.6973 - mse: 220.0957 - val_loss: 573.1796 - val_mae: 18.2130 - val_mse: 573.1796\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 214.8078 - mae: 10.7697 - mse: 214.8078 - val_loss: 594.4938 - val_mae: 18.7789 - val_mse: 594.4938\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 209.0017 - mae: 10.5725 - mse: 209.0017 - val_loss: 575.3069 - val_mae: 18.2085 - val_mse: 575.3069\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 205.2209 - mae: 10.3839 - mse: 205.2209 - val_loss: 583.3596 - val_mae: 18.7053 - val_mse: 583.3596\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.3109 - mae: 10.3498 - mse: 202.3109 - val_loss: 584.6224 - val_mae: 18.5512 - val_mse: 584.6224\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 199.1269 - mae: 10.3228 - mse: 199.1269 - val_loss: 581.2922 - val_mae: 18.4172 - val_mse: 581.2922\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.8089 - mae: 10.2021 - mse: 195.8089 - val_loss: 589.9075 - val_mae: 18.6954 - val_mse: 589.9075\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 190.6271 - mae: 10.0456 - mse: 190.6271 - val_loss: 598.8209 - val_mae: 18.8713 - val_mse: 598.8209\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 189.3175 - mae: 10.0006 - mse: 189.3175 - val_loss: 610.1231 - val_mae: 19.3005 - val_mse: 610.1231\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 183.2306 - mae: 9.8317 - mse: 183.2306 - val_loss: 631.8595 - val_mae: 19.0779 - val_mse: 631.8595\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 181.8792 - mae: 9.8090 - mse: 181.8792 - val_loss: 631.9651 - val_mae: 19.0343 - val_mse: 631.9651\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 176.2698 - mae: 9.5989 - mse: 176.2698 - val_loss: 618.3696 - val_mae: 18.8983 - val_mse: 618.3696\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 176.6344 - mae: 9.7076 - mse: 176.6344 - val_loss: 641.5378 - val_mae: 18.8336 - val_mse: 641.5378\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 175.7886 - mae: 9.6094 - mse: 175.7886 - val_loss: 637.1666 - val_mae: 19.2912 - val_mse: 637.1666\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.3987 - mae: 9.5277 - mse: 171.3987 - val_loss: 632.7845 - val_mae: 19.0899 - val_mse: 632.7845\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 168.2045 - mae: 9.4112 - mse: 168.2045 - val_loss: 649.2037 - val_mae: 19.2462 - val_mse: 649.2037\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 171.0506 - mae: 9.5423 - mse: 171.0506 - val_loss: 675.4341 - val_mae: 19.4216 - val_mse: 675.4341\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 167.1529 - mae: 9.3497 - mse: 167.1529 - val_loss: 661.6721 - val_mae: 19.4914 - val_mse: 661.6721\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 157.8326 - mae: 9.0896 - mse: 157.8326 - val_loss: 662.5737 - val_mae: 19.2938 - val_mse: 662.5737\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 156.7415 - mae: 9.0857 - mse: 156.7415 - val_loss: 672.7690 - val_mae: 20.0315 - val_mse: 672.7690\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 153.3025 - mae: 8.9901 - mse: 153.3025 - val_loss: 687.0602 - val_mae: 19.6466 - val_mse: 687.0602\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 153.8064 - mae: 8.9677 - mse: 153.8064 - val_loss: 660.1406 - val_mae: 19.6931 - val_mse: 660.1406\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.5812 - mae: 8.8227 - mse: 147.5812 - val_loss: 646.2503 - val_mae: 19.2705 - val_mse: 646.2503\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 146.6172 - mae: 8.7100 - mse: 146.6172 - val_loss: 662.7087 - val_mae: 19.5628 - val_mse: 662.7087\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 141.6480 - mae: 8.6112 - mse: 141.6480 - val_loss: 696.4136 - val_mae: 19.8653 - val_mse: 696.4136\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.2308 - mae: 8.5359 - mse: 140.2308 - val_loss: 675.4758 - val_mae: 19.7746 - val_mse: 675.4758\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6542 - mae: 8.4273 - mse: 136.6542 - val_loss: 669.3276 - val_mae: 19.6349 - val_mse: 669.3276\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2565 - mae: 8.3920 - mse: 134.2565 - val_loss: 686.1292 - val_mae: 19.8983 - val_mse: 686.1292\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.7356 - mae: 8.3393 - mse: 132.7356 - val_loss: 718.5313 - val_mae: 20.1434 - val_mse: 718.5313\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.1627 - mae: 8.4911 - mse: 134.1627 - val_loss: 699.2370 - val_mae: 19.9888 - val_mse: 699.2370\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.6164 - mae: 8.5269 - mse: 135.6164 - val_loss: 698.2330 - val_mae: 20.3521 - val_mse: 698.2330\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.7049 - mae: 8.2169 - mse: 126.7049 - val_loss: 706.7504 - val_mae: 20.4258 - val_mse: 706.7504\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7794 - mae: 8.3042 - mse: 128.7794 - val_loss: 706.2249 - val_mae: 19.9442 - val_mse: 706.2249\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4908 - mae: 8.3317 - mse: 128.4908 - val_loss: 697.6802 - val_mae: 20.0852 - val_mse: 697.6802\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.8584 - mae: 8.0513 - mse: 121.8584 - val_loss: 722.1714 - val_mae: 20.2290 - val_mse: 722.1714\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.0711 - mae: 8.0673 - mse: 122.0711 - val_loss: 737.4019 - val_mae: 20.5457 - val_mse: 737.4019\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.1796 - mae: 7.8699 - mse: 118.1796 - val_loss: 751.5908 - val_mae: 20.7820 - val_mse: 751.5908\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.7113 - mae: 7.7076 - mse: 114.7113 - val_loss: 733.6817 - val_mae: 20.5299 - val_mse: 733.6817\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.0477 - mae: 7.7017 - mse: 114.0477 - val_loss: 757.0280 - val_mae: 20.7442 - val_mse: 757.0280\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 120.8091 - mae: 7.9975 - mse: 120.8091 - val_loss: 725.3312 - val_mae: 20.7769 - val_mse: 725.3312\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 116.7880 - mae: 7.8752 - mse: 116.7880 - val_loss: 741.6437 - val_mae: 20.4434 - val_mse: 741.6437\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.9279 - mae: 7.6135 - mse: 112.9279 - val_loss: 757.6684 - val_mae: 20.9498 - val_mse: 757.6684\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.7833 - mae: 7.5082 - mse: 107.7833 - val_loss: 743.4960 - val_mae: 20.3855 - val_mse: 743.4960\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.3735 - mae: 7.5345 - mse: 109.3735 - val_loss: 767.4005 - val_mae: 20.8563 - val_mse: 767.4005\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.2023 - mae: 7.4484 - mse: 106.2023 - val_loss: 735.8165 - val_mae: 20.3734 - val_mse: 735.8165\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.5678 - mae: 7.4372 - mse: 106.5678 - val_loss: 741.6857 - val_mae: 20.7945 - val_mse: 741.6857\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9588 - mae: 7.2521 - mse: 101.9588 - val_loss: 764.1398 - val_mae: 20.9125 - val_mse: 764.1398\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline3_2 = baseline_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 5866.9878 - mae: 72.7818 - mse: 5866.9878 - val_loss: 5441.7329 - val_mae: 70.0525 - val_mse: 5441.7329\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 4661.4712 - mae: 64.4526 - mse: 4661.4712 - val_loss: 4027.0215 - val_mae: 59.7014 - val_mse: 4027.0215\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 3143.0669 - mae: 52.1421 - mse: 3143.0669 - val_loss: 2472.0081 - val_mae: 46.0076 - val_mse: 2472.0081\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 1706.3195 - mae: 37.3661 - mse: 1706.3195 - val_loss: 1213.7183 - val_mae: 31.2431 - val_mse: 1213.7183\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 832.3569 - mae: 24.9782 - mse: 832.3569 - val_loss: 676.0316 - val_mae: 22.1221 - val_mse: 676.0316\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.7731 - mae: 18.3451 - mse: 512.7731 - val_loss: 459.2562 - val_mae: 16.9291 - val_mse: 459.2562\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 441.5845 - mae: 16.2541 - mse: 441.5845 - val_loss: 450.6696 - val_mae: 16.4872 - val_mse: 450.6696\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 424.7069 - mae: 15.4769 - mse: 424.7069 - val_loss: 446.4685 - val_mae: 15.9859 - val_mse: 446.4685\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 412.9382 - mae: 15.2586 - mse: 412.9382 - val_loss: 449.1614 - val_mae: 16.0142 - val_mse: 449.1614\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 400.3046 - mae: 14.9203 - mse: 400.3046 - val_loss: 451.0849 - val_mae: 16.0207 - val_mse: 451.0849\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 397.9171 - mae: 14.8369 - mse: 397.9171 - val_loss: 455.1835 - val_mae: 16.2439 - val_mse: 455.1835\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 394.3323 - mae: 14.8609 - mse: 394.3323 - val_loss: 457.4597 - val_mae: 16.2118 - val_mse: 457.4597\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 385.4710 - mae: 14.6079 - mse: 385.4710 - val_loss: 465.0157 - val_mae: 16.5770 - val_mse: 465.0157\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 384.9205 - mae: 14.5747 - mse: 384.9205 - val_loss: 458.4398 - val_mae: 16.0008 - val_mse: 458.4398\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 376.9151 - mae: 14.4564 - mse: 376.9151 - val_loss: 470.0336 - val_mae: 16.3067 - val_mse: 470.0336\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 381.9337 - mae: 14.4393 - mse: 381.9337 - val_loss: 464.6107 - val_mae: 16.3114 - val_mse: 464.6107\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 372.0064 - mae: 14.2703 - mse: 372.0064 - val_loss: 464.6444 - val_mae: 16.2317 - val_mse: 464.6444\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 364.2359 - mae: 14.1109 - mse: 364.2359 - val_loss: 465.2691 - val_mae: 16.4874 - val_mse: 465.2691\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 356.8594 - mae: 14.0007 - mse: 356.8594 - val_loss: 460.3544 - val_mae: 16.1133 - val_mse: 460.3544\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 357.5927 - mae: 13.9377 - mse: 357.5927 - val_loss: 466.6535 - val_mae: 16.5870 - val_mse: 466.6535\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.1353 - mae: 13.8807 - mse: 353.1353 - val_loss: 466.7399 - val_mae: 16.2004 - val_mse: 466.7399\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 353.0158 - mae: 13.8273 - mse: 353.0158 - val_loss: 463.8322 - val_mae: 16.4347 - val_mse: 463.8322\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 349.6851 - mae: 13.8015 - mse: 349.6851 - val_loss: 470.5819 - val_mae: 16.7223 - val_mse: 470.5819\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 344.5558 - mae: 13.7877 - mse: 344.5558 - val_loss: 464.3658 - val_mae: 16.0960 - val_mse: 464.3658\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 337.2541 - mae: 13.5315 - mse: 337.2541 - val_loss: 473.1261 - val_mae: 16.5070 - val_mse: 473.1261\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 335.2006 - mae: 13.5423 - mse: 335.2006 - val_loss: 471.8835 - val_mae: 16.4423 - val_mse: 471.8835\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 331.1770 - mae: 13.3746 - mse: 331.1770 - val_loss: 477.9010 - val_mae: 16.4507 - val_mse: 477.9010\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 325.7626 - mae: 13.4064 - mse: 325.7626 - val_loss: 478.7518 - val_mae: 16.6151 - val_mse: 478.7518\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 327.3549 - mae: 13.3480 - mse: 327.3549 - val_loss: 471.8765 - val_mae: 16.2520 - val_mse: 471.8765\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 325.6339 - mae: 13.2860 - mse: 325.6339 - val_loss: 481.2211 - val_mae: 16.6347 - val_mse: 481.2211\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 314.9679 - mae: 13.1025 - mse: 314.9679 - val_loss: 481.1561 - val_mae: 16.5358 - val_mse: 481.1561\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 313.7764 - mae: 13.0008 - mse: 313.7764 - val_loss: 482.3104 - val_mae: 16.5673 - val_mse: 482.3104\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 315.0605 - mae: 13.0688 - mse: 315.0605 - val_loss: 469.6340 - val_mae: 16.3503 - val_mse: 469.6340\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 308.6010 - mae: 12.9189 - mse: 308.6010 - val_loss: 469.8265 - val_mae: 16.3978 - val_mse: 469.8265\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 302.0658 - mae: 12.7850 - mse: 302.0658 - val_loss: 481.9107 - val_mae: 16.8085 - val_mse: 481.9107\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 304.0415 - mae: 12.8768 - mse: 304.0415 - val_loss: 485.9336 - val_mae: 16.4845 - val_mse: 485.9336\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 307.5296 - mae: 12.8157 - mse: 307.5296 - val_loss: 477.2332 - val_mae: 16.5424 - val_mse: 477.2332\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 299.6761 - mae: 12.7099 - mse: 299.6761 - val_loss: 490.7467 - val_mae: 16.9306 - val_mse: 490.7467\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 291.6181 - mae: 12.4962 - mse: 291.6181 - val_loss: 495.7002 - val_mae: 16.6689 - val_mse: 495.7002\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 293.1833 - mae: 12.5437 - mse: 293.1833 - val_loss: 484.8248 - val_mae: 16.6809 - val_mse: 484.8248\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 283.3069 - mae: 12.3855 - mse: 283.3069 - val_loss: 496.7206 - val_mae: 16.6691 - val_mse: 496.7206\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 283.0588 - mae: 12.3464 - mse: 283.0588 - val_loss: 503.1490 - val_mae: 17.2267 - val_mse: 503.1490\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 280.0394 - mae: 12.2234 - mse: 280.0394 - val_loss: 499.8807 - val_mae: 16.8741 - val_mse: 499.8807\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 277.4629 - mae: 12.2164 - mse: 277.4629 - val_loss: 507.8779 - val_mae: 16.8089 - val_mse: 507.8779\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 274.7758 - mae: 12.1680 - mse: 274.7758 - val_loss: 508.6575 - val_mae: 17.1845 - val_mse: 508.6575\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 277.5538 - mae: 12.2126 - mse: 277.5538 - val_loss: 506.0362 - val_mae: 16.9816 - val_mse: 506.0362\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 267.9180 - mae: 11.9835 - mse: 267.9180 - val_loss: 505.0749 - val_mae: 16.9086 - val_mse: 505.0749\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 268.4735 - mae: 11.9260 - mse: 268.4735 - val_loss: 511.1898 - val_mae: 16.8433 - val_mse: 511.1898\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 261.0044 - mae: 11.7558 - mse: 261.0044 - val_loss: 518.0646 - val_mae: 17.1133 - val_mse: 518.0646\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 264.1172 - mae: 11.8962 - mse: 264.1172 - val_loss: 525.9396 - val_mae: 17.3493 - val_mse: 525.9396\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 254.5170 - mae: 11.7996 - mse: 254.5170 - val_loss: 529.6415 - val_mae: 17.5690 - val_mse: 529.6415\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 259.1226 - mae: 11.7465 - mse: 259.1226 - val_loss: 527.3185 - val_mae: 17.5603 - val_mse: 527.3185\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 250.8418 - mae: 11.5431 - mse: 250.8418 - val_loss: 529.4772 - val_mae: 17.3844 - val_mse: 529.4772\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 254.7495 - mae: 11.7982 - mse: 254.7495 - val_loss: 530.1727 - val_mae: 17.2655 - val_mse: 530.1727\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 253.8266 - mae: 11.6086 - mse: 253.8266 - val_loss: 537.9186 - val_mae: 17.4931 - val_mse: 537.9186\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 248.3782 - mae: 11.5386 - mse: 248.3782 - val_loss: 540.6470 - val_mae: 17.8119 - val_mse: 540.6470\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 243.4111 - mae: 11.3685 - mse: 243.4111 - val_loss: 540.0712 - val_mae: 17.9197 - val_mse: 540.0712\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 248.7368 - mae: 11.5906 - mse: 248.7368 - val_loss: 536.9722 - val_mae: 17.7332 - val_mse: 536.9722\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 238.5628 - mae: 11.2956 - mse: 238.5628 - val_loss: 545.8381 - val_mae: 17.7426 - val_mse: 545.8381\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 238.3579 - mae: 11.2783 - mse: 238.3579 - val_loss: 540.4854 - val_mae: 17.7453 - val_mse: 540.4854\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 226.6470 - mae: 10.9682 - mse: 226.6470 - val_loss: 547.1608 - val_mae: 17.6305 - val_mse: 547.1608\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 232.3131 - mae: 11.0624 - mse: 232.3131 - val_loss: 535.6311 - val_mae: 17.4598 - val_mse: 535.6311\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 233.7380 - mae: 11.1734 - mse: 233.7380 - val_loss: 541.5906 - val_mae: 17.4435 - val_mse: 541.5906\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 230.8068 - mae: 11.1507 - mse: 230.8068 - val_loss: 548.8479 - val_mae: 17.7583 - val_mse: 548.8479\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 223.9628 - mae: 10.9801 - mse: 223.9628 - val_loss: 541.9437 - val_mae: 17.5134 - val_mse: 541.9437\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 235.8235 - mae: 11.2977 - mse: 235.8235 - val_loss: 543.1762 - val_mae: 17.4922 - val_mse: 543.1762\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 225.1030 - mae: 10.9145 - mse: 225.1030 - val_loss: 541.0896 - val_mae: 17.4648 - val_mse: 541.0896\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 226.3839 - mae: 11.0066 - mse: 226.3839 - val_loss: 564.9969 - val_mae: 17.9952 - val_mse: 564.9969\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 221.5108 - mae: 10.9441 - mse: 221.5108 - val_loss: 565.8674 - val_mae: 17.9486 - val_mse: 565.8674\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 223.4545 - mae: 10.9491 - mse: 223.4545 - val_loss: 566.2095 - val_mae: 18.0752 - val_mse: 566.2095\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 218.3287 - mae: 10.7970 - mse: 218.3287 - val_loss: 562.3889 - val_mae: 17.9173 - val_mse: 562.3889\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.6367 - mae: 10.7840 - mse: 215.6367 - val_loss: 557.6080 - val_mae: 17.8526 - val_mse: 557.6080\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 221.2101 - mae: 10.8905 - mse: 221.2101 - val_loss: 546.6534 - val_mae: 17.6331 - val_mse: 546.6534\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 216.3825 - mae: 10.7594 - mse: 216.3825 - val_loss: 565.8367 - val_mae: 17.8717 - val_mse: 565.8367\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 210.8029 - mae: 10.6284 - mse: 210.8029 - val_loss: 573.1037 - val_mae: 18.1484 - val_mse: 573.1037\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 212.7604 - mae: 10.6628 - mse: 212.7604 - val_loss: 568.5330 - val_mae: 18.1579 - val_mse: 568.5330\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 208.5023 - mae: 10.6022 - mse: 208.5023 - val_loss: 571.2440 - val_mae: 18.0985 - val_mse: 571.2440\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 211.4844 - mae: 10.7471 - mse: 211.4844 - val_loss: 557.2679 - val_mae: 17.8960 - val_mse: 557.2679\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 215.2939 - mae: 10.7750 - mse: 215.2939 - val_loss: 566.6571 - val_mae: 18.0758 - val_mse: 566.6571\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 212.6004 - mae: 10.6770 - mse: 212.6004 - val_loss: 567.9473 - val_mae: 17.8868 - val_mse: 567.9473\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.7089 - mae: 10.4998 - mse: 203.7089 - val_loss: 560.5833 - val_mae: 18.1725 - val_mse: 560.5833\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 200.9450 - mae: 10.4617 - mse: 200.9450 - val_loss: 557.6107 - val_mae: 18.0139 - val_mse: 557.6107\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 203.7611 - mae: 10.3987 - mse: 203.7611 - val_loss: 557.4728 - val_mae: 17.9910 - val_mse: 557.4728\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 202.0396 - mae: 10.4846 - mse: 202.0396 - val_loss: 569.1538 - val_mae: 18.1557 - val_mse: 569.1538\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.7039 - mae: 10.3752 - mse: 195.7039 - val_loss: 583.2649 - val_mae: 18.2930 - val_mse: 583.2649\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 196.3322 - mae: 10.2210 - mse: 196.3322 - val_loss: 577.3466 - val_mae: 18.0782 - val_mse: 577.3466\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 200.4626 - mae: 10.3867 - mse: 200.4626 - val_loss: 576.5277 - val_mae: 18.1372 - val_mse: 576.5277\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 197.6616 - mae: 10.3311 - mse: 197.6616 - val_loss: 586.8721 - val_mae: 18.4099 - val_mse: 586.8721\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 199.5432 - mae: 10.5151 - mse: 199.5432 - val_loss: 567.8392 - val_mae: 18.1075 - val_mse: 567.8392\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 192.1346 - mae: 10.2349 - mse: 192.1346 - val_loss: 574.4958 - val_mae: 18.0917 - val_mse: 574.4958\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 197.9836 - mae: 10.2845 - mse: 197.9836 - val_loss: 567.5116 - val_mae: 17.9115 - val_mse: 567.5116\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 198.1748 - mae: 10.3916 - mse: 198.1748 - val_loss: 574.7110 - val_mae: 18.3141 - val_mse: 574.7110\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 196.9849 - mae: 10.3517 - mse: 196.9849 - val_loss: 570.9031 - val_mae: 17.9650 - val_mse: 570.9031\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 191.1976 - mae: 10.2090 - mse: 191.1976 - val_loss: 587.0274 - val_mae: 18.4660 - val_mse: 587.0274\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 194.6873 - mae: 10.3398 - mse: 194.6873 - val_loss: 582.4552 - val_mae: 18.1100 - val_mse: 582.4552\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.2665 - mae: 10.3365 - mse: 195.2665 - val_loss: 577.6946 - val_mae: 18.2511 - val_mse: 577.6946\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 191.2505 - mae: 10.1402 - mse: 191.2505 - val_loss: 581.4416 - val_mae: 18.1287 - val_mse: 581.4416\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 195.5191 - mae: 10.2593 - mse: 195.5191 - val_loss: 573.3727 - val_mae: 18.0882 - val_mse: 573.3727\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 189.0881 - mae: 10.1114 - mse: 189.0881 - val_loss: 569.4839 - val_mae: 18.3654 - val_mse: 569.4839\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 187.1733 - mae: 10.0975 - mse: 187.1733 - val_loss: 573.3924 - val_mae: 18.1703 - val_mse: 573.3924\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model3_2 = bnorm_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 2699.9878 - mae: 42.9134 - mse: 2699.9597 - val_loss: 655.1633 - val_mae: 21.8393 - val_mse: 655.1312\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 877.1313 - mae: 24.1740 - mse: 877.0978 - val_loss: 552.1523 - val_mae: 19.5945 - val_mse: 552.1185\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 827.9899 - mae: 23.2400 - mse: 827.9553 - val_loss: 527.2841 - val_mae: 19.1267 - val_mse: 527.2489\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 799.1243 - mae: 22.8544 - mse: 799.0889 - val_loss: 467.1892 - val_mae: 17.3312 - val_mse: 467.1527\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 737.2303 - mae: 21.9505 - mse: 737.1935 - val_loss: 507.0651 - val_mae: 18.5500 - val_mse: 507.0283\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 733.1163 - mae: 21.5839 - mse: 733.0792 - val_loss: 547.1879 - val_mae: 19.6434 - val_mse: 547.1505\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 726.4760 - mae: 21.5847 - mse: 726.4379 - val_loss: 493.8077 - val_mae: 18.1518 - val_mse: 493.7698\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 732.0952 - mae: 21.7908 - mse: 732.0567 - val_loss: 464.4268 - val_mae: 17.2329 - val_mse: 464.3880\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.4677 - mae: 21.4737 - mse: 709.4289 - val_loss: 478.1371 - val_mae: 17.8140 - val_mse: 478.0981\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 703.3143 - mae: 21.3230 - mse: 703.2752 - val_loss: 466.0837 - val_mae: 17.3452 - val_mse: 466.0443\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 679.2150 - mae: 20.9736 - mse: 679.1757 - val_loss: 461.8279 - val_mae: 17.2486 - val_mse: 461.7884\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.6072 - mae: 20.7926 - mse: 674.5679 - val_loss: 480.9797 - val_mae: 17.9876 - val_mse: 480.9402\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.4896 - mae: 20.5273 - mse: 665.4501 - val_loss: 471.7990 - val_mae: 17.7558 - val_mse: 471.7594\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.2434 - mae: 20.6405 - mse: 664.2038 - val_loss: 490.0572 - val_mae: 18.1827 - val_mse: 490.0177\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.0842 - mae: 20.4652 - mse: 653.0446 - val_loss: 494.0870 - val_mae: 18.4085 - val_mse: 494.0478\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.0980 - mae: 20.5665 - mse: 657.0585 - val_loss: 449.6897 - val_mae: 16.9544 - val_mse: 449.6502\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 664.5643 - mae: 20.7140 - mse: 664.5252 - val_loss: 473.1994 - val_mae: 17.6840 - val_mse: 473.1603\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.7613 - mae: 20.2277 - mse: 646.7220 - val_loss: 502.1192 - val_mae: 18.7093 - val_mse: 502.0802\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.6370 - mae: 20.1777 - mse: 634.5981 - val_loss: 477.1523 - val_mae: 17.8658 - val_mse: 477.1138\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 632.6215 - mae: 20.1404 - mse: 632.5833 - val_loss: 471.9774 - val_mae: 17.6859 - val_mse: 471.9391\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.0890 - mae: 20.0329 - mse: 624.0509 - val_loss: 469.2138 - val_mae: 17.5977 - val_mse: 469.1762\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.9440 - mae: 20.0887 - mse: 630.9061 - val_loss: 471.3345 - val_mae: 17.5622 - val_mse: 471.2968\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.0771 - mae: 20.1625 - mse: 635.0399 - val_loss: 467.8669 - val_mae: 17.5838 - val_mse: 467.8294\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 620.7756 - mae: 19.9544 - mse: 620.7382 - val_loss: 464.7070 - val_mae: 17.5406 - val_mse: 464.6699\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.7949 - mae: 19.7874 - mse: 616.7581 - val_loss: 471.5573 - val_mae: 17.7064 - val_mse: 471.5207\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.8621 - mae: 19.9372 - mse: 626.8256 - val_loss: 457.5323 - val_mae: 17.1159 - val_mse: 457.4958\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 607.3671 - mae: 19.6887 - mse: 607.3311 - val_loss: 477.3049 - val_mae: 17.8936 - val_mse: 477.2691\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.7769 - mae: 19.8022 - mse: 609.7413 - val_loss: 461.1885 - val_mae: 17.3280 - val_mse: 461.1531\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.3765 - mae: 19.6564 - mse: 606.3416 - val_loss: 491.7515 - val_mae: 18.3382 - val_mse: 491.7165\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.9058 - mae: 19.6416 - mse: 603.8710 - val_loss: 455.6653 - val_mae: 17.1682 - val_mse: 455.6307\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.2947 - mae: 19.7515 - mse: 611.2605 - val_loss: 490.2550 - val_mae: 18.2829 - val_mse: 490.2209\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.5119 - mae: 19.4143 - mse: 588.4781 - val_loss: 448.6614 - val_mae: 16.7411 - val_mse: 448.6277\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.9391 - mae: 19.5670 - mse: 596.9056 - val_loss: 447.4780 - val_mae: 16.8605 - val_mse: 447.4446\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.8622 - mae: 19.6592 - mse: 606.8291 - val_loss: 451.4127 - val_mae: 16.9833 - val_mse: 451.3796\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.9089 - mae: 19.2578 - mse: 582.8762 - val_loss: 462.3209 - val_mae: 17.3709 - val_mse: 462.2881\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.2286 - mae: 19.5939 - mse: 596.1962 - val_loss: 453.9679 - val_mae: 16.8232 - val_mse: 453.9356\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.7500 - mae: 19.4566 - mse: 594.7178 - val_loss: 461.2596 - val_mae: 17.3555 - val_mse: 461.2276\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.2617 - mae: 19.5007 - mse: 599.2297 - val_loss: 462.1499 - val_mae: 17.3968 - val_mse: 462.1181\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.2078 - mae: 19.5109 - mse: 595.1760 - val_loss: 479.7705 - val_mae: 18.0515 - val_mse: 479.7392\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.4805 - mae: 19.2231 - mse: 583.4492 - val_loss: 482.3332 - val_mae: 18.0100 - val_mse: 482.3020\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.2358 - mae: 19.4789 - mse: 586.2047 - val_loss: 450.3031 - val_mae: 16.7396 - val_mse: 450.2722\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 579.7890 - mae: 19.1410 - mse: 579.7586 - val_loss: 444.7226 - val_mae: 16.6006 - val_mse: 444.6920\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.8231 - mae: 19.2911 - mse: 583.7926 - val_loss: 467.8677 - val_mae: 17.6150 - val_mse: 467.8375\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 580.9232 - mae: 19.3041 - mse: 580.8929 - val_loss: 451.1052 - val_mae: 17.0699 - val_mse: 451.0750\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.2396 - mae: 19.3425 - mse: 580.2094 - val_loss: 451.6719 - val_mae: 16.9895 - val_mse: 451.6418\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 575.8730 - mae: 19.1499 - mse: 575.8431 - val_loss: 473.6396 - val_mae: 17.8598 - val_mse: 473.6101\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 578.8672 - mae: 19.2373 - mse: 578.8376 - val_loss: 458.7299 - val_mae: 17.3896 - val_mse: 458.7004\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.2045 - mae: 19.3511 - mse: 582.1750 - val_loss: 449.5809 - val_mae: 16.8661 - val_mse: 449.5516\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.7388 - mae: 18.9605 - mse: 571.7095 - val_loss: 451.4867 - val_mae: 17.0170 - val_mse: 451.4576\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.3334 - mae: 18.9519 - mse: 566.3043 - val_loss: 453.4833 - val_mae: 16.9750 - val_mse: 453.4543\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.6766 - mae: 18.8130 - mse: 555.6478 - val_loss: 444.4706 - val_mae: 16.1283 - val_mse: 444.4417\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 559.4786 - mae: 18.8973 - mse: 559.4500 - val_loss: 447.5214 - val_mae: 16.7026 - val_mse: 447.4927\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.4465 - mae: 19.0880 - mse: 569.4179 - val_loss: 457.4016 - val_mae: 17.2652 - val_mse: 457.3733\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.5757 - mae: 19.1285 - mse: 567.5477 - val_loss: 447.8289 - val_mae: 16.6455 - val_mse: 447.8008\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.4783 - mae: 18.7808 - mse: 555.4499 - val_loss: 443.9866 - val_mae: 16.5430 - val_mse: 443.9584\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.5856 - mae: 18.7910 - mse: 555.5577 - val_loss: 454.2576 - val_mae: 17.0280 - val_mse: 454.2296\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.7063 - mae: 18.8281 - mse: 563.6780 - val_loss: 441.6079 - val_mae: 16.3820 - val_mse: 441.5801\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 568.4222 - mae: 19.0096 - mse: 568.3945 - val_loss: 448.4235 - val_mae: 16.8210 - val_mse: 448.3957\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 557.7584 - mae: 18.7636 - mse: 557.7306 - val_loss: 459.5228 - val_mae: 17.3076 - val_mse: 459.4951\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.6462 - mae: 18.6563 - mse: 551.6185 - val_loss: 441.4968 - val_mae: 16.2092 - val_mse: 441.4691\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 558.0876 - mae: 18.8012 - mse: 558.0597 - val_loss: 448.4619 - val_mae: 16.5663 - val_mse: 448.4342\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.8842 - mae: 18.6760 - mse: 551.8568 - val_loss: 455.1687 - val_mae: 17.0464 - val_mse: 455.1412\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.5602 - mae: 18.5982 - mse: 544.5328 - val_loss: 451.1304 - val_mae: 17.0004 - val_mse: 451.1030\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 539.9414 - mae: 18.5146 - mse: 539.9139 - val_loss: 447.9300 - val_mae: 16.7539 - val_mse: 447.9026\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 539.1511 - mae: 18.5504 - mse: 539.1235 - val_loss: 454.3408 - val_mae: 16.8057 - val_mse: 454.3134\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.7576 - mae: 18.5255 - mse: 537.7303 - val_loss: 450.9364 - val_mae: 16.9210 - val_mse: 450.9089\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 540.1222 - mae: 18.4738 - mse: 540.0948 - val_loss: 449.8743 - val_mae: 16.8495 - val_mse: 449.8468\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 547.0176 - mae: 18.6083 - mse: 546.9903 - val_loss: 443.7357 - val_mae: 16.1128 - val_mse: 443.7083\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.6927 - mae: 18.5505 - mse: 541.6654 - val_loss: 449.5108 - val_mae: 16.8734 - val_mse: 449.4836\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.0793 - mae: 18.7892 - mse: 548.0521 - val_loss: 451.2350 - val_mae: 16.9946 - val_mse: 451.2080\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 534.1136 - mae: 18.3648 - mse: 534.0862 - val_loss: 447.0436 - val_mae: 16.8070 - val_mse: 447.0166\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 538.7796 - mae: 18.4450 - mse: 538.7526 - val_loss: 442.7869 - val_mae: 16.5145 - val_mse: 442.7600\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.8929 - mae: 18.5843 - mse: 537.8660 - val_loss: 446.4301 - val_mae: 16.7643 - val_mse: 446.4033\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 534.1005 - mae: 18.4162 - mse: 534.0739 - val_loss: 453.8603 - val_mae: 17.1420 - val_mse: 453.8335\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.1382 - mae: 18.6092 - mse: 544.1115 - val_loss: 458.8878 - val_mae: 17.4266 - val_mse: 458.8611\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 541.6786 - mae: 18.5276 - mse: 541.6519 - val_loss: 443.3396 - val_mae: 16.5242 - val_mse: 443.3129\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 533.8526 - mae: 18.3285 - mse: 533.8256 - val_loss: 446.4863 - val_mae: 16.5450 - val_mse: 446.4596\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.1078 - mae: 18.0338 - mse: 523.0812 - val_loss: 446.5092 - val_mae: 16.8586 - val_mse: 446.4824\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4077 - mae: 18.3821 - mse: 531.3808 - val_loss: 441.8688 - val_mae: 16.6643 - val_mse: 441.8419\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 520.9041 - mae: 18.1942 - mse: 520.8773 - val_loss: 450.2386 - val_mae: 16.9020 - val_mse: 450.2120\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 522.0189 - mae: 18.1962 - mse: 521.9919 - val_loss: 450.0806 - val_mae: 16.7623 - val_mse: 450.0539\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 523.8525 - mae: 18.1870 - mse: 523.8259 - val_loss: 460.1342 - val_mae: 17.3582 - val_mse: 460.1077\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 531.4272 - mae: 18.3456 - mse: 531.4003 - val_loss: 446.3467 - val_mae: 16.7663 - val_mse: 446.3199\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 519.7271 - mae: 18.2588 - mse: 519.7008 - val_loss: 450.4432 - val_mae: 16.8916 - val_mse: 450.4163\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 522.2610 - mae: 18.1714 - mse: 522.2343 - val_loss: 448.2872 - val_mae: 16.8938 - val_mse: 448.2605\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 521.3271 - mae: 18.1554 - mse: 521.3004 - val_loss: 439.4465 - val_mae: 16.3366 - val_mse: 439.4197\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 515.1349 - mae: 18.0212 - mse: 515.1079 - val_loss: 441.5893 - val_mae: 16.3981 - val_mse: 441.5623\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 520.8240 - mae: 18.1105 - mse: 520.7972 - val_loss: 443.1610 - val_mae: 16.3491 - val_mse: 443.1340\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 516.2036 - mae: 17.9859 - mse: 516.1766 - val_loss: 457.4094 - val_mae: 17.3441 - val_mse: 457.3825\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 504.5481 - mae: 17.7379 - mse: 504.5211 - val_loss: 447.1848 - val_mae: 16.6371 - val_mse: 447.1581\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 518.5012 - mae: 18.1689 - mse: 518.4744 - val_loss: 440.6575 - val_mae: 16.5706 - val_mse: 440.6306\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 505.0982 - mae: 17.8506 - mse: 505.0714 - val_loss: 449.1805 - val_mae: 16.7496 - val_mse: 449.1534\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.4738 - mae: 17.9799 - mse: 512.4469 - val_loss: 472.5655 - val_mae: 17.8911 - val_mse: 472.5386\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.2599 - mae: 17.9938 - mse: 512.2328 - val_loss: 448.1171 - val_mae: 16.5961 - val_mse: 448.0900\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 511.8910 - mae: 17.8440 - mse: 511.8642 - val_loss: 451.9624 - val_mae: 16.9372 - val_mse: 451.9354\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 501.7968 - mae: 17.8233 - mse: 501.7699 - val_loss: 462.8581 - val_mae: 17.3499 - val_mse: 462.8312\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 512.7311 - mae: 17.9569 - mse: 512.7040 - val_loss: 443.2213 - val_mae: 16.4805 - val_mse: 443.1942\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 498.4146 - mae: 17.6197 - mse: 498.3873 - val_loss: 449.6291 - val_mae: 16.8109 - val_mse: 449.6020\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 497.5008 - mae: 17.6150 - mse: 497.4735 - val_loss: 446.4661 - val_mae: 16.3010 - val_mse: 446.4388\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 496.2195 - mae: 17.6483 - mse: 496.1921 - val_loss: 452.2564 - val_mae: 16.9125 - val_mse: 452.2292\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_3_train_final.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_2 = reg_model_maxpeak_withgenre.fit(\n",
    "    X_withgenre_3_train_scaled, y_withgenre_3_train_final,\n",
    "    validation_data=(X_withgenre_3_val_scaled, y_withgenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 6.8788, Train MSE: 90.7993\n",
      "Val   MAE: 20.9125, Val   MSE: 764.1398\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 7.3418, Train MSE: 106.9902\n",
      "Val   MAE: 18.1703, Val   MSE: 573.3924\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 14.8380, Train MSE: 369.1338\n",
      "Val   MAE: 16.9125, Val   MSE: 452.2292\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX PEAK POSITION, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores3_2 = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores3_2   = baseline_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores3_2[1]:.4f}, Train MSE: {train_scores3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores3_2[1]:.4f}, Val   MSE: {val_scores3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn3_2 = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_bn3_2   = bnorm_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn3_2[1]:.4f}, Train MSE: {train_scores_bn3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn3_2[1]:.4f}, Val   MSE: {val_scores_bn3_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg3_2 = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_train_scaled, y_withgenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_2   = reg_model_maxpeak_withgenre.evaluate(X_withgenre_3_val_scaled, y_withgenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_2[1]:.4f}, Train MSE: {train_scores_reg3_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_2[1]:.4f}, Val   MSE: {val_scores_reg3_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - No Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 163.6994 - mae: 9.1939 - mse: 163.6994 - val_loss: 146.5902 - val_mae: 8.7993 - val_mse: 146.5902\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.6751 - mae: 8.6075 - mse: 139.6751 - val_loss: 142.5868 - val_mae: 8.9208 - val_mse: 142.5868\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.7392 - mae: 8.5368 - mse: 137.7392 - val_loss: 142.4232 - val_mae: 8.6669 - val_mse: 142.4232\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.5552 - mae: 8.5116 - mse: 136.5552 - val_loss: 145.1798 - val_mae: 8.6242 - val_mse: 145.1798\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8448 - mae: 8.4530 - mse: 135.8448 - val_loss: 140.7722 - val_mae: 8.7388 - val_mse: 140.7722\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.6794 - mae: 8.4913 - mse: 135.6794 - val_loss: 140.7099 - val_mae: 8.6030 - val_mse: 140.7099\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4535 - mae: 8.4366 - mse: 134.4535 - val_loss: 141.2067 - val_mae: 8.5922 - val_mse: 141.2067\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.2167 - mae: 8.3747 - mse: 133.2167 - val_loss: 141.4359 - val_mae: 8.6665 - val_mse: 141.4359\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.8638 - mae: 8.3753 - mse: 132.8638 - val_loss: 140.9899 - val_mae: 8.6244 - val_mse: 140.9899\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.1011 - mae: 8.3602 - mse: 132.1011 - val_loss: 144.0760 - val_mae: 8.6456 - val_mse: 144.0760\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5261 - mae: 8.3266 - mse: 131.5261 - val_loss: 140.4108 - val_mae: 8.7875 - val_mse: 140.4108\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.1434 - mae: 8.2949 - mse: 130.1434 - val_loss: 140.5952 - val_mae: 8.6727 - val_mse: 140.5952\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.7185 - mae: 8.2535 - mse: 129.7185 - val_loss: 141.3340 - val_mae: 8.6334 - val_mse: 141.3340\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.5713 - mae: 8.2283 - mse: 128.5713 - val_loss: 141.0575 - val_mae: 8.8117 - val_mse: 141.0575\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 127.3479 - mae: 8.2172 - mse: 127.3479 - val_loss: 144.6642 - val_mae: 8.7645 - val_mse: 144.6642\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.9542 - mae: 8.1597 - mse: 125.9542 - val_loss: 143.3976 - val_mae: 8.9220 - val_mse: 143.3976\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 125.4509 - mae: 8.1502 - mse: 125.4509 - val_loss: 146.0223 - val_mae: 9.0921 - val_mse: 146.0223\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 123.5753 - mae: 8.0880 - mse: 123.5753 - val_loss: 146.0869 - val_mae: 9.0358 - val_mse: 146.0869\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 123.0955 - mae: 8.1051 - mse: 123.0955 - val_loss: 144.4891 - val_mae: 8.9256 - val_mse: 144.4891\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9696 - mae: 8.0197 - mse: 120.9696 - val_loss: 146.2754 - val_mae: 8.9358 - val_mse: 146.2754\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 120.4910 - mae: 7.9959 - mse: 120.4910 - val_loss: 145.4303 - val_mae: 8.9014 - val_mse: 145.4303\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 119.2536 - mae: 7.9589 - mse: 119.2536 - val_loss: 146.3809 - val_mae: 9.0443 - val_mse: 146.3809\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.6051 - mae: 7.9208 - mse: 117.6051 - val_loss: 149.8065 - val_mae: 9.2909 - val_mse: 149.8065\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 116.6282 - mae: 7.8963 - mse: 116.6282 - val_loss: 147.9337 - val_mae: 9.0718 - val_mse: 147.9337\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 115.8159 - mae: 7.8588 - mse: 115.8159 - val_loss: 149.2712 - val_mae: 9.1155 - val_mse: 149.2712\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 113.8677 - mae: 7.8258 - mse: 113.8677 - val_loss: 149.9238 - val_mae: 9.2684 - val_mse: 149.9238\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 113.4410 - mae: 7.8044 - mse: 113.4410 - val_loss: 151.8965 - val_mae: 9.0818 - val_mse: 151.8965\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.1809 - mae: 7.7742 - mse: 112.1809 - val_loss: 156.7161 - val_mae: 9.0797 - val_mse: 156.7161\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4911 - mae: 7.7190 - mse: 110.4911 - val_loss: 153.5957 - val_mae: 9.3343 - val_mse: 153.5957\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.6604 - mae: 7.6934 - mse: 109.6604 - val_loss: 153.5593 - val_mae: 9.3030 - val_mse: 153.5593\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.2698 - mae: 7.6386 - mse: 108.2698 - val_loss: 155.9758 - val_mae: 9.1564 - val_mse: 155.9758\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 108.2494 - mae: 7.6403 - mse: 108.2494 - val_loss: 156.8181 - val_mae: 9.4661 - val_mse: 156.8181\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 105.5371 - mae: 7.5436 - mse: 105.5371 - val_loss: 159.3262 - val_mae: 9.6046 - val_mse: 159.3262\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 106.2799 - mae: 7.5966 - mse: 106.2799 - val_loss: 155.6282 - val_mae: 9.5144 - val_mse: 155.6282\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.9642 - mae: 7.5168 - mse: 103.9642 - val_loss: 158.0479 - val_mae: 9.5430 - val_mse: 158.0479\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.5565 - mae: 7.4959 - mse: 103.5565 - val_loss: 159.0953 - val_mae: 9.4178 - val_mse: 159.0953\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 100.9547 - mae: 7.4069 - mse: 100.9547 - val_loss: 159.4695 - val_mae: 9.4688 - val_mse: 159.4695\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.0795 - mae: 7.3972 - mse: 101.0795 - val_loss: 161.3044 - val_mae: 9.6116 - val_mse: 161.3044\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.8931 - mae: 7.3784 - mse: 99.8931 - val_loss: 164.5630 - val_mae: 9.6412 - val_mse: 164.5630\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.1165 - mae: 7.3793 - mse: 99.1165 - val_loss: 161.6770 - val_mae: 9.5400 - val_mse: 161.6770\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 97.7009 - mae: 7.2810 - mse: 97.7009 - val_loss: 162.8642 - val_mae: 9.5352 - val_mse: 162.8642\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 96.4544 - mae: 7.2676 - mse: 96.4544 - val_loss: 165.9525 - val_mae: 9.6618 - val_mse: 165.9525\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.5412 - mae: 7.2361 - mse: 95.5412 - val_loss: 169.7922 - val_mae: 9.7286 - val_mse: 169.7922\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.2807 - mae: 7.2246 - mse: 95.2807 - val_loss: 164.2546 - val_mae: 9.4558 - val_mse: 164.2546\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.5301 - mae: 7.1680 - mse: 93.5301 - val_loss: 169.4503 - val_mae: 9.5071 - val_mse: 169.4503\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.5097 - mae: 7.1862 - mse: 93.5097 - val_loss: 171.2588 - val_mae: 9.6116 - val_mse: 171.2588\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 91.1018 - mae: 7.0943 - mse: 91.1018 - val_loss: 167.5082 - val_mae: 9.6585 - val_mse: 167.5082\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.1476 - mae: 7.0749 - mse: 91.1476 - val_loss: 171.4738 - val_mae: 9.7714 - val_mse: 171.4738\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.8585 - mae: 7.0533 - mse: 89.8585 - val_loss: 169.8099 - val_mae: 9.6920 - val_mse: 169.8099\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.7339 - mae: 7.0436 - mse: 89.7339 - val_loss: 181.8004 - val_mae: 9.8077 - val_mse: 181.8004\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 88.0498 - mae: 6.9543 - mse: 88.0498 - val_loss: 172.7514 - val_mae: 9.7914 - val_mse: 172.7514\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 87.6393 - mae: 6.9405 - mse: 87.6393 - val_loss: 171.0560 - val_mae: 9.7843 - val_mse: 171.0560\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.2995 - mae: 7.0573 - mse: 89.2995 - val_loss: 174.9364 - val_mae: 10.0138 - val_mse: 174.9364\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 85.6382 - mae: 6.8651 - mse: 85.6382 - val_loss: 173.1906 - val_mae: 9.8124 - val_mse: 173.1906\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 85.0885 - mae: 6.8737 - mse: 85.0885 - val_loss: 174.5753 - val_mae: 9.7240 - val_mse: 174.5753\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.6962 - mae: 6.8449 - mse: 84.6962 - val_loss: 177.0687 - val_mae: 9.8937 - val_mse: 177.0687\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.7378 - mae: 6.7886 - mse: 82.7378 - val_loss: 179.1032 - val_mae: 10.0657 - val_mse: 179.1032\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.7162 - mae: 6.7789 - mse: 82.7162 - val_loss: 178.2224 - val_mae: 9.9927 - val_mse: 178.2224\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.9106 - mae: 6.8213 - mse: 82.9106 - val_loss: 178.6161 - val_mae: 10.1090 - val_mse: 178.6161\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 81.0066 - mae: 6.7574 - mse: 81.0066 - val_loss: 183.7741 - val_mae: 9.9509 - val_mse: 183.7741\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 81.2601 - mae: 6.7511 - mse: 81.2601 - val_loss: 176.0390 - val_mae: 9.8469 - val_mse: 176.0390\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 79.2529 - mae: 6.6525 - mse: 79.2529 - val_loss: 179.6586 - val_mae: 9.9941 - val_mse: 179.6586\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 77.6975 - mae: 6.5982 - mse: 77.6975 - val_loss: 183.5479 - val_mae: 10.0085 - val_mse: 183.5479\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 78.1089 - mae: 6.6363 - mse: 78.1089 - val_loss: 189.2261 - val_mae: 9.9719 - val_mse: 189.2261\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 77.2047 - mae: 6.5808 - mse: 77.2047 - val_loss: 183.1960 - val_mae: 10.0293 - val_mse: 183.1960\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.8369 - mae: 6.5034 - mse: 75.8369 - val_loss: 181.6039 - val_mae: 10.0097 - val_mse: 181.6039\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.6088 - mae: 6.5025 - mse: 75.6088 - val_loss: 185.0665 - val_mae: 10.1556 - val_mse: 185.0665\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 75.8699 - mae: 6.5571 - mse: 75.8699 - val_loss: 188.2630 - val_mae: 10.2544 - val_mse: 188.2630\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.1594 - mae: 6.4087 - mse: 73.1594 - val_loss: 182.7262 - val_mae: 10.0353 - val_mse: 182.7262\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 72.7649 - mae: 6.4306 - mse: 72.7649 - val_loss: 185.5446 - val_mae: 10.1193 - val_mse: 185.5446\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.1563 - mae: 6.3276 - mse: 71.1563 - val_loss: 186.3555 - val_mae: 10.2473 - val_mse: 186.3555\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 71.2269 - mae: 6.3394 - mse: 71.2269 - val_loss: 189.5376 - val_mae: 10.2179 - val_mse: 189.5376\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 71.4683 - mae: 6.3284 - mse: 71.4683 - val_loss: 187.6027 - val_mae: 10.1683 - val_mse: 187.6027\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 70.5083 - mae: 6.3166 - mse: 70.5083 - val_loss: 187.3468 - val_mae: 10.1728 - val_mse: 187.3468\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 69.5165 - mae: 6.2678 - mse: 69.5165 - val_loss: 188.6333 - val_mae: 10.1744 - val_mse: 188.6333\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.6138 - mae: 6.1575 - mse: 67.6138 - val_loss: 193.6031 - val_mae: 10.3609 - val_mse: 193.6031\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 68.8259 - mae: 6.2901 - mse: 68.8259 - val_loss: 184.5905 - val_mae: 9.9600 - val_mse: 184.5905\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.0607 - mae: 6.1297 - mse: 67.0607 - val_loss: 198.4982 - val_mae: 10.6384 - val_mse: 198.4982\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 67.2416 - mae: 6.1509 - mse: 67.2416 - val_loss: 198.2409 - val_mae: 10.5752 - val_mse: 198.2409\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 65.9747 - mae: 6.1179 - mse: 65.9747 - val_loss: 197.1183 - val_mae: 10.4870 - val_mse: 197.1183\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.2949 - mae: 6.1631 - mse: 66.2949 - val_loss: 193.3045 - val_mae: 10.2911 - val_mse: 193.3045\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.0802 - mae: 6.1151 - mse: 65.0802 - val_loss: 192.4500 - val_mae: 10.1665 - val_mse: 192.4500\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.9513 - mae: 6.0429 - mse: 63.9513 - val_loss: 194.8931 - val_mae: 10.2247 - val_mse: 194.8931\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.4745 - mae: 6.0489 - mse: 64.4745 - val_loss: 198.8596 - val_mae: 10.3209 - val_mse: 198.8596\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 63.6069 - mae: 6.0232 - mse: 63.6069 - val_loss: 195.1287 - val_mae: 10.3807 - val_mse: 195.1287\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.8266 - mae: 5.8670 - mse: 60.8266 - val_loss: 206.7375 - val_mae: 10.7635 - val_mse: 206.7375\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 62.8106 - mae: 5.9873 - mse: 62.8106 - val_loss: 195.0881 - val_mae: 10.1797 - val_mse: 195.0881\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.9522 - mae: 5.9313 - mse: 60.9522 - val_loss: 201.1508 - val_mae: 10.4595 - val_mse: 201.1508\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.4340 - mae: 5.8591 - mse: 60.4340 - val_loss: 200.7374 - val_mae: 10.3991 - val_mse: 200.7374\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.3661 - mae: 5.8215 - mse: 59.3661 - val_loss: 206.0656 - val_mae: 10.6274 - val_mse: 206.0656\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.2402 - mae: 5.7472 - mse: 58.2402 - val_loss: 205.1778 - val_mae: 10.6182 - val_mse: 205.1778\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 59.6572 - mae: 5.8316 - mse: 59.6572 - val_loss: 201.8430 - val_mae: 10.4406 - val_mse: 201.8430\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.8766 - mae: 5.7897 - mse: 58.8766 - val_loss: 202.0848 - val_mae: 10.4834 - val_mse: 202.0848\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 56.9950 - mae: 5.7094 - mse: 56.9950 - val_loss: 207.6714 - val_mae: 10.7088 - val_mse: 207.6714\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 57.7192 - mae: 5.7404 - mse: 57.7192 - val_loss: 221.2529 - val_mae: 11.1979 - val_mse: 221.2529\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.5192 - mae: 5.7947 - mse: 58.5192 - val_loss: 203.7799 - val_mae: 10.5138 - val_mse: 203.7799\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 56.5748 - mae: 5.6652 - mse: 56.5748 - val_loss: 202.0991 - val_mae: 10.5297 - val_mse: 202.0991\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.3109 - mae: 5.5820 - mse: 54.3109 - val_loss: 209.4045 - val_mae: 10.6295 - val_mse: 209.4045\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.2451 - mae: 5.5601 - mse: 54.2451 - val_loss: 209.4900 - val_mae: 10.6832 - val_mse: 209.4900\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.0082 - mae: 5.5728 - mse: 54.0082 - val_loss: 206.0707 - val_mae: 10.4803 - val_mse: 206.0707\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_1 = baseline_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 247.5984 - mae: 11.4104 - mse: 247.5984 - val_loss: 235.4777 - val_mae: 11.2651 - val_mse: 235.4777\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 165.1202 - mae: 9.0374 - mse: 165.1202 - val_loss: 159.6051 - val_mae: 8.9333 - val_mse: 159.6051\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3735 - mae: 8.3451 - mse: 136.3735 - val_loss: 142.8192 - val_mae: 8.6185 - val_mse: 142.8192\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5479 - mae: 8.3087 - mse: 131.5479 - val_loss: 143.1145 - val_mae: 8.7279 - val_mse: 143.1145\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.2044 - mae: 8.3153 - mse: 130.2044 - val_loss: 142.1322 - val_mae: 8.7200 - val_mse: 142.1322\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7501 - mae: 8.2483 - mse: 128.7501 - val_loss: 142.3866 - val_mae: 8.8997 - val_mse: 142.3866\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4380 - mae: 8.2627 - mse: 128.4380 - val_loss: 141.1958 - val_mae: 8.7414 - val_mse: 141.1958\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4285 - mae: 8.2114 - mse: 127.4285 - val_loss: 141.9936 - val_mae: 8.8748 - val_mse: 141.9936\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 126.1214 - mae: 8.1568 - mse: 126.1214 - val_loss: 143.2634 - val_mae: 8.7664 - val_mse: 143.2634\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.8694 - mae: 8.1672 - mse: 125.8694 - val_loss: 144.8573 - val_mae: 8.7578 - val_mse: 144.8573\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.6261 - mae: 8.1627 - mse: 124.6261 - val_loss: 142.3284 - val_mae: 8.7864 - val_mse: 142.3284\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1815 - mae: 8.1456 - mse: 124.1815 - val_loss: 145.6023 - val_mae: 8.9521 - val_mse: 145.6023\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.8567 - mae: 8.1262 - mse: 123.8567 - val_loss: 147.0350 - val_mae: 9.0388 - val_mse: 147.0350\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.7364 - mae: 8.1238 - mse: 122.7364 - val_loss: 145.3135 - val_mae: 8.9169 - val_mse: 145.3135\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.3613 - mae: 8.0858 - mse: 122.3613 - val_loss: 145.0243 - val_mae: 8.9683 - val_mse: 145.0243\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9691 - mae: 8.0650 - mse: 120.9691 - val_loss: 145.9814 - val_mae: 8.9570 - val_mse: 145.9814\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.3455 - mae: 8.0598 - mse: 120.3455 - val_loss: 147.5744 - val_mae: 9.0449 - val_mse: 147.5744\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.7127 - mae: 7.9850 - mse: 118.7127 - val_loss: 147.8299 - val_mae: 9.1306 - val_mse: 147.8299\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.2331 - mae: 8.0116 - mse: 119.2331 - val_loss: 147.6138 - val_mae: 9.0588 - val_mse: 147.6138\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.5877 - mae: 8.0290 - mse: 118.5877 - val_loss: 150.8034 - val_mae: 9.0548 - val_mse: 150.8034\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.7278 - mae: 7.9341 - mse: 116.7278 - val_loss: 150.5605 - val_mae: 9.1111 - val_mse: 150.5605\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.6613 - mae: 8.0021 - mse: 117.6613 - val_loss: 149.2334 - val_mae: 9.0985 - val_mse: 149.2334\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.3404 - mae: 7.8846 - mse: 115.3404 - val_loss: 146.9989 - val_mae: 9.0333 - val_mse: 146.9989\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.3909 - mae: 7.9375 - mse: 116.3909 - val_loss: 151.4994 - val_mae: 9.3081 - val_mse: 151.4994\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.8293 - mae: 7.8900 - mse: 113.8293 - val_loss: 155.4615 - val_mae: 9.2647 - val_mse: 155.4615\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.3351 - mae: 7.8563 - mse: 114.3351 - val_loss: 152.1231 - val_mae: 9.2545 - val_mse: 152.1231\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.2566 - mae: 7.8584 - mse: 114.2566 - val_loss: 152.9240 - val_mae: 9.0529 - val_mse: 152.9240\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.5988 - mae: 7.7792 - mse: 112.5988 - val_loss: 153.8217 - val_mae: 9.2339 - val_mse: 153.8217\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.6743 - mae: 7.8058 - mse: 111.6743 - val_loss: 148.7007 - val_mae: 9.1642 - val_mse: 148.7007\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.7598 - mae: 7.7445 - mse: 110.7598 - val_loss: 153.4382 - val_mae: 9.1908 - val_mse: 153.4382\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.5570 - mae: 7.8161 - mse: 111.5570 - val_loss: 157.3539 - val_mae: 9.2684 - val_mse: 157.3539\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4225 - mae: 7.7974 - mse: 110.4225 - val_loss: 157.1826 - val_mae: 9.2503 - val_mse: 157.1826\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.3080 - mae: 7.7633 - mse: 109.3080 - val_loss: 155.3094 - val_mae: 9.2153 - val_mse: 155.3094\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8155 - mae: 7.7179 - mse: 108.8155 - val_loss: 156.8951 - val_mae: 9.3047 - val_mse: 156.8951\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9454 - mae: 7.6932 - mse: 107.9454 - val_loss: 156.2481 - val_mae: 9.3160 - val_mse: 156.2481\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.6165 - mae: 7.6507 - mse: 107.6165 - val_loss: 154.3568 - val_mae: 9.2302 - val_mse: 154.3568\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8985 - mae: 7.6659 - mse: 108.8985 - val_loss: 157.2781 - val_mae: 9.3106 - val_mse: 157.2781\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.1801 - mae: 7.5935 - mse: 105.1801 - val_loss: 157.9465 - val_mae: 9.2510 - val_mse: 157.9465\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.4131 - mae: 7.6273 - mse: 106.4131 - val_loss: 160.1235 - val_mae: 9.4201 - val_mse: 160.1235\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.9819 - mae: 7.5492 - mse: 104.9819 - val_loss: 160.2046 - val_mae: 9.5112 - val_mse: 160.2046\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.3627 - mae: 7.6145 - mse: 104.3627 - val_loss: 158.2147 - val_mae: 9.3068 - val_mse: 158.2147\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.8930 - mae: 7.6262 - mse: 105.8930 - val_loss: 161.2203 - val_mae: 9.4742 - val_mse: 161.2203\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.7386 - mae: 7.5535 - mse: 104.7386 - val_loss: 160.1093 - val_mae: 9.3491 - val_mse: 160.1093\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.3412 - mae: 7.5487 - mse: 103.3412 - val_loss: 159.5777 - val_mae: 9.2731 - val_mse: 159.5777\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.7248 - mae: 7.5077 - mse: 102.7248 - val_loss: 160.8370 - val_mae: 9.5076 - val_mse: 160.8370\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6129 - mae: 7.4856 - mse: 101.6129 - val_loss: 164.6996 - val_mae: 9.4638 - val_mse: 164.6996\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.0041 - mae: 7.4990 - mse: 102.0041 - val_loss: 164.6512 - val_mae: 9.6210 - val_mse: 164.6512\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6507 - mae: 7.5820 - mse: 101.6507 - val_loss: 161.3277 - val_mae: 9.4896 - val_mse: 161.3277\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6515 - mae: 7.4872 - mse: 101.6515 - val_loss: 158.6392 - val_mae: 9.3518 - val_mse: 158.6392\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.7848 - mae: 7.3915 - mse: 98.7848 - val_loss: 164.1393 - val_mae: 9.4980 - val_mse: 164.1393\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.8782 - mae: 7.3414 - mse: 98.8782 - val_loss: 161.2203 - val_mae: 9.4840 - val_mse: 161.2203\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.0479 - mae: 7.3956 - mse: 99.0479 - val_loss: 166.1731 - val_mae: 9.6877 - val_mse: 166.1731\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.8396 - mae: 7.4661 - mse: 100.8396 - val_loss: 164.8414 - val_mae: 9.4956 - val_mse: 164.8414\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.5965 - mae: 7.3776 - mse: 97.5965 - val_loss: 165.0360 - val_mae: 9.5769 - val_mse: 165.0360\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.2586 - mae: 7.2912 - mse: 96.2586 - val_loss: 168.0140 - val_mae: 9.6748 - val_mse: 168.0140\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.5316 - mae: 7.4081 - mse: 98.5316 - val_loss: 165.2034 - val_mae: 9.4899 - val_mse: 165.2034\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.9094 - mae: 7.3947 - mse: 97.9094 - val_loss: 162.8026 - val_mae: 9.4295 - val_mse: 162.8026\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.0997 - mae: 7.2895 - mse: 96.0997 - val_loss: 163.1887 - val_mae: 9.4548 - val_mse: 163.1887\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.1257 - mae: 7.2628 - mse: 95.1257 - val_loss: 165.0116 - val_mae: 9.6234 - val_mse: 165.0116\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 97.2509 - mae: 7.4064 - mse: 97.2509 - val_loss: 163.5095 - val_mae: 9.4748 - val_mse: 163.5095\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.6674 - mae: 7.2598 - mse: 94.6674 - val_loss: 168.1011 - val_mae: 9.6351 - val_mse: 168.1011\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.7941 - mae: 7.2687 - mse: 95.7941 - val_loss: 170.5729 - val_mae: 9.6862 - val_mse: 170.5729\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.6397 - mae: 7.2156 - mse: 93.6397 - val_loss: 169.7390 - val_mae: 9.6785 - val_mse: 169.7390\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.8529 - mae: 7.2515 - mse: 93.8529 - val_loss: 167.8690 - val_mae: 9.4817 - val_mse: 167.8690\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.6009 - mae: 7.1706 - mse: 92.6009 - val_loss: 169.4846 - val_mae: 9.6368 - val_mse: 169.4846\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.0449 - mae: 7.1864 - mse: 92.0449 - val_loss: 171.1148 - val_mae: 9.6764 - val_mse: 171.1148\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 93.4900 - mae: 7.2203 - mse: 93.4900 - val_loss: 171.6473 - val_mae: 9.5700 - val_mse: 171.6473\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 92.9964 - mae: 7.2173 - mse: 92.9964 - val_loss: 174.7949 - val_mae: 9.7926 - val_mse: 174.7949\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.4757 - mae: 7.1443 - mse: 91.4757 - val_loss: 172.0092 - val_mae: 9.7661 - val_mse: 172.0092\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 92.4805 - mae: 7.1835 - mse: 92.4805 - val_loss: 171.4970 - val_mae: 9.7283 - val_mse: 171.4970\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.9706 - mae: 7.1329 - mse: 90.9706 - val_loss: 173.3345 - val_mae: 9.8543 - val_mse: 173.3345\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.7169 - mae: 7.1805 - mse: 91.7169 - val_loss: 166.9821 - val_mae: 9.5171 - val_mse: 166.9821\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.0778 - mae: 7.0974 - mse: 90.0778 - val_loss: 174.3049 - val_mae: 9.7913 - val_mse: 174.3049\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.7824 - mae: 7.1527 - mse: 91.7824 - val_loss: 169.8535 - val_mae: 9.6779 - val_mse: 169.8535\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.1445 - mae: 7.1059 - mse: 90.1445 - val_loss: 174.8537 - val_mae: 9.8768 - val_mse: 174.8537\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.5445 - mae: 6.9904 - mse: 88.5445 - val_loss: 174.6725 - val_mae: 9.7522 - val_mse: 174.6725\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.5072 - mae: 7.0028 - mse: 87.5072 - val_loss: 173.9221 - val_mae: 9.8444 - val_mse: 173.9221\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.3801 - mae: 7.0531 - mse: 90.3801 - val_loss: 174.2487 - val_mae: 9.8130 - val_mse: 174.2487\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.0257 - mae: 7.0663 - mse: 89.0257 - val_loss: 174.3065 - val_mae: 9.8603 - val_mse: 174.3065\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.0637 - mae: 7.1105 - mse: 89.0637 - val_loss: 176.1683 - val_mae: 9.9135 - val_mse: 176.1683\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8796 - mae: 7.0569 - mse: 88.8796 - val_loss: 175.8932 - val_mae: 9.7858 - val_mse: 175.8932\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.7311 - mae: 6.9827 - mse: 87.7311 - val_loss: 173.3930 - val_mae: 9.8080 - val_mse: 173.3930\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.0489 - mae: 6.9569 - mse: 87.0489 - val_loss: 176.5896 - val_mae: 9.8747 - val_mse: 176.5896\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.0093 - mae: 6.9911 - mse: 86.0093 - val_loss: 179.6147 - val_mae: 9.9981 - val_mse: 179.6147\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 89.7597 - mae: 7.0719 - mse: 89.7597 - val_loss: 177.2987 - val_mae: 9.8647 - val_mse: 177.2987\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.7876 - mae: 7.0468 - mse: 88.7876 - val_loss: 171.1225 - val_mae: 9.7284 - val_mse: 171.1225\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8485 - mae: 7.0940 - mse: 88.8485 - val_loss: 173.4394 - val_mae: 9.7386 - val_mse: 173.4394\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.5924 - mae: 6.9602 - mse: 86.5924 - val_loss: 179.5074 - val_mae: 9.9893 - val_mse: 179.5074\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.4019 - mae: 7.0100 - mse: 86.4019 - val_loss: 178.5220 - val_mae: 9.8964 - val_mse: 178.5220\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.1849 - mae: 6.9364 - mse: 86.1849 - val_loss: 179.9277 - val_mae: 10.0068 - val_mse: 179.9277\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.7952 - mae: 6.9094 - mse: 84.7952 - val_loss: 178.5181 - val_mae: 9.9323 - val_mse: 178.5181\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.7605 - mae: 6.8852 - mse: 83.7605 - val_loss: 176.7695 - val_mae: 9.8616 - val_mse: 176.7695\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.2041 - mae: 6.8537 - mse: 83.2041 - val_loss: 178.1810 - val_mae: 9.9514 - val_mse: 178.1810\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.1400 - mae: 6.8484 - mse: 84.1400 - val_loss: 178.0415 - val_mae: 9.8664 - val_mse: 178.0415\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 85.2397 - mae: 6.9042 - mse: 85.2397 - val_loss: 179.7577 - val_mae: 9.8960 - val_mse: 179.7577\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.3273 - mae: 6.8553 - mse: 83.3273 - val_loss: 175.7792 - val_mae: 9.8158 - val_mse: 175.7792\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 84.6359 - mae: 6.9136 - mse: 84.6359 - val_loss: 183.0036 - val_mae: 9.9984 - val_mse: 183.0036\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.2593 - mae: 6.9139 - mse: 86.2593 - val_loss: 177.9559 - val_mae: 9.8041 - val_mse: 177.9559\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 81.5784 - mae: 6.7807 - mse: 81.5784 - val_loss: 180.8874 - val_mae: 9.9697 - val_mse: 180.8874\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 82.7775 - mae: 6.8267 - mse: 82.7775 - val_loss: 181.1145 - val_mae: 9.9698 - val_mse: 181.1145\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_1 = bnorm_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 184.8586 - mae: 9.7674 - mse: 184.8427 - val_loss: 145.5124 - val_mae: 8.6903 - val_mse: 145.4963\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 153.4001 - mae: 8.9627 - mse: 153.3840 - val_loss: 148.2971 - val_mae: 8.6866 - val_mse: 148.2811\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 149.6286 - mae: 8.8920 - mse: 149.6125 - val_loss: 143.3225 - val_mae: 8.6073 - val_mse: 143.3064\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.4624 - mae: 8.8655 - mse: 147.4463 - val_loss: 143.7707 - val_mae: 8.5932 - val_mse: 143.7546\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.1608 - mae: 8.8135 - mse: 147.1447 - val_loss: 142.5448 - val_mae: 8.5872 - val_mse: 142.5287\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 144.5305 - mae: 8.7481 - mse: 144.5144 - val_loss: 145.4015 - val_mae: 8.5968 - val_mse: 145.3856\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 144.1734 - mae: 8.7255 - mse: 144.1575 - val_loss: 143.0185 - val_mae: 8.5857 - val_mse: 143.0025\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5416 - mae: 8.6301 - mse: 142.5257 - val_loss: 139.6987 - val_mae: 8.5743 - val_mse: 139.6826\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5748 - mae: 8.7054 - mse: 142.5588 - val_loss: 146.4006 - val_mae: 8.6097 - val_mse: 146.3847\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.8417 - mae: 8.6044 - mse: 140.8256 - val_loss: 141.9977 - val_mae: 8.5368 - val_mse: 141.9817\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 143.1664 - mae: 8.6803 - mse: 143.1504 - val_loss: 140.1859 - val_mae: 8.5790 - val_mse: 140.1699\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.9545 - mae: 8.6191 - mse: 140.9386 - val_loss: 141.3959 - val_mae: 8.5268 - val_mse: 141.3799\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.6898 - mae: 8.6635 - mse: 141.6738 - val_loss: 141.3783 - val_mae: 8.5510 - val_mse: 141.3624\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.7580 - mae: 8.5832 - mse: 139.7421 - val_loss: 141.2381 - val_mae: 8.5587 - val_mse: 141.2222\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.4827 - mae: 8.5738 - mse: 138.4668 - val_loss: 140.5972 - val_mae: 8.5632 - val_mse: 140.5813\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.9393 - mae: 8.5529 - mse: 139.9235 - val_loss: 140.5279 - val_mae: 8.5556 - val_mse: 140.5120\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 138.3905 - mae: 8.5504 - mse: 138.3747 - val_loss: 139.7779 - val_mae: 8.5904 - val_mse: 139.7621\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.4282 - mae: 8.6223 - mse: 140.4125 - val_loss: 138.8164 - val_mae: 8.6549 - val_mse: 138.8006\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.0870 - mae: 8.5850 - mse: 140.0712 - val_loss: 140.1086 - val_mae: 8.5473 - val_mse: 140.0928\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 138.1979 - mae: 8.5443 - mse: 138.1822 - val_loss: 140.3788 - val_mae: 8.5598 - val_mse: 140.3630\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.0611 - mae: 8.5868 - mse: 140.0453 - val_loss: 143.6061 - val_mae: 8.5681 - val_mse: 143.5903\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 140.0822 - mae: 8.5815 - mse: 140.0664 - val_loss: 143.9062 - val_mae: 8.5647 - val_mse: 143.8905\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 139.5349 - mae: 8.5472 - mse: 139.5191 - val_loss: 140.5637 - val_mae: 8.5544 - val_mse: 140.5478\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.4373 - mae: 8.5373 - mse: 139.4215 - val_loss: 139.4457 - val_mae: 8.5766 - val_mse: 139.4298\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7419 - mae: 8.5551 - mse: 138.7260 - val_loss: 141.9120 - val_mae: 8.5712 - val_mse: 141.8962\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.1335 - mae: 8.5233 - mse: 137.1176 - val_loss: 141.5430 - val_mae: 8.5574 - val_mse: 141.5271\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.5289 - mae: 8.4477 - mse: 137.5130 - val_loss: 140.3367 - val_mae: 8.5674 - val_mse: 140.3207\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.0491 - mae: 8.5457 - mse: 138.0331 - val_loss: 140.9082 - val_mae: 8.5650 - val_mse: 140.8922\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.4627 - mae: 8.5007 - mse: 137.4467 - val_loss: 141.5446 - val_mae: 8.5669 - val_mse: 141.5286\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.4302 - mae: 8.5051 - mse: 137.4141 - val_loss: 140.6466 - val_mae: 8.5760 - val_mse: 140.6305\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1166 - mae: 8.5243 - mse: 138.1004 - val_loss: 139.1218 - val_mae: 8.5931 - val_mse: 139.1056\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.9010 - mae: 8.5321 - mse: 136.8847 - val_loss: 143.0862 - val_mae: 8.5584 - val_mse: 143.0700\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.3363 - mae: 8.4590 - mse: 135.3200 - val_loss: 139.3717 - val_mae: 8.5842 - val_mse: 139.3553\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.9167 - mae: 8.4762 - mse: 134.9003 - val_loss: 140.3321 - val_mae: 8.5534 - val_mse: 140.3156\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.0851 - mae: 8.4693 - mse: 136.0687 - val_loss: 140.4544 - val_mae: 8.5733 - val_mse: 140.4378\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.7149 - mae: 8.4490 - mse: 134.6983 - val_loss: 142.1852 - val_mae: 8.5696 - val_mse: 142.1684\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.6889 - mae: 8.5095 - mse: 137.6721 - val_loss: 141.9231 - val_mae: 8.5858 - val_mse: 141.9063\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.8188 - mae: 8.4628 - mse: 135.8020 - val_loss: 140.3885 - val_mae: 8.6142 - val_mse: 140.3715\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.3018 - mae: 8.5004 - mse: 136.2848 - val_loss: 143.8772 - val_mae: 8.5778 - val_mse: 143.8603\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.9806 - mae: 8.4632 - mse: 134.9635 - val_loss: 142.1642 - val_mae: 8.5513 - val_mse: 142.1470\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.5133 - mae: 8.5273 - mse: 136.4962 - val_loss: 143.0370 - val_mae: 8.5762 - val_mse: 143.0198\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.2795 - mae: 8.4572 - mse: 136.2622 - val_loss: 142.7816 - val_mae: 8.5681 - val_mse: 142.7643\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.4536 - mae: 8.4633 - mse: 134.4363 - val_loss: 141.4570 - val_mae: 8.5972 - val_mse: 141.4395\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.5393 - mae: 8.4648 - mse: 134.5217 - val_loss: 142.1177 - val_mae: 8.5778 - val_mse: 142.1000\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.8352 - mae: 8.4346 - mse: 134.8175 - val_loss: 144.7634 - val_mae: 8.5806 - val_mse: 144.7457\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 135.0972 - mae: 8.4387 - mse: 135.0795 - val_loss: 141.7296 - val_mae: 8.6041 - val_mse: 141.7117\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1247 - mae: 8.4693 - mse: 135.1067 - val_loss: 142.9007 - val_mae: 8.5939 - val_mse: 142.8828\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.8335 - mae: 8.4374 - mse: 134.8156 - val_loss: 143.1390 - val_mae: 8.6089 - val_mse: 143.1210\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9144 - mae: 8.4330 - mse: 133.8963 - val_loss: 142.1110 - val_mae: 8.5975 - val_mse: 142.0928\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6674 - mae: 8.4038 - mse: 133.6492 - val_loss: 142.8239 - val_mae: 8.5960 - val_mse: 142.8056\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.3118 - mae: 8.4426 - mse: 134.2935 - val_loss: 141.8967 - val_mae: 8.6124 - val_mse: 141.8783\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.3753 - mae: 8.4417 - mse: 134.3569 - val_loss: 142.4393 - val_mae: 8.6394 - val_mse: 142.4207\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 134.0813 - mae: 8.4538 - mse: 134.0627 - val_loss: 143.7713 - val_mae: 8.6110 - val_mse: 143.7526\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.9356 - mae: 8.4495 - mse: 133.9167 - val_loss: 142.1321 - val_mae: 8.6142 - val_mse: 142.1132\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.6818 - mae: 8.4303 - mse: 132.6628 - val_loss: 144.9672 - val_mae: 8.6126 - val_mse: 144.9481\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.7416 - mae: 8.4263 - mse: 133.7224 - val_loss: 142.4402 - val_mae: 8.6031 - val_mse: 142.4209\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.2498 - mae: 8.4582 - mse: 133.2305 - val_loss: 144.7819 - val_mae: 8.6228 - val_mse: 144.7626\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5761 - mae: 8.3889 - mse: 131.5567 - val_loss: 144.6856 - val_mae: 8.6293 - val_mse: 144.6661\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.4501 - mae: 8.3790 - mse: 133.4306 - val_loss: 141.4031 - val_mae: 8.6410 - val_mse: 141.3834\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6531 - mae: 8.4406 - mse: 133.6333 - val_loss: 144.0704 - val_mae: 8.5892 - val_mse: 144.0506\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.7709 - mae: 8.3430 - mse: 131.7510 - val_loss: 142.8871 - val_mae: 8.6277 - val_mse: 142.8671\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.1493 - mae: 8.4060 - mse: 133.1293 - val_loss: 142.9602 - val_mae: 8.6024 - val_mse: 142.9401\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.4891 - mae: 8.4247 - mse: 133.4689 - val_loss: 142.7030 - val_mae: 8.6125 - val_mse: 142.6826\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3583 - mae: 8.4084 - mse: 132.3378 - val_loss: 144.0322 - val_mae: 8.5958 - val_mse: 144.0116\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.7677 - mae: 8.3890 - mse: 132.7470 - val_loss: 143.2183 - val_mae: 8.5949 - val_mse: 143.1975\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.1645 - mae: 8.4171 - mse: 133.1435 - val_loss: 145.0832 - val_mae: 8.6272 - val_mse: 145.0622\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.0820 - mae: 8.3892 - mse: 132.0608 - val_loss: 143.8825 - val_mae: 8.6275 - val_mse: 143.8611\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.8586 - mae: 8.3722 - mse: 130.8372 - val_loss: 146.1688 - val_mae: 8.6307 - val_mse: 146.1473\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.5276 - mae: 8.3918 - mse: 131.5060 - val_loss: 143.8951 - val_mae: 8.6199 - val_mse: 143.8733\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7664 - mae: 8.4156 - mse: 131.7446 - val_loss: 144.6709 - val_mae: 8.6203 - val_mse: 144.6489\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3107 - mae: 8.3857 - mse: 132.2886 - val_loss: 143.2753 - val_mae: 8.6381 - val_mse: 143.2531\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5637 - mae: 8.3922 - mse: 132.5414 - val_loss: 144.3784 - val_mae: 8.6233 - val_mse: 144.3559\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.2849 - mae: 8.4105 - mse: 132.2625 - val_loss: 143.2732 - val_mae: 8.6347 - val_mse: 143.2505\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.6207 - mae: 8.3502 - mse: 130.5979 - val_loss: 143.1509 - val_mae: 8.6387 - val_mse: 143.1281\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.3034 - mae: 8.3989 - mse: 131.2804 - val_loss: 144.2000 - val_mae: 8.6411 - val_mse: 144.1769\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.3348 - mae: 8.3830 - mse: 132.3115 - val_loss: 142.7035 - val_mae: 8.6637 - val_mse: 142.6801\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3513 - mae: 8.4256 - mse: 133.3279 - val_loss: 143.6877 - val_mae: 8.6457 - val_mse: 143.6642\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.5668 - mae: 8.3658 - mse: 130.5432 - val_loss: 143.0252 - val_mae: 8.6459 - val_mse: 143.0013\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.2181 - mae: 8.3628 - mse: 130.1940 - val_loss: 144.8686 - val_mae: 8.6624 - val_mse: 144.8445\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.9491 - mae: 8.3763 - mse: 131.9249 - val_loss: 142.8505 - val_mae: 8.6231 - val_mse: 142.8262\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.7681 - mae: 8.3432 - mse: 130.7437 - val_loss: 144.0482 - val_mae: 8.6330 - val_mse: 144.0236\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.6645 - mae: 8.3538 - mse: 129.6398 - val_loss: 144.7215 - val_mae: 8.6566 - val_mse: 144.6967\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.1600 - mae: 8.3730 - mse: 131.1351 - val_loss: 143.9108 - val_mae: 8.6805 - val_mse: 143.8858\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.8605 - mae: 8.3342 - mse: 128.8352 - val_loss: 145.6736 - val_mae: 8.6629 - val_mse: 145.6482\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8574 - mae: 8.3678 - mse: 129.8318 - val_loss: 147.4540 - val_mae: 8.6614 - val_mse: 147.4283\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 130.7361 - mae: 8.3289 - mse: 130.7103 - val_loss: 144.7823 - val_mae: 8.6700 - val_mse: 144.7564\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.2824 - mae: 8.3851 - mse: 130.2563 - val_loss: 145.1832 - val_mae: 8.6507 - val_mse: 145.1570\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.0860 - mae: 8.2806 - mse: 129.0596 - val_loss: 145.4065 - val_mae: 8.6653 - val_mse: 145.3800\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.5686 - mae: 8.3654 - mse: 129.5418 - val_loss: 145.3940 - val_mae: 8.6410 - val_mse: 145.3671\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.3251 - mae: 8.3463 - mse: 131.2982 - val_loss: 143.7665 - val_mae: 8.6289 - val_mse: 143.7395\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.1049 - mae: 8.3063 - mse: 129.0776 - val_loss: 144.3714 - val_mae: 8.6567 - val_mse: 144.3439\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.2364 - mae: 8.3168 - mse: 129.2087 - val_loss: 146.0615 - val_mae: 8.6389 - val_mse: 146.0337\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.6616 - mae: 8.3683 - mse: 131.6338 - val_loss: 146.0207 - val_mae: 8.6405 - val_mse: 145.9928\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 128.1987 - mae: 8.2956 - mse: 128.1705 - val_loss: 145.8173 - val_mae: 8.6558 - val_mse: 145.7889\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.0585 - mae: 8.3186 - mse: 129.0300 - val_loss: 145.8145 - val_mae: 8.6639 - val_mse: 145.7858\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.2785 - mae: 8.3463 - mse: 129.2497 - val_loss: 147.4288 - val_mae: 8.7048 - val_mse: 147.3998\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9611 - mae: 8.3355 - mse: 129.9319 - val_loss: 147.0152 - val_mae: 8.6904 - val_mse: 146.9859\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2383 - mae: 8.2814 - mse: 128.2089 - val_loss: 146.3531 - val_mae: 8.6874 - val_mse: 146.3235\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.6755 - mae: 8.3037 - mse: 129.6457 - val_loss: 146.1844 - val_mae: 8.7301 - val_mse: 146.1544\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.8901 - mae: 8.3443 - mse: 127.8599 - val_loss: 146.7063 - val_mae: 8.6852 - val_mse: 146.6760\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 5.3295, Train MSE: 50.0978\n",
      "Val   MAE: 10.4803, Val   MSE: 206.0707\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8031, Train MSE: 58.0731\n",
      "Val   MAE: 9.9698, Val   MSE: 181.1145\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.9646, Train MSE: 124.6105\n",
      "Val   MAE: 8.6852, Val   MSE: 146.6760\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, NO GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_1 = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores4_1   = baseline_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_1[1]:.4f}, Train MSE: {train_scores4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_1[1]:.4f}, Val   MSE: {val_scores4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_1 = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_1   = bnorm_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Max Rank Change - With Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 2ms/step - loss: 161.4726 - mae: 9.1934 - mse: 161.4726 - val_loss: 140.9807 - val_mae: 8.5906 - val_mse: 140.9807\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0031 - mae: 8.2058 - mse: 131.0031 - val_loss: 137.8750 - val_mae: 8.5968 - val_mse: 137.8750\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2051 - mae: 8.0609 - mse: 128.2051 - val_loss: 138.8955 - val_mae: 8.6845 - val_mse: 138.8955\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.1012 - mae: 8.0178 - mse: 126.1012 - val_loss: 138.6035 - val_mae: 8.6133 - val_mse: 138.6035\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.9222 - mae: 7.9838 - mse: 124.9222 - val_loss: 139.7602 - val_mae: 8.5596 - val_mse: 139.7602\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 123.5303 - mae: 7.9078 - mse: 123.5303 - val_loss: 138.7037 - val_mae: 8.6376 - val_mse: 138.7037\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.0633 - mae: 7.8472 - mse: 122.0633 - val_loss: 139.1320 - val_mae: 8.6394 - val_mse: 139.1320\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 121.2353 - mae: 7.8144 - mse: 121.2353 - val_loss: 140.5667 - val_mae: 8.7631 - val_mse: 140.5667\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 119.4185 - mae: 7.7670 - mse: 119.4185 - val_loss: 141.9588 - val_mae: 8.6418 - val_mse: 141.9588\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.9765 - mae: 7.7086 - mse: 117.9765 - val_loss: 140.7823 - val_mae: 8.5916 - val_mse: 140.7823\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.0236 - mae: 7.6430 - mse: 117.0236 - val_loss: 139.5932 - val_mae: 8.7460 - val_mse: 139.5932\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.6427 - mae: 7.5900 - mse: 114.6427 - val_loss: 141.4575 - val_mae: 8.7016 - val_mse: 141.4575\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6479 - mae: 7.5448 - mse: 113.6479 - val_loss: 141.2963 - val_mae: 8.8562 - val_mse: 141.2963\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.4678 - mae: 7.4443 - mse: 110.4678 - val_loss: 143.5450 - val_mae: 8.9974 - val_mse: 143.5450\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7563 - mae: 7.3614 - mse: 108.7563 - val_loss: 142.8491 - val_mae: 8.8511 - val_mse: 142.8491\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 106.6163 - mae: 7.3035 - mse: 106.6163 - val_loss: 143.1089 - val_mae: 8.8603 - val_mse: 143.1089\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.9976 - mae: 7.1860 - mse: 103.9976 - val_loss: 146.6412 - val_mae: 8.9852 - val_mse: 146.6412\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 101.5579 - mae: 7.1065 - mse: 101.5579 - val_loss: 147.5126 - val_mae: 8.8336 - val_mse: 147.5126\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.6691 - mae: 7.0436 - mse: 99.6691 - val_loss: 151.5329 - val_mae: 9.1810 - val_mse: 151.5329\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 95.4079 - mae: 6.9123 - mse: 95.4079 - val_loss: 151.9431 - val_mae: 9.3025 - val_mse: 151.9431\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 93.9827 - mae: 6.8395 - mse: 93.9827 - val_loss: 152.3937 - val_mae: 9.2086 - val_mse: 152.3937\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.2824 - mae: 6.7228 - mse: 90.2824 - val_loss: 159.0290 - val_mae: 9.2628 - val_mse: 159.0290\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 89.0609 - mae: 6.6845 - mse: 89.0609 - val_loss: 157.5204 - val_mae: 9.3202 - val_mse: 157.5204\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 84.4606 - mae: 6.4505 - mse: 84.4606 - val_loss: 158.7558 - val_mae: 9.2066 - val_mse: 158.7558\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 82.6047 - mae: 6.3787 - mse: 82.6047 - val_loss: 157.8629 - val_mae: 9.2331 - val_mse: 157.8629\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 80.0564 - mae: 6.2868 - mse: 80.0564 - val_loss: 165.5187 - val_mae: 9.7654 - val_mse: 165.5187\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 80.2578 - mae: 6.2686 - mse: 80.2578 - val_loss: 165.4089 - val_mae: 9.3981 - val_mse: 165.4089\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 76.1484 - mae: 6.1205 - mse: 76.1484 - val_loss: 166.0064 - val_mae: 9.4336 - val_mse: 166.0064\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 72.7964 - mae: 5.9652 - mse: 72.7964 - val_loss: 174.7423 - val_mae: 9.5493 - val_mse: 174.7423\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 73.3494 - mae: 5.9682 - mse: 73.3494 - val_loss: 169.4640 - val_mae: 9.5949 - val_mse: 169.4640\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.8724 - mae: 5.7773 - mse: 68.8724 - val_loss: 168.6990 - val_mae: 9.4906 - val_mse: 168.6990\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 66.0972 - mae: 5.6229 - mse: 66.0972 - val_loss: 172.5446 - val_mae: 9.6460 - val_mse: 172.5446\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 64.6686 - mae: 5.5937 - mse: 64.6686 - val_loss: 175.3035 - val_mae: 9.6815 - val_mse: 175.3035\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 62.3165 - mae: 5.4798 - mse: 62.3165 - val_loss: 175.3754 - val_mae: 9.7154 - val_mse: 175.3754\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 60.3155 - mae: 5.3613 - mse: 60.3155 - val_loss: 181.8994 - val_mae: 9.8883 - val_mse: 181.8994\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 59.6393 - mae: 5.3313 - mse: 59.6393 - val_loss: 178.7400 - val_mae: 9.7845 - val_mse: 178.7400\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 58.4754 - mae: 5.2262 - mse: 58.4754 - val_loss: 176.7740 - val_mae: 9.8613 - val_mse: 176.7740\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 54.2489 - mae: 5.1032 - mse: 54.2489 - val_loss: 181.5812 - val_mae: 9.8598 - val_mse: 181.5812\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.1621 - mae: 5.1088 - mse: 54.1621 - val_loss: 183.3923 - val_mae: 9.8059 - val_mse: 183.3923\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.0787 - mae: 4.9136 - mse: 51.0787 - val_loss: 207.2942 - val_mae: 10.7947 - val_mse: 207.2942\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 50.5626 - mae: 4.9017 - mse: 50.5626 - val_loss: 181.2331 - val_mae: 9.9564 - val_mse: 181.2331\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 48.3393 - mae: 4.7734 - mse: 48.3393 - val_loss: 192.9110 - val_mae: 10.2519 - val_mse: 192.9110\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 45.8952 - mae: 4.6701 - mse: 45.8952 - val_loss: 189.4877 - val_mae: 10.0818 - val_mse: 189.4877\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 44.9024 - mae: 4.6262 - mse: 44.9024 - val_loss: 190.8255 - val_mae: 10.1355 - val_mse: 190.8255\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 43.5674 - mae: 4.5187 - mse: 43.5674 - val_loss: 193.3514 - val_mae: 10.2865 - val_mse: 193.3514\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.0047 - mae: 4.4352 - mse: 42.0047 - val_loss: 192.7274 - val_mae: 10.1199 - val_mse: 192.7274\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 42.2410 - mae: 4.4761 - mse: 42.2410 - val_loss: 186.8794 - val_mae: 9.9859 - val_mse: 186.8794\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 41.5240 - mae: 4.4035 - mse: 41.5240 - val_loss: 190.9940 - val_mae: 10.0872 - val_mse: 190.9940\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 41.2658 - mae: 4.4574 - mse: 41.2658 - val_loss: 190.3036 - val_mae: 10.2231 - val_mse: 190.3036\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 38.8430 - mae: 4.2890 - mse: 38.8430 - val_loss: 195.0031 - val_mae: 10.2526 - val_mse: 195.0031\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 37.4284 - mae: 4.1635 - mse: 37.4284 - val_loss: 199.5318 - val_mae: 10.4248 - val_mse: 199.5318\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 36.8557 - mae: 4.1771 - mse: 36.8557 - val_loss: 192.8505 - val_mae: 10.1140 - val_mse: 192.8505\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 36.8438 - mae: 4.1598 - mse: 36.8438 - val_loss: 203.9976 - val_mae: 10.3916 - val_mse: 203.9976\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 35.8060 - mae: 4.0767 - mse: 35.8060 - val_loss: 190.6319 - val_mae: 10.0648 - val_mse: 190.6319\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 33.7931 - mae: 3.9732 - mse: 33.7931 - val_loss: 197.0139 - val_mae: 10.2856 - val_mse: 197.0139\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 34.1264 - mae: 3.9878 - mse: 34.1264 - val_loss: 200.7989 - val_mae: 10.3514 - val_mse: 200.7989\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 32.2341 - mae: 3.8585 - mse: 32.2341 - val_loss: 198.1495 - val_mae: 10.2873 - val_mse: 198.1495\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 31.9303 - mae: 3.8473 - mse: 31.9303 - val_loss: 210.9775 - val_mae: 10.6164 - val_mse: 210.9775\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.2038 - mae: 3.7561 - mse: 30.2038 - val_loss: 202.9928 - val_mae: 10.4315 - val_mse: 202.9928\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.4943 - mae: 3.7457 - mse: 30.4943 - val_loss: 210.9052 - val_mae: 10.6706 - val_mse: 210.9052\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 30.1650 - mae: 3.7456 - mse: 30.1650 - val_loss: 206.6323 - val_mae: 10.4172 - val_mse: 206.6323\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 29.4358 - mae: 3.7125 - mse: 29.4358 - val_loss: 206.6526 - val_mae: 10.4790 - val_mse: 206.6526\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 27.8217 - mae: 3.5797 - mse: 27.8217 - val_loss: 205.0745 - val_mae: 10.3959 - val_mse: 205.0745\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 27.8769 - mae: 3.5410 - mse: 27.8769 - val_loss: 207.5512 - val_mae: 10.4414 - val_mse: 207.5512\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.7936 - mae: 3.5367 - mse: 26.7936 - val_loss: 206.1088 - val_mae: 10.4904 - val_mse: 206.1088\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 25.6102 - mae: 3.4624 - mse: 25.6102 - val_loss: 217.8742 - val_mae: 10.7538 - val_mse: 217.8742\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 25.7527 - mae: 3.4707 - mse: 25.7527 - val_loss: 211.8038 - val_mae: 10.4651 - val_mse: 211.8038\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 25.1089 - mae: 3.3996 - mse: 25.1089 - val_loss: 210.2323 - val_mae: 10.5837 - val_mse: 210.2323\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 26.0278 - mae: 3.4125 - mse: 26.0278 - val_loss: 209.0444 - val_mae: 10.6329 - val_mse: 209.0444\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.8155 - mae: 3.4238 - mse: 24.8155 - val_loss: 211.2266 - val_mae: 10.6878 - val_mse: 211.2266\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.4379 - mae: 3.3517 - mse: 24.4379 - val_loss: 214.8246 - val_mae: 10.5790 - val_mse: 214.8246\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 24.8630 - mae: 3.3798 - mse: 24.8630 - val_loss: 208.7324 - val_mae: 10.4740 - val_mse: 208.7324\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 23.8204 - mae: 3.3143 - mse: 23.8204 - val_loss: 216.2418 - val_mae: 10.6944 - val_mse: 216.2418\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.0728 - mae: 3.2231 - mse: 22.0728 - val_loss: 215.3469 - val_mae: 10.6628 - val_mse: 215.3469\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 23.0477 - mae: 3.2508 - mse: 23.0477 - val_loss: 214.4421 - val_mae: 10.5805 - val_mse: 214.4421\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.3491 - mae: 3.1580 - mse: 21.3491 - val_loss: 218.2379 - val_mae: 10.7007 - val_mse: 218.2379\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.9084 - mae: 3.1750 - mse: 21.9084 - val_loss: 217.6043 - val_mae: 10.7433 - val_mse: 217.6043\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.1702 - mae: 3.0756 - mse: 21.1702 - val_loss: 215.6024 - val_mae: 10.7471 - val_mse: 215.6024\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.7097 - mae: 3.2019 - mse: 22.7097 - val_loss: 207.8883 - val_mae: 10.3474 - val_mse: 207.8883\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.5941 - mae: 3.1698 - mse: 21.5941 - val_loss: 221.2746 - val_mae: 10.8910 - val_mse: 221.2746\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.5705 - mae: 3.0126 - mse: 19.5705 - val_loss: 218.0820 - val_mae: 10.7315 - val_mse: 218.0820\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.8291 - mae: 2.9993 - mse: 19.8291 - val_loss: 227.2062 - val_mae: 10.9708 - val_mse: 227.2062\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 21.4722 - mae: 3.1353 - mse: 21.4722 - val_loss: 215.6814 - val_mae: 10.7302 - val_mse: 215.6814\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 19.7174 - mae: 3.0210 - mse: 19.7174 - val_loss: 213.9366 - val_mae: 10.6585 - val_mse: 213.9366\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.4368 - mae: 3.0043 - mse: 19.4368 - val_loss: 215.9455 - val_mae: 10.6511 - val_mse: 215.9455\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.9892 - mae: 2.9271 - mse: 18.9892 - val_loss: 215.3099 - val_mae: 10.5833 - val_mse: 215.3099\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 22.9285 - mae: 3.2095 - mse: 22.9285 - val_loss: 220.5648 - val_mae: 10.8196 - val_mse: 220.5648\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 19.8599 - mae: 2.9742 - mse: 19.8599 - val_loss: 225.2145 - val_mae: 11.0549 - val_mse: 225.2145\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.0075 - mae: 2.8528 - mse: 18.0075 - val_loss: 218.7548 - val_mae: 10.7770 - val_mse: 218.7548\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.4457 - mae: 2.7891 - mse: 17.4457 - val_loss: 230.3237 - val_mae: 11.1410 - val_mse: 230.3237\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.0110 - mae: 2.8617 - mse: 18.0110 - val_loss: 222.1540 - val_mae: 10.8680 - val_mse: 222.1540\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 17.3777 - mae: 2.7825 - mse: 17.3777 - val_loss: 214.7461 - val_mae: 10.6090 - val_mse: 214.7461\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 20.9071 - mae: 2.9932 - mse: 20.9071 - val_loss: 218.8286 - val_mae: 10.8262 - val_mse: 218.8286\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 17.6439 - mae: 2.8074 - mse: 17.6439 - val_loss: 223.4378 - val_mae: 10.7986 - val_mse: 223.4378\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 18.9368 - mae: 2.8958 - mse: 18.9368 - val_loss: 215.9441 - val_mae: 10.6715 - val_mse: 215.9441\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.1077 - mae: 2.6862 - mse: 16.1077 - val_loss: 214.5085 - val_mae: 10.7360 - val_mse: 214.5085\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 16.4696 - mae: 2.6984 - mse: 16.4696 - val_loss: 225.2644 - val_mae: 10.9313 - val_mse: 225.2644\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 15.2490 - mae: 2.6107 - mse: 15.2490 - val_loss: 230.2008 - val_mae: 11.1243 - val_mse: 230.2008\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.8547 - mae: 2.5916 - mse: 14.8547 - val_loss: 216.5108 - val_mae: 10.7424 - val_mse: 216.5108\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 14.3457 - mae: 2.5502 - mse: 14.3457 - val_loss: 215.8615 - val_mae: 10.7414 - val_mse: 215.8615\n"
     ]
    }
   ],
   "source": [
    "# deep learning model, no regularization or dropout\n",
    "\n",
    "baseline_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "baseline_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_baseline4_2 = baseline_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 228.5887 - mae: 10.8400 - mse: 228.5887 - val_loss: 215.8089 - val_mae: 10.6629 - val_mse: 215.8089\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 150.6140 - mae: 8.5490 - mse: 150.6140 - val_loss: 150.1284 - val_mae: 8.7162 - val_mse: 150.1284\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.8971 - mae: 8.0235 - mse: 128.8971 - val_loss: 140.8296 - val_mae: 8.6181 - val_mse: 140.8296\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.9577 - mae: 8.0057 - mse: 125.9577 - val_loss: 139.2443 - val_mae: 8.6329 - val_mse: 139.2443\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 123.6529 - mae: 7.9166 - mse: 123.6529 - val_loss: 140.9102 - val_mae: 8.6520 - val_mse: 140.9102\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.2403 - mae: 7.8375 - mse: 121.2403 - val_loss: 141.7994 - val_mae: 8.8073 - val_mse: 141.7994\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.2463 - mae: 7.8486 - mse: 121.2463 - val_loss: 141.2703 - val_mae: 8.6385 - val_mse: 141.2703\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.0794 - mae: 7.7744 - mse: 119.0794 - val_loss: 140.9797 - val_mae: 8.7462 - val_mse: 140.9797\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4061 - mae: 7.7590 - mse: 118.4061 - val_loss: 140.8953 - val_mae: 8.7891 - val_mse: 140.8953\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.0918 - mae: 7.7112 - mse: 116.0918 - val_loss: 141.1821 - val_mae: 8.7707 - val_mse: 141.1821\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.8404 - mae: 7.6062 - mse: 113.8404 - val_loss: 143.2003 - val_mae: 8.8444 - val_mse: 143.2003\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.2393 - mae: 7.6193 - mse: 112.2393 - val_loss: 144.1846 - val_mae: 8.8445 - val_mse: 144.1846\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.2765 - mae: 7.4954 - mse: 110.2765 - val_loss: 147.7897 - val_mae: 8.9526 - val_mse: 147.7897\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.4216 - mae: 7.4770 - mse: 109.4216 - val_loss: 148.7707 - val_mae: 8.8872 - val_mse: 148.7707\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.3138 - mae: 7.4479 - mse: 109.3138 - val_loss: 146.1940 - val_mae: 8.9234 - val_mse: 146.1940\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.7955 - mae: 7.3547 - mse: 106.7955 - val_loss: 148.3199 - val_mae: 8.9804 - val_mse: 148.3199\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.7870 - mae: 7.2966 - mse: 104.7870 - val_loss: 147.4205 - val_mae: 8.8806 - val_mse: 147.4205\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.9115 - mae: 7.2472 - mse: 103.9115 - val_loss: 147.8915 - val_mae: 8.9054 - val_mse: 147.8915\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.9126 - mae: 7.2422 - mse: 102.9126 - val_loss: 150.4281 - val_mae: 9.0770 - val_mse: 150.4281\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.6428 - mae: 7.1726 - mse: 101.6428 - val_loss: 150.5000 - val_mae: 8.9858 - val_mse: 150.5000\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.8264 - mae: 7.1295 - mse: 99.8264 - val_loss: 151.1312 - val_mae: 8.9835 - val_mse: 151.1312\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.3070 - mae: 7.0191 - mse: 96.3070 - val_loss: 151.5411 - val_mae: 9.0299 - val_mse: 151.5411\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 96.3641 - mae: 6.9898 - mse: 96.3641 - val_loss: 152.6515 - val_mae: 9.0869 - val_mse: 152.6515\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.7598 - mae: 6.9390 - mse: 94.7598 - val_loss: 150.9450 - val_mae: 9.0547 - val_mse: 150.9450\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 94.5450 - mae: 6.9799 - mse: 94.5450 - val_loss: 151.5098 - val_mae: 8.8880 - val_mse: 151.5098\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 91.0264 - mae: 6.7963 - mse: 91.0264 - val_loss: 153.3621 - val_mae: 9.1662 - val_mse: 153.3621\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 90.6348 - mae: 6.8187 - mse: 90.6348 - val_loss: 151.9318 - val_mae: 9.0334 - val_mse: 151.9318\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 88.8049 - mae: 6.7094 - mse: 88.8049 - val_loss: 158.7911 - val_mae: 9.1994 - val_mse: 158.7911\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 87.5300 - mae: 6.6723 - mse: 87.5300 - val_loss: 156.9970 - val_mae: 9.2121 - val_mse: 156.9970\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.3825 - mae: 6.6762 - mse: 86.3825 - val_loss: 156.4699 - val_mae: 9.1633 - val_mse: 156.4699\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 86.6482 - mae: 6.6947 - mse: 86.6482 - val_loss: 157.5306 - val_mae: 9.0986 - val_mse: 157.5306\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.5897 - mae: 6.5470 - mse: 83.5897 - val_loss: 159.1537 - val_mae: 9.2306 - val_mse: 159.1537\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 83.1537 - mae: 6.5285 - mse: 83.1537 - val_loss: 159.0829 - val_mae: 9.2167 - val_mse: 159.0829\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.0175 - mae: 6.4520 - mse: 80.0175 - val_loss: 166.0707 - val_mae: 9.4977 - val_mse: 166.0707\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 80.0244 - mae: 6.4300 - mse: 80.0244 - val_loss: 158.3020 - val_mae: 9.2328 - val_mse: 158.3020\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 79.9303 - mae: 6.3537 - mse: 79.9303 - val_loss: 159.7334 - val_mae: 9.2853 - val_mse: 159.7334\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 78.6536 - mae: 6.3439 - mse: 78.6536 - val_loss: 158.3983 - val_mae: 9.2453 - val_mse: 158.3983\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 78.1751 - mae: 6.2545 - mse: 78.1751 - val_loss: 158.0094 - val_mae: 9.2079 - val_mse: 158.0094\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 81.3343 - mae: 6.5214 - mse: 81.3343 - val_loss: 155.4272 - val_mae: 9.2748 - val_mse: 155.4272\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 77.0667 - mae: 6.2610 - mse: 77.0667 - val_loss: 158.2077 - val_mae: 9.2472 - val_mse: 158.2077\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.6951 - mae: 6.1874 - mse: 73.6951 - val_loss: 160.5908 - val_mae: 9.2671 - val_mse: 160.5908\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 73.3524 - mae: 6.1453 - mse: 73.3524 - val_loss: 161.6114 - val_mae: 9.3862 - val_mse: 161.6114\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 72.7775 - mae: 6.1027 - mse: 72.7775 - val_loss: 158.0458 - val_mae: 9.2080 - val_mse: 158.0458\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 72.0577 - mae: 6.1016 - mse: 72.0577 - val_loss: 163.2254 - val_mae: 9.4679 - val_mse: 163.2254\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 71.8582 - mae: 6.0355 - mse: 71.8582 - val_loss: 162.2205 - val_mae: 9.4003 - val_mse: 162.2205\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 70.9060 - mae: 6.0286 - mse: 70.9060 - val_loss: 161.4151 - val_mae: 9.3810 - val_mse: 161.4151\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.5991 - mae: 5.9371 - mse: 68.5991 - val_loss: 157.6324 - val_mae: 9.1656 - val_mse: 157.6324\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.2235 - mae: 5.9494 - mse: 68.2235 - val_loss: 164.1641 - val_mae: 9.3547 - val_mse: 164.1641\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 68.1397 - mae: 5.9507 - mse: 68.1397 - val_loss: 158.8388 - val_mae: 9.2215 - val_mse: 158.8388\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.8671 - mae: 5.7452 - mse: 64.8671 - val_loss: 160.0213 - val_mae: 9.2944 - val_mse: 160.0213\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 67.3076 - mae: 5.9231 - mse: 67.3076 - val_loss: 161.5705 - val_mae: 9.3378 - val_mse: 161.5705\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.9358 - mae: 5.7990 - mse: 64.9358 - val_loss: 172.6468 - val_mae: 9.7320 - val_mse: 172.6468\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 66.4746 - mae: 5.8770 - mse: 66.4746 - val_loss: 161.1747 - val_mae: 9.3543 - val_mse: 161.1747\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 64.4635 - mae: 5.7550 - mse: 64.4635 - val_loss: 154.8273 - val_mae: 9.1430 - val_mse: 154.8273\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 66.2916 - mae: 5.8410 - mse: 66.2916 - val_loss: 166.3676 - val_mae: 9.5250 - val_mse: 166.3676\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 65.6026 - mae: 5.8040 - mse: 65.6026 - val_loss: 160.9932 - val_mae: 9.3599 - val_mse: 160.9932\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 63.3861 - mae: 5.7096 - mse: 63.3861 - val_loss: 163.5392 - val_mae: 9.3611 - val_mse: 163.5392\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.5200 - mae: 5.6762 - mse: 62.5200 - val_loss: 165.4363 - val_mae: 9.3532 - val_mse: 165.4363\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 63.8047 - mae: 5.7653 - mse: 63.8047 - val_loss: 168.3450 - val_mae: 9.5645 - val_mse: 168.3450\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.2511 - mae: 5.5913 - mse: 60.2511 - val_loss: 167.9265 - val_mae: 9.5169 - val_mse: 167.9265\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 62.6129 - mae: 5.7125 - mse: 62.6129 - val_loss: 165.3417 - val_mae: 9.4361 - val_mse: 165.3417\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.9369 - mae: 5.6181 - mse: 60.9369 - val_loss: 162.0472 - val_mae: 9.3146 - val_mse: 162.0472\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.7645 - mae: 5.5681 - mse: 59.7645 - val_loss: 168.9467 - val_mae: 9.5939 - val_mse: 168.9467\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.5845 - mae: 5.5548 - mse: 59.5845 - val_loss: 165.9582 - val_mae: 9.4363 - val_mse: 165.9582\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.6505 - mae: 5.6003 - mse: 60.6505 - val_loss: 169.3410 - val_mae: 9.6225 - val_mse: 169.3410\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.6496 - mae: 5.4901 - mse: 58.6496 - val_loss: 170.2224 - val_mae: 9.6935 - val_mse: 170.2224\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 59.6621 - mae: 5.5535 - mse: 59.6621 - val_loss: 159.8146 - val_mae: 9.3029 - val_mse: 159.8146\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.8749 - mae: 5.5048 - mse: 58.8749 - val_loss: 165.8739 - val_mae: 9.3806 - val_mse: 165.8739\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.0118 - mae: 5.3255 - mse: 56.0118 - val_loss: 164.5335 - val_mae: 9.3751 - val_mse: 164.5335\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 60.7606 - mae: 5.6286 - mse: 60.7606 - val_loss: 167.7990 - val_mae: 9.4400 - val_mse: 167.7990\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 58.7523 - mae: 5.4997 - mse: 58.7523 - val_loss: 163.5680 - val_mae: 9.3824 - val_mse: 163.5680\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 55.1109 - mae: 5.3648 - mse: 55.1109 - val_loss: 161.6992 - val_mae: 9.3647 - val_mse: 161.6992\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 55.7196 - mae: 5.3588 - mse: 55.7196 - val_loss: 166.4328 - val_mae: 9.4043 - val_mse: 166.4328\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 56.1517 - mae: 5.3872 - mse: 56.1517 - val_loss: 166.5158 - val_mae: 9.4248 - val_mse: 166.5158\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.8807 - mae: 5.3445 - mse: 54.8807 - val_loss: 165.6010 - val_mae: 9.4459 - val_mse: 165.6010\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 54.3155 - mae: 5.3372 - mse: 54.3155 - val_loss: 163.3861 - val_mae: 9.3069 - val_mse: 163.3861\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.6630 - mae: 5.2970 - mse: 53.6630 - val_loss: 169.3163 - val_mae: 9.5078 - val_mse: 169.3163\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.2636 - mae: 5.2637 - mse: 52.2636 - val_loss: 170.2025 - val_mae: 9.5243 - val_mse: 170.2025\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.2807 - mae: 5.2810 - mse: 52.2807 - val_loss: 164.7175 - val_mae: 9.4391 - val_mse: 164.7175\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.9424 - mae: 5.2893 - mse: 53.9424 - val_loss: 164.4215 - val_mae: 9.3843 - val_mse: 164.4215\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.7228 - mae: 5.2974 - mse: 53.7228 - val_loss: 178.5916 - val_mae: 9.7588 - val_mse: 178.5916\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.2288 - mae: 5.2479 - mse: 53.2288 - val_loss: 164.3759 - val_mae: 9.3724 - val_mse: 164.3759\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.8252 - mae: 5.2684 - mse: 53.8252 - val_loss: 170.3187 - val_mae: 9.5161 - val_mse: 170.3187\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.5267 - mae: 5.1088 - mse: 51.5267 - val_loss: 163.5145 - val_mae: 9.2755 - val_mse: 163.5145\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 53.1963 - mae: 5.2739 - mse: 53.1963 - val_loss: 165.7485 - val_mae: 9.4379 - val_mse: 165.7485\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 52.3554 - mae: 5.2387 - mse: 52.3554 - val_loss: 175.2908 - val_mae: 9.7283 - val_mse: 175.2908\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.1037 - mae: 5.0836 - mse: 50.1037 - val_loss: 169.9044 - val_mae: 9.4618 - val_mse: 169.9044\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 51.9829 - mae: 5.2172 - mse: 51.9829 - val_loss: 161.8248 - val_mae: 9.3035 - val_mse: 161.8248\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.8648 - mae: 5.1163 - mse: 50.8648 - val_loss: 168.8173 - val_mae: 9.6260 - val_mse: 168.8173\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.7151 - mae: 5.1629 - mse: 50.7151 - val_loss: 167.6453 - val_mae: 9.4827 - val_mse: 167.6453\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.4314 - mae: 5.0305 - mse: 49.4314 - val_loss: 171.2142 - val_mae: 9.5475 - val_mse: 171.2142\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 50.8307 - mae: 5.1175 - mse: 50.8307 - val_loss: 165.9234 - val_mae: 9.4119 - val_mse: 165.9234\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.6934 - mae: 5.1567 - mse: 50.6934 - val_loss: 167.8779 - val_mae: 9.4675 - val_mse: 167.8779\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.6356 - mae: 5.1539 - mse: 50.6356 - val_loss: 164.6260 - val_mae: 9.4726 - val_mse: 164.6260\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 49.6492 - mae: 5.1057 - mse: 49.6492 - val_loss: 166.2095 - val_mae: 9.4359 - val_mse: 166.2095\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 50.3419 - mae: 5.1207 - mse: 50.3419 - val_loss: 166.5183 - val_mae: 9.3597 - val_mse: 166.5183\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 48.4203 - mae: 5.0473 - mse: 48.4203 - val_loss: 168.9840 - val_mae: 9.5448 - val_mse: 168.9840\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 47.7431 - mae: 5.0188 - mse: 47.7431 - val_loss: 164.2905 - val_mae: 9.3106 - val_mse: 164.2905\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 45.4315 - mae: 4.9093 - mse: 45.4315 - val_loss: 171.1413 - val_mae: 9.4024 - val_mse: 171.1413\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 46.2897 - mae: 4.8958 - mse: 46.2897 - val_loss: 168.2096 - val_mae: 9.4547 - val_mse: 168.2096\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with batch normalization\n",
    "\n",
    "bnorm_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='linear', input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    \n",
    "    layers.Dense(64, activation='linear'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "bnorm_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_bnorm_model4_2 = bnorm_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 172.2329 - mae: 9.4199 - mse: 172.2080 - val_loss: 146.2268 - val_mae: 8.5796 - val_mse: 146.2008\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.1977 - mae: 8.7237 - mse: 146.1714 - val_loss: 143.2088 - val_mae: 8.5322 - val_mse: 143.1823\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.8656 - mae: 8.6204 - mse: 142.8390 - val_loss: 141.2887 - val_mae: 8.5337 - val_mse: 141.2619\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.8969 - mae: 8.5458 - mse: 140.8701 - val_loss: 142.4988 - val_mae: 8.5350 - val_mse: 142.4719\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.2815 - mae: 8.4815 - mse: 139.2545 - val_loss: 145.3491 - val_mae: 8.5526 - val_mse: 145.3222\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1884 - mae: 8.4563 - mse: 138.1613 - val_loss: 146.3978 - val_mae: 8.6033 - val_mse: 146.3708\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2265 - mae: 8.4130 - mse: 136.1993 - val_loss: 144.6232 - val_mae: 8.5803 - val_mse: 144.5961\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.0520 - mae: 8.3952 - mse: 136.0249 - val_loss: 142.9586 - val_mae: 8.5823 - val_mse: 142.9316\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.2320 - mae: 8.2168 - mse: 134.2050 - val_loss: 140.3635 - val_mae: 8.5710 - val_mse: 140.3363\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.4396 - mae: 8.3087 - mse: 134.4127 - val_loss: 142.0224 - val_mae: 8.5956 - val_mse: 141.9953\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3878 - mae: 8.2592 - mse: 132.3607 - val_loss: 144.1301 - val_mae: 8.6063 - val_mse: 144.1032\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.4749 - mae: 8.3059 - mse: 134.4479 - val_loss: 140.4869 - val_mae: 8.5629 - val_mse: 140.4600\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.2868 - mae: 8.2819 - mse: 133.2600 - val_loss: 141.1147 - val_mae: 8.5715 - val_mse: 141.0879\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.6877 - mae: 8.2530 - mse: 132.6608 - val_loss: 143.2128 - val_mae: 8.5772 - val_mse: 143.1860\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.1440 - mae: 8.1826 - mse: 130.1171 - val_loss: 140.7945 - val_mae: 8.5670 - val_mse: 140.7676\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.7963 - mae: 8.1913 - mse: 131.7694 - val_loss: 142.1677 - val_mae: 8.5743 - val_mse: 142.1408\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.3632 - mae: 8.1947 - mse: 130.3364 - val_loss: 141.4183 - val_mae: 8.5572 - val_mse: 141.3914\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.3891 - mae: 8.2005 - mse: 130.3623 - val_loss: 142.9663 - val_mae: 8.5834 - val_mse: 142.9396\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6410 - mae: 8.1477 - mse: 129.6144 - val_loss: 146.7997 - val_mae: 8.6223 - val_mse: 146.7731\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.9724 - mae: 8.0828 - mse: 128.9456 - val_loss: 141.8740 - val_mae: 8.5667 - val_mse: 141.8472\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6548 - mae: 8.1507 - mse: 129.6281 - val_loss: 141.0468 - val_mae: 8.5538 - val_mse: 141.0199\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9468 - mae: 8.0855 - mse: 127.9199 - val_loss: 144.2395 - val_mae: 8.6073 - val_mse: 144.2127\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9381 - mae: 8.1026 - mse: 127.9113 - val_loss: 143.6216 - val_mae: 8.5854 - val_mse: 143.5946\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.3337 - mae: 8.0313 - mse: 127.3067 - val_loss: 142.3750 - val_mae: 8.5783 - val_mse: 142.3478\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.6238 - mae: 8.0814 - mse: 127.5967 - val_loss: 140.2678 - val_mae: 8.5524 - val_mse: 140.2405\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.9058 - mae: 8.0248 - mse: 124.8786 - val_loss: 140.7648 - val_mae: 8.5475 - val_mse: 140.7375\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.8342 - mae: 8.0272 - mse: 124.8068 - val_loss: 148.1098 - val_mae: 8.6590 - val_mse: 148.0825\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.7544 - mae: 7.9785 - mse: 124.7270 - val_loss: 143.6451 - val_mae: 8.6217 - val_mse: 143.6175\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.4543 - mae: 8.0066 - mse: 122.4266 - val_loss: 142.6391 - val_mae: 8.6067 - val_mse: 142.6114\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.3346 - mae: 8.0122 - mse: 124.3068 - val_loss: 145.1747 - val_mae: 8.6401 - val_mse: 145.1468\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 122.5445 - mae: 7.9709 - mse: 122.5165 - val_loss: 143.3630 - val_mae: 8.6240 - val_mse: 143.3350\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1657 - mae: 8.0478 - mse: 124.1376 - val_loss: 144.3729 - val_mae: 8.6530 - val_mse: 144.3447\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.5382 - mae: 7.9138 - mse: 121.5099 - val_loss: 144.3369 - val_mae: 8.6525 - val_mse: 144.3085\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 121.9240 - mae: 7.9363 - mse: 121.8954 - val_loss: 142.4773 - val_mae: 8.5921 - val_mse: 142.4486\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 122.0870 - mae: 7.9437 - mse: 122.0582 - val_loss: 143.4388 - val_mae: 8.5969 - val_mse: 143.4099\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9363 - mae: 7.9370 - mse: 120.9072 - val_loss: 145.2041 - val_mae: 8.6572 - val_mse: 145.1750\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4859 - mae: 7.8833 - mse: 120.4566 - val_loss: 142.9342 - val_mae: 8.6078 - val_mse: 142.9047\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.2653 - mae: 7.9248 - mse: 119.2356 - val_loss: 142.2544 - val_mae: 8.6036 - val_mse: 142.2246\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4596 - mae: 7.8926 - mse: 120.4297 - val_loss: 142.3389 - val_mae: 8.6216 - val_mse: 142.3088\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.4369 - mae: 7.8995 - mse: 120.4067 - val_loss: 144.9455 - val_mae: 8.6737 - val_mse: 144.9153\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9876 - mae: 7.9019 - mse: 120.9572 - val_loss: 145.9210 - val_mae: 8.6738 - val_mse: 145.8904\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2576 - mae: 7.8296 - mse: 118.2269 - val_loss: 143.6654 - val_mae: 8.6594 - val_mse: 143.6345\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 116.5000 - mae: 7.8044 - mse: 116.4689 - val_loss: 143.0874 - val_mae: 8.6257 - val_mse: 143.0561\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.3101 - mae: 7.8003 - mse: 117.2789 - val_loss: 144.5740 - val_mae: 8.6609 - val_mse: 144.5426\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4976 - mae: 7.7961 - mse: 118.4660 - val_loss: 141.6870 - val_mae: 8.6223 - val_mse: 141.6552\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.6169 - mae: 7.7688 - mse: 115.5849 - val_loss: 144.2662 - val_mae: 8.6684 - val_mse: 144.2341\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 117.7217 - mae: 7.8411 - mse: 117.6894 - val_loss: 144.9301 - val_mae: 8.6414 - val_mse: 144.8977\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.3526 - mae: 7.7748 - mse: 118.3200 - val_loss: 143.1316 - val_mae: 8.6173 - val_mse: 143.0989\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.0795 - mae: 7.7772 - mse: 114.0466 - val_loss: 144.4593 - val_mae: 8.6747 - val_mse: 144.4263\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.5923 - mae: 7.7674 - mse: 114.5591 - val_loss: 144.9316 - val_mae: 8.6437 - val_mse: 144.8982\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.6796 - mae: 7.7472 - mse: 113.6460 - val_loss: 144.0377 - val_mae: 8.6480 - val_mse: 144.0040\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 115.6487 - mae: 7.7941 - mse: 115.6147 - val_loss: 143.5474 - val_mae: 8.6235 - val_mse: 143.5133\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 114.3232 - mae: 7.7427 - mse: 114.2888 - val_loss: 145.9073 - val_mae: 8.6875 - val_mse: 145.8728\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.9403 - mae: 7.7192 - mse: 112.9057 - val_loss: 142.6536 - val_mae: 8.6436 - val_mse: 142.6188\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 112.4681 - mae: 7.7054 - mse: 112.4331 - val_loss: 144.9102 - val_mae: 8.7120 - val_mse: 144.8750\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.1936 - mae: 7.7454 - mse: 113.1582 - val_loss: 142.5117 - val_mae: 8.6421 - val_mse: 142.4761\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 113.7728 - mae: 7.7481 - mse: 113.7370 - val_loss: 146.5298 - val_mae: 8.7014 - val_mse: 146.4939\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 111.5365 - mae: 7.6650 - mse: 111.5004 - val_loss: 143.7491 - val_mae: 8.6547 - val_mse: 143.7128\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 110.3987 - mae: 7.6223 - mse: 110.3622 - val_loss: 146.3931 - val_mae: 8.6871 - val_mse: 146.3565\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 111.2176 - mae: 7.6724 - mse: 111.1809 - val_loss: 144.0108 - val_mae: 8.6764 - val_mse: 143.9739\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.7983 - mae: 7.6248 - mse: 108.7612 - val_loss: 145.4747 - val_mae: 8.7012 - val_mse: 145.4373\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 109.8878 - mae: 7.6564 - mse: 109.8503 - val_loss: 145.6251 - val_mae: 8.7504 - val_mse: 145.5873\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.1050 - mae: 7.6256 - mse: 108.0670 - val_loss: 148.1740 - val_mae: 8.7959 - val_mse: 148.1359\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 110.5856 - mae: 7.6502 - mse: 110.5472 - val_loss: 147.4284 - val_mae: 8.7382 - val_mse: 147.3899\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 109.1474 - mae: 7.5798 - mse: 109.1087 - val_loss: 145.8545 - val_mae: 8.7095 - val_mse: 145.8156\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.8896 - mae: 7.5640 - mse: 108.8505 - val_loss: 146.2674 - val_mae: 8.6885 - val_mse: 146.2280\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.5795 - mae: 7.5678 - mse: 108.5400 - val_loss: 146.4636 - val_mae: 8.7062 - val_mse: 146.4239\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.9802 - mae: 7.6191 - mse: 107.9403 - val_loss: 148.1125 - val_mae: 8.7296 - val_mse: 148.0723\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.8633 - mae: 7.5059 - mse: 107.8231 - val_loss: 148.3330 - val_mae: 8.7201 - val_mse: 148.2925\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 108.4574 - mae: 7.6072 - mse: 108.4167 - val_loss: 147.7589 - val_mae: 8.7383 - val_mse: 147.7180\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 105.2132 - mae: 7.4938 - mse: 105.1721 - val_loss: 146.1678 - val_mae: 8.7342 - val_mse: 146.1265\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.6760 - mae: 7.4559 - mse: 103.6345 - val_loss: 147.3804 - val_mae: 8.7514 - val_mse: 147.3387\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.1063 - mae: 7.5759 - mse: 106.0644 - val_loss: 146.7662 - val_mae: 8.7381 - val_mse: 146.7241\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 106.5234 - mae: 7.5411 - mse: 106.4811 - val_loss: 149.3772 - val_mae: 8.7609 - val_mse: 149.3347\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 107.7348 - mae: 7.5932 - mse: 107.6921 - val_loss: 146.9877 - val_mae: 8.7236 - val_mse: 146.9447\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.0529 - mae: 7.3913 - mse: 103.0098 - val_loss: 148.5652 - val_mae: 8.7478 - val_mse: 148.5219\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.2480 - mae: 7.4278 - mse: 104.2045 - val_loss: 148.0592 - val_mae: 8.7360 - val_mse: 148.0155\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9610 - mae: 7.4153 - mse: 101.9171 - val_loss: 147.2341 - val_mae: 8.7145 - val_mse: 147.1901\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.1638 - mae: 7.4554 - mse: 104.1196 - val_loss: 147.0531 - val_mae: 8.6978 - val_mse: 147.0086\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.2715 - mae: 7.4100 - mse: 102.2268 - val_loss: 148.8810 - val_mae: 8.7517 - val_mse: 148.8361\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.8687 - mae: 7.4424 - mse: 102.8235 - val_loss: 149.1584 - val_mae: 8.7374 - val_mse: 149.1132\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 104.1611 - mae: 7.4125 - mse: 104.1157 - val_loss: 145.6856 - val_mae: 8.6807 - val_mse: 145.6399\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.1440 - mae: 7.4424 - mse: 103.0981 - val_loss: 149.1925 - val_mae: 8.7773 - val_mse: 149.1463\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 103.4559 - mae: 7.4626 - mse: 103.4096 - val_loss: 147.0263 - val_mae: 8.6948 - val_mse: 146.9798\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 101.9912 - mae: 7.4217 - mse: 101.9445 - val_loss: 146.2417 - val_mae: 8.6967 - val_mse: 146.1946\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 104.0916 - mae: 7.4856 - mse: 104.0443 - val_loss: 149.0478 - val_mae: 8.7525 - val_mse: 149.0005\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.9350 - mae: 7.3835 - mse: 100.8874 - val_loss: 148.8420 - val_mae: 8.7696 - val_mse: 148.7943\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 102.1079 - mae: 7.4153 - mse: 102.0599 - val_loss: 150.4041 - val_mae: 8.8059 - val_mse: 150.3560\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 102.1013 - mae: 7.3627 - mse: 102.0531 - val_loss: 149.4193 - val_mae: 8.7370 - val_mse: 149.3710\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.8585 - mae: 7.4338 - mse: 103.8099 - val_loss: 148.8768 - val_mae: 8.7124 - val_mse: 148.8281\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 99.6518 - mae: 7.3243 - mse: 99.6028 - val_loss: 148.3242 - val_mae: 8.7366 - val_mse: 148.2751\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.3496 - mae: 7.3526 - mse: 100.3003 - val_loss: 149.2192 - val_mae: 8.7739 - val_mse: 149.1695\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 103.2299 - mae: 7.4323 - mse: 103.1801 - val_loss: 146.5369 - val_mae: 8.7107 - val_mse: 146.4868\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 100.0050 - mae: 7.3477 - mse: 99.9547 - val_loss: 149.9954 - val_mae: 8.7692 - val_mse: 149.9450\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 98.4780 - mae: 7.3198 - mse: 98.4273 - val_loss: 150.1174 - val_mae: 8.7660 - val_mse: 150.0664\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 101.4495 - mae: 7.3630 - mse: 101.3984 - val_loss: 151.0921 - val_mae: 8.7646 - val_mse: 151.0409\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 95.4617 - mae: 7.2355 - mse: 95.4102 - val_loss: 148.1281 - val_mae: 8.7388 - val_mse: 148.0763\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 99.9246 - mae: 7.3454 - mse: 99.8727 - val_loss: 149.0641 - val_mae: 8.7482 - val_mse: 149.0120\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.1327 - mae: 7.2928 - mse: 98.0804 - val_loss: 147.5014 - val_mae: 8.7082 - val_mse: 147.4488\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 98.2837 - mae: 7.3076 - mse: 98.2309 - val_loss: 148.3302 - val_mae: 8.7153 - val_mse: 148.2770\n"
     ]
    }
   ],
   "source": [
    "# deep learning model with regularization (L2 and dropout)\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_withgenre = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_withgenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_withgenre.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_2 = reg_model_maxrank_withgenre.fit(\n",
    "    X_withgenre_4_train_scaled, y_withgenre_4_train_final,\n",
    "    validation_data=(X_withgenre_4_val_scaled, y_withgenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\n",
      "\n",
      "=== Baseline Model ===\n",
      "Train MAE: 2.4526, Train MSE: 13.5034\n",
      "Val   MAE: 10.7414, Val   MSE: 215.8615\n",
      "\n",
      "=== BatchNorm Model ===\n",
      "Train MAE: 5.8031, Train MSE: 58.0731\n",
      "Val   MAE: 9.9698, Val   MSE: 181.1145\n",
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "Train MAE: 7.0905, Train MSE: 94.4091\n",
      "Val   MAE: 8.7153, Val   MSE: 148.2770\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"MODEL EVALUATION: MAX RANK CHANGE, WITH GENRE\")\n",
    "\n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "train_scores4_2 = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores4_2   = baseline_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores4_2[1]:.4f}, Train MSE: {train_scores4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores4_2[1]:.4f}, Val   MSE: {val_scores4_2[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== BatchNorm Model ===\")\n",
    "train_scores_bn4_2 = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_bn4_2   = bnorm_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_bn4_1[1]:.4f}, Train MSE: {train_scores_bn4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_bn4_1[1]:.4f}, Val   MSE: {val_scores_bn4_1[2]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "train_scores_reg4_2 = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_train_scaled, y_withgenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_2   = reg_model_maxrank_withgenre.evaluate(X_withgenre_4_val_scaled, y_withgenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_2[1]:.4f}, Train MSE: {train_scores_reg4_2[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_2[1]:.4f}, Val   MSE: {val_scores_reg4_2[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all versions of the deep learning model, the regularized model has the best combination of low MAE (accuracy) and small difference between training and validation (least overfitting). \n",
    "\n",
    "Additionally, the max peak position model with genre data had a lower MAE score. For max rank change, the two models had essentially the same MAE.\n",
    "\n",
    "Next I'll optimize the regularized, max peak position model with genre and the regularized, max rank change model without genre (in the interest of maximizing model efficiency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning | Optimizing Regularized Models**\n",
    "\n",
    "Max Peak Position (regularized, with genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2637.5242 - mae: 42.3891 - mse: 2637.5012 - val_loss: 787.6920 - val_mae: 24.0844 - val_mse: 787.6674\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1063.5612 - mae: 26.7792 - mse: 1063.5369 - val_loss: 663.6807 - val_mae: 21.7454 - val_mse: 663.6561\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 986.6328 - mae: 25.7667 - mse: 986.6082 - val_loss: 698.2437 - val_mae: 22.8379 - val_mse: 698.2192\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 898.9559 - mae: 24.4286 - mse: 898.9312 - val_loss: 627.2560 - val_mae: 21.2902 - val_mse: 627.2315\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 840.7985 - mae: 23.6541 - mse: 840.7739 - val_loss: 617.2349 - val_mae: 21.2061 - val_mse: 617.2105\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 828.8493 - mae: 23.2666 - mse: 828.8248 - val_loss: 559.0635 - val_mae: 19.7727 - val_mse: 559.0392\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 805.7038 - mae: 22.8687 - mse: 805.6796 - val_loss: 569.1693 - val_mae: 20.1579 - val_mse: 569.1452\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 791.1954 - mae: 22.6961 - mse: 791.1714 - val_loss: 550.3447 - val_mae: 19.6908 - val_mse: 550.3208\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 762.0412 - mae: 22.0111 - mse: 762.0175 - val_loss: 608.3972 - val_mae: 21.2633 - val_mse: 608.3735\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 779.4661 - mae: 22.6219 - mse: 779.4424 - val_loss: 573.0119 - val_mae: 20.3544 - val_mse: 572.9883\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 769.0422 - mae: 22.2899 - mse: 769.0186 - val_loss: 523.8784 - val_mae: 18.9577 - val_mse: 523.8547\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 725.5492 - mae: 21.5103 - mse: 725.5258 - val_loss: 588.0566 - val_mae: 20.7732 - val_mse: 588.0333\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 744.7244 - mae: 21.9056 - mse: 744.7010 - val_loss: 531.4764 - val_mae: 19.2680 - val_mse: 531.4531\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 701.7850 - mae: 21.1913 - mse: 701.7618 - val_loss: 531.8601 - val_mae: 19.3203 - val_mse: 531.8369\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 718.7404 - mae: 21.5242 - mse: 718.7173 - val_loss: 533.1886 - val_mae: 19.3593 - val_mse: 533.1654\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 719.3492 - mae: 21.4788 - mse: 719.3263 - val_loss: 505.3168 - val_mae: 18.4826 - val_mse: 505.2938\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 723.9532 - mae: 21.4882 - mse: 723.9302 - val_loss: 538.2142 - val_mae: 19.4737 - val_mse: 538.1915\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 692.6707 - mae: 21.0897 - mse: 692.6479 - val_loss: 546.3524 - val_mae: 19.7114 - val_mse: 546.3300\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 685.3653 - mae: 20.9989 - mse: 685.3428 - val_loss: 516.8538 - val_mae: 18.8609 - val_mse: 516.8311\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.6763 - mae: 20.9423 - mse: 687.6538 - val_loss: 533.9154 - val_mae: 19.3988 - val_mse: 533.8930\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 681.6442 - mae: 20.7607 - mse: 681.6219 - val_loss: 499.7081 - val_mae: 18.3070 - val_mse: 499.6859\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.7054 - mae: 20.6236 - mse: 671.6832 - val_loss: 516.1550 - val_mae: 18.8041 - val_mse: 516.1328\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 676.3180 - mae: 20.6502 - mse: 676.2960 - val_loss: 494.1901 - val_mae: 17.9817 - val_mse: 494.1678\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.8241 - mae: 20.5432 - mse: 668.8020 - val_loss: 519.7742 - val_mae: 18.9676 - val_mse: 519.7522\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.7075 - mae: 20.6422 - mse: 673.6857 - val_loss: 524.3220 - val_mae: 19.0950 - val_mse: 524.3002\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.7618 - mae: 20.5057 - mse: 665.7403 - val_loss: 530.2259 - val_mae: 19.2823 - val_mse: 530.2043\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 659.5175 - mae: 20.4147 - mse: 659.4961 - val_loss: 502.1443 - val_mae: 18.3170 - val_mse: 502.1227\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 656.6533 - mae: 20.2846 - mse: 656.6321 - val_loss: 503.6998 - val_mae: 18.3511 - val_mse: 503.6783\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 663.1359 - mae: 20.4154 - mse: 663.1141 - val_loss: 529.4603 - val_mae: 19.1812 - val_mse: 529.4390\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 673.3648 - mae: 20.6512 - mse: 673.3434 - val_loss: 547.8799 - val_mae: 19.7503 - val_mse: 547.8585\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 660.6510 - mae: 20.3991 - mse: 660.6298 - val_loss: 515.6743 - val_mae: 18.7595 - val_mse: 515.6532\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 654.1020 - mae: 20.2363 - mse: 654.0806 - val_loss: 517.4538 - val_mae: 18.8341 - val_mse: 517.4327\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.4595 - mae: 20.0693 - mse: 643.4385 - val_loss: 507.3864 - val_mae: 18.4492 - val_mse: 507.3652\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.2739 - mae: 20.2869 - mse: 655.2531 - val_loss: 508.5467 - val_mae: 18.4890 - val_mse: 508.5258\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.5474 - mae: 20.4340 - mse: 665.5264 - val_loss: 546.6007 - val_mae: 19.6829 - val_mse: 546.5798\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 655.3766 - mae: 20.2460 - mse: 655.3553 - val_loss: 515.5179 - val_mae: 18.7632 - val_mse: 515.4972\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 654.8655 - mae: 20.3486 - mse: 654.8451 - val_loss: 500.7202 - val_mae: 18.1622 - val_mse: 500.6997\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.4797 - mae: 20.1701 - mse: 645.4588 - val_loss: 533.5328 - val_mae: 19.3129 - val_mse: 533.5122\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 646.4099 - mae: 20.1922 - mse: 646.3891 - val_loss: 521.5591 - val_mae: 18.9523 - val_mse: 521.5385\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.9392 - mae: 20.0895 - mse: 641.9186 - val_loss: 508.7217 - val_mae: 18.5331 - val_mse: 508.7012\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.9070 - mae: 20.2031 - mse: 647.8863 - val_loss: 499.3183 - val_mae: 18.0074 - val_mse: 499.2977\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.2271 - mae: 20.2554 - mse: 651.2064 - val_loss: 505.5118 - val_mae: 18.3691 - val_mse: 505.4913\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 650.6272 - mae: 20.1421 - mse: 650.6070 - val_loss: 504.1235 - val_mae: 18.2725 - val_mse: 504.1032\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.5176 - mae: 20.1832 - mse: 647.4971 - val_loss: 537.1807 - val_mae: 19.4067 - val_mse: 537.1605\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 644.1755 - mae: 20.0960 - mse: 644.1553 - val_loss: 497.9360 - val_mae: 17.9893 - val_mse: 497.9157\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 637.2231 - mae: 19.9372 - mse: 637.2029 - val_loss: 507.8427 - val_mae: 18.4236 - val_mse: 507.8226\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 645.7903 - mae: 20.1500 - mse: 645.7697 - val_loss: 501.8157 - val_mae: 18.1105 - val_mse: 501.7956\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.3427 - mae: 19.8945 - mse: 630.3227 - val_loss: 515.8507 - val_mae: 18.7522 - val_mse: 515.8306\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.6007 - mae: 20.0279 - mse: 633.5807 - val_loss: 498.6514 - val_mae: 17.9871 - val_mse: 498.6313\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.7400 - mae: 20.1366 - mse: 643.7202 - val_loss: 525.0330 - val_mae: 19.0047 - val_mse: 525.0131\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.6149 - mae: 20.0384 - mse: 642.5948 - val_loss: 517.9816 - val_mae: 18.7936 - val_mse: 517.9617\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.5749 - mae: 20.0476 - mse: 642.5549 - val_loss: 499.7908 - val_mae: 18.0682 - val_mse: 499.7709\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.8812 - mae: 20.0251 - mse: 636.8611 - val_loss: 539.8171 - val_mae: 19.4634 - val_mse: 539.7975\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.2925 - mae: 19.9117 - mse: 636.2726 - val_loss: 551.5005 - val_mae: 19.7850 - val_mse: 551.4810\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.4844 - mae: 19.9404 - mse: 634.4646 - val_loss: 529.2404 - val_mae: 19.1510 - val_mse: 529.2209\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 651.5452 - mae: 20.1773 - mse: 651.5261 - val_loss: 526.6722 - val_mae: 19.0857 - val_mse: 526.6528\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.0021 - mae: 20.0271 - mse: 635.9825 - val_loss: 511.1419 - val_mae: 18.4964 - val_mse: 511.1224\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4112 - mae: 19.8982 - mse: 635.3918 - val_loss: 523.0687 - val_mae: 18.9475 - val_mse: 523.0493\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.8182 - mae: 20.1372 - mse: 642.7986 - val_loss: 517.2814 - val_mae: 18.7861 - val_mse: 517.2620\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 619.4564 - mae: 19.5494 - mse: 619.4371 - val_loss: 518.4246 - val_mae: 18.8715 - val_mse: 518.4053\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5644 - mae: 20.0621 - mse: 636.5451 - val_loss: 498.7797 - val_mae: 18.0344 - val_mse: 498.7603\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.6397 - mae: 19.7749 - mse: 629.6203 - val_loss: 508.1760 - val_mae: 18.4650 - val_mse: 508.1566\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.1559 - mae: 19.9317 - mse: 633.1365 - val_loss: 522.9647 - val_mae: 18.9874 - val_mse: 522.9454\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.0758 - mae: 19.8535 - mse: 626.0565 - val_loss: 522.4543 - val_mae: 18.9987 - val_mse: 522.4350\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 638.7549 - mae: 19.9939 - mse: 638.7354 - val_loss: 512.5812 - val_mae: 18.6797 - val_mse: 512.5620\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.6474 - mae: 19.9792 - mse: 636.6285 - val_loss: 506.3384 - val_mae: 18.4452 - val_mse: 506.3190\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.2679 - mae: 19.9235 - mse: 629.2484 - val_loss: 506.1094 - val_mae: 18.4488 - val_mse: 506.0901\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.0809 - mae: 19.5790 - mse: 621.0613 - val_loss: 517.5500 - val_mae: 18.8239 - val_mse: 517.5309\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.5034 - mae: 19.7120 - mse: 625.4841 - val_loss: 494.3990 - val_mae: 18.0008 - val_mse: 494.3797\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.7723 - mae: 19.5048 - mse: 609.7534 - val_loss: 501.3704 - val_mae: 18.3097 - val_mse: 501.3512\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.0745 - mae: 19.6606 - mse: 621.0552 - val_loss: 493.1142 - val_mae: 18.0540 - val_mse: 493.0948\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5077 - mae: 19.8044 - mse: 621.4884 - val_loss: 515.4812 - val_mae: 18.8636 - val_mse: 515.4619\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.0442 - mae: 19.5073 - mse: 610.0250 - val_loss: 504.3161 - val_mae: 18.4555 - val_mse: 504.2967\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.0364 - mae: 19.3210 - mse: 604.0172 - val_loss: 497.7630 - val_mae: 18.1735 - val_mse: 497.7437\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.0826 - mae: 19.5191 - mse: 614.0633 - val_loss: 507.3298 - val_mae: 18.5796 - val_mse: 507.3104\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.4748 - mae: 19.2772 - mse: 600.4555 - val_loss: 493.5315 - val_mae: 18.0484 - val_mse: 493.5120\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.4202 - mae: 19.5886 - mse: 610.4006 - val_loss: 494.6381 - val_mae: 18.0752 - val_mse: 494.6187\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.7144 - mae: 19.5084 - mse: 605.6953 - val_loss: 503.9870 - val_mae: 18.4488 - val_mse: 503.9676\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.5853 - mae: 19.6346 - mse: 620.5658 - val_loss: 521.4331 - val_mae: 19.0363 - val_mse: 521.4138\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.8193 - mae: 19.3898 - mse: 599.7997 - val_loss: 513.2712 - val_mae: 18.7813 - val_mse: 513.2517\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.8187 - mae: 19.5117 - mse: 611.7991 - val_loss: 514.7736 - val_mae: 18.8461 - val_mse: 514.7542\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.7655 - mae: 19.2373 - mse: 600.7458 - val_loss: 505.4450 - val_mae: 18.5522 - val_mse: 505.4255\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.4352 - mae: 19.4259 - mse: 603.4156 - val_loss: 484.6104 - val_mae: 17.6683 - val_mse: 484.5908\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.2169 - mae: 19.4034 - mse: 609.1973 - val_loss: 491.2191 - val_mae: 18.0218 - val_mse: 491.1994\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.2939 - mae: 19.1560 - mse: 597.2740 - val_loss: 499.3791 - val_mae: 18.3193 - val_mse: 499.3595\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.4918 - mae: 19.4893 - mse: 614.4719 - val_loss: 503.0061 - val_mae: 18.4178 - val_mse: 502.9863\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.9703 - mae: 19.1757 - mse: 595.9506 - val_loss: 501.2331 - val_mae: 18.3544 - val_mse: 501.2134\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.3693 - mae: 19.4699 - mse: 608.3497 - val_loss: 510.7640 - val_mae: 18.7066 - val_mse: 510.7442\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.1156 - mae: 19.4193 - mse: 605.0958 - val_loss: 522.8132 - val_mae: 19.1099 - val_mse: 522.7935\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.6736 - mae: 19.0966 - mse: 591.6536 - val_loss: 511.4246 - val_mae: 18.7206 - val_mse: 511.4048\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.5034 - mae: 19.3851 - mse: 609.4835 - val_loss: 517.4868 - val_mae: 18.9055 - val_mse: 517.4667\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.4907 - mae: 19.2595 - mse: 595.4707 - val_loss: 494.7609 - val_mae: 18.0711 - val_mse: 494.7408\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.1793 - mae: 19.4053 - mse: 603.1591 - val_loss: 493.3702 - val_mae: 18.0409 - val_mse: 493.3501\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 586.3113 - mae: 18.9860 - mse: 586.2912 - val_loss: 488.6969 - val_mae: 17.8457 - val_mse: 488.6768\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.6484 - mae: 19.1151 - mse: 595.6282 - val_loss: 493.4080 - val_mae: 18.0841 - val_mse: 493.3878\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.0280 - mae: 19.1957 - mse: 597.0079 - val_loss: 496.4456 - val_mae: 18.2021 - val_mse: 496.4254\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.1672 - mae: 19.1335 - mse: 590.1471 - val_loss: 495.1434 - val_mae: 18.1295 - val_mse: 495.1230\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 594.4115 - mae: 19.2551 - mse: 594.3913 - val_loss: 490.4645 - val_mae: 17.9729 - val_mse: 490.4441\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 603.5989 - mae: 19.3759 - mse: 603.5790 - val_loss: 492.8329 - val_mae: 18.0911 - val_mse: 492.8127\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.5779 - mae: 18.9267 - mse: 583.5576 - val_loss: 511.7914 - val_mae: 18.7369 - val_mse: 511.7708\n"
     ]
    }
   ],
   "source": [
    "# adding a layer\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxpeak_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_1.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding an additional deep layer\n",
      "Train MAE: 18.3038, Train MSE: 498.7998\n",
      "Val   MAE: 18.7369, Val   MSE: 511.7708\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding an additional deep layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_1.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a layer resulted in a larger MAE for both training and validation. Removing the additional layer and adding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2063.6829 - mae: 36.3444 - mse: 2063.6509 - val_loss: 689.2535 - val_mae: 22.0733 - val_mse: 689.2202\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 826.1360 - mae: 23.2645 - mse: 826.1028 - val_loss: 593.9785 - val_mae: 19.9290 - val_mse: 593.9451\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 814.2825 - mae: 22.9193 - mse: 814.2485 - val_loss: 562.1478 - val_mae: 19.3306 - val_mse: 562.1143\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 766.1361 - mae: 22.2481 - mse: 766.1028 - val_loss: 551.6454 - val_mae: 19.1931 - val_mse: 551.6119\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 720.8075 - mae: 21.6047 - mse: 720.7739 - val_loss: 540.4574 - val_mae: 18.5377 - val_mse: 540.4238\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 716.8436 - mae: 21.5159 - mse: 716.8102 - val_loss: 529.9467 - val_mae: 18.6217 - val_mse: 529.9132\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 704.5610 - mae: 21.1665 - mse: 704.5273 - val_loss: 540.5358 - val_mae: 19.3349 - val_mse: 540.5023\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 690.4141 - mae: 20.9290 - mse: 690.3810 - val_loss: 525.1923 - val_mae: 18.6695 - val_mse: 525.1588\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 674.4059 - mae: 20.5534 - mse: 674.3724 - val_loss: 510.6354 - val_mae: 17.7198 - val_mse: 510.6020\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.2300 - mae: 20.5712 - mse: 667.1963 - val_loss: 521.4208 - val_mae: 18.6187 - val_mse: 521.3874\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.0100 - mae: 20.3180 - mse: 660.9769 - val_loss: 541.0872 - val_mae: 19.3431 - val_mse: 541.0541\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.3366 - mae: 20.1949 - mse: 653.3036 - val_loss: 506.7249 - val_mae: 18.1787 - val_mse: 506.6917\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 640.4178 - mae: 20.1199 - mse: 640.3845 - val_loss: 504.3607 - val_mae: 18.0174 - val_mse: 504.3275\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4736 - mae: 19.8075 - mse: 635.4406 - val_loss: 497.6169 - val_mae: 17.8358 - val_mse: 497.5838\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.8004 - mae: 19.8562 - mse: 635.7673 - val_loss: 521.9502 - val_mae: 18.9723 - val_mse: 521.9172\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.4069 - mae: 20.0055 - mse: 639.3740 - val_loss: 509.9736 - val_mae: 18.3243 - val_mse: 509.9404\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6353 - mae: 19.7719 - mse: 635.6024 - val_loss: 521.1716 - val_mae: 18.8760 - val_mse: 521.1384\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.8526 - mae: 19.8068 - mse: 629.8189 - val_loss: 489.0791 - val_mae: 17.4591 - val_mse: 489.0457\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.1804 - mae: 19.6546 - mse: 624.1472 - val_loss: 487.7478 - val_mae: 17.3666 - val_mse: 487.7143\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.7040 - mae: 19.5317 - mse: 616.6709 - val_loss: 490.1374 - val_mae: 17.7633 - val_mse: 490.1041\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.8480 - mae: 19.3442 - mse: 613.8148 - val_loss: 490.2246 - val_mae: 17.7977 - val_mse: 490.1913\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.1722 - mae: 19.2415 - mse: 606.1391 - val_loss: 496.8153 - val_mae: 18.1447 - val_mse: 496.7819\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.0172 - mae: 19.5272 - mse: 617.9839 - val_loss: 490.0059 - val_mae: 17.8076 - val_mse: 489.9726\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 606.3236 - mae: 19.3385 - mse: 606.2903 - val_loss: 491.7262 - val_mae: 17.7017 - val_mse: 491.6928\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.1060 - mae: 19.4716 - mse: 613.0730 - val_loss: 489.3952 - val_mae: 17.8854 - val_mse: 489.3619\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 618.0970 - mae: 19.4754 - mse: 618.0638 - val_loss: 505.4539 - val_mae: 18.4565 - val_mse: 505.4208\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 606.0465 - mae: 19.2575 - mse: 606.0137 - val_loss: 495.0088 - val_mae: 18.0628 - val_mse: 494.9757\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.8764 - mae: 19.4395 - mse: 615.8434 - val_loss: 483.6725 - val_mae: 17.3013 - val_mse: 483.6393\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 611.2694 - mae: 19.1591 - mse: 611.2365 - val_loss: 484.9034 - val_mae: 17.3457 - val_mse: 484.8702\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.6193 - mae: 19.0162 - mse: 596.5858 - val_loss: 515.6871 - val_mae: 18.7712 - val_mse: 515.6540\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 587.8530 - mae: 18.9259 - mse: 587.8202 - val_loss: 498.0406 - val_mae: 18.2178 - val_mse: 498.0076\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0739 - mae: 19.1231 - mse: 601.0406 - val_loss: 498.7513 - val_mae: 18.1641 - val_mse: 498.7184\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 594.5905 - mae: 19.0746 - mse: 594.5576 - val_loss: 486.6208 - val_mae: 17.7439 - val_mse: 486.5878\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.8818 - mae: 19.1108 - mse: 592.8489 - val_loss: 482.6638 - val_mae: 17.2366 - val_mse: 482.6308\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0429 - mae: 19.0356 - mse: 596.0098 - val_loss: 484.8214 - val_mae: 17.4956 - val_mse: 484.7884\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.4198 - mae: 18.9672 - mse: 593.3869 - val_loss: 492.8668 - val_mae: 18.0092 - val_mse: 492.8339\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 595.6039 - mae: 19.0538 - mse: 595.5710 - val_loss: 482.6855 - val_mae: 17.3479 - val_mse: 482.6524\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.7720 - mae: 18.8500 - mse: 592.7392 - val_loss: 487.7218 - val_mae: 17.7466 - val_mse: 487.6888\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.6913 - mae: 19.2397 - mse: 598.6585 - val_loss: 487.2433 - val_mae: 17.6640 - val_mse: 487.2102\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 602.4775 - mae: 19.0450 - mse: 602.4445 - val_loss: 481.3237 - val_mae: 17.4564 - val_mse: 481.2908\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 580.5073 - mae: 18.7169 - mse: 580.4745 - val_loss: 488.9723 - val_mae: 17.7244 - val_mse: 488.9394\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 583.3596 - mae: 18.8683 - mse: 583.3268 - val_loss: 503.5258 - val_mae: 18.4629 - val_mse: 503.4931\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.3982 - mae: 18.9759 - mse: 591.3657 - val_loss: 493.7786 - val_mae: 18.1133 - val_mse: 493.7460\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 591.1117 - mae: 18.9606 - mse: 591.0793 - val_loss: 505.6937 - val_mae: 18.5484 - val_mse: 505.6612\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 582.2828 - mae: 18.8860 - mse: 582.2501 - val_loss: 483.1662 - val_mae: 17.4713 - val_mse: 483.1336\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 579.1772 - mae: 18.6986 - mse: 579.1448 - val_loss: 482.8763 - val_mae: 17.5999 - val_mse: 482.8437\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 589.3599 - mae: 18.9190 - mse: 589.3273 - val_loss: 485.8301 - val_mae: 17.7447 - val_mse: 485.7976\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 585.3544 - mae: 18.8262 - mse: 585.3218 - val_loss: 482.2225 - val_mae: 17.3220 - val_mse: 482.1901\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 592.6801 - mae: 18.9135 - mse: 592.6476 - val_loss: 484.6383 - val_mae: 17.5806 - val_mse: 484.6059\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.1219 - mae: 18.9306 - mse: 590.0895 - val_loss: 485.7797 - val_mae: 17.8399 - val_mse: 485.7474\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 588.0735 - mae: 18.8575 - mse: 588.0412 - val_loss: 480.4764 - val_mae: 17.3719 - val_mse: 480.4442\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 578.6832 - mae: 18.7766 - mse: 578.6511 - val_loss: 493.9861 - val_mae: 18.1682 - val_mse: 493.9540\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 590.4916 - mae: 18.9476 - mse: 590.4597 - val_loss: 473.9874 - val_mae: 16.9695 - val_mse: 473.9552\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 578.5526 - mae: 18.7244 - mse: 578.5204 - val_loss: 488.8027 - val_mae: 18.0312 - val_mse: 488.7705\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 575.3963 - mae: 18.6701 - mse: 575.3640 - val_loss: 477.5664 - val_mae: 17.2039 - val_mse: 477.5341\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.2013 - mae: 18.4085 - mse: 566.1693 - val_loss: 477.0541 - val_mae: 17.0290 - val_mse: 477.0218\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 561.7394 - mae: 18.4777 - mse: 561.7072 - val_loss: 478.0927 - val_mae: 17.0780 - val_mse: 478.0604\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.0107 - mae: 18.7890 - mse: 576.9786 - val_loss: 481.3262 - val_mae: 17.6549 - val_mse: 481.2939\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 579.5758 - mae: 18.6505 - mse: 579.5435 - val_loss: 493.7932 - val_mae: 18.1582 - val_mse: 493.7609\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.6399 - mae: 18.5772 - mse: 567.6077 - val_loss: 489.0762 - val_mae: 18.0386 - val_mse: 489.0440\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 574.9600 - mae: 18.6202 - mse: 574.9274 - val_loss: 484.0294 - val_mae: 17.7431 - val_mse: 483.9970\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.5184 - mae: 18.5897 - mse: 567.4857 - val_loss: 480.2362 - val_mae: 17.6287 - val_mse: 480.2040\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.8011 - mae: 18.7486 - mse: 577.7687 - val_loss: 489.0372 - val_mae: 17.9650 - val_mse: 489.0049\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.8035 - mae: 18.6299 - mse: 571.7712 - val_loss: 478.3602 - val_mae: 17.3808 - val_mse: 478.3278\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.3093 - mae: 18.5521 - mse: 569.2769 - val_loss: 477.0341 - val_mae: 17.2017 - val_mse: 477.0016\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.2319 - mae: 18.6109 - mse: 571.1995 - val_loss: 488.2876 - val_mae: 18.0145 - val_mse: 488.2553\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 568.1151 - mae: 18.6261 - mse: 568.0825 - val_loss: 474.6282 - val_mae: 17.1484 - val_mse: 474.5958\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 569.6682 - mae: 18.6274 - mse: 569.6357 - val_loss: 475.8278 - val_mae: 17.3527 - val_mse: 475.7954\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 566.9432 - mae: 18.5724 - mse: 566.9108 - val_loss: 478.0833 - val_mae: 17.5742 - val_mse: 478.0506\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 571.2289 - mae: 18.5980 - mse: 571.1957 - val_loss: 491.9922 - val_mae: 18.1580 - val_mse: 491.9597\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 563.5851 - mae: 18.5325 - mse: 563.5527 - val_loss: 474.6701 - val_mae: 16.9458 - val_mse: 474.6373\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 567.9595 - mae: 18.5407 - mse: 567.9268 - val_loss: 476.4854 - val_mae: 17.2172 - val_mse: 476.4528\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 577.2883 - mae: 18.6916 - mse: 577.2557 - val_loss: 491.8947 - val_mae: 18.1575 - val_mse: 491.8620\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 572.3086 - mae: 18.5347 - mse: 572.2759 - val_loss: 471.3790 - val_mae: 17.2950 - val_mse: 471.3463\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 561.6018 - mae: 18.3063 - mse: 561.5685 - val_loss: 472.3777 - val_mae: 16.8421 - val_mse: 472.3448\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 565.2369 - mae: 18.4742 - mse: 565.2039 - val_loss: 491.8140 - val_mae: 18.2074 - val_mse: 491.7812\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 564.7375 - mae: 18.4219 - mse: 564.7045 - val_loss: 477.3441 - val_mae: 17.3961 - val_mse: 477.3110\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.4319 - mae: 18.3369 - mse: 552.3987 - val_loss: 481.1107 - val_mae: 17.6875 - val_mse: 481.0775\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 558.9868 - mae: 18.3541 - mse: 558.9532 - val_loss: 475.7746 - val_mae: 17.4473 - val_mse: 475.7412\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 553.8135 - mae: 18.4379 - mse: 553.7802 - val_loss: 476.8531 - val_mae: 16.7508 - val_mse: 476.8194\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.1340 - mae: 18.2214 - mse: 551.1006 - val_loss: 471.6800 - val_mae: 16.9736 - val_mse: 471.6460\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.0787 - mae: 18.2012 - mse: 552.0449 - val_loss: 477.5336 - val_mae: 17.4844 - val_mse: 477.4996\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.2136 - mae: 18.2206 - mse: 549.1796 - val_loss: 471.9548 - val_mae: 17.1773 - val_mse: 471.9204\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.6270 - mae: 18.2514 - mse: 549.5929 - val_loss: 475.8790 - val_mae: 17.3385 - val_mse: 475.8447\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 552.9986 - mae: 18.2985 - mse: 552.9641 - val_loss: 474.8471 - val_mae: 16.7697 - val_mse: 474.8124\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 556.0417 - mae: 18.3448 - mse: 556.0071 - val_loss: 472.0791 - val_mae: 16.9819 - val_mse: 472.0442\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.9942 - mae: 18.0864 - mse: 548.9596 - val_loss: 473.7958 - val_mae: 17.2211 - val_mse: 473.7608\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 549.5380 - mae: 18.1561 - mse: 549.5029 - val_loss: 483.1813 - val_mae: 17.8658 - val_mse: 483.1463\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 554.2013 - mae: 18.3096 - mse: 554.1661 - val_loss: 476.0331 - val_mae: 17.2323 - val_mse: 475.9977\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 551.3806 - mae: 18.1607 - mse: 551.3451 - val_loss: 485.6524 - val_mae: 17.9194 - val_mse: 485.6172\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 555.4897 - mae: 18.2159 - mse: 555.4540 - val_loss: 489.9293 - val_mae: 18.0352 - val_mse: 489.8935\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 549.8048 - mae: 18.2680 - mse: 549.7688 - val_loss: 479.9194 - val_mae: 17.5817 - val_mse: 479.8834\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 544.1467 - mae: 17.9981 - mse: 544.1105 - val_loss: 484.8286 - val_mae: 17.8535 - val_mse: 484.7925\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 545.8397 - mae: 18.1333 - mse: 545.8032 - val_loss: 481.0097 - val_mae: 17.6744 - val_mse: 480.9730\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 547.1404 - mae: 18.1408 - mse: 547.1041 - val_loss: 487.7214 - val_mae: 17.9139 - val_mse: 487.6846\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 529.8124 - mae: 17.8488 - mse: 529.7758 - val_loss: 488.6172 - val_mae: 17.9323 - val_mse: 488.5800\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 543.0217 - mae: 18.0920 - mse: 542.9844 - val_loss: 476.3470 - val_mae: 17.1372 - val_mse: 476.3095\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 537.9143 - mae: 17.9622 - mse: 537.8768 - val_loss: 482.8395 - val_mae: 17.5574 - val_mse: 482.8018\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 548.5068 - mae: 18.1021 - mse: 548.4690 - val_loss: 482.3041 - val_mae: 17.6814 - val_mse: 482.2663\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 550.2599 - mae: 18.2865 - mse: 550.2221 - val_loss: 478.9218 - val_mae: 17.1875 - val_mse: 478.8835\n"
     ]
    }
   ],
   "source": [
    "reg_model_maxpeak_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_2.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    3 deep layers, 128 kernels per layer\n",
      "Train MAE: 16.2949, Train MSE: 454.0456\n",
      "Val   MAE: 17.1875, Val   MSE: 478.8835\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_2.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing kernels per layer gave a result very similar to the original model but increased overfitting a little bit. Leaving 128 kernels per layer and increasing the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2370.4817 - mae: 39.8835 - mse: 2370.4497 - val_loss: 707.7922 - val_mae: 22.4548 - val_mse: 707.7590\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1031.9341 - mae: 26.3930 - mse: 1031.9010 - val_loss: 596.6560 - val_mae: 19.8617 - val_mse: 596.6224\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 966.9283 - mae: 25.4896 - mse: 966.8953 - val_loss: 643.0316 - val_mae: 21.4751 - val_mse: 642.9983\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 905.8526 - mae: 24.4060 - mse: 905.8193 - val_loss: 573.0706 - val_mae: 19.8401 - val_mse: 573.0372\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 893.2500 - mae: 24.2132 - mse: 893.2168 - val_loss: 623.0960 - val_mae: 21.1503 - val_mse: 623.0629\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 870.3020 - mae: 24.0005 - mse: 870.2688 - val_loss: 557.1016 - val_mae: 19.5457 - val_mse: 557.0682\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 854.3907 - mae: 23.5718 - mse: 854.3577 - val_loss: 538.7375 - val_mae: 18.9882 - val_mse: 538.7041\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 844.8894 - mae: 23.4831 - mse: 844.8561 - val_loss: 562.2311 - val_mae: 19.7859 - val_mse: 562.1980\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 802.4953 - mae: 22.8672 - mse: 802.4619 - val_loss: 551.2480 - val_mae: 19.5142 - val_mse: 551.2148\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 794.9246 - mae: 22.8921 - mse: 794.8914 - val_loss: 533.1952 - val_mae: 18.9642 - val_mse: 533.1620\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 797.9401 - mae: 22.8359 - mse: 797.9071 - val_loss: 566.0074 - val_mae: 20.0079 - val_mse: 565.9745\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 777.1449 - mae: 22.5327 - mse: 777.1119 - val_loss: 513.5373 - val_mae: 18.3665 - val_mse: 513.5042\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 790.2823 - mae: 22.5970 - mse: 790.2498 - val_loss: 569.4741 - val_mae: 20.1819 - val_mse: 569.4413\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 757.5637 - mae: 22.2042 - mse: 757.5308 - val_loss: 579.7106 - val_mae: 20.4741 - val_mse: 579.6780\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 755.5649 - mae: 22.0759 - mse: 755.5323 - val_loss: 537.5193 - val_mae: 19.2939 - val_mse: 537.4868\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 745.4221 - mae: 22.0480 - mse: 745.3898 - val_loss: 555.8333 - val_mae: 19.8769 - val_mse: 555.8010\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 737.8729 - mae: 21.6997 - mse: 737.8408 - val_loss: 513.9667 - val_mae: 18.4935 - val_mse: 513.9344\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 741.1009 - mae: 21.7715 - mse: 741.0690 - val_loss: 514.5392 - val_mae: 18.5344 - val_mse: 514.5071\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 741.4383 - mae: 21.7390 - mse: 741.4062 - val_loss: 521.2806 - val_mae: 18.8260 - val_mse: 521.2487\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 743.7592 - mae: 21.7819 - mse: 743.7276 - val_loss: 513.8451 - val_mae: 18.5683 - val_mse: 513.8133\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 710.1010 - mae: 21.3589 - mse: 710.0692 - val_loss: 564.0231 - val_mae: 20.1432 - val_mse: 563.9917\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 728.2238 - mae: 21.6471 - mse: 728.1923 - val_loss: 517.0915 - val_mae: 18.6641 - val_mse: 517.0602\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 711.6091 - mae: 21.3805 - mse: 711.5779 - val_loss: 515.7766 - val_mae: 18.6934 - val_mse: 515.7454\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 718.7947 - mae: 21.5033 - mse: 718.7635 - val_loss: 511.7367 - val_mae: 18.5750 - val_mse: 511.7055\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 713.5811 - mae: 21.3877 - mse: 713.5500 - val_loss: 513.3278 - val_mae: 18.6715 - val_mse: 513.2968\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.7830 - mae: 21.3000 - mse: 708.7523 - val_loss: 496.7324 - val_mae: 17.9501 - val_mse: 496.7014\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 719.8051 - mae: 21.2720 - mse: 719.7742 - val_loss: 511.8014 - val_mae: 18.6154 - val_mse: 511.7708\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 708.8479 - mae: 21.2594 - mse: 708.8172 - val_loss: 490.9307 - val_mae: 17.7041 - val_mse: 490.9000\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 720.3104 - mae: 21.4057 - mse: 720.2799 - val_loss: 503.9372 - val_mae: 18.2918 - val_mse: 503.9066\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 698.5232 - mae: 21.0462 - mse: 698.4930 - val_loss: 506.8420 - val_mae: 18.4600 - val_mse: 506.8118\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 703.0370 - mae: 20.9897 - mse: 703.0067 - val_loss: 503.2796 - val_mae: 18.3218 - val_mse: 503.2494\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 691.7380 - mae: 20.9063 - mse: 691.7078 - val_loss: 489.9079 - val_mae: 17.4940 - val_mse: 489.8777\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 695.0565 - mae: 20.9082 - mse: 695.0264 - val_loss: 489.8180 - val_mae: 17.6209 - val_mse: 489.7877\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 698.6441 - mae: 20.9108 - mse: 698.6138 - val_loss: 486.9463 - val_mae: 17.4825 - val_mse: 486.9163\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 689.9433 - mae: 20.8928 - mse: 689.9135 - val_loss: 490.2141 - val_mae: 17.6608 - val_mse: 490.1843\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 678.9495 - mae: 20.7361 - mse: 678.9194 - val_loss: 491.6341 - val_mae: 17.8498 - val_mse: 491.6044\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 695.8645 - mae: 21.0201 - mse: 695.8348 - val_loss: 518.3657 - val_mae: 18.8653 - val_mse: 518.3364\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 687.1659 - mae: 20.8057 - mse: 687.1367 - val_loss: 497.3813 - val_mae: 18.0993 - val_mse: 497.3520\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 686.4155 - mae: 20.6745 - mse: 686.3865 - val_loss: 520.7607 - val_mae: 18.9433 - val_mse: 520.7315\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 696.3968 - mae: 20.9450 - mse: 696.3677 - val_loss: 487.9097 - val_mae: 17.4476 - val_mse: 487.8805\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 693.1377 - mae: 20.8443 - mse: 693.1088 - val_loss: 489.3266 - val_mae: 17.7114 - val_mse: 489.2975\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.9763 - mae: 20.7167 - mse: 677.9471 - val_loss: 484.3979 - val_mae: 17.4469 - val_mse: 484.3688\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 669.1261 - mae: 20.4164 - mse: 669.0970 - val_loss: 496.7319 - val_mae: 18.1407 - val_mse: 496.7032\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 680.4749 - mae: 20.7381 - mse: 680.4462 - val_loss: 490.6148 - val_mae: 17.8961 - val_mse: 490.5858\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.2322 - mae: 20.3606 - mse: 659.2034 - val_loss: 481.7736 - val_mae: 17.4203 - val_mse: 481.7447\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 664.7844 - mae: 20.3866 - mse: 664.7556 - val_loss: 498.0884 - val_mae: 18.2144 - val_mse: 498.0598\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 659.8715 - mae: 20.3182 - mse: 659.8427 - val_loss: 490.5023 - val_mae: 17.9175 - val_mse: 490.4739\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 670.8020 - mae: 20.5636 - mse: 670.7737 - val_loss: 481.8839 - val_mae: 17.4094 - val_mse: 481.8555\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.2516 - mae: 20.4513 - mse: 677.2231 - val_loss: 484.8761 - val_mae: 17.6780 - val_mse: 484.8478\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.0927 - mae: 20.4247 - mse: 671.0650 - val_loss: 487.6601 - val_mae: 17.8072 - val_mse: 487.6320\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 677.8250 - mae: 20.7011 - mse: 677.7968 - val_loss: 486.7062 - val_mae: 17.7193 - val_mse: 486.6781\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.2039 - mae: 20.3201 - mse: 665.1757 - val_loss: 484.3996 - val_mae: 17.5805 - val_mse: 484.3715\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.0454 - mae: 20.4325 - mse: 662.0170 - val_loss: 482.1376 - val_mae: 17.5276 - val_mse: 482.1096\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 653.0865 - mae: 20.2359 - mse: 653.0589 - val_loss: 488.1039 - val_mae: 17.8716 - val_mse: 488.0760\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 667.3917 - mae: 20.3824 - mse: 667.3642 - val_loss: 498.6682 - val_mae: 18.2573 - val_mse: 498.6405\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 655.4000 - mae: 20.2152 - mse: 655.3726 - val_loss: 488.4238 - val_mae: 17.8672 - val_mse: 488.3961\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.0429 - mae: 20.2100 - mse: 649.0149 - val_loss: 493.4773 - val_mae: 18.0856 - val_mse: 493.4497\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.5632 - mae: 20.4506 - mse: 665.5358 - val_loss: 496.5226 - val_mae: 18.2152 - val_mse: 496.4952\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 657.0017 - mae: 20.3352 - mse: 656.9742 - val_loss: 486.4055 - val_mae: 17.8098 - val_mse: 486.3781\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.4045 - mae: 19.9747 - mse: 640.3768 - val_loss: 489.5039 - val_mae: 17.9322 - val_mse: 489.4765\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 649.2098 - mae: 20.0409 - mse: 649.1822 - val_loss: 478.7143 - val_mae: 17.2445 - val_mse: 478.6869\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 647.8448 - mae: 19.9278 - mse: 647.8175 - val_loss: 482.9593 - val_mae: 17.6334 - val_mse: 482.9320\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 648.1691 - mae: 20.0164 - mse: 648.1416 - val_loss: 496.0704 - val_mae: 18.2108 - val_mse: 496.0433\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 648.3317 - mae: 20.0606 - mse: 648.3044 - val_loss: 485.0473 - val_mae: 17.7449 - val_mse: 485.0201\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.0553 - mae: 19.9766 - mse: 639.0279 - val_loss: 484.5574 - val_mae: 17.6937 - val_mse: 484.5302\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 642.6790 - mae: 20.0066 - mse: 642.6519 - val_loss: 482.9801 - val_mae: 17.6219 - val_mse: 482.9529\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 629.2778 - mae: 19.7229 - mse: 629.2505 - val_loss: 488.0178 - val_mae: 17.9015 - val_mse: 487.9907\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 640.7576 - mae: 19.8970 - mse: 640.7301 - val_loss: 479.1344 - val_mae: 17.4168 - val_mse: 479.1073\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.5248 - mae: 19.8142 - mse: 634.4976 - val_loss: 490.6411 - val_mae: 18.0173 - val_mse: 490.6139\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 658.8738 - mae: 20.3833 - mse: 658.8469 - val_loss: 481.0026 - val_mae: 17.5946 - val_mse: 480.9754\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 648.3746 - mae: 20.1422 - mse: 648.3475 - val_loss: 485.2260 - val_mae: 17.7416 - val_mse: 485.1987\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.6077 - mae: 19.7911 - mse: 635.5804 - val_loss: 485.2171 - val_mae: 17.7743 - val_mse: 485.1898\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 636.5091 - mae: 19.8978 - mse: 636.4820 - val_loss: 496.4600 - val_mae: 18.1811 - val_mse: 496.4329\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 628.7330 - mae: 19.7818 - mse: 628.7059 - val_loss: 488.4967 - val_mae: 17.9166 - val_mse: 488.4696\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 633.3363 - mae: 19.7873 - mse: 633.3091 - val_loss: 490.0869 - val_mae: 17.9737 - val_mse: 490.0597\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 634.6699 - mae: 19.6026 - mse: 634.6428 - val_loss: 487.9421 - val_mae: 17.9305 - val_mse: 487.9149\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 617.4261 - mae: 19.5634 - mse: 617.3990 - val_loss: 486.0007 - val_mae: 17.7886 - val_mse: 485.9734\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.0632 - mae: 19.6455 - mse: 620.0357 - val_loss: 484.1146 - val_mae: 17.6957 - val_mse: 484.0871\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 628.3881 - mae: 19.6155 - mse: 628.3602 - val_loss: 486.2995 - val_mae: 17.8432 - val_mse: 486.2719\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.5321 - mae: 19.3312 - mse: 613.5043 - val_loss: 484.3665 - val_mae: 17.7828 - val_mse: 484.3387\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.4154 - mae: 19.8486 - mse: 635.3877 - val_loss: 475.6969 - val_mae: 17.2685 - val_mse: 475.6692\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.7649 - mae: 19.6513 - mse: 623.7369 - val_loss: 478.0510 - val_mae: 17.3644 - val_mse: 478.0229\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.1729 - mae: 19.4859 - mse: 616.1447 - val_loss: 484.4632 - val_mae: 17.7986 - val_mse: 484.4350\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 610.3753 - mae: 19.4800 - mse: 610.3473 - val_loss: 483.6250 - val_mae: 17.7543 - val_mse: 483.5967\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.8707 - mae: 19.4377 - mse: 616.8427 - val_loss: 474.3017 - val_mae: 17.1186 - val_mse: 474.2733\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 635.1155 - mae: 19.8856 - mse: 635.0873 - val_loss: 479.4889 - val_mae: 17.4790 - val_mse: 479.4603\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.2742 - mae: 19.4196 - mse: 614.2454 - val_loss: 475.7259 - val_mae: 17.2529 - val_mse: 475.6974\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.1202 - mae: 19.4561 - mse: 613.0916 - val_loss: 483.0739 - val_mae: 17.7150 - val_mse: 483.0451\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 609.0538 - mae: 19.3359 - mse: 609.0245 - val_loss: 478.5358 - val_mae: 17.5016 - val_mse: 478.5069\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 616.2075 - mae: 19.5229 - mse: 616.1786 - val_loss: 498.6602 - val_mae: 18.2924 - val_mse: 498.6313\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.1732 - mae: 19.6631 - mse: 626.1437 - val_loss: 474.4667 - val_mae: 17.1515 - val_mse: 474.4373\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.7549 - mae: 19.6208 - mse: 620.7253 - val_loss: 480.5599 - val_mae: 17.5718 - val_mse: 480.5302\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 600.8147 - mae: 19.4336 - mse: 600.7848 - val_loss: 474.3376 - val_mae: 17.1914 - val_mse: 474.3077\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.8564 - mae: 19.4106 - mse: 614.8265 - val_loss: 479.1705 - val_mae: 17.4530 - val_mse: 479.1406\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.9748 - mae: 19.6012 - mse: 621.9449 - val_loss: 487.0316 - val_mae: 17.9372 - val_mse: 487.0014\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.9178 - mae: 19.4602 - mse: 614.8876 - val_loss: 478.7530 - val_mae: 17.5391 - val_mse: 478.7224\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.4213 - mae: 19.3787 - mse: 605.3906 - val_loss: 484.9724 - val_mae: 17.7589 - val_mse: 484.9415\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 615.2260 - mae: 19.5059 - mse: 615.1950 - val_loss: 477.7089 - val_mae: 17.3433 - val_mse: 477.6777\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.5020 - mae: 19.2249 - mse: 602.4709 - val_loss: 478.5802 - val_mae: 17.3325 - val_mse: 478.5489\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.6281 - mae: 19.2195 - mse: 601.5967 - val_loss: 477.0312 - val_mae: 17.3380 - val_mse: 476.9997\n"
     ]
    }
   ],
   "source": [
    "# increasing dropout rate\n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_3 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_3.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_3.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    3 deep layers, 128 kernels per layer\n",
      "Train MAE: 17.0017, Train MSE: 482.7560\n",
      "Val   MAE: 17.3380, Val   MSE: 476.9997\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    3 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_3.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the dropout rate brought the training and validation MAEs closer together but the result is similar to the model before optimization. Trying another layer again, keeping the increased kernels and higher dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 2154.9417 - mae: 37.8534 - mse: 2154.8975 - val_loss: 866.1996 - val_mae: 25.5505 - val_mse: 866.1552\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1137.0728 - mae: 27.6327 - mse: 1137.0280 - val_loss: 860.4778 - val_mae: 25.8384 - val_mse: 860.4332\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 1024.1622 - mae: 26.1851 - mse: 1024.1172 - val_loss: 795.1439 - val_mae: 24.6692 - val_mse: 795.0990\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 976.2333 - mae: 25.5275 - mse: 976.1888 - val_loss: 794.0362 - val_mae: 24.7748 - val_mse: 793.9913\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 949.3926 - mae: 25.0022 - mse: 949.3472 - val_loss: 821.3254 - val_mae: 25.4429 - val_mse: 821.2805\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 906.3994 - mae: 24.5855 - mse: 906.3545 - val_loss: 629.3447 - val_mae: 21.5867 - val_mse: 629.2996\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 867.0803 - mae: 23.9324 - mse: 867.0353 - val_loss: 734.7047 - val_mae: 23.8791 - val_mse: 734.6599\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 874.6105 - mae: 23.9893 - mse: 874.5656 - val_loss: 635.2013 - val_mae: 21.7867 - val_mse: 635.1565\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 832.3945 - mae: 23.4804 - mse: 832.3498 - val_loss: 625.8828 - val_mae: 21.6204 - val_mse: 625.8381\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 834.0496 - mae: 23.4565 - mse: 834.0052 - val_loss: 646.4335 - val_mae: 22.0875 - val_mse: 646.3888\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 824.6174 - mae: 23.1036 - mse: 824.5726 - val_loss: 750.8664 - val_mae: 24.3312 - val_mse: 750.8220\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 818.6053 - mae: 23.2537 - mse: 818.5607 - val_loss: 641.1705 - val_mae: 22.0328 - val_mse: 641.1260\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 799.1076 - mae: 22.8414 - mse: 799.0632 - val_loss: 627.8763 - val_mae: 21.6786 - val_mse: 627.8318\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 780.1438 - mae: 22.5605 - mse: 780.0992 - val_loss: 700.4547 - val_mae: 23.3380 - val_mse: 700.4104\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 751.4824 - mae: 22.1496 - mse: 751.4378 - val_loss: 591.4529 - val_mae: 20.8409 - val_mse: 591.4083\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 760.1786 - mae: 22.0283 - mse: 760.1346 - val_loss: 626.7730 - val_mae: 21.7044 - val_mse: 626.7288\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 742.0889 - mae: 21.9682 - mse: 742.0447 - val_loss: 612.7074 - val_mae: 21.4077 - val_mse: 612.6631\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 734.3730 - mae: 21.8063 - mse: 734.3290 - val_loss: 572.7029 - val_mae: 20.3444 - val_mse: 572.6586\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 738.5729 - mae: 21.8140 - mse: 738.5287 - val_loss: 594.1309 - val_mae: 20.9489 - val_mse: 594.0868\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 722.6121 - mae: 21.5739 - mse: 722.5682 - val_loss: 589.8914 - val_mae: 20.8310 - val_mse: 589.8475\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 726.9529 - mae: 21.5877 - mse: 726.9090 - val_loss: 611.0002 - val_mae: 21.3770 - val_mse: 610.9562\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 720.4545 - mae: 21.4948 - mse: 720.4105 - val_loss: 580.3383 - val_mae: 20.6102 - val_mse: 580.2946\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 726.3301 - mae: 21.5814 - mse: 726.2864 - val_loss: 652.4115 - val_mae: 22.3682 - val_mse: 652.3679\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.0776 - mae: 21.2808 - mse: 709.0335 - val_loss: 561.3838 - val_mae: 20.0718 - val_mse: 561.3400\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 708.0976 - mae: 21.2893 - mse: 708.0536 - val_loss: 582.0026 - val_mae: 20.6647 - val_mse: 581.9589\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 706.0010 - mae: 21.1757 - mse: 705.9574 - val_loss: 553.0450 - val_mae: 19.8914 - val_mse: 553.0012\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 691.4828 - mae: 21.0597 - mse: 691.4391 - val_loss: 515.3265 - val_mae: 18.7642 - val_mse: 515.2828\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 693.5110 - mae: 20.9171 - mse: 693.4673 - val_loss: 553.4686 - val_mae: 19.9429 - val_mse: 553.4249\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 709.8601 - mae: 21.2148 - mse: 709.8164 - val_loss: 556.6267 - val_mae: 20.0002 - val_mse: 556.5832\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 694.7885 - mae: 20.9559 - mse: 694.7446 - val_loss: 566.1856 - val_mae: 20.2767 - val_mse: 566.1420\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 661.1754 - mae: 20.4129 - mse: 661.1313 - val_loss: 528.8629 - val_mae: 19.2432 - val_mse: 528.8190\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 682.6413 - mae: 20.8466 - mse: 682.5972 - val_loss: 521.7114 - val_mae: 19.0066 - val_mse: 521.6674\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 668.3447 - mae: 20.4964 - mse: 668.3007 - val_loss: 512.7366 - val_mae: 18.6749 - val_mse: 512.6926\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.9238 - mae: 20.5391 - mse: 665.8797 - val_loss: 536.8917 - val_mae: 19.4548 - val_mse: 536.8477\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 669.7986 - mae: 20.5144 - mse: 669.7540 - val_loss: 497.2353 - val_mae: 18.1167 - val_mse: 497.1910\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 669.8341 - mae: 20.5306 - mse: 669.7902 - val_loss: 499.3320 - val_mae: 18.2661 - val_mse: 499.2876\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 662.6592 - mae: 20.4028 - mse: 662.6149 - val_loss: 500.4725 - val_mae: 18.2873 - val_mse: 500.4282\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 665.6938 - mae: 20.3837 - mse: 665.6494 - val_loss: 495.1460 - val_mae: 18.1375 - val_mse: 495.1016\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.1176 - mae: 20.4769 - mse: 672.0734 - val_loss: 506.5137 - val_mae: 18.5039 - val_mse: 506.4693\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 672.6395 - mae: 20.4631 - mse: 672.5951 - val_loss: 499.2678 - val_mae: 18.2162 - val_mse: 499.2232\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 656.4976 - mae: 20.1560 - mse: 656.4532 - val_loss: 550.8789 - val_mae: 19.8270 - val_mse: 550.8346\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 671.3124 - mae: 20.5062 - mse: 671.2677 - val_loss: 502.0875 - val_mae: 18.3084 - val_mse: 502.0428\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 666.9319 - mae: 20.4180 - mse: 666.8868 - val_loss: 507.6384 - val_mae: 18.5031 - val_mse: 507.5936\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 663.4995 - mae: 20.2984 - mse: 663.4547 - val_loss: 504.1674 - val_mae: 18.4142 - val_mse: 504.1225\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 660.1348 - mae: 20.3224 - mse: 660.0897 - val_loss: 522.7314 - val_mae: 18.9916 - val_mse: 522.6865\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 642.0040 - mae: 20.0064 - mse: 641.9590 - val_loss: 496.1809 - val_mae: 18.0375 - val_mse: 496.1359\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 648.7177 - mae: 20.1878 - mse: 648.6729 - val_loss: 521.4437 - val_mae: 18.9493 - val_mse: 521.3986\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.5725 - mae: 20.2354 - mse: 651.5274 - val_loss: 512.5760 - val_mae: 18.6214 - val_mse: 512.5308\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 651.0632 - mae: 20.1314 - mse: 651.0177 - val_loss: 500.2267 - val_mae: 18.2550 - val_mse: 500.1812\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.9503 - mae: 20.0613 - mse: 641.9049 - val_loss: 499.4483 - val_mae: 18.2255 - val_mse: 499.4026\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 639.7725 - mae: 19.9390 - mse: 639.7268 - val_loss: 506.2184 - val_mae: 18.4163 - val_mse: 506.1729\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.6553 - mae: 19.8917 - mse: 641.6094 - val_loss: 501.0995 - val_mae: 18.2436 - val_mse: 501.0536\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 641.0501 - mae: 19.9380 - mse: 641.0043 - val_loss: 509.7984 - val_mae: 18.5709 - val_mse: 509.7525\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 653.0039 - mae: 20.2009 - mse: 652.9579 - val_loss: 517.8643 - val_mae: 18.8392 - val_mse: 517.8183\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 642.6546 - mae: 19.9452 - mse: 642.6085 - val_loss: 488.6543 - val_mae: 17.7068 - val_mse: 488.6078\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 639.3681 - mae: 19.8847 - mse: 639.3217 - val_loss: 513.5941 - val_mae: 18.6720 - val_mse: 513.5477\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 641.1001 - mae: 20.0636 - mse: 641.0535 - val_loss: 499.6726 - val_mae: 18.1407 - val_mse: 499.6259\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 640.8742 - mae: 19.8167 - mse: 640.8274 - val_loss: 499.8205 - val_mae: 18.1987 - val_mse: 499.7736\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 639.1833 - mae: 19.8990 - mse: 639.1362 - val_loss: 493.5721 - val_mae: 17.8873 - val_mse: 493.5248\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 634.3763 - mae: 19.8093 - mse: 634.3292 - val_loss: 510.4951 - val_mae: 18.5111 - val_mse: 510.4479\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 632.4431 - mae: 19.8234 - mse: 632.3957 - val_loss: 501.0948 - val_mae: 18.1606 - val_mse: 501.0472\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5638 - mae: 19.5484 - mse: 621.5162 - val_loss: 500.7289 - val_mae: 18.1388 - val_mse: 500.6813\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.9314 - mae: 19.6593 - mse: 623.8836 - val_loss: 498.4855 - val_mae: 18.1260 - val_mse: 498.4374\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 630.4550 - mae: 19.8172 - mse: 630.4067 - val_loss: 496.4584 - val_mae: 17.9957 - val_mse: 496.4101\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.5922 - mae: 19.5725 - mse: 621.5441 - val_loss: 495.8011 - val_mae: 17.9290 - val_mse: 495.7524\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 630.1591 - mae: 19.8147 - mse: 630.1103 - val_loss: 502.8672 - val_mae: 18.2376 - val_mse: 502.8184\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 626.2939 - mae: 19.7686 - mse: 626.2451 - val_loss: 506.2541 - val_mae: 18.4039 - val_mse: 506.2051\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 643.8990 - mae: 20.0727 - mse: 643.8499 - val_loss: 493.4131 - val_mae: 17.8551 - val_mse: 493.3638\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 624.6761 - mae: 19.6660 - mse: 624.6265 - val_loss: 490.5192 - val_mae: 17.5994 - val_mse: 490.4695\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 620.4990 - mae: 19.5090 - mse: 620.4492 - val_loss: 502.5094 - val_mae: 18.1541 - val_mse: 502.4596\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 621.3821 - mae: 19.6353 - mse: 621.3320 - val_loss: 502.7688 - val_mae: 18.2471 - val_mse: 502.7187\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 622.1290 - mae: 19.5394 - mse: 622.0783 - val_loss: 498.7411 - val_mae: 18.1238 - val_mse: 498.6906\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 598.9103 - mae: 19.1758 - mse: 598.8597 - val_loss: 493.1137 - val_mae: 17.7813 - val_mse: 493.0628\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 629.6525 - mae: 19.7937 - mse: 629.6014 - val_loss: 485.8556 - val_mae: 17.3152 - val_mse: 485.8043\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.0275 - mae: 19.4620 - mse: 613.9763 - val_loss: 498.8578 - val_mae: 18.0670 - val_mse: 498.8064\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 617.7936 - mae: 19.5666 - mse: 617.7422 - val_loss: 496.8887 - val_mae: 18.0111 - val_mse: 496.8369\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 625.7433 - mae: 19.7060 - mse: 625.6911 - val_loss: 488.1200 - val_mae: 17.4149 - val_mse: 488.0677\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.9290 - mae: 19.3170 - mse: 604.8766 - val_loss: 498.8098 - val_mae: 18.0322 - val_mse: 498.7571\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 623.5310 - mae: 19.6814 - mse: 623.4781 - val_loss: 496.2546 - val_mae: 17.8974 - val_mse: 496.2018\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 614.3730 - mae: 19.4609 - mse: 614.3202 - val_loss: 494.1033 - val_mae: 17.8210 - val_mse: 494.0502\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 614.7858 - mae: 19.5133 - mse: 614.7327 - val_loss: 503.5404 - val_mae: 18.2465 - val_mse: 503.4871\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 612.9753 - mae: 19.3881 - mse: 612.9220 - val_loss: 499.7571 - val_mae: 18.0416 - val_mse: 499.7033\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 613.5526 - mae: 19.4963 - mse: 613.4985 - val_loss: 493.4690 - val_mae: 17.8828 - val_mse: 493.4147\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.0770 - mae: 19.1776 - mse: 598.0228 - val_loss: 497.1225 - val_mae: 17.9222 - val_mse: 497.0681\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.1599 - mae: 19.1631 - mse: 604.1053 - val_loss: 495.5492 - val_mae: 17.8792 - val_mse: 495.4945\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 608.0568 - mae: 19.3420 - mse: 608.0020 - val_loss: 500.0040 - val_mae: 18.0439 - val_mse: 499.9489\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 598.8804 - mae: 19.1036 - mse: 598.8253 - val_loss: 492.1984 - val_mae: 17.5782 - val_mse: 492.1428\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 609.5414 - mae: 19.3778 - mse: 609.4859 - val_loss: 504.1074 - val_mae: 18.1326 - val_mse: 504.0516\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0741 - mae: 19.1296 - mse: 601.0181 - val_loss: 511.7747 - val_mae: 18.4640 - val_mse: 511.7184\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 601.0271 - mae: 19.1866 - mse: 600.9705 - val_loss: 488.9930 - val_mae: 17.1257 - val_mse: 488.9363\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 605.0302 - mae: 19.1785 - mse: 604.9736 - val_loss: 508.4807 - val_mae: 18.3530 - val_mse: 508.4238\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.5561 - mae: 19.3024 - mse: 604.4990 - val_loss: 500.9649 - val_mae: 18.0183 - val_mse: 500.9077\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 604.2015 - mae: 19.2291 - mse: 604.1444 - val_loss: 494.8755 - val_mae: 17.5464 - val_mse: 494.8181\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 596.0103 - mae: 19.1861 - mse: 595.9526 - val_loss: 519.1091 - val_mae: 18.6407 - val_mse: 519.0513\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 597.4541 - mae: 19.2732 - mse: 597.3961 - val_loss: 511.5906 - val_mae: 18.4944 - val_mse: 511.5327\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 597.9136 - mae: 19.2602 - mse: 597.8554 - val_loss: 496.8972 - val_mae: 17.8510 - val_mse: 496.8387\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 602.8374 - mae: 19.3442 - mse: 602.7787 - val_loss: 500.0653 - val_mae: 17.9994 - val_mse: 500.0065\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 593.3226 - mae: 19.0373 - mse: 593.2635 - val_loss: 493.0575 - val_mae: 17.6811 - val_mse: 492.9982\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 589.5729 - mae: 18.9771 - mse: 589.5134 - val_loss: 501.1159 - val_mae: 18.0700 - val_mse: 501.0563\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 599.2280 - mae: 19.2664 - mse: 599.1681 - val_loss: 490.5354 - val_mae: 17.3484 - val_mse: 490.4753\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.6\n",
    "\n",
    "reg_model_maxpeak_nogenre_4 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_3_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxpeak_nogenre_4.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model3_1 = reg_model_maxpeak_nogenre_4.fit(\n",
    "    X_nogenre_3_train_scaled, y_nogenre_3_train_final,\n",
    "    validation_data=(X_nogenre_3_val_scaled, y_nogenre_3_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    4 deep layers, 128 kernels per layer\n",
      "Train MAE: 16.8372, Train MSE: 476.2779\n",
      "Val   MAE: 17.3484, Val   MSE: 490.4753\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg3_1 = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_train_scaled, y_nogenre_3_train_final, verbose=0)\n",
    "val_scores_reg3_1   = reg_model_maxpeak_nogenre_4.evaluate(X_nogenre_3_val_scaled, y_nogenre_3_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg3_1[1]:.4f}, Train MSE: {train_scores_reg3_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg3_1[1]:.4f}, Val   MSE: {val_scores_reg3_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model continues to have the best combination of train/val MAE and limited overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_maxpeak_model = reg_model_maxpeak_nogenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Rank Change (regularized, no genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 185.6404 - mae: 9.9279 - mse: 185.6183 - val_loss: 153.0399 - val_mae: 8.8688 - val_mse: 153.0175\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 154.9768 - mae: 9.0846 - mse: 154.9545 - val_loss: 149.9039 - val_mae: 8.7539 - val_mse: 149.8815\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 149.5989 - mae: 8.9075 - mse: 149.5766 - val_loss: 147.7548 - val_mae: 8.7104 - val_mse: 147.7323\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 148.4724 - mae: 8.8419 - mse: 148.4500 - val_loss: 150.3177 - val_mae: 8.7239 - val_mse: 150.2953\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 147.3181 - mae: 8.8303 - mse: 147.2956 - val_loss: 145.1816 - val_mae: 8.6158 - val_mse: 145.1592\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 146.1712 - mae: 8.7592 - mse: 146.1489 - val_loss: 146.9011 - val_mae: 8.6395 - val_mse: 146.8788\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.6194 - mae: 8.7570 - mse: 144.5971 - val_loss: 144.3659 - val_mae: 8.6022 - val_mse: 144.3436\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5418 - mae: 8.6809 - mse: 142.5195 - val_loss: 145.1015 - val_mae: 8.5977 - val_mse: 145.0792\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 144.3413 - mae: 8.7341 - mse: 144.3189 - val_loss: 144.2987 - val_mae: 8.5695 - val_mse: 144.2764\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.9597 - mae: 8.6952 - mse: 142.9374 - val_loss: 146.7326 - val_mae: 8.6001 - val_mse: 146.7104\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.3297 - mae: 8.6129 - mse: 141.3076 - val_loss: 145.7192 - val_mae: 8.5828 - val_mse: 145.6970\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.5415 - mae: 8.6021 - mse: 141.5193 - val_loss: 140.4306 - val_mae: 8.5545 - val_mse: 140.4085\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.1872 - mae: 8.6050 - mse: 140.1651 - val_loss: 147.2967 - val_mae: 8.6165 - val_mse: 147.2745\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.3527 - mae: 8.6061 - mse: 140.3304 - val_loss: 142.4047 - val_mae: 8.5292 - val_mse: 142.3825\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5134 - mae: 8.6051 - mse: 140.4913 - val_loss: 141.6037 - val_mae: 8.5277 - val_mse: 141.5816\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.1872 - mae: 8.5976 - mse: 140.1650 - val_loss: 143.9831 - val_mae: 8.5564 - val_mse: 143.9610\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.0159 - mae: 8.5808 - mse: 139.9938 - val_loss: 147.0474 - val_mae: 8.6022 - val_mse: 147.0253\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.4542 - mae: 8.5936 - mse: 140.4320 - val_loss: 144.5712 - val_mae: 8.5610 - val_mse: 144.5491\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.3758 - mae: 8.5102 - mse: 138.3537 - val_loss: 140.7147 - val_mae: 8.5383 - val_mse: 140.6925\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.2290 - mae: 8.5564 - mse: 138.2067 - val_loss: 140.1227 - val_mae: 8.5321 - val_mse: 140.1005\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 140.5728 - mae: 8.5804 - mse: 140.5506 - val_loss: 144.8643 - val_mae: 8.5566 - val_mse: 144.8421\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.0222 - mae: 8.5032 - mse: 138.0000 - val_loss: 141.0152 - val_mae: 8.5142 - val_mse: 140.9929\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.4387 - mae: 8.5008 - mse: 137.4164 - val_loss: 141.1543 - val_mae: 8.5177 - val_mse: 141.1319\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.3318 - mae: 8.5621 - mse: 137.3093 - val_loss: 142.2667 - val_mae: 8.5228 - val_mse: 142.2443\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.0850 - mae: 8.5149 - mse: 137.0624 - val_loss: 139.5884 - val_mae: 8.5447 - val_mse: 139.5658\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1284 - mae: 8.5639 - mse: 138.1058 - val_loss: 141.1020 - val_mae: 8.5190 - val_mse: 141.0794\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.7666 - mae: 8.5343 - mse: 138.7439 - val_loss: 140.8071 - val_mae: 8.5149 - val_mse: 140.7844\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.1392 - mae: 8.4919 - mse: 137.1165 - val_loss: 139.4441 - val_mae: 8.5469 - val_mse: 139.4213\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.1032 - mae: 8.5429 - mse: 138.0804 - val_loss: 138.8628 - val_mae: 8.5433 - val_mse: 138.8399\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.8159 - mae: 8.5366 - mse: 137.7930 - val_loss: 140.4274 - val_mae: 8.5120 - val_mse: 140.4044\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.2311 - mae: 8.5034 - mse: 137.2081 - val_loss: 140.1375 - val_mae: 8.5201 - val_mse: 140.1145\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.5889 - mae: 8.4956 - mse: 137.5657 - val_loss: 139.8516 - val_mae: 8.5407 - val_mse: 139.8284\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.4123 - mae: 8.5041 - mse: 136.3890 - val_loss: 139.4075 - val_mae: 8.5223 - val_mse: 139.3841\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 137.7919 - mae: 8.5145 - mse: 137.7685 - val_loss: 141.8356 - val_mae: 8.5142 - val_mse: 141.8123\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.4682 - mae: 8.4565 - mse: 136.4448 - val_loss: 140.4936 - val_mae: 8.5311 - val_mse: 140.4701\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 136.4084 - mae: 8.4675 - mse: 136.3849 - val_loss: 141.2007 - val_mae: 8.5117 - val_mse: 141.1770\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2537 - mae: 8.4405 - mse: 136.2299 - val_loss: 138.8529 - val_mae: 8.5409 - val_mse: 138.8291\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4799 - mae: 8.4970 - mse: 135.4559 - val_loss: 141.8116 - val_mae: 8.5029 - val_mse: 141.7878\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9371 - mae: 8.4277 - mse: 135.9132 - val_loss: 139.6461 - val_mae: 8.5128 - val_mse: 139.6221\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5671 - mae: 8.4611 - mse: 135.5428 - val_loss: 139.2698 - val_mae: 8.5149 - val_mse: 139.2455\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.1739 - mae: 8.5080 - mse: 136.1497 - val_loss: 139.2148 - val_mae: 8.5417 - val_mse: 139.1904\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5615 - mae: 8.4748 - mse: 136.5371 - val_loss: 138.8166 - val_mae: 8.5368 - val_mse: 138.7921\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1186 - mae: 8.5075 - mse: 135.0941 - val_loss: 139.9830 - val_mae: 8.5254 - val_mse: 139.9584\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.0293 - mae: 8.4761 - mse: 136.0046 - val_loss: 139.7086 - val_mae: 8.5604 - val_mse: 139.6838\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.3847 - mae: 8.4762 - mse: 135.3598 - val_loss: 141.9727 - val_mae: 8.5047 - val_mse: 141.9478\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9017 - mae: 8.4905 - mse: 135.8768 - val_loss: 139.0233 - val_mae: 8.5729 - val_mse: 138.9981\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.6533 - mae: 8.4476 - mse: 134.6281 - val_loss: 140.3326 - val_mae: 8.5431 - val_mse: 140.3073\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.1310 - mae: 8.4362 - mse: 135.1056 - val_loss: 139.8172 - val_mae: 8.5506 - val_mse: 139.7917\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.2646 - mae: 8.4847 - mse: 136.2391 - val_loss: 140.1552 - val_mae: 8.5542 - val_mse: 140.1295\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.6565 - mae: 8.4702 - mse: 135.6308 - val_loss: 140.9275 - val_mae: 8.5268 - val_mse: 140.9017\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3149 - mae: 8.4684 - mse: 134.2890 - val_loss: 141.0142 - val_mae: 8.5382 - val_mse: 140.9882\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.0630 - mae: 8.4872 - mse: 135.0370 - val_loss: 142.3656 - val_mae: 8.5106 - val_mse: 142.3396\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.5713 - mae: 8.4673 - mse: 135.5451 - val_loss: 139.7197 - val_mae: 8.5305 - val_mse: 139.6936\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.1850 - mae: 8.4374 - mse: 134.1588 - val_loss: 139.9125 - val_mae: 8.5183 - val_mse: 139.8862\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.5234 - mae: 8.4796 - mse: 134.4970 - val_loss: 139.5965 - val_mae: 8.5400 - val_mse: 139.5700\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0437 - mae: 8.4471 - mse: 134.0170 - val_loss: 139.7214 - val_mae: 8.5424 - val_mse: 139.6947\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0918 - mae: 8.3616 - mse: 132.0649 - val_loss: 141.0916 - val_mae: 8.5161 - val_mse: 141.0647\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.0291 - mae: 8.4365 - mse: 134.0020 - val_loss: 139.9357 - val_mae: 8.5430 - val_mse: 139.9086\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 133.6928 - mae: 8.4193 - mse: 133.6654 - val_loss: 141.0993 - val_mae: 8.5114 - val_mse: 141.0719\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3661 - mae: 8.4102 - mse: 133.3385 - val_loss: 141.4367 - val_mae: 8.5090 - val_mse: 141.4090\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.3040 - mae: 8.3966 - mse: 134.2761 - val_loss: 139.3528 - val_mae: 8.5394 - val_mse: 139.3248\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.2141 - mae: 8.4942 - mse: 135.1861 - val_loss: 142.3911 - val_mae: 8.5116 - val_mse: 142.3631\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.4249 - mae: 8.3706 - mse: 131.3967 - val_loss: 140.1526 - val_mae: 8.5340 - val_mse: 140.1242\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.5224 - mae: 8.4469 - mse: 134.4939 - val_loss: 139.6628 - val_mae: 8.5317 - val_mse: 139.6341\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.2102 - mae: 8.3879 - mse: 131.1814 - val_loss: 140.3748 - val_mae: 8.5344 - val_mse: 140.3459\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3113 - mae: 8.4649 - mse: 133.2822 - val_loss: 141.3019 - val_mae: 8.5070 - val_mse: 141.2726\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5851 - mae: 8.3777 - mse: 132.5558 - val_loss: 140.8809 - val_mae: 8.5427 - val_mse: 140.8514\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0728 - mae: 8.3976 - mse: 131.0432 - val_loss: 141.1810 - val_mae: 8.5354 - val_mse: 141.1512\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5413 - mae: 8.4215 - mse: 132.5116 - val_loss: 139.3936 - val_mae: 8.5528 - val_mse: 139.3636\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.8450 - mae: 8.4131 - mse: 132.8149 - val_loss: 140.4918 - val_mae: 8.5381 - val_mse: 140.4616\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0323 - mae: 8.4411 - mse: 132.0021 - val_loss: 140.6239 - val_mae: 8.5193 - val_mse: 140.5934\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.5534 - mae: 8.4047 - mse: 132.5227 - val_loss: 142.1323 - val_mae: 8.5230 - val_mse: 142.1017\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.2930 - mae: 8.3214 - mse: 131.2621 - val_loss: 141.0707 - val_mae: 8.5368 - val_mse: 141.0397\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 132.6532 - mae: 8.3885 - mse: 132.6220 - val_loss: 140.9773 - val_mae: 8.5364 - val_mse: 140.9460\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.6695 - mae: 8.3549 - mse: 131.6381 - val_loss: 141.2665 - val_mae: 8.5173 - val_mse: 141.2349\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.0108 - mae: 8.3569 - mse: 131.9792 - val_loss: 140.7110 - val_mae: 8.5657 - val_mse: 140.6790\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 131.9874 - mae: 8.3808 - mse: 131.9555 - val_loss: 141.2395 - val_mae: 8.5235 - val_mse: 141.2073\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.1205 - mae: 8.3877 - mse: 131.0882 - val_loss: 139.7785 - val_mae: 8.5629 - val_mse: 139.7460\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0206 - mae: 8.3856 - mse: 129.9882 - val_loss: 140.6365 - val_mae: 8.5506 - val_mse: 140.6038\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.6348 - mae: 8.3941 - mse: 130.6019 - val_loss: 141.9392 - val_mae: 8.5278 - val_mse: 141.9062\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.9631 - mae: 8.3449 - mse: 129.9300 - val_loss: 141.1113 - val_mae: 8.5177 - val_mse: 141.0780\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0258 - mae: 8.3632 - mse: 129.9923 - val_loss: 141.6522 - val_mae: 8.5292 - val_mse: 141.6185\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8562 - mae: 8.3276 - mse: 129.8224 - val_loss: 140.5197 - val_mae: 8.5239 - val_mse: 140.4856\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0236 - mae: 8.3668 - mse: 129.9893 - val_loss: 141.2959 - val_mae: 8.5350 - val_mse: 141.2617\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.2784 - mae: 8.3357 - mse: 129.2439 - val_loss: 142.6967 - val_mae: 8.5438 - val_mse: 142.6622\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0056 - mae: 8.3316 - mse: 129.9711 - val_loss: 140.2737 - val_mae: 8.5505 - val_mse: 140.2389\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0963 - mae: 8.3527 - mse: 130.0613 - val_loss: 140.7341 - val_mae: 8.5247 - val_mse: 140.6991\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 1ms/step - loss: 129.3027 - mae: 8.3517 - mse: 129.2675 - val_loss: 140.0058 - val_mae: 8.5361 - val_mse: 139.9704\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8526 - mae: 8.3593 - mse: 129.8173 - val_loss: 140.4399 - val_mae: 8.5344 - val_mse: 140.4043\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.7123 - mae: 8.3107 - mse: 128.6767 - val_loss: 140.7914 - val_mae: 8.5447 - val_mse: 140.7556\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.8034 - mae: 8.3495 - mse: 129.7675 - val_loss: 141.1859 - val_mae: 8.5150 - val_mse: 141.1499\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.5850 - mae: 8.2940 - mse: 127.5488 - val_loss: 140.6683 - val_mae: 8.5271 - val_mse: 140.6319\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3728 - mae: 8.3511 - mse: 128.3361 - val_loss: 142.0614 - val_mae: 8.5248 - val_mse: 142.0246\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.4046 - mae: 8.3369 - mse: 128.3675 - val_loss: 142.1432 - val_mae: 8.5241 - val_mse: 142.1059\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.7517 - mae: 8.3114 - mse: 127.7141 - val_loss: 142.1891 - val_mae: 8.5324 - val_mse: 142.1515\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.0766 - mae: 8.2900 - mse: 128.0390 - val_loss: 140.8091 - val_mae: 8.5365 - val_mse: 140.7713\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.0006 - mae: 8.2918 - mse: 127.9627 - val_loss: 141.9430 - val_mae: 8.5441 - val_mse: 141.9049\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.5049 - mae: 8.2782 - mse: 128.4667 - val_loss: 140.7656 - val_mae: 8.5828 - val_mse: 140.7271\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.2908 - mae: 8.3149 - mse: 128.2522 - val_loss: 140.5533 - val_mae: 8.5605 - val_mse: 140.5144\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.8713 - mae: 8.2875 - mse: 128.8325 - val_loss: 141.6661 - val_mae: 8.5604 - val_mse: 141.6273\n"
     ]
    }
   ],
   "source": [
    "# adding a layer \n",
    "\n",
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(64, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_1.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      "    Adding a layer\n",
      "Train MAE: 8.0288, Train MSE: 123.9662\n",
      "Val   MAE: 8.5604, Val   MSE: 141.6273\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\"    Adding a layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very nearly identical to the initial configurations. Doubling the kernels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 167.2343 - mae: 9.3601 - mse: 167.1924 - val_loss: 142.3192 - val_mae: 8.6937 - val_mse: 142.2769\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.9986 - mae: 8.8403 - mse: 146.9561 - val_loss: 143.6896 - val_mae: 8.6442 - val_mse: 143.6469\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 146.8293 - mae: 8.8142 - mse: 146.7865 - val_loss: 142.2674 - val_mae: 8.6695 - val_mse: 142.2244\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.7292 - mae: 8.7351 - mse: 142.6861 - val_loss: 148.5261 - val_mae: 8.6831 - val_mse: 148.4829\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 142.2989 - mae: 8.7025 - mse: 142.2555 - val_loss: 145.1137 - val_mae: 8.6189 - val_mse: 145.0701\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 143.5256 - mae: 8.7281 - mse: 143.4818 - val_loss: 145.0394 - val_mae: 8.5886 - val_mse: 144.9955\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 142.5378 - mae: 8.6921 - mse: 142.4936 - val_loss: 148.0239 - val_mae: 8.6391 - val_mse: 147.9796\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 141.7583 - mae: 8.6883 - mse: 141.7138 - val_loss: 146.7876 - val_mae: 8.6268 - val_mse: 146.7430\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.8806 - mae: 8.5904 - mse: 139.8357 - val_loss: 141.9741 - val_mae: 8.5532 - val_mse: 141.9289\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.4467 - mae: 8.5163 - mse: 138.4015 - val_loss: 142.3516 - val_mae: 8.5186 - val_mse: 142.3063\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 138.6581 - mae: 8.5976 - mse: 138.6126 - val_loss: 143.3703 - val_mae: 8.5701 - val_mse: 143.3246\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9484 - mae: 8.5278 - mse: 137.9025 - val_loss: 143.4153 - val_mae: 8.5852 - val_mse: 143.3693\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 138.1365 - mae: 8.5443 - mse: 138.0903 - val_loss: 144.6257 - val_mae: 8.5770 - val_mse: 144.5794\n",
      "Epoch 14/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 139.1040 - mae: 8.5595 - mse: 139.0575 - val_loss: 141.5736 - val_mae: 8.5273 - val_mse: 141.5269\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.8542 - mae: 8.4963 - mse: 135.8073 - val_loss: 142.2528 - val_mae: 8.5657 - val_mse: 142.2057\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.9015 - mae: 8.5317 - mse: 137.8542 - val_loss: 143.4198 - val_mae: 8.5549 - val_mse: 143.3724\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 137.7767 - mae: 8.5380 - mse: 137.7291 - val_loss: 141.6530 - val_mae: 8.5109 - val_mse: 141.6052\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.6791 - mae: 8.5568 - mse: 136.6310 - val_loss: 142.7033 - val_mae: 8.5489 - val_mse: 142.6550\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 136.5706 - mae: 8.4977 - mse: 136.5221 - val_loss: 140.1360 - val_mae: 8.5392 - val_mse: 140.0872\n",
      "Epoch 20/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 134.9641 - mae: 8.4559 - mse: 134.9152 - val_loss: 139.2952 - val_mae: 8.5437 - val_mse: 139.2460\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7619 - mae: 8.5309 - mse: 135.7125 - val_loss: 139.8416 - val_mae: 8.5255 - val_mse: 139.7920\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.4162 - mae: 8.4779 - mse: 135.3664 - val_loss: 144.0311 - val_mae: 8.5329 - val_mse: 143.9810\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.7253 - mae: 8.4024 - mse: 135.6751 - val_loss: 137.8393 - val_mae: 8.5547 - val_mse: 137.7887\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 134.9588 - mae: 8.4673 - mse: 134.9080 - val_loss: 139.6904 - val_mae: 8.5463 - val_mse: 139.6392\n",
      "Epoch 25/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 135.9683 - mae: 8.4899 - mse: 135.9169 - val_loss: 139.4721 - val_mae: 8.5090 - val_mse: 139.4205\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.4305 - mae: 8.4107 - mse: 133.3785 - val_loss: 141.4364 - val_mae: 8.5308 - val_mse: 141.3842\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.0132 - mae: 8.4213 - mse: 132.9607 - val_loss: 143.8673 - val_mae: 8.5166 - val_mse: 143.8147\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 134.8278 - mae: 8.4693 - mse: 134.7749 - val_loss: 141.6443 - val_mae: 8.5036 - val_mse: 141.5913\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.8729 - mae: 8.3675 - mse: 132.8193 - val_loss: 139.0052 - val_mae: 8.5789 - val_mse: 138.9514\n",
      "Epoch 30/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.3246 - mae: 8.4146 - mse: 133.2705 - val_loss: 139.8519 - val_mae: 8.5917 - val_mse: 139.7974\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.3360 - mae: 8.4220 - mse: 132.2812 - val_loss: 140.3040 - val_mae: 8.5418 - val_mse: 140.2487\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 133.8846 - mae: 8.4455 - mse: 133.8291 - val_loss: 141.3894 - val_mae: 8.5181 - val_mse: 141.3336\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.8174 - mae: 8.4420 - mse: 132.7612 - val_loss: 143.1489 - val_mae: 8.4935 - val_mse: 143.0927\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.7212 - mae: 8.3805 - mse: 131.6645 - val_loss: 142.6922 - val_mae: 8.5238 - val_mse: 142.6352\n",
      "Epoch 35/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 132.4975 - mae: 8.3838 - mse: 132.4403 - val_loss: 143.6801 - val_mae: 8.5531 - val_mse: 143.6228\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 130.9808 - mae: 8.4080 - mse: 130.9229 - val_loss: 143.4050 - val_mae: 8.5860 - val_mse: 143.3468\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.0766 - mae: 8.3692 - mse: 131.0181 - val_loss: 142.1825 - val_mae: 8.5578 - val_mse: 142.1237\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.9281 - mae: 8.3781 - mse: 131.8689 - val_loss: 144.2332 - val_mae: 8.5767 - val_mse: 144.1739\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.4735 - mae: 8.3273 - mse: 129.4137 - val_loss: 142.1961 - val_mae: 8.5680 - val_mse: 142.1360\n",
      "Epoch 40/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 131.5219 - mae: 8.3848 - mse: 131.4615 - val_loss: 142.1891 - val_mae: 8.5514 - val_mse: 142.1282\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 130.0459 - mae: 8.3502 - mse: 129.9849 - val_loss: 143.1303 - val_mae: 8.5454 - val_mse: 143.0690\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.9299 - mae: 8.3293 - mse: 127.8679 - val_loss: 142.3903 - val_mae: 8.5558 - val_mse: 142.3279\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 129.6933 - mae: 8.3736 - mse: 129.6305 - val_loss: 144.4853 - val_mae: 8.5685 - val_mse: 144.4222\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 130.2331 - mae: 8.3830 - mse: 130.1696 - val_loss: 143.3303 - val_mae: 8.5560 - val_mse: 143.2665\n",
      "Epoch 45/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3790 - mae: 8.3095 - mse: 128.3148 - val_loss: 141.1558 - val_mae: 8.5748 - val_mse: 141.0911\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.8945 - mae: 8.2859 - mse: 125.8292 - val_loss: 148.7896 - val_mae: 8.6283 - val_mse: 148.7239\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.2449 - mae: 8.2599 - mse: 127.1786 - val_loss: 142.7781 - val_mae: 8.5782 - val_mse: 142.7115\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 128.3915 - mae: 8.3311 - mse: 128.3246 - val_loss: 142.1332 - val_mae: 8.5774 - val_mse: 142.0658\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.8932 - mae: 8.2957 - mse: 126.8253 - val_loss: 142.5808 - val_mae: 8.6131 - val_mse: 142.5125\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.4585 - mae: 8.3014 - mse: 127.3897 - val_loss: 142.0307 - val_mae: 8.5772 - val_mse: 141.9615\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 125.3658 - mae: 8.2324 - mse: 125.2961 - val_loss: 142.2643 - val_mae: 8.5864 - val_mse: 142.1942\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 126.4737 - mae: 8.2313 - mse: 126.4031 - val_loss: 141.6476 - val_mae: 8.5615 - val_mse: 141.5765\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.0957 - mae: 8.2242 - mse: 125.0241 - val_loss: 144.8544 - val_mae: 8.5502 - val_mse: 144.7825\n",
      "Epoch 54/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 127.5320 - mae: 8.2880 - mse: 127.4597 - val_loss: 142.9434 - val_mae: 8.5553 - val_mse: 142.8706\n",
      "Epoch 55/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.1300 - mae: 8.1972 - mse: 124.0567 - val_loss: 141.6886 - val_mae: 8.5606 - val_mse: 141.6147\n",
      "Epoch 56/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 125.3518 - mae: 8.2462 - mse: 125.2774 - val_loss: 143.1861 - val_mae: 8.5661 - val_mse: 143.1111\n",
      "Epoch 57/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.0992 - mae: 8.2291 - mse: 124.0236 - val_loss: 142.3804 - val_mae: 8.5474 - val_mse: 142.3043\n",
      "Epoch 58/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 123.9983 - mae: 8.2476 - mse: 123.9217 - val_loss: 147.0419 - val_mae: 8.5826 - val_mse: 146.9649\n",
      "Epoch 59/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 125.2617 - mae: 8.2452 - mse: 125.1843 - val_loss: 142.9872 - val_mae: 8.5335 - val_mse: 142.9093\n",
      "Epoch 60/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 124.1462 - mae: 8.2328 - mse: 124.0676 - val_loss: 143.7397 - val_mae: 8.5639 - val_mse: 143.6607\n",
      "Epoch 61/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.3945 - mae: 8.2399 - mse: 124.3150 - val_loss: 143.1200 - val_mae: 8.5599 - val_mse: 143.0401\n",
      "Epoch 62/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.5267 - mae: 8.2182 - mse: 124.4465 - val_loss: 144.3668 - val_mae: 8.5402 - val_mse: 144.2862\n",
      "Epoch 63/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 124.4436 - mae: 8.2121 - mse: 124.3625 - val_loss: 144.6765 - val_mae: 8.5492 - val_mse: 144.5949\n",
      "Epoch 64/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 124.0374 - mae: 8.2037 - mse: 123.9556 - val_loss: 142.8904 - val_mae: 8.5322 - val_mse: 142.8083\n",
      "Epoch 65/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.1619 - mae: 8.1572 - mse: 121.0793 - val_loss: 141.5815 - val_mae: 8.6053 - val_mse: 141.4982\n",
      "Epoch 66/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.2630 - mae: 8.1516 - mse: 120.1794 - val_loss: 140.6527 - val_mae: 8.6211 - val_mse: 140.5681\n",
      "Epoch 67/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6034 - mae: 8.1723 - mse: 120.5185 - val_loss: 144.6385 - val_mae: 8.5692 - val_mse: 144.5532\n",
      "Epoch 68/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.9402 - mae: 8.1227 - mse: 120.8546 - val_loss: 143.4853 - val_mae: 8.5932 - val_mse: 143.3990\n",
      "Epoch 69/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.6059 - mae: 8.1439 - mse: 120.5192 - val_loss: 143.5187 - val_mae: 8.5557 - val_mse: 143.4315\n",
      "Epoch 70/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.9470 - mae: 8.0636 - mse: 117.8592 - val_loss: 144.4667 - val_mae: 8.6474 - val_mse: 144.3784\n",
      "Epoch 71/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.9726 - mae: 8.1714 - mse: 121.8844 - val_loss: 142.7219 - val_mae: 8.5928 - val_mse: 142.6335\n",
      "Epoch 72/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 120.1376 - mae: 8.1479 - mse: 120.0485 - val_loss: 144.1719 - val_mae: 8.5664 - val_mse: 144.0824\n",
      "Epoch 73/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.2621 - mae: 8.0513 - mse: 118.1720 - val_loss: 145.5330 - val_mae: 8.5838 - val_mse: 145.4428\n",
      "Epoch 74/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 121.7539 - mae: 8.1283 - mse: 121.6630 - val_loss: 145.5918 - val_mae: 8.5583 - val_mse: 145.5007\n",
      "Epoch 75/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 120.5285 - mae: 8.1143 - mse: 120.4370 - val_loss: 143.1920 - val_mae: 8.5901 - val_mse: 143.0999\n",
      "Epoch 76/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.1266 - mae: 8.1329 - mse: 119.0342 - val_loss: 142.7547 - val_mae: 8.5803 - val_mse: 142.6619\n",
      "Epoch 77/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.6071 - mae: 8.1491 - mse: 119.5139 - val_loss: 143.6204 - val_mae: 8.5820 - val_mse: 143.5264\n",
      "Epoch 78/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 119.4226 - mae: 8.1369 - mse: 119.3280 - val_loss: 146.5015 - val_mae: 8.5855 - val_mse: 146.4067\n",
      "Epoch 79/100\n",
      "158/158 [==============================] - 1s 4ms/step - loss: 117.6746 - mae: 8.0619 - mse: 117.5791 - val_loss: 145.6877 - val_mae: 8.5810 - val_mse: 145.5917\n",
      "Epoch 80/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 118.4643 - mae: 8.0976 - mse: 118.3677 - val_loss: 145.3576 - val_mae: 8.6285 - val_mse: 145.2605\n",
      "Epoch 81/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 116.1383 - mae: 8.0324 - mse: 116.0407 - val_loss: 147.1670 - val_mae: 8.6217 - val_mse: 147.0690\n",
      "Epoch 82/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 120.1077 - mae: 8.1593 - mse: 120.0093 - val_loss: 144.7285 - val_mae: 8.6745 - val_mse: 144.6296\n",
      "Epoch 83/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.9701 - mae: 8.0076 - mse: 115.8707 - val_loss: 145.2923 - val_mae: 8.6285 - val_mse: 145.1924\n",
      "Epoch 84/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 115.7361 - mae: 8.0149 - mse: 115.6357 - val_loss: 146.9341 - val_mae: 8.6500 - val_mse: 146.8333\n",
      "Epoch 85/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 115.9289 - mae: 8.0171 - mse: 115.8276 - val_loss: 145.7986 - val_mae: 8.6626 - val_mse: 145.6969\n",
      "Epoch 86/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 117.7450 - mae: 8.0315 - mse: 117.6426 - val_loss: 144.6985 - val_mae: 8.6306 - val_mse: 144.5953\n",
      "Epoch 87/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.9291 - mae: 8.0375 - mse: 117.8258 - val_loss: 144.3044 - val_mae: 8.6536 - val_mse: 144.2004\n",
      "Epoch 88/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.9409 - mae: 7.9505 - mse: 113.8365 - val_loss: 145.9460 - val_mae: 8.6339 - val_mse: 145.8409\n",
      "Epoch 89/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 114.6720 - mae: 7.9751 - mse: 114.5663 - val_loss: 146.9050 - val_mae: 8.6889 - val_mse: 146.7985\n",
      "Epoch 90/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.3056 - mae: 7.9143 - mse: 113.1987 - val_loss: 147.6449 - val_mae: 8.6230 - val_mse: 147.5374\n",
      "Epoch 91/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 113.9317 - mae: 7.9802 - mse: 113.8237 - val_loss: 146.0756 - val_mae: 8.5843 - val_mse: 145.9674\n",
      "Epoch 92/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.9066 - mae: 7.9147 - mse: 113.7979 - val_loss: 146.6966 - val_mae: 8.6101 - val_mse: 146.5875\n",
      "Epoch 93/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1304 - mae: 7.8935 - mse: 113.0206 - val_loss: 147.4060 - val_mae: 8.6228 - val_mse: 147.2956\n",
      "Epoch 94/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1933 - mae: 7.9547 - mse: 113.0825 - val_loss: 146.3798 - val_mae: 8.6741 - val_mse: 146.2683\n",
      "Epoch 95/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 114.4721 - mae: 7.9588 - mse: 114.3603 - val_loss: 145.5726 - val_mae: 8.6007 - val_mse: 145.4605\n",
      "Epoch 96/100\n",
      "158/158 [==============================] - 1s 3ms/step - loss: 115.3004 - mae: 7.9820 - mse: 115.1878 - val_loss: 146.2556 - val_mae: 8.6051 - val_mse: 146.1426\n",
      "Epoch 97/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 112.6000 - mae: 7.9371 - mse: 112.4864 - val_loss: 147.5810 - val_mae: 8.6040 - val_mse: 147.4670\n",
      "Epoch 98/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 112.9418 - mae: 7.9294 - mse: 112.8273 - val_loss: 146.5905 - val_mae: 8.6325 - val_mse: 146.4753\n",
      "Epoch 99/100\n",
      "158/158 [==============================] - 0s 3ms/step - loss: 113.1401 - mae: 7.9114 - mse: 113.0246 - val_loss: 148.1808 - val_mae: 8.6684 - val_mse: 148.0648\n",
      "Epoch 100/100\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 117.5969 - mae: 7.9860 - mse: 117.4809 - val_loss: 145.3819 - val_mae: 8.5919 - val_mse: 145.2657\n"
     ]
    }
   ],
   "source": [
    "l2_reg = 1e-4\n",
    "dropout_rate = 0.4\n",
    "\n",
    "reg_model_maxrank_nogenre_2 = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                 input_shape=(X_nogenre_4_train.shape[1],)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "        \n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout_rate),\n",
    "\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "reg_model_maxrank_nogenre_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mse']\n",
    ")\n",
    "\n",
    "history_reg_model4_1 = reg_model_maxrank_nogenre_2.fit(\n",
    "    X_nogenre_4_train_scaled, y_nogenre_4_train_final,\n",
    "    validation_data=(X_nogenre_4_val_scaled, y_nogenre_4_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regularized Model (L2 + Dropout) ===\n",
      " 4 deep layers, 128 kernels per layer\n",
      "Train MAE: 8.0288, Train MSE: 123.9662\n",
      "Val   MAE: 8.5604, Val   MSE: 141.6273\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Regularized Model (L2 + Dropout) ===\")\n",
    "print(\" 4 deep layers, 128 kernels per layer\")\n",
    "train_scores_reg4_1 = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_train_scaled, y_nogenre_4_train_final, verbose=0)\n",
    "val_scores_reg4_1   = reg_model_maxrank_nogenre_1.evaluate(X_nogenre_4_val_scaled, y_nogenre_4_val, verbose=0)\n",
    "print(f\"Train MAE: {train_scores_reg4_1[1]:.4f}, Train MSE: {train_scores_reg4_1[2]:.4f}\")\n",
    "print(f\"Val   MAE: {val_scores_reg4_1[1]:.4f}, Val   MSE: {val_scores_reg4_1[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original regularized model has the best MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_maxrankchange_model = reg_model_maxrank_nogenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Summary of Model Performance\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "The XGBoost model did not return meaningful results for this dataset. The highest r<sup>2</sup> value XGB achieved was 0.271, not high enough to indicate that the model strongly fits the underlying data.\n",
    "\n",
    " The best metrics for XGBoost were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- RMSE: 21.607\n",
    "- r<sup>2</sup>: 0.271\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- RMSE: 11.764\n",
    "- r<sup>2</sup>: 0.045\n",
    "\n",
    "**k-Nearest Neighbors**\n",
    "\n",
    "This model also did not perform well on this data set--its accuracy was not above 22% in any scenario.\n",
    "\n",
    "Its best metrics were:\n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "- Accuracy: 0.035\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "- Accuracy: 0.217\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "The deep learning model returned the most promising results and are described in the next section.\n",
    "\n",
    "### Final Models\n",
    "\n",
    "The final models are both based on a deep learning architecture. \n",
    "\n",
    "_Maximum Peak Position_\n",
    "\n",
    "The best model for the Maximum Peak Position is a 3-layer, regularized model with 64 kernels per layer. It was trained a song dataset without genre information.\n",
    "\n",
    "The final metrics for the best model were:\n",
    "\n",
    "- Training mean absolute error: 14.84\n",
    "- Validation mean absolute error: 16.91\n",
    "\n",
    "_Maximum Rank Increase_\n",
    "\n",
    "The best model for the Maximum Rank Increase is named reg_model4_2. It is also a 3-layer, regularized model trained on the song dataset without genre.\n",
    "Its final metrics:\n",
    "\n",
    "- Training mean absolute error: 7.96\n",
    "- Validation mean absolute error: 8.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genre of a song, at least how it is captured on Spotify, has very little influence on its movement on the Billboard Hot 100 list. This gives FutureProduct Advisors consultants and their customers significant leeway when they are selecting music for their social and advertising campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characteristics of a given song appear to be the most important factors in its performance on the Hot 100 list. Features such as tempo, danceability, etc. have the most predictive power for the Hot 100 list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital music has a large number of characteristics that can be pulled into machine learning models. The factors that have the largest influence on popularity are often unexpected, and modeling and analysis of songs requires methodical and careful selection and vetting of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "reg_model3_1 (predicting highest ranking on the Billboard Hot 100) and reg_model4_2 (predicting largest week over week ranking increase) are ready for initial deployment to FutureProduct Advisors. \n",
    "\n",
    "Detailed training will be provided, but at a high level here is how users will engage:\n",
    "\n",
    "1. Consultants will identify a list of songs they'd like to explore.\n",
    "    - We recommend using the songs in places 90-100 of the Billboard Hot 100 at a minimum\n",
    "2. Consultants download those songs' metadata from Spotify as a CSV file (detailed instructions to come)\n",
    "3. Consultants load the CSV file into these models, and the models will return predicted activity for each song.\n",
    "\n",
    "There is also an additional opportunity to built on and refine these models by indluding additional data and experimenting with other modeling approaches. If the FutureProduct Advisors consultants have positive feedback on these prototype tools, we will be happy to partner on future enhancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
